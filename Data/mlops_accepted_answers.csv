"no","Id","PostTypeId","ParentId","CreationDate","Score","Body","OwnerUserId","CommentCount"
1,25838584,2,25838512,2014-09-14 22:22:38,4,"<p>I was not able to compile and run the code, but you probably need something like this:</p>

<pre><code>let invokeRequestResponseService() = async {
    use client = new HttpClient()
    let scoreData = (...)
    let apiKey = ""abc123""
    client.DefaultRequestHeaders.Authorization &lt;- 
        new AuthenticationHeaderValue(""Bearer"", apiKey)
    client.BaseAddress &lt;- Uri(""https://ussouthcentral..../score"");
    let! response = client.PostAsJsonAsync("""", scoreRequest) |&gt; Async.AwaitTask
    if response.IsSuccessStatusCode then
        let! result = response.Content.ReadAsStringAsync() |&gt; Async.AwaitTask
        Console.WriteLine(""Result: {0}"", result);
    else
        Console.WriteLine(""Failed with status code: {0}"", response.StatusCode) }
</code></pre>

<ul>
<li><p>Wrapping the code in the <code>async { .. }</code> block makes it asynchronous and lets you use <code>let!</code> inside the block to perform asynchronous waiting (i.e. in places where you'd use <code>await</code> in C#)</p></li>
<li><p>F# uses type <code>Async&lt;T&gt;</code> instead of .NET Task, so when you're awaiting a task, you need to insert <code>Async.AwaitTask</code> (or you can write wrappers for the most frequently used operations)</p></li>
<li><p>The <code>invokeRequestResponseService()</code> function returns F# async, so if you need to pass it to some other library function (or if it needs to return a task), you can use <code>Async.StartAsTask</code></p></li>
</ul>
","33518",5
2,27551102,2,27461432,2014-12-18 16:18:15,6,"<p>Actually Azure ML can't process JSON data. It will probably be added in a future update, but the easiest way (in my opinion) to consume that data is to convert it into CSV format. This can be done quickly with Power Query. Then you upload the CSV file as a new dataset.</p>
","1602990",0
3,29344026,2,29297579,2015-03-30 10:38:05,1,"<p><em>UPDATE</em> 1/28/2016</p>

<p>Network I/O for <code>Execute Python Script</code> is now supported.</p>

<p><em>Out of date</em></p>

<p>Network I/O is not support from Execute Python Modules. In order to execute such a program, you should instead launch a virtual machine(Windows or Linux your choice). </p>

<p>Windows:</p>

<ol>
<li>RDP into Virtual Machine</li>
<li>Install your choice of Python</li>
<li>You can drag and drop your Python program from your Local Windows machine onto your RDP screen to transfer your code</li>
<li>Then run your program </li>
</ol>

<p>Ubuntu:</p>

<ol>
<li>SSH into your virtual machine using Cygwin or Putty(Windows) or Terminal SSH (mac) <code>ssh yourUserName@yourAzureVM.cloudapps.net</code></li>
<li>install Python <code>sudo apt-get install python</code></li>
<li>open your preferred Linux text editor <code>vi myProgram.py</code></li>
<li>Copy and Paste your code into the editor (leave vi with esc <code>:wq</code> )</li>
<li>Run your code <code>python myProgram.py</code></li>
</ol>

<p>To move data from your VM to AzureML please check out the <a href=""https://github.com/Azure/Azure-MachineLearning-ClientLibrary-Python"" rel=""nofollow"">Azure-MachineLearning-ClientLibrary-Python</a> on Github</p>
","1341806",4
4,29357309,2,29283841,2015-03-30 22:14:48,4,"<p>Azure public IP address are published and refreshed at regular intervals it can be found here: <a href=""http://www.microsoft.com/en-us/download/details.aspx?id=41653"" rel=""nofollow"">http://www.microsoft.com/en-us/download/details.aspx?id=41653</a></p>

<p>You can use these to specify the restricted IP range for access</p>
","4731677",2
5,29495886,2,29485562,2015-04-07 15:56:06,2,"<p>You need to use the Feature Hashing module to convert the text into word features. This, however, might not be enough as words are not good features for your problem. You may want to do some processing of the text and create more useful features (perhaps detecting the presence of zip codes, positions of numbers, etc...)</p>

<p>Edit: Using the raw text column as one feature will not get you anywhere. You don’t want your model to learn the addresses the way they are written. Instead, you need to learn patterns in the text that provide evidence for address vs. non-address instances.
When you use feature hashing, the text column will be transformed to multiple word (or n-gram) columns, where the values represent counts of those words in each text input. The problem here is overfitting. For example, these two addresses have no words in common:
“100 broadway st, GA” and “200 main rd, NY” but it’s clear they have similar structure. One way to create ‘useful features’ is to replace the words with tags: “#NUM #TXT, #STATE” and use feature hashing (bi-grams) to create features such as “#NUM #TXT” and “, #STATE”. As you can see, these bi-grams count as evidence in both addresses and suggest some kind of similarity between them (compared to other non-address instances). Of course this is an oversimplification of the problem but I hope you see why you can’t use the raw text or plain feature hashing.<br>
You can still use the Azure ML modules for feature hashing, training, and scoring in addition to an ‘Execute R’ module to do the text processing before training.</p>

<p>Edit: Example of feature hashing usage: <a href=""http://gallery.azureml.net/Details/cf65bf129fee4190b6f48a53e599a755"" rel=""nofollow"">http://gallery.azureml.net/Details/cf65bf129fee4190b6f48a53e599a755</a> </p>
","4621861",4
6,30070326,2,30068341,2015-05-06 07:34:50,2,"<p>Don't set output port and use Batch execution service - details are provided here - <a href=""http://azure.microsoft.com/en-us/documentation/articles/machine-learning-publish-a-machine-learning-web-service/"" rel=""nofollow"">Publish web service</a> and <a href=""http://azure.microsoft.com/en-us/documentation/articles/machine-learning-consume-web-services/"" rel=""nofollow"">consume web service</a></p>
","4589073",5
7,30106785,2,30092360,2015-05-07 16:29:00,2,"<p>You will need to first create a new job in the Azure management portal (<a href=""https://msdn.microsoft.com/en-us/library/azure/dn495651.aspx"" rel=""nofollow"">https://msdn.microsoft.com/en-us/library/azure/dn495651.aspx</a>), where you can configure the URL and the HTTP method to POST, and specify the body. However, the initial configuration screens don't let you add any headers, so once you have created the job, go in and edit it to add the following headers:</p>

<p>Content-Type: application/json<br>
Accept: application/json<br>
Authorization: Bearer </p>

<p>This will work, but am wondering if this actually serves your purpose. If you're calling the synchronous (request response) endpoint of the AzureML service, you need to specify the inputs in the request payload, which is statically configured with the Azure Scheduler job. So you will effectively be repeating the same call over and over again. You may also want to explore <a href=""http://azure.microsoft.com/en-us/services/data-factory/"" rel=""nofollow"">Azure Data Factory</a> if your needs are served by calling the asynchronous (batch) endpoint of the AzureML service.</p>
","1153282",0
8,30237070,2,30214698,2015-05-14 12:10:11,1,"<p>You would use Azure Data Factory instead of the scheduler. This would allow you to schedule the BES call into the future while identifying where the result file will end up. </p>

<p>There are lots of examples online on how to do that.</p>
","4899801",0
9,30268859,2,30172382,2015-05-15 21:16:20,1,"<p>If you are using BES with web service input and output, you would need to provide the Storage information for the data. 
With the Reader and Writer modules, you can remove the web service input and output ports.
Then when the web service is called, it executes without using the Storage blob. It will read from the Reader and write to the destination specified in the Writer.
I have uploaded a <a href=""https://azuremlbesclienttemplate.codeplex.com/documentation"" rel=""nofollow"">Visual Studio template to CodePlex</a> that you can install. The NoInputOutput.aspx of that project does the above. And it should show you the workflow.</p>
","4905427",0
10,30290362,2,30133814,2015-05-17 18:13:01,5,"<p>Another way is to have R or python code that replicates the status for each image and then use add-columns. I think R/Python code to just replicate the status for each image may be easier and faster than outer join.</p>
","4589073",2
11,30495978,2,30396392,2015-05-28 01:58:36,2,"<p>You could also add project columns to exclude label as part of scoring experiment and connect web service output port to the output of project columns</p>
","4589073",8
12,32005243,2,30115812,2015-08-14 08:09:05,1,"<p>I found the problem. I was facing this error because the R module in the Azure ML was was taking variable as the other data type and not producing any outputs results which i was checking through for loop which is why i was getting this error.</p>
","3768302",0
13,32060236,2,32060196,2015-08-17 21:47:56,0,"<p>I've run into this error before, and unfortunately, the only workaround I found was to create a new ML workspace backed by a storage account that you know is online. Then copy your experiment over to the new workspace, and things should work. It can be a bit cumbersome, but it should get rid of your error message. With the service being relatively new, things sometimes get corrupted as updates are being made, so I recommend checking the box labeled ""disable updates"" within your experiment.  Hope that helps!</p>
","5212902",1
14,32252449,2,32252282,2015-08-27 14:41:38,2,"<p>Currently, Azure Machine Learning will bring that data onto ML servers.  </p>
","1535478",0
15,32423130,2,32422626,2015-09-06 11:39:08,3,"<p>Take a look at <a href=""http://www.nltk.org/book/ch05.html"" rel=""nofollow"">NTLK book</a>, Categorizing and Tagging Words section.</p>

<p>Simple example, it uses the Penn Treebank tagset:</p>

<pre><code>from nltk.tag import pos_tag
from nltk.tokenize import word_tokenize
pos_tag(word_tokenize(""John's big idea isn't all that bad."")) 

[('John', 'NNP'),
(""'s"", 'POS'),
 ('big', 'JJ'),
 ('idea', 'NN'),
 ('is', 'VBZ'),
 (""n't"", 'RB'),
 ('all', 'DT'),
 ('that', 'DT'),
 ('bad', 'JJ'),
 ('.', '.')]
</code></pre>

<p>Then you can use</p>

<pre><code>from collections import defaultdict
counts = defaultdict(int)
for (word, tag) in pos_tag(word_tokenize(""John's big idea isn't all that bad."")):
    counts[tag] += 1
</code></pre>

<p>to get frequencies:</p>

<pre><code>defaultdict(&lt;type 'int'&gt;, {'JJ': 2, 'NN': 1, 'POS': 1, '.': 1, 'RB': 1, 'VBZ': 1, 'DT': 2, 'NNP': 1})
</code></pre>
","4016674",7
16,32465554,2,32451243,2015-09-08 19:17:16,1,"<p>Look at the Azure CDN <a href=""http://azure.microsoft.com/en-us/services/cdn/"" rel=""nofollow"">http://azure.microsoft.com/en-us/services/cdn/</a> , after which the blobs will get an alternative url. My blob downloads became about 4 times faster after switching.</p>
","user152949",1
17,32620509,2,32612311,2015-09-16 23:45:00,2,"<p>It sounds like you are trying to output all those thousands of columns as an output. What you really only need is the scored probability or the scored label. To solve this, just drop all the feature hashed columns from the score model module. To do this add in a project columns module, and tell it to start with ""no columns"" then ""include"" by ""column names"", and just add predicted column (scored probability/scored label). </p>

<p>Then hook up the output of that project columns module to your web service output module. Your web service should now be returning only 1-3 columns rather than thousands.</p>
","4582041",1
18,32760764,2,32752659,2015-09-24 11:59:56,3,"<p>You cannot use the tarball packages. If you are on windows you need to do the following:</p>

<p>Once you install a package (+ it's dependencies) it will download the packages in a directory </p>

<blockquote>
  <p>C:\Users\xxxxx\AppData\Local\Temp\some directory
  name\downloaded_packages</p>
</blockquote>

<p>These will be in a zip format. These are the packages you need. </p>

<p>Or download the windows binaries from cran.</p>

<p>Next you need to put all the needed packages in one total zip-file and upload this to AzureML as a new dataset.</p>

<p>in AzureML load the data package connected to a r-script</p>

<pre><code>install.packages(""src/ada.zip"", lib = ""."", repos = NULL, verbose = TRUE)
library(ada, lib.loc=""."", verbose=TRUE)
</code></pre>

<p>Be sure to check that all dependent packages are available in Azure. Rpart is available.</p>

<p>For a complete overview, look at this <a href=""http://blogs.msdn.com/b/benjguin/archive/2014/09/24/how-to-upload-an-r-package-to-azure-machine-learning.aspx"" rel=""nofollow"">msdn blog</a> explaining it a bit better with some visuals.</p>
","4985176",0
19,32857098,2,32431471,2015-09-30 03:07:41,0,"<p>@Anuj Shankar,
After my colleague tested, we can read data from <code>CSV</code> files and get the expected results. Please refer to this experience:</p>

<p>1)  Input data  - It has <code>apple.zip</code> file which has two <code>csv</code> files similar to you and each csv file includes bag of words related to company.</p>

<p><a href=""https://i.stack.imgur.com/yrXst.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/yrXst.jpg"" alt=""enter image description here""></a>
2)  Python script: </p>

<pre><code># The script MUST contain a function named azureml_main
# which is the entry point for this module.
#
# The entry point function can contain up to two input arguments:
#   Param&lt;dataframe1&gt;: a pandas.DataFrame
#   Param&lt;dataframe2&gt;: a pandas.DataFrame
import csv
import numpy as np
import pandas as pd

def azureml_main(dataframe1 = None, dataframe2 = None):
    # Execution logic goes here
    #print('Input pandas.DataFrame #1:\r\n\r\n{0}'.format(dataframe1))

    # If a zip file is connected to the third input port is connected,
    # it is unzipped under "".\Script Bundle"". This directory is added
    # to sys.path. Therefore, if your zip file contains a Python file
    # mymodule.py you can import it using:
    # import mymodule

    apple = {}
    microsoft = {}
  #Reading from the csv files
    with open('.\Script Bundle\\apple.csv', 'rb') as f:
      reader = csv.reader(f)
      apple = list_to_dict(list(reader)[0])

    with open('.\Script Bundle\\microsoft.csv', 'rb') as f:
      reader = csv.reader(f)
      microsoft = list_to_dict(list(reader)[0])

#    print('hello world' + ' '.join(apple[0]))
    applecount = 0
    microsoftcount = 0

    input = ""i want to buy surface which runs on windows""
    splitted_input = input.split(' ')

    for word in splitted_input:
        if word in apple:
            applecount = applecount + 1
        if word in microsoft:
            microsoftcount = microsoftcount + 1

    print(""apple bag of words count - "" + str(applecount))
    print(""microsoft bag of words count - "" + str(microsoftcount))
    mydata = [{'input words': len(splitted_input)}, {'applecount':applecount},
        {'microsoftcount':microsoftcount}]       
    # Return value must be of a sequence of pandas.DataFrame
    return pd.DataFrame(mydata),


def list_to_dict(li):      
    dct = {}  
    for item in li:
        if dct.has_key(item):              
            dct[item] = dct[item] + 1  
        else:  
            dct[item] = 1  
    return dct  
</code></pre>

<p>3)  Output  - if I consider a string ""i want to buy surface which runs on windows"". It has 2 words related to microsoft and 0 related to apple which are visualized in below snapshot.
<a href=""https://i.stack.imgur.com/ifE1t.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ifE1t.png"" alt=""enter image description here""></a></p>
","4836342",1
20,33867833,2,33708725,2015-11-23 09:44:23,1,"<p><strong>Moncef</strong> provided <a href=""https://social.msdn.microsoft.com/Forums/azure/en-US/aabb9080-adf8-4fc0-b332-2a3c8ee29a28/azure-machine-learning-error-with-multiclass-classification-algo?forum=MachineLearning"" rel=""nofollow"">here</a> the solution to my problem:</p>

<p>The point is that Azure has limitations on maximum categories 8192, this is why the number should be decreased by R or python modules or own evaluation module may be created. Or there is another way, evaluation step may be skipped, because model`ve been trained successfully. </p>
","1739325",0
21,34005075,2,32434805,2015-11-30 18:06:05,0,"<p>Read about <code>linear regression</code>. This is answer for your question. And here is the link to Azure ML tutorial <a href=""https://azure.microsoft.com/en-us/documentation/articles/machine-learning-create-experiment/"" rel=""nofollow"">link</a></p>
","1168786",0
22,34208081,2,34047782,2015-12-10 17:25:00,8,"<p>It very much could be memory usage - a 647054, 5196 data frame has 3,362,092,584 elements, which would be 24GB just for the pointers to the objects on a 64-bit system.  On AzureML while the VM has a large amount of memory you're actually limited in how much memory you have available (currently 2GB, soon to be 4GB) - and when you hit the limit the kernel typically dies.  So it seems very likely it is a memory usage issue.</p>

<p>You might try doing <a href=""http://pandas.pydata.org/pandas-docs/stable/sparse.html"" rel=""noreferrer"">.to_sparse()</a> on the data frame first before doing any additional manipulations.  That should allow Pandas to keep most of the data frame out of memory.</p>
","114000",0
23,34325425,2,34320449,2015-12-17 02:00:13,1,"<p>You can <a href=""https://azure.microsoft.com/en-us/documentation/articles/machine-learning-publish-a-machine-learning-web-service/"" rel=""nofollow"">publish</a> an experiment (machine learning functions you hooked together in Azure ML Studio) as an API. When you call that API in your custom code you give it your data and all the computation runs in the cloud in Azure ML. </p>
","5070440",4
24,34328911,2,33229576,2015-12-17 07:27:41,0,"<p>Machine Learning Studio services that you create needs to receive requests from a device that has SSL capabilities to perform HTTPS requests. AFAIK, Arduino doesn't support SSL capabilities.</p>

<p>One usual scenario is to attach the Arduino to a third device like Raspberry Pi 2 etc to use it as a gateway and do the call from the Pi itself.</p>

<p>Here's a sample <a href=""https://github.com/Azure/connectthedots/blob/master/GettingStarted.md"" rel=""nofollow"">project</a> from Microsoft Open Technologies team that utilizes Arduino Uno, Raspberry pi and Azure stuff.</p>

<p>Hope this helps!</p>
","4840671",0
25,34335626,2,33802274,2015-12-17 13:20:02,0,"<p>After a while, an answer came back from the support team, so I am going to post the relevant part as an answer here for anyone who lands here with the same problem. </p>

<p>""This is a known issue. The container (a sandbox technology known as ""drawbridge"" running on top of Azure PaaS VM) executing the Execute R module doesn't support outbound HTTPS traffic. Please try to switch to HTTP and that should work.""</p>

<p>As well as that a solution is on the way :</p>

<p>""We are actively looking at how to fix this bug. ""</p>

<p>Here is the original <a href=""https://social.msdn.microsoft.com/Forums/azure/en-US/5866e16c-a145-481e-8764-f7c7823742b0/https-call-from-inside-r-module-possible-?forum=MachineLearning"" rel=""nofollow"">link</a> as a reference.
hth</p>
","4950019",0
26,34545225,2,34545078,2015-12-31 10:47:14,3,"<p>Azure Machine Learning is part of the Cortana analytics suite</p>

<p>You will find more info with the link below</p>

<p><a href=""http://www.sqlchick.com/entries/2015/8/22/what-is-the-cortana-analytics-suite"" rel=""nofollow"">All the details on the Cortana link here</a></p>

<p>All the best</p>
","2572645",1
27,34615413,2,34614582,2016-01-05 15:32:53,3,"<p>You'll need to serialize the object to a JSON string (I recommend using NewtonSoft.Json to make it easier) and set the content type accordingly. Here's an implementation I'm using in my UWP apps (note that <code>_client</code> is an <code>HttpClient</code>):</p>

<pre><code>    public async Task&lt;HttpResponseMessage&gt; PostAsJsonAsync&lt;T&gt;(Uri uri, T item)
    {
        var itemAsJson = JsonConvert.SerializeObject(item);
        var content = new StringContent(itemAsJson);
        content.Headers.ContentType = new MediaTypeHeaderValue(""application/json"");

        return await _client.PostAsync(uri, content);
    }
</code></pre>
","3447474",0
28,34673140,2,34335483,2016-01-08 09:07:08,0,"<p>This question was resolved thanks to some people on the Azure ML forum so 
I'm going to post an answer for anyone landing here in search for some answers...</p>

<p>The short answer is no, this is not possible. The more detailed version is:<br>
""From within the R script you cannot identify the internal AzureML IP addresses or the unique web service instances. When you make an external network call from the R script to an outside URL, that URL will see one of the AzureML public virtual IP's as the source IP. These are IP's of the load balancers, and not of the machines that are physically running the web service. AzureML dynamically allocates the instances of R engine in the backend, handles failures, and uses multiple nodes for running the web service for high availability. The exact layout of these for a given web service is not programmatically discoverable.""<br>
Here is also the <a href=""https://social.msdn.microsoft.com/Forums/azure/en-US/dd1f0658-7b0b-46d8-8e32-3fe4e96ec4be/uniquely-identify-instances-of-vms-web-services?forum=MachineLearning#cde28631-828d-4d83-9c93-1a1cf0dfb6fb"" rel=""nofollow"">link</a> to the original discussion. </p>
","4950019",0
29,35020997,2,34990561,2016-01-26 18:20:06,8,"<p>First, I am assuming you are doing your timing test on the published AML endpoint.</p>

<p>When a call is made to the AML the first call must warm up the container. By default a web service has 20 containers. Each container is cold, and a cold container can cause a large(30 sec) delay. In the string returned by the AML endpoint, only count requests that have the <code>isWarm</code> flag set to true. By smashing the service with MANY requests(relative to how many containers you have running) can get all your containers warmed.</p>

<p>If you are sending out dozens of requests a instance, the endpoint might be getting throttled. You can adjust the number of calls your endpoint can accept by going to manage.windowsazure.com/</p>

<ol>
<li>manage.windowsazure.com/</li>
<li>Azure ML Section from left bar</li>
<li>select your workspace</li>
<li>go to web services tab</li>
<li>Select your web service from list</li>
<li>adjust the number of calls with slider</li>
</ol>

<p>By enabling debugging onto your endpoint you can get logs about the execution time for each of your modules to complete. You can use this to determine if a module is not running as you intended which may add to the time.</p>

<p>Overall, there is an overhead when using the Execute python module, but I'd expect this request to complete in under 3 secs. </p>
","1341806",11
30,35414580,2,35376910,2016-02-15 16:41:27,2,"<p>The score module also returns the result with scored probabilities. I think you can add a simple math operation to compare the scored probability and add a new column or write a simple R script - see the image below with ""apply math operation"" to generate output based on probability exceeding 0.6 instead of 0.5</p>

<p><a href=""https://i.stack.imgur.com/xNAtf.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/xNAtf.png"" alt=""enter image description here""></a></p>
","4589073",0
31,35423846,2,35411741,2016-02-16 04:37:02,0,"<p>This was an Azure problem. I quote the Microsoft guy, </p>

<blockquote>
  <p>We believe we have isolated the issue impacting tour service and we are currently working on a fix. We will be able to deploy this in the next couple of days. The problem is impacting only the ASIA AzureML region at this time, so if this is an option for you, might I suggest using a workspace in either the US or EU region until the fix gets rolled out here.</p>
</blockquote>

<p>To view the complete discussion, click <a href=""https://social.msdn.microsoft.com/Forums/en-US/985e253e-5e54-45a5-a359-5c501152c445/getting-error-503-nomoreresources-to-any-web-service-api-even-when-i-only-make-1-request?forum=MachineLearning&amp;prof=required"" rel=""nofollow"">here</a></p>
","4646197",0
32,35566273,2,35562896,2016-02-22 23:54:32,1,"<p>You need to use project columns in your AML experiment. Currently, you have a module connected to Web Service Output. Use a <code>project columns</code> module before your <code>web service output</code> to select just the columns you would like to send to our output instead. </p>
","1341806",1
33,35601231,2,35600907,2016-02-24 11:42:31,2,"<p>The answer to your problem is that the “Numeric” datatype which is written in the input parameters in Azure ML is in fact a float and not an integer for your income measure. So when trying to request a response from Azure ML, you are not providing it the “adequate” information needed in the right format for it to respond correctly, resulting in it not giving you any response.</p>

<p>I believe your model would look something similar to this based on your input parameters:</p>

<pre><code>public class Person
    {
        public string Gender { get; set; }
        public int Age { get; set; }
        public int Income { get; set; }


        public override string ToString()
        {
            return Gender + "","" + Age + "","" + Income;
        }
    }
</code></pre>

<p>You would have to change your Income datatype into float like so:</p>

<pre><code>public class Person
{
    public string Gender { get; set; }
    public int Age { get; set; }
    public float Income { get; set; }

    public override string ToString()
    {
        return Gender + "","" + Age + "","" + Income;
    }
}
</code></pre>

<p>And then your post-method would look something like this:</p>

<pre><code>    [HttpPost]
    public ActionResult GetPredictionFromWebService()
    {
        var gender = Request.Form[""gender""];
        var age = Request.Form[""age""];

        if (!string.IsNullOrEmpty(gender) &amp;&amp; !string.IsNullOrEmpty(age))
        {
            var resultResponse = _incomeWebService.InvokeRequestResponseService&lt;ResultOutcome&gt;(gender, age).Result;

                if (resultResponse != null)
                {
                    var result = resultResponse.Results.Output1.Value.Values;
                    PersonResult = new Person
                    {
                        Gender = result[0, 0],
                        Age = Int32.Parse(result[0, 1]),
                        Income = float.Parse(result[0, 3], CultureInfo.InvariantCulture.NumberFormat)
                };
            }
        }

        ViewBag.myData = PersonResult.Income.ToString();
        return View(""Index"");
    }
</code></pre>

<p>The key here is simply:</p>

<pre><code>Income = float.Parse(result[0, 3], CultureInfo.InvariantCulture.NumberFormat)
</code></pre>

<p>Rather than your legacy </p>

<pre><code>Income = Int32.Parse(result[0, 2])
</code></pre>
","5535030",1
34,35643096,2,35627916,2016-02-26 03:51:29,3,"<p>I was able to do this using a combination of <a href=""https://msdn.microsoft.com/library/azure/70530644-c97a-4ab6-85f7-88bf30a8be5f"" rel=""nofollow"">Split Data</a>, <a href=""https://msdn.microsoft.com/library/azure/a8726e34-1b3e-4515-b59a-3e4a475654b8"" rel=""nofollow"">Partition and Sample</a>, and <a href=""https://msdn.microsoft.com/library/azure/b2ebdabd-217d-4915-86cc-5b05972f7270"" rel=""nofollow"">Add Rows</a> modules.  There may be an easier way to do it, but I did confirm it works.  :)  I published my work at <a href=""http://gallery.azureml.net/Details/1245147fd7004e91bc7a3683cda19cc7"" rel=""nofollow"">http://gallery.azureml.net/Details/1245147fd7004e91bc7a3683cda19cc7</a> so you can grab it directly from there, and run to confirm it does what you expect.  </p>

<p>Since you said you wanted a sampling of the data, I just reduced each of the labels to 10% to have all labels represented equally.  Since you have a good understanding of the distribution in your dataset, leave label 3, 4, and 5 all at about 10%, and reduce label 1 by 1/4 and label 2 by 1/2 to get about 10% of them as well.  </p>

<p>To explain what I did in the workspace linked above:</p>

<ul>
<li>I used some ""Split Data"" modules to filter out the label1 and label2 data.  In the Split Data module, change the Splitting mode to ""Regular Expression"" and set the regular expression to <strong>\""Label"" ^label1</strong> (to get the label1 data, for example).  </li>
<li>Then I used some ""Partition and Sample"" modules to reduce the size of the label1 and label2 data appropriately.  </li>
<li>Finally, I used some ""Add Rows"" modules to join all of the data back together again.  </li>
</ul>

<p>Finally, I didn't include this in my work, but you can also look at the <a href=""https://msdn.microsoft.com/library/azure/9f3fe1c4-520e-49ac-a152-2e104169912a"" rel=""nofollow"">SMOTE</a> module.  It will increase the number of low-occurring samples using synthetic minority oversampling.  </p>
","1535478",1
35,35650197,2,35631267,2016-02-26 11:11:05,2,"<p>I reproduced your issue and reviewed the <code>SQL Queries</code> doc of <code>pandas.io.sql</code> at <a href=""http://pandas.pydata.org/pandas-docs/stable/io.html#sql-queries"" rel=""nofollow"">http://pandas.pydata.org/pandas-docs/stable/io.html#sql-queries</a>. I tried to use <code>read_sql_query</code> to solve it, but failed.</p>

<p>According to the <code>pandas</code> doc, tt seems that <code>Pandas</code> not support the usage for this SQL syntax.</p>

<p>Base on my experience and according to your SQL, I tried to do the SQL <code>select * from (select * from t1) as tbl</code> instead of your SQL that work for <code>Pandas</code>.</p>

<p>Hope it helps. Best Regards. </p>
","4989676",0
36,35765912,2,35682879,2016-03-03 07:37:05,2,"<p>I got an answer from azure support:</p>

<blockquote>
  <p>Currently it is not possible to access sql azure dbs from within  an
  “execute python script” module. As you suspected this is due to
  missing odbc drivers in the execution environment.   Suggested
  workarounds are to  a) use reader module   or   b) export to blobs
  and use the Azure Python SDK for accessing those blobs
  <a href=""http://blogs.msdn.com/b/bigdatasupport/archive/2015/10/02/using-azure-sdk-for-python.aspx"" rel=""nofollow"">http://blogs.msdn.com/b/bigdatasupport/archive/2015/10/02/using-azure-sdk-for-python.aspx</a></p>
</blockquote>

<p>So currently it it <strong>impossible</strong> to connect to SQL server from “execute python script” module in Azure-ML. If you like to change it, please vote <a href=""https://feedback.azure.com/forums/257792-machine-learning/suggestions/12589266-enable-odbc-connection-from-excute-python-script"" rel=""nofollow"">here</a> </p>
","1021911",0
37,35980492,2,35973168,2016-03-14 06:01:52,4,"<p>I have read the source code in the python package <strong>azureml</strong>, and found out that they are using a simple request post when uploading a dataset, which has a limited content length 4194304 bytes.</p>

<p>I tried to modify the code inside ""http.py"" within the python package <strong>azureml</strong>. I posted the request with a chunked data, and I got the following error:</p>

<pre><code>Traceback (most recent call last):
  File "".\azuremltest.py"", line 10, in &lt;module&gt;
    ws.datasets.add_from_dataframe(frame, 'GenericCSV', 'output2.csv', 'Uotput results')
  File ""C:\Python34\lib\site-packages\azureml\__init__.py"", line 507, in add_from_dataframe
    return self._upload(raw_data, data_type_id, name, description)
  File ""C:\Python34\lib\site-packages\azureml\__init__.py"", line 550, in _upload
raw_data, None)
  File ""C:\Python34\lib\site-packages\azureml\http.py"", line 135, in upload_dataset
    upload_result = self._send_post_req(api_path, raw_data)
  File ""C:\Python34\lib\site-packages\azureml\http.py"", line 197, in _send_post_req
    raise AzureMLHttpError(response.text, response.status_code)
azureml.errors.AzureMLHttpError: Chunked transfer encoding is not permitted. Upload size must be indicated in the Content-Length header.
Request ID: 7b692d82-845c-4106-b8ec-896a91ecdf2d 2016-03-14 04:32:55Z
</code></pre>

<p>The REST API in <strong>azureml</strong> package does not support chunked transfer encoding. Hence, I took a look at how the Azure ML studio implements this, and I found out this:</p>

<ol>
<li><p>It post a request with content-length=0 to <code>https://studioapi.azureml.net/api/resourceuploads/workspaces/&lt;workspace_id&gt;/?userStorage=true&amp;dataTypeId=GenericCSV</code>, which will return an id in the response body.</p></li>
<li><p>Break the .csv file into chunks less than 4194304 bytes, and post them to <code>https://studioapi.azureml.net/api/blobuploads/workspaces/&lt;workspace_id&gt;/?numberOfBlocks=&lt;the number of chunks&gt;&amp;blockId=&lt;index of chunk&gt;&amp;uploadId=&lt;the id you get from previous request&gt;&amp;dataTypeId=GenericCSV</code></p></li>
</ol>

<p>If you really want this functionality, you can implement it with python and the above REST API.</p>

<p>If you think it's too complicated, report the issue to <a href=""https://github.com/Azure/Azure-MachineLearning-ClientLibrary-Python/issues"" rel=""nofollow"">this</a>. The <strong>azureml</strong> python package is still under development, so your suggestion would be very helpful for them.</p>
","5760649",4
38,35983684,2,35970126,2016-03-14 09:26:29,2,"<p>The <code>/dev/shm</code> is a virtual filesystem for passing data between programs that implementation of traditional shared memory on Linux.</p>

<p>So you could not increase it via set up some options on Application Layout.</p>

<p>But for example, you can remount <code>/dev/shm</code> with 8G size in Linux Shell with administrator permission like <code>root</code> as follows.</p>

<p><code>mount -o remount,size=8G /dev/shm</code></p>

<p>However, it seems that Azure ML studio not support remote access via SSH protocol, so the feasible plan is upgrade the standard tier if using free tier at present.</p>
","4989676",1
39,36000440,2,35886113,2016-03-15 00:10:13,1,"<p>Traditional Node Performance:</p>

<p>You can currently only view relative gain in gini within the boosted decision tree models. Right-click and visualize the output of a trained boosted decision tree, image linked below. From that, wait for the trees to load. You may then click on the nodes of each individual tree to view the split gains at each level.</p>

<p><img src=""https://i.stack.imgur.com/NVnll.png"" alt=""Split Gain at Each Node""></p>

<p>Entropy/Information Gain:</p>

<p>Though let's step back and ask why would we want to view entropy? Entropy is a node specific measurement within an individual tree. Azure Machine Learning does not have single tree classifiers such as rpart in R, only ensembles of trees in the form of decision forest, decision jungle, and boosted decision tree modules. </p>

<p>Variable Importance:</p>

<p>Therefore, I am guessing that you are looking for variable/feature importance measurements, which is the aggregation or average of the overall gini/entropy/information gain of all node splits in all trees within the ensemble. Azure ML has a module that calculates feature importance from a trained algorithm called Permutation Feature Importance module. It works by running random predictor values through your trained model to see the magnitude at which the response class changes. </p>

<p><img src=""https://i.stack.imgur.com/NxQ1b.png"" alt=""Permutation Feature Importance Module""></p>
","5952764",0
40,36002193,2,35998155,2016-03-15 03:30:04,2,"<p>Can you force the type using Meta-Editor before passing to execute-R</p>
","4589073",0
41,36132435,2,36125274,2016-03-21 13:32:42,1,"<p>From the R doc.</p>

<blockquote>
  <p>The AzureML API does not support uploads for <em>replacing</em> datasets with
  new data by re-using a name. If you need to do this, first delete the
  dataset from the AzureML Studio interface, then upload a new version.</p>
</blockquote>

<p>Now, I think this is particular for the R sdk, as the Python SDK, and the AzureML Studio UI lets you upload a new dataset. Will check in with the R team about this.</p>

<p>I would recommend uploading it as a new dataset with a new name, and then replacing the dataset in your experiment with this new dataset. Sorry this seem's round about, but I think is the easier option.</p>

<p>Unless you want to upload the new version using the AzureML Studio, in which case go to +NEW, Dataset, select your file and select the checkbox that says this is an existing dataset. The filename should be the same. </p>
","1341806",1
42,36134909,2,36126897,2016-03-21 15:16:38,0,"<p>I have found that if I go from notebook menu to File->Open, then I see all my notebooks, their statuses and I could shutdown them.</p>

<p>Also I have found that some of my closed notebooks were still alive and have shut them down.</p>

<p>After this my working notebook came back to life.</p>
","5769555",0
43,36148545,2,36132719,2016-03-22 07:35:18,0,"<p>Based on my understanding, I think you want to dynamically get the selected columns data via request the Azure ML webservice with some parameters on the client.</p>

<p>You can refer to the offical document <a href=""https://azure.microsoft.com/en-us/documentation/articles/machine-learning-web-service-parameters/"" rel=""nofollow"">Use Azure Machine Learning Web Service Parameters</a> and the blog <a href=""https://blogs.technet.microsoft.com/machinelearning/2014/11/25/azureml-web-service-parameters/"" rel=""nofollow"">AzureML Web Service Parameters</a> to know how to set and use the web service parameters to implement your needs via add the selected column names as array into the json parameter <code>GlobalParameters</code>.</p>

<p>Meanwhile, there is a client sample on GitHub <a href=""https://github.com/nk773/AzureML_RRSApp"" rel=""nofollow"">https://github.com/nk773/AzureML_RRSApp</a>. Althought it was writen in Java, I think it is easy to understand, then you can rewrite in Python with <code>requests</code> package.</p>
","4989676",1
44,36178693,2,36154971,2016-03-23 12:49:37,2,"<p>No. Currently we do not feature exporting weights from the models including with Azure Machine Learning. </p>

<p>If you have a method for extracting weights from Python models, you may be able to work this out using the execute Python Script module.</p>

<p>The primary purpose of Azure Machine Learning is to make deployable and scalable web services from the machine learning modules. Though the authoring experience for creating ML models is great, it is not intended to be a place to create and export models, but instead a place to create and operationalize your models. </p>

<p><em>UPDATE</em> New features may make this answer outdated. </p>
","1341806",2
45,36183493,2,36127510,2016-03-23 16:22:08,4,"<p>Right now, Azure Data Lake Store is not a supported source, as you note.  That said, Azure Data Lake Analytics can also be used to write data out to Azure Blob Store, and so you can use that as an approach to process the data in U-SQL and then stage it for Azure Machine Learning to process it from Blob store.  When Azure ML supports Data Lake store, then you can switch that over. </p>
","500945",0
46,36250327,2,36249716,2016-03-27 17:24:16,4,"<p>I'm assuming you've created the webservice from the experiment and asking about the consumption of the webservice. You can consume the webservice from anything that can do an API call to the endpoint. I don't know the exact architecture of your solution but take a look at this as it might suit your scenario. </p>

<p>Stream analytics on Azure has a new feature called Functions(just a heads-up, its still in preview) that can automate the usage of deployed ML services from your account.Since you are trying to gather info from IoT devices, you might use <a href=""https://azure.microsoft.com/en-us/documentation/articles/event-hubs-csharp-ephcs-getstarted/"" rel=""nofollow"">Event Hubs</a> or <a href=""https://azure.microsoft.com/en-us/documentation/articles/iot-hub-csharp-csharp-getstarted/"" rel=""nofollow"">IoT Hubs</a> to get the data and process it using Stream Analytics and during the process you can use the Webservice as Function in SA to achieve on-the-go ML results.</p>

<p>Usage is relatively simple if you are familiar with Stream Analytics or SQL queries in general.This <a href=""https://azure.microsoft.com/en-us/documentation/articles/stream-analytics-machine-learning-integration-tutorial/"" rel=""nofollow"">link</a> shows the step by step implementation and the usage is below;</p>

<pre><code>    WITH subquery AS (  
    SELECT text, ""webservicealias""(text) as result from input  
    )  

    Select text, result.[Score]  
    Into output  
    From subquery  
</code></pre>

<p>Hope this helps!</p>

<p>Mert</p>
","4840671",0
47,36300150,2,36285329,2016-03-30 05:07:07,0,"<p>You will have to upload all dependent packages of casualImpact as a zip file - see sample <a href=""http://gallery.azureml.net/Details/7507f907deb845d9b9b193b455a8615d"" rel=""nofollow"">here</a> which shows uploading two packages required for xgboost</p>
","4589073",1
48,36357537,2,36344278,2016-04-01 13:23:36,1,"<p>If you are an owner in the workspace, you can open your dataset in Python inside of a Jupyter Notebook. By the visualize should be an open in notebook button. Then just execute the code that is provided for you, and it should print your dataset. You can then also select specific columns to visualize as well.</p>
","1341806",0
49,36485746,2,36485084,2016-04-07 19:43:50,1,"<p>You can use ""<strong>import images</strong>"" module in Azure ML Studio that can read images from Azure blob storage - <a href=""https://gallery.cortanaintelligence.com/Experiment/Face-detection-2"" rel=""nofollow"">here</a> is the sample experiment </p>
","4589073",3
50,36563933,2,36563769,2016-04-12 04:57:38,14,"<p>Please try the Convert to CSV module: <a href=""https://msdn.microsoft.com/library/azure/faa6ba63-383c-4086-ba58-7abf26b85814"" rel=""noreferrer"">https://msdn.microsoft.com/library/azure/faa6ba63-383c-4086-ba58-7abf26b85814</a></p>

<p>After you run the experiment, right click on the output of the module to download the CSV file.</p>
","6191402",1
51,36584067,2,36450108,2016-04-12 21:08:01,3,"<p>That page is missing references to content mentioned at other locations.  See this page for a more complete guide...</p>

<p><a href=""https://azure.microsoft.com/en-us/documentation/articles/machine-learning-recommendation-api-documentation/"" rel=""nofollow"">https://azure.microsoft.com/en-us/documentation/articles/machine-learning-recommendation-api-documentation/</a></p>

<p>It describes Cold Items in the Rank Build section in the document as...</p>

<p>Features can enhance the recommendation model, but to do so requires the use of meaningful features. For this purpose a new build was introduced - a rank build. This build will rank the usefulness of features. A meaningful feature is a feature with a rank score of 2 and up. After understanding which of the features are meaningful, trigger a recommendation build with the list (or sublist) of meaningful features. It is possible to use these feature for the enhancement of both warm items and cold items. In order to use them for warm items, the UseFeatureInModel build parameter should be set up. In order to use features for cold items, the AllowColdItemPlacement build parameter should be enabled. Note: It is not possible to enable AllowColdItemPlacement without enabling UseFeatureInModel.</p>

<p>It also describes the ReasoningFeatureList in the Recommendation Reasoning section as...</p>

<p>Recommendation reasoning is another aspect of feature usage. Indeed, the Azure Machine Learning Recommendations engine can use features to provide recommendation explanations (a.k.a. reasoning), leading to more confidence in the recommended item from the recommendation consumer. To enable reasoning, the AllowFeatureCorrelation and ReasoningFeatureList parameters should be setup prior to requesting a recommendation build.</p>
","6195563",1
52,36772718,2,36763479,2016-04-21 14:31:25,1,"<p>The models you compare should be of the same type - binary classification, regression, multi-class classification etc. For example, you can't compare effectiveness of linear regression to the effectiveness of logistics regression. They solve absolutely different tasks.</p>

<p>This is the case for you - you try to compare linear regression (which outputs real value) with the multiclass decision forest, which tries to classify input to some class.</p>
","3633250",0
53,36816601,2,36781843,2016-04-23 21:22:26,0,"<p>Finally I found the visualization. Just put the code in AzureML Studio Execute R Script module will be fine.
After the code has been executed successfully, right click Execute R Script module, and choose R device, the visualization is there</p>

<p><a href=""https://i.stack.imgur.com/1aZ0y.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/1aZ0y.png"" alt=""ggplot through AzureML R Script""></a></p>
","2085454",0
54,36820022,2,36260727,2016-04-24 06:30:22,0,"<p>You can use the ""Execute R Script"" module and just plug in your R code there.</p>

<pre><code>df &lt;- maml.mapInputPort(1)
df &lt;- df[!df$col1 == ""None"",] 
maml.mapOutputPort(""df"");
</code></pre>
","4884260",0
55,36950208,2,36917948,2016-04-30 02:59:34,1,"<p>You can use permutation feature importance module but that will give importance of the features across the sample set. Retrieving the weights on per call basis is not available in Azure ML.</p>
","4589073",2
56,36968954,2,36967126,2016-05-01 16:22:44,6,"<p>First, it has been well established that a variety of classification models yield incredibly good results on Iris (Iris is very predictable); see <a href=""http://lab.fs.uni-lj.si/lasin/wp/IMIT_files/neural/doc/seminar8.pdf"" rel=""noreferrer"">here</a>, for example.</p>

<p>Secondly, we can observe that there are relatively few features in the Iris dataset. Moreover, if you look at the <a href=""https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.names"" rel=""noreferrer"">dataset description</a> you can see that two of the features are very highly correlated with the class outcomes.</p>

<p>These correlation values are linear, single-feature correlations, which indicates that one can most likely apply a linear model and observe good results. Neural nets are highly nonlinear; they become more and more complex and capture greater and greater nonlinear feature combinations as the number of hidden nodes and hidden layers is increased.</p>

<p>Taking these facts into account, that (a) there are few features to begin with and (b) that there are high linear correlations with class, would all point to a less complex, linear function as being the appropriate predictive model-- by using a single hidden node, you are very nearly using a linear model.</p>

<p>It can also be noted that, in the absence of any hidden layer (i.e., just input and output nodes), and when the logistic transfer function is used, this is equivalent to logistic regression.</p>
","2172472",2
57,37003425,2,37000397,2016-05-03 11:53:36,1,"<p>The R Azure ML API has that. Excerpt from <a href=""https://htmlpreview.github.io/?https://github.com/RevolutionAnalytics/AzureML/blob/master/vignettes/getting_started.html"" rel=""nofollow"">https://htmlpreview.github.io/?https://github.com/RevolutionAnalytics/AzureML/blob/master/vignettes/getting_started.html</a> : </p>

<p><code>
(webservices &lt;- services(ws, name = ""AzureML-vignette-silly""))
</code></p>
","623419",1
58,37198119,2,37183494,2016-05-12 21:56:26,3,"<p>Go over this <a href=""http://download.microsoft.com/download/0/5/A/05AE6B94-E688-403E-90A5-6035DBE9EEC5/machine-learning-basics-infographic-with-algorithm-examples.pdf"" rel=""nofollow"">http://download.microsoft.com/download/0/5/A/05AE6B94-E688-403E-90A5-6035DBE9EEC5/machine-learning-basics-infographic-with-algorithm-examples.pdf</a></p>

<p>If above infographic doesn't help, then you can try all of the learners by going over this experiment and use the one with best results - <a href=""https://gallery.cortanaintelligence.com/Experiment/Algo-Evaluater-Compare-Performance-of-Multiple-Algos-against-Your-Data-1"" rel=""nofollow"">https://gallery.cortanaintelligence.com/Experiment/Algo-Evaluater-Compare-Performance-of-Multiple-Algos-against-Your-Data-1</a></p>
","4589073",1
59,37212991,2,37140987,2016-05-13 14:43:12,3,"<p>I used something called Azure Data Factory (ADF). It allows you to schedule a job by defining a pipeline with activities. There are activities for training your model or scoring your predictive ML. The scoring result, I am storing it in Azure DB (it could be another storage) and connected it to Power BI.</p>
","5748174",1
60,37228199,2,37228077,2016-05-14 15:00:35,2,"<p>As of today, there's no automatic way of converting a ""Classic"" storage account into ""Azure Resource Manager (ARM)"" storage account. Today, you would need to copy data from a classic storage account to a new storage account.</p>

<p>Having said that, there's no difference in how the data is stored in both kinds of storage accounts. Both of them support connecting via account name/key and/or shared access signature. The difference is how these storage account themselves are managed. In ARM storage accounts, you can assign granular role-based access control (RBAC) to control what a user can do as far as managing the storage accounts (like updating, deleting, viewing/regenerating keys).</p>

<p>Regarding your question about using new storage accounts with ML workspace, I don't think it's possible today (I may be wrong though). Reason being, ML is still managed via old portal which doesn't have the capability to manage ARM storage accounts.</p>
","188096",1
61,37228218,2,37228027,2016-05-14 15:02:38,1,"<p>Just upload it as a dataset. <a href=""https://azure.microsoft.com/en-us/documentation/articles/machine-learning-execute-python-scripts/"" rel=""nofollow"">Reference.</a> (search for it, as it is not on the first page).</p>

<p><a href=""https://azure.microsoft.com/en-us/documentation/articles/machine-learning-walkthrough-2-upload-data/#upload-the-dataset-to-machine-learning-studio"" rel=""nofollow"">Reference</a> on how to upload the dataset. </p>
","1963868",2
62,37255927,2,37250368,2016-05-16 14:08:37,3,"<p>You can try to use Azure ML PowerShell module to discover and delete web service endpoints, and web service.</p>

<p><a href=""http://aka.ms/amlps"" rel=""nofollow"">http://aka.ms/amlps</a></p>
","6191408",2
63,37378876,2,37363883,2016-05-22 19:43:50,1,"<p>What is your source location - SQL or Blob or http?</p>

<p>If SQL, then you can use query to start from line 6.</p>

<p>If Blob/http, I would suggest reading a file as a single column TSV format, use simple R/Python script to drop first 6 rows and convert to csv</p>
","4589073",0
64,37409955,2,37375506,2016-05-24 09:46:39,2,"<p>The matchbox recommender requires that ratings be numerical or categorical. Also when training, your ratings cannot all be the same.</p>

<p>You need to use a metadata editor <a href=""https://msdn.microsoft.com/en-us/library/azure/dn905986.aspx"" rel=""nofollow"">https://msdn.microsoft.com/en-us/library/azure/dn905986.aspx</a> to convert the ratings into numerical features and you need to make sure you are using a range of ratings.</p>

<p>Then this should work!</p>
","2596276",1
65,37511658,2,37503141,2016-05-29 15:26:07,1,"<p>Currently no. There is no odbc driver in the container. </p>

<p>If you provide some more details about what you want to delete maybe I can offer a workaround. </p>

<p>But to update a Db... You could use execute python to send an event to an event hub(in a service bus). Then connect to that event hub via stream analytics. There you can set the azure SQL Db as an output port to update your row.</p>

<p>Let me know if you need more details about any step. </p>
","1341806",1
66,37538120,2,37519858,2016-05-31 06:15:35,0,"<p>Try using this sample and compare with yours - <a href=""https://gallery.cortanaintelligence.com/Experiment/Compare-Sample-5-in-R-vs-Azure-ML-1"" rel=""nofollow"">https://gallery.cortanaintelligence.com/Experiment/Compare-Sample-5-in-R-vs-Azure-ML-1</a></p>
","4589073",2
67,37541187,2,37540703,2016-05-31 08:58:13,1,"<p>The module you are looking for, is the one called “<strong>Cross-Validate Model</strong>”. It basically splits whatever comes in from the input-port (dataset) into 10 pieces, then reserves the last piece as the “answer”; and trains the nine other subset models and returns a set of accuracy statistics measured towards the last subset. What you would look at is the column called “Mean absolute error” which is the average error for the trained models. You can connect whatever algorithm you want to one of the ports, and subsequently you will receive the result for that algorithm in particular after you “right-click” the port which gives the score.</p>

<p>After that you can assess which algorithm did the best. And as a pro-tip; you could use the <strong>Filter-based-feature selection</strong> to actually see which column had a significant impact on the result.</p>
","5085210",0
68,37552793,2,37418265,2016-05-31 18:10:10,2,"<p>Well after a lot of RnD, I was able to finally call Azure ML using some workarounds.</p>

<p>Wrapping Azure ML webservice on Azure API is one option.</p>

<p>But, what I did was that I created a python webservice which calls the Azure webservice.</p>

<p>So now my HTML App calls the python webservice which calls Azure ML and returns data to the HTML App.</p>
","6206103",0
69,35237286,2,35237226,2016-02-06 04:23:52,1,"<p>You can use: PyPy to run your program with less memory usage and more speed. see this <a href=""http://pypy.org/"" rel=""nofollow"">pypy site</a></p>
","5685031",1
70,33091869,2,33091830,2015-10-12 23:38:59,18,"<p>I think you want to use <code>get_blob_to_bytes</code>, <code>or get_blob_to_text</code>; these should output a string which you can use to create a dataframe as</p>

<pre><code>from io import StringIO
blobstring = blob_service.get_blob_to_text(CONTAINERNAME,BLOBNAME)
df = pd.read_csv(StringIO(blobstring))
</code></pre>
","839957",2
71,33006711,2,32993148,2015-10-08 04:35:40,2,"<p>Update: visualization of decision trees is available now!  Right-click on the output node of the ""Train Model"" module and select ""Visualize"".  </p>

<p>My old answer:</p>

<p><em>I'm sorry; visualization of decision trees isn't available yet.  (I really want it too!  You can upvote this feature request at <a href=""http://feedback.azure.com/forums/257792-machine-learning/suggestions/7419469-show-variable-importance-after-experiment-runs"" rel=""nofollow"">http://feedback.azure.com/forums/257792-machine-learning/suggestions/7419469-show-variable-importance-after-experiment-runs</a>, but they are currently working on it.)<br>
Just FYI, you can currently see what the model builds for linear algorithms by right-clicking on the ""Train Model"" module output node and selecting ""Visualize"".  It will show the initial parameter values and the feature weights.  But for non-linear algorithms like decision trees, that visibility is still forthcoming.</em>  </p>
","1535478",1
72,32974719,2,32884296,2015-10-06 16:13:01,3,"<p>You can remove the web service input block and publish the web service without it. That way the Pdelta input will be passed in only from the Reader module.</p>
","5414914",1
73,31657926,2,31630745,2015-07-27 16:02:43,2,"<p>RAE and RSE closer to 0 is a good sign...you want error to be as low as possible.  See <a href=""https://azure.microsoft.com/en-us/documentation/articles/machine-learning-evaluate-model-performance/"" rel=""nofollow"">this article</a> for more information on evaluating your model.  From that page:</p>

<blockquote>
  <p>The term ""error"" here represents the difference between the predicted value and the true value. The absolute value or the square of this difference are usually computed to capture the total magnitude of error across all instances, as the difference between the predicted and true value could be negative in some cases. The error metrics measure the predictive performance of a regression model in terms of the mean deviation of its predictions from the true values. Lower error values mean the model is more accurate in making predictions. An overall error metric of 0 means that the model fits the data perfectly.</p>
</blockquote>

<p>Yes, with your current results, the boosted decision tree performs best.  I don't know the details of your work well enough to determine if that is good enough.  It honestly may be.  But if you determine it's not, you can also tweak the input parameters in your ""Boosted Decision Tree Regression"" module to try to get even better results.  The ""<a href=""https://msdn.microsoft.com/library/azure/038d91b6-c2f2-42a1-9215-1f2c20ed1b40/"" rel=""nofollow"">ParameterSweep</a>"" module can help with that by trying many different input parameters for you and you specify the parameter that you want to optimize for (such as your RAE, RSE, or COD referenced in your question).  See <a href=""https://azure.microsoft.com/en-us/documentation/articles/machine-learning-algorithm-parameters-optimize/"" rel=""nofollow"">this article</a> for a brief description.  Hope this helps.
<br/><br/>
P.S. I'm glad that you're looking into the black carbon levels in Westeros...I'm sure Cersei doesn't even care.  </p>
","1535478",1
74,30944383,2,30937903,2015-06-19 17:53:00,2,"<p>Azure ML supports ""execute-R"" module which can be easily used to accomplish this in R - few examples below</p>

<p>x&lt;-as.Date(""12/3/2009"", ""%m/%d/%Y"")</p>

<blockquote>
  <p>months.Date(x)</p>
</blockquote>

<p>[1] ""December""</p>

<blockquote>
  <p>weekdays.Date(x)</p>
</blockquote>

<p>[1] ""Thursday""</p>

<blockquote>
  <p>quarters(x)</p>
</blockquote>

<p>[1] ""Q4""</p>
","4589073",2
75,30872172,2,30859824,2015-06-16 15:38:50,22,"<p>As far as I know, Project Oxford (MS Azure CV API) wouldn't be suitable for your task. Their APIs are very focused to Face related tasks (detection, verification, etc), OCR and Image description. And apparently you can't extend their models or train new ones from the existing ones.</p>

<p>However, even though I don't know an out of the box solution for your object detection problem; there are easy enough approaches that you could try and that would give you some start point results.</p>

<p>For instance, here is a naive method you could use:</p>

<p><strong>1) Create your dataset:</strong>
    This is probably the more tedious step and paradoxically a crucial one. I will assume you have a good amount of images to work with. What would you need to do is to pick a fixed window size and extract positive and negative examples.
<img src=""https://i.stack.imgur.com/H4uC5.png"" alt=""enter image description here""></p>

<p>If some of the images in your dataset are in different sizes you would need to rescale them to a common size. You don't need to get too crazy about the size, probably 30x30 images would be more than enough. To make things easier I would turn the images to gray scale too. </p>

<p><strong>2) Pick a classification algorithm and train it:</strong>
    There is an awful amount of classification algorithms out there. But if you are new to machine learning I will pick the one I would understand the most. Keeping that in mind, I would check out logistic regression which give decent results, it's easy enough for starters and have a lot of libraries and tutorials. For instance, <a href=""http://blog.yhathq.com/posts/logistic-regression-and-python.html"" rel=""noreferrer"">this one</a> or <a href=""https://msdn.microsoft.com/en-us/magazine/dn948113.aspx"" rel=""noreferrer"">this one</a>. At first I would say to focus in a binary classification problem (like if there is an UD logo in the picture or not) and when you master that one you can jump to the multi-class case. There are resources for that <a href=""http://www.codeproject.com/Articles/821347/MultiClass-Logistic-Classifier-in-Python"" rel=""noreferrer"">too</a> or you can always have several models one per logo and run this recipe for each one separately. </p>

<p>To train your model, you just need to read the images generated in the step 1 and turn them into a vector and label them accordingly. That would be the  dataset that will feed your model. If you are using images in gray scale, then each position in the vector would correspond to a pixel value in the range 0-255. Depending on the algorithm you might need to rescale those values to the range [0-1] (this is because some algorithms perform better with values in that range). Notice that rescaling the range in this case is fairly easy (new_value = value/255).</p>

<p>You also need to split your dataset, reserving some examples for training, a subset for validation and another one for testing. Again, there are different ways to do this, but I'm keeping this answer as naive as possible.</p>

<p><strong>3) Perform the detection:</strong>
    So now let's start the fun part. Given any image you want to run your model and produce coordinates in the picture where there is a logo. There are different ways to do this and I will describe one that probably <strong>is not the best nor the more efficient</strong>, but it's easier to develop in my opinion.</p>

<p>You are going to scan the picture, extracting the pixels in a ""window"", rescaling those pixels to the size you selected in step 1 and then feed them to your model. </p>

<p><img src=""https://i.stack.imgur.com/VGk3f.png"" alt=""Extracting windows to feed the model""></p>

<p>If the model give you a positive answer then you mark that window in the original image. Since the logo might appear in different scales you need to repeat this process with different window sizes. You also would need to tweak the amount of space between windows.</p>

<p><strong>4) Rinse and repeat:</strong>
    At the first iteration it's very likely that you will get a lot of false positives. Then you need to take those as negative examples and retrain your model. This would be an iterative process and hopefully on each iteration you will have less and less false positives and fewer false negatives.</p>

<p>Once you are reasonable happy with your solution, you might want to improve it. You might want to try other classification algorithms like <a href=""https://en.wikipedia.org/wiki/Support_vector_machine"" rel=""noreferrer"">SVM</a> or <a href=""https://en.wikipedia.org/wiki/Deep_learning"" rel=""noreferrer"">Deep Learning Artificial Neural Networks</a>, or to try better object detection frameworks like <a href=""https://en.wikipedia.org/wiki/Viola%E2%80%93Jones_object_detection_framework"" rel=""noreferrer"">Viola-Jones</a>. Also, you will probably need to use <a href=""https://en.wikipedia.org/wiki/Cross-validation_%28statistics%29"" rel=""noreferrer"">crossvalidation</a> to compare all your solutions (you can actually use crossvalidation from the beginning). By this moment I bet you would be confident enough that you would like to use OpenCV or another ready to use framework in which case you will have a fair understanding of what is going on under the hood. </p>

<p>Also you could just disregard all this answer and go for an OpenCV object detection tutorial like this <a href=""http://note.sonots.com/SciSoftware/haartraining.html"" rel=""noreferrer"">one</a>. Or take another answer from another question like this <a href=""https://stackoverflow.com/questions/10168686/algorithm-improvement-for-coca-cola-can-shape-recognition?rq=1"">one</a>. Good luck!</p>
","1112142",3
76,30017719,2,30016116,2015-05-03 18:28:28,1,"<p><strong>Answering my own question:</strong></p>

<p>It turned out the join type I needed was a <em>Full Outer Join.</em></p>

<p>Background information:</p>

<ul>
<li>For those pursuing AzureML related to this question in the future, I had to enable the functionality to save columns from the 'Right' table input.</li>
<li>I then ran this through a 'Project Columns' module and a 'Metadata' module to rename them to the form I desired.</li>
</ul>
","4196127",0
77,29781576,2,29391016,2015-04-21 19:37:00,1,"<p>I believe Azure ML already does this.  When you run it the second time, if nothing upstream from that tick has changed it should just load the results from the previous run.  It may take a few seconds for Azure ML to recognize that it is cached and reload it.</p>
","276310",0
78,28845910,2,27568624,2015-03-04 02:57:40,5,"<p>To install a package this way, you have to create a .zip of a .zip. The outer layer of packaging will get unzipped into the src/ folder when the dataset is passed in to the module, and you'll be able to install the inner package from there.</p>
","4630014",0
79,28844298,2,28590690,2015-03-03 23:52:28,1,"<p>The Azure ML retraining API is designed to handle the workflow you describe:</p>

<p><a href=""http://azure.microsoft.com/en-us/documentation/articles/machine-learning-retrain-models-programmatically/"" rel=""nofollow"">http://azure.microsoft.com/en-us/documentation/articles/machine-learning-retrain-models-programmatically/</a></p>

<p>Hope this helps,</p>

<p>Roope - Microsoft Azure ML Team</p>
","4630014",0
80,26226283,2,26193051,2014-10-06 23:13:28,4,"<p>I found a solution <a href=""http://social.msdn.microsoft.com/Forums/en-US/bf8f76c7-f976-4552-8553-8e54133ff2c6/replacing-specific-values-in-dataset-with-azure-ml?forum=MachineLearning"" rel=""nofollow"">there</a>, by using a <code>Convert to Dataset</code> module.</p>
","1602990",0
81,35327300,2,35246826,2016-02-10 22:31:24,5,"<p><strong>Bottom Line Up Front:</strong> Use HTTP instead of HTTPS for accessing Azure storage.</p>

<p>When declaring BlobService pass in <code>protocol='http'</code> to force the service to communicate over HTTP. Note that you must have your container configured to allow requests over HTTP (which it does by default).</p>

<p><code>client = BlobService(STORAGE_ACCOUNT, STORAGE_KEY, protocol=""http"")</code></p>

<p>History and credit:</p>

<p>I posted a query on this topic to @AzureHelps and they opened a ticket on the MSDN forums: <a href=""https://social.msdn.microsoft.com/Forums/azure/en-US/46166b22-47ae-4808-ab87-402388dd7a5c/trouble-writing-blob-storage-file-in-azure-ml-experiment?forum=MachineLearning&amp;prof=required"">https://social.msdn.microsoft.com/Forums/azure/en-US/46166b22-47ae-4808-ab87-402388dd7a5c/trouble-writing-blob-storage-file-in-azure-ml-experiment?forum=MachineLearning&amp;prof=required</a> </p>

<p>Sudarshan Raghunathan replied with the magic.  Here are the steps to make it easy for everyone to duplicate my fix:</p>

<ol>
<li>Download azure.zip which provides the required libraries: <a href=""https://azuremlpackagesupport.blob.core.windows.net/python/azure.zip"">https://azuremlpackagesupport.blob.core.windows.net/python/azure.zip</a></li>
<li>Upload them as a DataSet to the Azure ML Studio</li>
<li>Connect them to the Zip input on an <code>Execute Python Script</code> module</li>
<li>Write your script as you would normally, being sure to create your <code>BlobService</code> object with <code>protocol='http'</code></li>
<li>Run the Experiment - you should now be able to write to blob storage.</li>
</ol>

<p>Some example code can be found here: <a href=""https://gist.github.com/drdarshan/92fff2a12ad9946892df"">https://gist.github.com/drdarshan/92fff2a12ad9946892df</a></p>

<p>The code I used was the following, which doesn't first write the CSV to the file system, but sends as a text stream.</p>

<pre><code>from azure.storage.blob import BlobService

def azureml_main(dataframe1 = None, dataframe2 = None):
    account_name = 'mystorageaccount'
    account_key='p8kSy3FACx...redacted...ebz3plQ=='
    container_name = ""upload""
    json_output_file_name = 'testfromml.json'
    json_orient = 'records' # Can be index, records, split, columns, values
    json_force_ascii=False;

    blob_service = BlobService(account_name, account_key, protocol='http')

    blob_service.put_block_blob_from_text(container_name,json_output_file_name,dataframe1.to_json(orient=json_orient, force_ascii=json_force_ascii))

    # Return value must be of a sequence of pandas.DataFrame
    return dataframe1,
</code></pre>

<p>Some thoughts:</p>

<ol>
<li>I would prefer if the azure Python libraries were imported by default. Microsoft imports hundreds of 3rd party libraries into Azure ML as part of the Anaconda distribution. They should also include those necessary to work with Azure. We're in Azure, we've committed to Azure. Embrace it.</li>
<li>I don't like that I have to use HTTP, instead of HTTPS. Granted, this is internal Azure communication, so it's likely no big deal. However, most of the documentation suggests the use of SSL / HTTPS when working with blob storage, so I'd prefer to be able to do that. </li>
<li>I still get random timeout errors in the Experiment. Sometimes the Python code will execute in milliseconds, other times it runs for several 60 or seconds and then times out. This makes running it in an experiment very frustrating at times. However, when published as a Web Service I do not seem to have this problem.</li>
<li>I would prefer that the experience from my local code matched more closely Azure ML. Locally, I can use HTTPS and never time out. It's blazing fast, and easy to write. But moving to an Azure ML experiment means some debugging, nearly every time. </li>
</ol>

<p>Huge props to Dan, Peter and Sudarshan, all from Microsoft, for their help in resolving this. I very much appreciate it!</p>
","5893121",0
82,31858425,2,31609319,2015-08-06 14:28:45,0,"<p>I have worked internally to request a confirmation of your concern - </p>

<blockquote>
  <p>'Enter Data' as list instead of list of lists in Azure ML Web Service</p>
</blockquote>

<p>but you expected feature is not available today in Azure ML Studio (The reason behind is Azure ML has to be able to read the input data as a tabular format, rows and columns). Such being the case, I would like to suggest you to submit a new feature request via below option:</p>

<p>On Azure ML Studio -> the upper right corner, there is a smiley face, please click that and send the feedback.</p>

<p>Should you have any further concerns, please feel free to let me know.</p>
","1338161",3
83,35871511,2,35870839,2016-03-08 15:32:13,2,"<p>I found it out myself. It is documenten on Microsoft's site <a href=""https://azure.microsoft.com/en-us/documentation/articles/machine-learning-execute-python-scripts/"" rel=""nofollow noreferrer"">here</a>.</p>

<p>The steps, very short, are:</p>

<ol>
<li>Include all the python you want in a .zip</li>
<li>Upload that zip as a dataset</li>
<li>Drag the dataset as the third option parameter in the 'execute python'-block (example below)</li>
</ol>

<p><a href=""https://i.stack.imgur.com/9FuMr.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/9FuMr.png"" alt=""Example dragging zip to Python script""></a></p>

<ol start=""4"">
<li>execute said function by importing <code>import Hello</code> (the name of the file, not the zip) and running <code>Hello.do_something_usefull()</code></li>
</ol>
","2781698",0
84,35145088,2,27987910,2016-02-02 04:31:32,4,"<p>Currently, we don't support disabling CORS on API side but you can either use the above option or you can use the API management service to disable CORS. The links below should help you with this</p>

<p>Here are the links: <a href=""http://azure.microsoft.com/en-us/documentation/articles/api-management-get-started/"" rel=""noreferrer"">step by step</a> guide, also this <a href=""http://channel9.msdn.com/Blogs/AzureApiMgmt/Last-mile-Security"" rel=""noreferrer"">video</a> on setting headers, and <a href=""https://msdn.microsoft.com/en-us/library/azure/dn894084.aspx#JSONP"" rel=""noreferrer"">this doc</a> on policies.</p>

<p>API Management service allow CORS by enabling it in the API configuration page</p>
","4589073",1
85,33917558,2,33741912,2015-11-25 13:19:15,1,"<p>To do real time Fraud detection typically you will create a Model on Azure ML, then publish that model to oWeb service, then on you Spark or Storm system you will call that Web service, in  sequence ( like payment happened on commercial sites for example), then you will get an immediate answer about the actual parameters you had sent in you web service call.</p>
","5604351",0
86,35851994,2,35851851,2016-03-07 19:12:12,3,"<p>Subset to numeric IDs:</p>

<pre><code>subset(df, grepl('^\\d+$', df$ID))
</code></pre>

<p>The pattern should match values of ID that start and end with digits, and only contain digits.</p>
","1488944",0
87,35325647,2,35304901,2016-02-10 20:51:04,2,"<p>A lot of this will probably depend on the design of your tables. Table Storage is a key / value store (think of it as a dictionary). It has some capabilities for scanning within a partition and across partitions - but the latencies will differ greatly. Ideally if you want to query 1000 rows they should be localized within a partition. See <a href=""https://azure.microsoft.com/en-us/documentation/articles/storage-table-design-guide/"" rel=""nofollow"">Table Design Guide</a> and <a href=""https://azure.microsoft.com/en-us/documentation/articles/storage-performance-checklist/"" rel=""nofollow"">Perf and Scalability Checklist</a> for full details.  </p>
","3590921",0
88,37671274,2,37520849,2016-06-07 05:36:24,5,"<p>Big multiplication function gradient forces the net probably almost immediately into some horrifying state where all its hidden nodes have zero gradient.
We can use two approaches:</p>

<p>1) Devide by constant. We are just deviding everything before the learning and multiply after.</p>

<p>2) Make log-normalization. It makes multiplication into addition:</p>

<pre><code>m = x*y =&gt; ln(m) = ln(x) + ln(y).
</code></pre>
","1876627",1
89,37705346,2,37702759,2016-06-08 14:27:50,3,"<p>You can install ggplot2 in your solution in the Visual Studio extension Open R (<a href=""https://www.visualstudio.com/en-us/features/rtvs-vs.aspx"" rel=""nofollow noreferrer"">https://www.visualstudio.com/en-us/features/rtvs-vs.aspx</a>) through this line of code and visualize it within the R Plot window in Visual Studio after creating your R-project: </p>

<pre><code>install.packages('ggplot2', dep = TRUE)

library(ggplot2)
</code></pre>

<p>The reason I have «library(ggplot2)» is to check if the package got successfully installed, else you would get an error like this: <strong>Error in library(ggplot2) : there is no package called ‘ggplot2’</strong></p>

<p>So if you don’t get that error; you should be good to go.</p>

<p>For your question about how to output charts; you simply have to populate the ggplot2 charts from a datasource, like in my example below (csv-file):</p>

<pre><code>dataset1 &lt;- read.csv(""Adult Census Income Binary Classification dataset.csv"", header = TRUE, sep = "","", quote = """", fill = TRUE, comment.char = """")

head(dataset1)

install.packages('ggplot2', dep = TRUE)

library(ggplot2)

names(dataset1) &lt;- sub(pattern = ',', replacement = '.', x = names(dataset1))

foo = qplot(age, data = dataset1, geom = ""histogram"", fill = income, position = ""dodge"");

print(foo)

bar = qplot(age, data = dataset1, geom = ""density"", alpha = 1, fill = income);

print(bar)
</code></pre>

<p>Here you can see that I create two charts, one histogram and one density-chart.</p>

<p><a href=""https://i.stack.imgur.com/sLxMN.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/sLxMN.png"" alt=""enter image description here""></a></p>

<p>In Azure ML, the same charts (this time I included a histogram for Relationships as well), would look like this:</p>

<pre><code>// Map 1-based optional input ports to variables

dataset1 &lt;- maml.mapInputPort(1) # class: data.frame

library(ggplot2)

library(data.table)

names(dataset1) &lt;- sub(pattern=',', replacement='.', x=names(dataset1))

// This time we need to specify the X to be sex; which we didn’t need in Visual Studio

foo = qplot(x=sex, data=dataset1, geom=""histogram"", fill=income, position=""dodge"");

print(foo)

foo = qplot(x=relationship, data=dataset1, geom=""histogram"", fill=income, position=""dodge"");

print(foo)

foo = qplot(x=age, data=dataset1, geom=""density"", alpha=0.5, fill=income);

print(foo)

// Select data.frame to be sent to the output Dataset port maml.mapOutputPort(""dataset1"");
</code></pre>

<p>Remember to put all of this in a “Execute R Script module” in order to run it correctly. After that, you can right lick the module and visualize the result.</p>

<p><a href=""https://i.stack.imgur.com/2vTlD.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/2vTlD.png"" alt=""enter image description here""></a></p>
","5085210",0
90,37729506,2,37728314,2016-06-09 14:56:24,0,"<p>What you are doing is essentially recreating this experiment made by Raja Iqbal for the Titanic dataset. I recommend you check that out here: <a href=""http://gallery.cortanaintelligence.com/Experiment/Tutorial-Building-a-classification-model-in-Azure-ML-8?share=1"" rel=""nofollow noreferrer"">http://gallery.cortanaintelligence.com/Experiment/Tutorial-Building-a-classification-model-in-Azure-ML-8?share=1</a></p>

<p>To answer your question, the module you can drag to your canvas in order to make the features into categories; is the Edit Metadata module where you select the columns you want and change the “unchanged” into “Make categorical” within the Categorical-properties pane like in the image below:</p>

<p><a href=""https://i.stack.imgur.com/2NDht.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/2NDht.png"" alt=""enter image description here""></a></p>

<p>You can also use the same module to make better sense from your columns by giving them a different column name. SibSp means SiblingSpouse like I have renamed it to in the image below:</p>

<p><a href=""https://i.stack.imgur.com/Gm9Rr.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Gm9Rr.png"" alt=""enter image description here""></a></p>

<p>And at last you can assign the targeted value (survived) and make the field into a label for ease of use.</p>

<p><a href=""https://i.stack.imgur.com/LyN0j.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/LyN0j.png"" alt=""enter image description here""></a></p>
","5085210",1
91,37811031,2,37807158,2016-06-14 11:42:44,2,"<p>What you would want to do is to train each model and save them as already trained models.
So create a new experiment, train your models and save them by right clicking on each model and they will show up in the left nav bar in the Studio. Now you are able to drag your models into the canvas and have them score predictions where you eventually make them end up in the same output as I have done in my example through the “Add columns” module. I made this example for Ronaldo (Real Madrid CF player) on how he will perform in match after training day. You can see my demo on <a href=""http://ronaldoinform.azurewebsites.net"" rel=""nofollow noreferrer"">http://ronaldoinform.azurewebsites.net</a></p>

<p><a href=""https://i.stack.imgur.com/ZwzUy.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ZwzUy.png"" alt=""Ronaldo InForm""></a></p>

<p>For more detailed explanation on how to save the models and train multiple values; you can check out Raymond Langaeian (MSFT) answer in the comment section on this link:
<a href=""https://azure.microsoft.com/en-us/documentation/articles/machine-learning-convert-training-experiment-to-scoring-experiment/"" rel=""nofollow noreferrer"">https://azure.microsoft.com/en-us/documentation/articles/machine-learning-convert-training-experiment-to-scoring-experiment/</a></p>
","5085210",0
92,37915437,2,34948242,2016-06-20 05:57:16,0,"<p>using ggplot2() in AzureML is bit different. It can use to have plots with labels. Here's the <a href=""https://gallery.cortanaintelligence.com/Experiment/b1c26728eb6c4e4d80dddceae992d653"" rel=""nofollow"">Cortana Intelligence gallery example</a> for the particular task.  </p>
","5383733",0
93,37968630,2,37943572,2016-06-22 12:52:42,0,"<p>The way Azure ML Web Services work in the background means that instances hosting the models are provisioned and moved in a very dynamic multi-tenant environment. Caching data (warming up) can be helpful but this doesn't mean all subsequent calls will land on the same instance with the same data available in the cache. </p>

<p>For models that need a lot of in-memory data there is a limit to what the Azure ML Web Services hosting layer can offer at this point. Microsoft R server could be an alternative to host these big ML workloads and looking at Service Fabric to scale </p>
","1662124",0
94,37971585,2,37812686,2016-06-22 14:51:52,0,"<p>At the end I have solved adding an additional package. The problem is in the fact that you have to check the log of the error and not only the error output (that does not insert all you need). At the end I have solved in this way:</p>

<pre><code>install.packages(""src/scales_0.4.0.zip"" ,lib = ""."", repos = NULL, verbose = TRUE)
install.packages(""src/ggplot2_2.1.0.zip"",lib = ""."", repos = NULL, verbose = TRUE)
install.packages(""src/ggrepel.zip""      ,lib = ""."", repos = NULL, verbose = TRUE)

library(scales,  lib.loc=""."", verbose=TRUE)
library(ggplot2, lib.loc=""."", verbose=TRUE)
library(ggrepel, lib.loc=""."", verbose=TRUE)
...
</code></pre>
","5097722",0
95,38041388,2,37947524,2016-06-26 17:46:56,0,"<p>You can use web service parameters as shown here - <a href=""https://azure.microsoft.com/en-us/documentation/articles/machine-learning-web-service-parameters/"" rel=""nofollow"">https://azure.microsoft.com/en-us/documentation/articles/machine-learning-web-service-parameters</a>/</p>
","4589073",0
96,38046577,2,37805113,2016-06-27 05:39:27,-1,"<p>You can start from Azure Data Factory for automating batch scoring. In your model instead of web service output, you can use DataWriter exporter module to write the output directly into an Azure Table etc. You can check Microsoft MyDriving reference guide (<a href=""http://aka.ms/mydrivingdocs"" rel=""nofollow"">http://aka.ms/mydrivingdocs</a>) at page 107-8 where the machine learning section starts at page 100.</p>
","6516863",0
97,38060560,2,37749862,2016-06-27 18:14:52,0,"<p>I am not quite sure I understand the workflow you described. Can you provide more details on what are you trying to accomplish with your web app and your experiment? For example, what do you mean when you say ""I have to add a bunch of stuff""?</p>

<p>Azure ML does support multiple web service inputs and outputs. Adding a new model to the experiment requires you to re-deploy your web service.</p>
","4905427",0
98,38115484,2,38077884,2016-06-30 06:29:36,2,"<p>Currently, you will have to use R or python within Azure ML for confidence interval </p>
","4589073",1
99,38121201,2,38119062,2016-06-30 11:00:56,3,"<p>Stream Analytics has a functionality called the “<a href=""https://blogs.technet.microsoft.com/machinelearning/2015/12/10/azure-ml-now-available-as-a-function-in-azure-stream-analytics/"" rel=""nofollow"">Functions</a>”. You can call any web service you’ve published using AML from within Stream Analytics and apply it within your Stream Analytics query. Check this <a href=""https://azure.microsoft.com/en-us/documentation/articles/stream-analytics-machine-learning-integration-tutorial/"" rel=""nofollow"">link for a tutorial</a>.
Example workflow in your case would be like the following;</p>

<ul>
<li>Telemetry arrives and reaches Stream Analytics</li>
<li>Streaming Analytics (SA) calls the Machine Learning function to apply it on the data</li>
<li>SA redirects it to the output accordingly, here you can use the PowerBI to create a predictions dashboards.</li>
</ul>

<p>Another way would be using R, and here’s a good tutorial showing that <a href=""https://blogs.technet.microsoft.com/machinelearning/2015/12/10/azure-ml-now-available-as-a-function-in-azure-stream-analytics/"" rel=""nofollow"">https://blogs.technet.microsoft.com/machinelearning/2015/12/10/azure-ml-now-available-as-a-function-in-azure-stream-analytics/</a> . 
It is more work of course but can give you more control as you control the code.</p>
","694697",0
100,38290069,2,38189399,2016-07-10 08:30:23,0,"<p>After discussion with the product team from Microsoft. the issue was resolved.
the product team rolled out an update to the web service first, and only later to the ML-Studio, which fixed an issue with categorical attributes in ""Execute python script"".
the issue was in a earlier stage of the flow and has nothing to do with the python code above.</p>
","1021911",2
101,38290105,2,37462268,2016-07-10 08:36:14,1,"<p>after talking to Microsoft support, the problem was that the ""Execute Python Script"" module cannot return empty values.
this can be solved by adding a ""Clean Missing Data"" module before reading it from python:
<a href=""https://i.stack.imgur.com/BzCUZ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/BzCUZ.png"" alt=""enter image description here""></a></p>
","1021911",1
102,38583669,2,38563051,2016-07-26 07:31:51,1,"<p>To saving the images into Azure Blob Storage with R, you need to do two steps, which include getting the images from the R device output of <code>Execute R Script</code> and uploading the images to Blob Storage.</p>

<p>There are two ways to implement the steps above.</p>

<ol>
<li><p>You can publish the experiment as a webservice, then get the images with base64 encoding from the response of the webservice request and use Azure Blob Storage <a href=""https://msdn.microsoft.com/en-us/library/dd179451.aspx"" rel=""nofollow"">REST API</a> with R to upload the images. Please refer to the article <a href=""https://blogs.msdn.microsoft.com/benjguin/2014/10/24/how-to-retrieve-r-data-visualization-from-azure-machine-learning/"" rel=""nofollow"">How to retrieve R data visualization from Azure Machine Learning</a>.</p></li>
<li><p>You can directly add a module in C# to get &amp; upload the images from the output of <code>Execute R Script</code>. Please refer to the article <a href=""https://blogs.msdn.microsoft.com/data_insights_global_practice/2015/12/15/accessing-a-visual-generated-from-r-code-in-azureml/"" rel=""nofollow"">Accessing a Visual Generated from R Code in AzureML</a>.</p></li>
</ol>
","4989676",2
103,38927256,2,38927230,2016-08-12 22:26:09,14,"<p>Try this:</p>

<pre><code>dataframe_output = pd.DataFrame(tagged, columns=['Output', 'temp'])
</code></pre>
","2740380",0
104,38976398,2,38632533,2016-08-16 13:34:13,1,"<p>If you're hoping to use the Matchbox Recommender in AML, you're correct that you need to identify some user-movie pairs that <em>are</em> not present in the raw dataset, and add these in with a rating of zero. (I'll assume that you have already set all of the real user-movie pairs to have a rating of one, as you described above.)</p>

<p>I would recommend generating some random candidate pairs and confirming their absence from the training data in an Execute R (or Python) Script module. I don't know the names of your dataset's features, but here is some pseudocode in R to do that:</p>

<pre><code>library(dplyr)
df &lt;- maml.mapInputPort(1)  # input dataset of observed user-movie pairs
all_movies &lt;- unique(df[['movie']])
all_users &lt;- unique(df[['user']])
n &lt;- 30  # number of random pairs to start with

negative_observations &lt;- data.frame(movie = sample(all_movies, n, replace=TRUE),
                                    user = sample(all_users, n, replace=TRUE),
                                    rating = rep(0, n))          
acceptable_negative_observations &lt;- anti_join(unique(negative_observations), df, by=c('movie', 'user'))
df &lt;- rbind(df, acceptable_negative_observations)
maml.mapOutputPort(""df"");
</code></pre>

<p>Alternatively, you could try a method like <a href=""https://en.wikipedia.org/wiki/Association_rule_learning"" rel=""nofollow"">association rule learning</a> which would not require you to add in the fake zero ratings. Martin Machac has posted a <a href=""https://gallery.cortanaintelligence.com/Experiment/Frequently-bought-together-market-basket-analyses-using-ARULES-1"" rel=""nofollow"">nice example</a> of how to do this in R/AML in the Cortana Intelligence Gallery.</p>
","6722050",1
105,38979354,2,38978361,2016-08-16 15:52:25,4,"<p>As per serhiyb's answer, the win was to assign it to another variable:</p>

<p><code>Select cast(""field"" as float) as 'someAlias' FROM ""Table""</code></p>
","2883603",0
106,39073079,2,39016846,2016-08-22 07:00:06,0,"<ol>
<li>Check the Q/A s that you using is not having missing values. If there's any missing values follow data preprocessing techniques to fill those. </li>
<li>What kind of answers do you have as inputs? (yes/no, numeric values, different textual answers, etc...) In  my opinion numerical values and yes/no inputs makes your model more accurate.</li>
<li>Try different regression algorithms (<a href=""https://azure.microsoft.com/en-us/documentation/articles/machine-learning-algorithm-cheat-sheet/"" rel=""nofollow"">https://azure.microsoft.com/en-us/documentation/articles/machine-learning-algorithm-cheat-sheet/</a>) and check their accuracy.</li>
</ol>
","5383733",2
107,39208183,2,31507547,2016-08-29 14:00:54,2,"<p>It looks like you've chosen an unfortunate example: the custom scripts in the Retail Forecasting web service explicitly drop all but the first ID pair. To see this, try loading the ""Retail Forecasting: Step 6A of 6"" experiment and check out the code in the ""Create a complete time series. Add future time stamps"" module. You will find the following:</p>

<pre><code>all.time &lt;- data.frame(ID1 = data$ID1[1], ID2 = data$ID2[1], time = all.time)
data &lt;- join(all.time, data, by = c(""ID1"", ""ID2"", ""time""), type = ""left"")
maml.mapOutputPort(""data"");
</code></pre>

<p>The left join statement will ignore any rows where data$ID1 != data$ID1[1]  and data$ID2 != data$ID2[1]. That is why you are losing everything but the first ID pair.</p>

<p>It appears batch prediction for multiple ID pairs in a single job was not a use case that the custom script authors envisioned for their web service. If you are proficient in R and particularly interested in this use case, you could modify the scripts in this experiment to support processing multiple time series concurrently. Otherwise, you might want to simply try another example experiment.</p>
","6722050",0
108,39289918,2,39104538,2016-09-02 10:13:59,1,"<p>One thing we can do is exporting the output plot to a pdf file and storing it to an Azure blob storage. for that you should create a blob storage in your azure storage. 
Then modify the script as follows.</p>

<pre><code>d &lt;- maml.mapInputPort(1)
library(GGally)
library(caTools)
pdf()
df &lt;- d
names(df) &lt;- gsub(""[- ]"",""x"",names(df))
d &lt;- ggpairs(df,  alpha=0.4)
b64ePDF &lt;- function(filename) {
                maxFileSizeInBytes &lt;- 5 * 1024 * 1024 # 5 MB
                return(base64encode(readBin(filename, ""raw"", n = maxFileSizeInBytes)))
}
d2 &lt;- data.frame(pdf = b64ePDF(""Rplots.pdf""))
maml.mapOutputPort(""d2"");
</code></pre>

<p>Set your azure blob storage destination. The plot will save as a pdf.
<a href=""https://i.stack.imgur.com/mTgNH.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/mTgNH.png"" alt=""enter image description here""></a></p>
","5383733",0
109,39327742,2,39327655,2016-09-05 09:38:07,2,"<p>Your JSON path in the SelectToken seems to be wrong. 
Try this:</p>

<pre><code>string valv = (string)o.SelectToken(""$.Results.output1.value.Values[0][1]"");
</code></pre>
","2273277",3
110,39656795,2,36110109,2016-09-23 09:01:53,1,"<p>You need to zip &amp; load the package windows binaries in dataset &amp; import it to the R environment.</p>
<p>You can follow the instructions over <a href=""https://gallery.azure.ai/Experiment/Using-XGBoost-to-build-Graduation-Admit-Model-1"" rel=""nofollow noreferrer"">here</a>. I couldn't import it for the latest version, so I simply downloaded the xgboost version from this experiment &amp; loaded it to my saved datasets</p>
<p><a href=""https://learn.microsoft.com/en-us/archive/blogs/benjguin/how-to-upload-an-r-package-to-azure-machine-learning"" rel=""nofollow noreferrer"">This</a> is for any generic packages which are not preloaded in the environment</p>
<p>The following is a <a href=""https://web.archive.org/web/20161020215633/https://azure.microsoft.com/en-in/documentation/articles/machine-learning-r-csharp-web-service-examples/"" rel=""nofollow noreferrer"">list of experiments</a> to publish R models as a web service</p>
<p>Hope this helps!</p>
<p>Edit: You can also simply change the R version to Microsoft Open R (current version 3.2.2) and you can import xgboost as any common library</p>
","4357068",0
111,39671497,2,39652384,2016-09-24 01:03:09,1,"<p>There is no generic way to incrementally update a dataset. </p>

<p>However, depending on what you want to do with the data, there are different options for adding new data:</p>

<p>The Add Rows module effectively concatenates two datasets. So you could use the old, cached dataset on the left-hand input and add the new data on the right-hand input. That way you only have to read in the new data.
However, you would have to create some complex logic for figuring out which rows were new and old, and then maintain that outside Azure ML.</p>

<p>You could create an OData feed based on table storage, to enable filtering and get the new data that way. Just be aware that right now only public feeds are supported. And you would have to use Join or Add Rows to recombine the old and new data as described above. </p>

<p>You might also look into ways of using the <a href=""https://blog.maartenballiauw.be/post/2012/10/08/what-partitionkey-and-rowkey-are-for-in-windows-azure-table-storage.html"" rel=""nofollow"">table names</a>, partitions, and rowkeys to chunk your data. </p>

<p>If you are retraining a model and you want to update your feature statistics, the <a href=""https://msdn.microsoft.com/library/dn913056.aspx"" rel=""nofollow"">Learning with Counts</a> modules support incremental updates of count-based features. </p>
","5792309",0
112,39786037,2,39785621,2016-09-30 07:31:05,0,"<p>you need to use</p>

<pre><code>dataset2 &lt;- dataset[1:5,] 
</code></pre>

<p>if you want to select the first 5 rows.</p>
","4706171",0
113,39794560,2,39692921,2016-09-30 15:02:16,5,"<p>There isn't a straight forward way to compute the confidence interval from the results of the Boosted Decision Tree model in Azure ML. </p>

<p>Here are some alternate suggestions:</p>

<ol>
<li><p>Rebuild the model using the library(gbm) <a href=""http://artax.karlin.mff.cuni.cz/r-help/library/gbm/html/gbm.html"">http://artax.karlin.mff.cuni.cz/r-help/library/gbm/html/gbm.html</a> or the library(glm) <a href=""https://stat.ethz.ch/R-manual/R-devel/library/stats/html/glm.html"">https://stat.ethz.ch/R-manual/R-devel/library/stats/html/glm.html</a> </p></li>
<li><p>Then build the confidence interval using confint function: <a href=""https://stat.ethz.ch/R-manual/R-devel/library/stats/html/confint.html"">https://stat.ethz.ch/R-manual/R-devel/library/stats/html/confint.html</a></p></li>
<li><p>For a linear model, the confidence interval computation is simpler: <a href=""http://www.r-tutor.com/elementary-statistics/simple-linear-regression/confidence-interval-linear-regression"">http://www.r-tutor.com/elementary-statistics/simple-linear-regression/confidence-interval-linear-regression</a> </p></li>
</ol>
","4589073",0
114,40008547,2,40007515,2016-10-12 21:21:00,1,"<p>The dropdown list indicates the ""Destination"" datatype for the new DATASET file you are creating, not the source type.</p>

<p>I just uploaded a <code>.xlsx</code> file successfully into a <code>.CSV</code> file in AML.</p>
","4784394",6
115,40137429,2,40108999,2016-10-19 16:58:20,1,"<p>To convert a column of numbers into a semicolon delimited text, do this to your table:</p>

<ol>
<li>Convert your Energy column is type text.</li>
<li>Add <code>[Energy]</code> after the name of your table, which gives you a list of the numbers.</li>
<li>Use <a href=""https://msdn.microsoft.com/en-us/library/mt253358.aspx"" rel=""nofollow""><code>Text.Combine</code></a> to turn the list into a text value seperated by <code>;</code></li>
</ol>

<p>Here's a mashup that does that:</p>

<pre><code>let
    Source = Table.FromRows(Json.Document(Binary.Decompress(Binary.FromText(""NcjBCQAgDAPAXfKWYqKR7iLdfw1F8J63N9Q70bBCKQ5Ue6VbnEHl9L9xz2GniaoD"", BinaryEncoding.Base64), Compression.Deflate)), let _t = ((type text) meta [Serialized.Text = true]) in type table [Year = _t, Energy = _t]),
    #""Changed Type"" = Table.TransformColumnTypes(Source,{{""Year"", Int64.Type}, {""Energy"", type text}}),
    Custom1 = #""Changed Type""[Energy],
    Custom2 = Text.Combine(Custom1, "";"")
in
    Custom2
</code></pre>

<hr>

<p>Once you have a function, you'll invoke it like <code>YourFunction(Custum2, 5)</code></p>
","771768",2
116,40242073,2,40234432,2016-10-25 14:00:12,0,"<p>sounds like very similar to this sample:</p>

<p><a href=""https://gallery.cortanaintelligence.com/Experiment/df7c518dcba7407fb855377339d6589f"" rel=""nofollow"">https://gallery.cortanaintelligence.com/Experiment/df7c518dcba7407fb855377339d6589f</a></p>

<p>Unfortunately there is going to be a bit of R code involved. Yes you should be able to retrain the model with new data.</p>
","6191408",0
117,40405444,2,40302499,2016-11-03 15:27:31,1,"<p>We did not mean to convey a difference in meaning between null recommendations and empty recommendations. I will check why we may be sending two different types of results. Either way, don't treat those two cases as different cases. </p>

<p>If you are not getting results for user-to-item recommendations, most likely there was no data for that user when the build was created or the items that the user interacted with do not have enough co-occurrences with other items in the usage.</p>

<p>What to do when you get empty recommendations is up to you, you may decide to not show any recommendations, or back-fill with popular items you may want to promote.</p>

<p>Thanks!</p>

<p>Luis Cabrera
Program Manager - Recommendations API.</p>
","6483167",0
118,40807763,2,40798184,2016-11-25 15:02:05,1,"<p>I figured out the problem :</p>

<pre><code>{
...
ds &lt;- cast(zw, product + location ~ index + variable, value = ""value"")
data.set &lt;- rbind(data.set, ds)
}
# Select data.frame to be sent to the output Dataset port
maml.mapOutputPort(""data.set"");
</code></pre>

<p>I should be merging the rows and then output outside of the loop.</p>
","2699762",0
119,41451124,2,41451123,2017-01-03 19:40:53,0,"<p><a href=""http://blog.learningtree.com/how-to-build-a-predictive-model-using-azure-machine-learning/"" rel=""nofollow noreferrer"">Here</a> is a good sample to create your first model.
I should notice that I can't load data from url, as there is a forbidden error to load from url, and I don't know why! 
Anyhow, you can import data manually by copy the data from <a href=""http://blog.learningtree.com/wp-content/uploads/2015/01/breast-cancer-wisconsin.data_.arff_.txt"" rel=""nofollow noreferrer"">here</a>.</p>

<p>Also, you can find the created model which is published here: 
<a href=""https://gallery.cortanaintelligence.com/Experiment/Cancer-Model-1"" rel=""nofollow noreferrer"">https://gallery.cortanaintelligence.com/Experiment/Cancer-Model-1</a></p>

<p>About see the result of the training model, you can right click on the tick (highlighted by a red circle in the following picture) of Evaluation Model. Then, in the opened menu, go to ""Evaluation Result -> Visualization"".
<a href=""https://i.stack.imgur.com/vhJWE.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/vhJWE.jpg"" alt=""enter image description here""></a></p>

<p>After that you can see a window like the following (which shows ROC curve and some related result such as accuracy of the training model):</p>

<p><a href=""https://i.stack.imgur.com/FAuzU.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/FAuzU.png"" alt=""enter image description here""></a>
<a href=""https://i.stack.imgur.com/B39lI.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/B39lI.png"" alt=""enter image description here""></a></p>

<p>Besides, you can see <a href=""https://learn.microsoft.com/en-us/azure/machine-learning/machine-learning-azure-ml-customer-churn-scenario"" rel=""nofollow noreferrer"">this example</a> as an another sample.</p>
","3768871",0
120,41629358,2,41185998,2017-01-13 07:23:17,1,"<p>So, after a lot of deliberate attempts, the only thing which worked is imputation. As suggested in the question comments, the issue was with the missing data, and as soon as we handled the missing data case, Azure ML worked and predicted results for all the records.</p>
","2138524",0
121,41645796,2,41236871,2017-01-14 01:18:43,1,"<p>Models can be trained, scored, saved, and run in AzureML studio, but can't downloaded to your local machine. There's no way to do anything with a model outside of AzureML.</p>
","2267790",4
122,41686872,2,41686871,2017-01-16 23:50:22,1,"<p>Like this:</p>

<pre><code>def azureml_main(dataframe1 = None, dataframe2 = None):
  dataframe1[1] = dataframe1['text'].str.replace('&lt;[^&lt;]+?&gt;', ' ', case=False)
  return dataframe1,
</code></pre>

<p>Remember to precede the <code>Execute Python Script</code> step with <code>Clean Missing Data</code> step and change the action to remove the entire row (if appropriate). This is important because the <code>Execute Python Script</code> step cannot return an empty <code>dataframe</code>. Only you know your data, in this case.</p>

<p>Let me also point out that the <code>Preprocessing Text</code> step allows you to apply a Regular Expression. That is another alternative that might be right for your situation.</p>
","265706",0
123,41701996,2,41603082,2017-01-17 16:17:25,7,"<p>Answering my own question here:</p>

<p>I have found my LUIS.ai programmatic API key. It is found by:
LUIS.ai dashboard -> username (upper-right) -> settings in dropdown -> Subscription Keys tab -> Programmatic API Key</p>

<p>It was not immediately obvious since it's found nowhere else: not alongside any of the other key listings in cognitive services or the LUIS.</p>
","1549998",1
124,41842341,2,41614428,2017-01-25 02:16:45,2,"<p>From one of your comments above, it sounds like the version of the <code>caret</code> package you've used requires an R version >3.1.2. I recommend using an older version of the package: the <code>caret</code> binary from <a href=""http://cran.cnr.berkeley.edu/bin/windows/contrib/3.1/"" rel=""nofollow noreferrer"">this 3.1 archive</a> (6.0-68) worked for me. I used these statements to load the package:</p>

<pre><code>install.packages(""src/caret_6.0-68.zip"", lib=""."", repos= NULL, verbose=TRUE)
library(""caret"", lib.loc=""."", verbose=TRUE)
</code></pre>
","6722050",0
125,41852811,2,41743792,2017-01-25 13:26:55,0,"<p>To my knowledge, there's no built-in module to do this succinctly (to my knowledge). If you prefer to use built-ins, you could:</p>

<ol>
<li>Use a Split Dataset module to split the entries based on credit
score</li>
<li>Divide the credit score in large-credit-score rows by 10 using
Apply Math Operation</li>
<li>Concatenate the two datasets row-wise with an Add Rows module</li>
</ol>
","6722050",1
126,41917028,2,41916570,2017-01-29 02:21:35,3,"<p>put the cursor on the name text when an experiment is open, and edit away.</p>
","6191408",0
127,41949944,2,41855344,2017-01-31 04:59:31,7,"<p>Azure ML only accepts the comma <code>,</code> separated CSV. Do a little work around.
Open your data file using a text editor. (Notepad will do the trick). Find and replace all semicolons with 'tab' (Make it a TSV) and the commas in data values may not occur a problem then. Make sure to define that the input is a TSV; not a CSV. </p>
","5383733",0
128,41981268,2,41032108,2017-02-01 13:41:08,3,"<p>As you noticed, the active user doesn't have permissions to write to the <code>site-packages</code> directory in Azure Machine Learning Studio notebooks. You could try installing the package to another directory where you do have write permissions (like the default working directory) and importing from there, but I recommend the following lower-hassle option.</p>

<p><a href=""https://notebooks.azure.com"" rel=""nofollow noreferrer"">Azure Notebooks</a> is a separate Jupyter Notebook service that will allow you to install tensorflow, theano, and keras. Like the notebooks in AML Studio, these notebooks will persist in your account. The primary downside is that if you want to access your workspace through e.g. the Python <code>azureml</code> package, you'll need to <a href=""https://github.com/Azure/Azure-MachineLearning-ClientLibrary-Python"" rel=""nofollow noreferrer"">provide your workspace id/authorization token</a> to set up the connection. (In Azure ML Studio, those values are loaded automatically from the current workspace.) Otherwise I believe Azure Notebooks can do everything you are used to doing inside AML Studio only.</p>
","6722050",0
129,41994181,2,41903982,2017-02-02 04:24:46,0,"<p>Make is a classic web service and see the JSON output getting from it. If it's providing all data you need.. go for it.</p>
","5383733",0
130,42018311,2,42017727,2017-02-03 06:47:19,2,"<p>Here is an example of uploading a .rda file for scoring:
<a href=""https://gallery.cortanaintelligence.com/Experiment/Womens-Health-Risk-Assessment-using-the-XGBoost-classification-algorithm-1"" rel=""nofollow noreferrer"">https://gallery.cortanaintelligence.com/Experiment/Womens-Health-Risk-Assessment-using-the-XGBoost-classification-algorithm-1</a></p>
","6191408",1
131,42184341,2,42145256,2017-02-12 04:53:12,2,"<p>I guess you are using Jupyter notebook on AzureML to do the experiment. In that case the <code>'mini.csv00'</code> should be in your experiments with <code>workspace_id='toto'</code>. </p>

<p>Create a new experiment in your workspace named toto and put the dataset into it first. Then open the dataset using 'open in a new Notebook'. </p>

<p><a href=""https://i.stack.imgur.com/ztIw0.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ztIw0.png"" alt=""enter image description here""></a> </p>
","5383733",2
132,42462403,2,42458982,2017-02-25 22:36:27,1,"<p>Basically, for the false/true positives and false/true negatives :
You have detected almost all the CANDIDATE samples in your dataset, 3420 of them were correctly predicted as TRUE and 31 of them were predicted as FALSE. This information is captured in the Recall ratio : 3420/(3420+31) = 99.1%. It is very high, so very good. </p>

<p>However, you have predicted <strong>too many</strong> CANDIDATE. Indeed, in all the TRUE values predicted by the model, 3420 were actually TRUE and 2002 were actually FALSE. This makes the Precision ratio bad : 3420/(3420+2002)=63.1%. Which is not that good. </p>

<p>F1 is a combinaison between Precision and Recall, it summarizes them into one value, some kind of weighted average. The formula is 2*(P*R)/(P+R). So if one of Precision or Recall is bad : the F1score will capture it. </p>

<p>You can see that you have a total of 5999 examples in your data set. Out of those, 3451 are really TRUE and 2548 are really FALSE. So you have 57% of your data that is TRUE. If you make a really stupid classifier that classifies everything as TRUE whatever the features are, then you will get 57% accuracy. Given that, 66.1% accuracy is not really good. 
If you look at the second column of that table, you only predict 577 FALSE out of the 5999 samples. Your classifier is heavily biased towards TRUE predictions. </p>

<p>For the AUC, it stands for Area Under the Curve. You can read <a href=""http://fastml.com/what-you-wanted-to-know-about-auc/"" rel=""nofollow noreferrer"">more detailed info about it here</a>. To summarize : when you predic a value, you don't really get True or False directly. You get a real number between 0 (False) and 1 (True). The way to classify a predicted value, say 0.2, is to use a Threshold. The threshold is by default set to 0.5. So if you predict 0.2, your model will predict to classify it as a False because 0.2&lt;0.5. But you could make that treshold move between 0 and 1. If the classifier is really good, if it discriminates really well the Falses and Trues predictions, then the AUC will be close to 1. If it's really bad, it will be close to 0.5. Refer to the link if you need more information. </p>
","7137636",1
133,42502264,2,42501472,2017-02-28 07:05:34,2,"<p>The answer is going to depend on the method you invoke on <code>Experiment</code>. Presumably you are going to call train_and_evaluate, e.g., if <code>TF_CONFIG</code>'s task type is set to ""master"" (cf <a href=""https://github.com/tensorflow/tensorflow/blob/r1.0/tensorflow/contrib/learn/python/learn/learn_runner.py#L135"" rel=""nofollow noreferrer"">this code</a>).</p>

<p>In that case, you'll want to set <code>min_eval_frequency</code> to <code>0</code> or <code>None</code> (cf <a href=""https://github.com/tensorflow/tensorflow/blob/r1.0/tensorflow/contrib/learn/python/learn/experiment.py#L453"" rel=""nofollow noreferrer"">this code</a>)</p>
","1399222",2
134,42540689,2,41303697,2017-03-01 19:48:19,0,"<p>Confirmed that this is not possible, AML only allows use of AML-created gateways.</p>
","1854159",0
135,42588631,2,42324035,2017-03-03 21:21:08,2,"<p>You can access the response using the amlresult.[Scored Probabilities] notation, where amlresult is an alias for the return value from your AzureML call.</p>
","7655788",0
136,42686789,2,42685041,2017-03-09 04:32:14,2,"<p>google-cloud-ml-engine is now GA. Specifically training and batch prediction are GA. Online prediction is still in beta.</p>

<p>You are correct that if you update gcloud to the latest components the new commands are</p>

<pre><code>gcloud ml-engine
</code></pre>
","4392784",0
137,42765641,2,42761075,2017-03-13 14:06:49,17,"<p>Python's <code>open</code> function cannot read files from GCS. You will need to use a library capable of doing so. TensorFlow includes one such library:</p>

<pre><code>import tensorflow as tf
from tensorflow.python.lib.io import file_io

with file_io.FileIO(args.wavenet_params, 'r') as f:
  wavenet_params = json.load(f)
</code></pre>
","1399222",4
138,42828217,2,42827797,2017-03-16 07:53:48,1,"<p>Correct.</p>

<ul>
<li><strong>probabilities</strong>: are the probabilities of &lt; $50K vs >=$50K.</li>
<li><strong>classes</strong>: the predicted class (0, i.e. &lt; $50K)</li>
<li><a href=""https://en.wikipedia.org/wiki/Logit"" rel=""nofollow noreferrer""><strong>logits</strong></a>: ln(p/(1-p)) = ln(0.00371/(1-.00371)) = -5.593</li>
<li><strong>logistic</strong>: 1/(1+exp(-logit)) = 1/(1+exp(5.593)) = 0.0037</li>
</ul>
","1399222",2
139,42840536,2,42821093,2017-03-16 17:08:09,0,"<p>I was able to reproduce this issue on Cloud ML, and it seems to be an issue with the version of file_io in Tensorflow 0.12.1, and goes away if Tensorflow 1.0 is installed.</p>

<p>If you can, upgrade to the 1.0 build of TF.</p>

<p>If you need a 0.12 version, the Cloud ML ""0.12"" runtime uses the official 0.12.1 build of TF, but you can upload your own version to install if you like.  I did not track down exactly when the issue was fixed but a <a href=""http://ci.tensorflow.org/view/Nightly/"" rel=""nofollow noreferrer"">Nightly</a> Tensorflow build from Feb 2nd seemed to work.</p>
","5441818",0
140,42911005,2,42865818,2017-03-20 18:12:42,4,"<p>TensorFlow serving expects you to point to a base directory which includes a version subdirectory. In your case, ""Servo"" is the directory you want to point to and ""1489706933289"" is the directory for the version.</p>

<p>The following should work:</p>

<pre><code>bazel-bin/tensorflow_serving/model_servers/tensorflow_model_server \
  --port=9000 \
  --model_base_path=/serving/tf_models/extrato/output/Servo
</code></pre>

<p>(Note the addition of ""Servo"" to the base path, and the absence of ""1489706933289"")</p>

<p>Note that in CloudML you directly deploy a version, so you'll want to point to a subdirectory on GCS akin to <code>gs://my_bucket/.../tf_models/extrato/output/Servo/1489706933289</code></p>
","1399222",0
141,42937050,2,42651900,2017-03-21 19:59:41,4,"<p>I recommend the following:</p>

<ul>
<li>Get a tenant ID, client ID, and client secret for your ADLS using the tutorial <a href=""https://learn.microsoft.com/en-us/azure/data-lake-store/data-lake-store-authenticate-using-active-directory#step-2-get-client-id-client-secret-and-tenant-id"" rel=""nofollow noreferrer"">here</a>.</li>
<li>Install the <a href=""https://github.com/Azure/azure-data-lake-store-python"" rel=""nofollow noreferrer""><code>azure-datalake-store</code></a> Python package on AML Studio by attaching it as a Script Bundle to an Execute Python Script module.</li>
<li>In the Execute Python Script module, import the <code>azure-datalake-store</code> package and connect to the ADLS with your tenant ID, client ID, and client secret.</li>
<li>Download the data you need from ADLS and convert it into a dataframe within the Python Script module; return that dataframe to make the data available in the rest of AML Studio.</li>
</ul>
","6722050",1
142,42940356,2,42924077,2017-03-21 23:55:31,6,"<p>Python templates are available as of April 2017 (see <a href=""https://cloud.google.com/dataflow/docs/templates/overview"" rel=""nofollow noreferrer"">documentation</a>). The way to operate them is the following:</p>

<ul>
<li>Define UserOptions subclassed from PipelineOptions.</li>
<li>Use the add_value_provider_argument API to add specific arguments to be parameterized.</li>
<li>Regular non-parameterizable options will continue to be defined using argparse's add_argument.</li>
</ul>

<pre>class UserOptions(PipelineOptions):
     @classmethod
     def _add_argparse_args(cls, parser):
         parser.add_value_provider_argument('--value_provider_arg', default='some_value')
         parser.add_argument('--non_value_provider_arg', default='some_other_value')</pre>

<p>Note that Python doesn't have a TemplatingDataflowPipelineRunner, and neither does Java 2.X (unlike what happened in Java 1.X).</p>
","7748447",2
143,42954095,2,42935110,2017-03-22 14:10:43,3,"<p>The gcloud tool has been updated so that the <code>ml-engine</code> set of commands is the officially supported way to interact with the service. The <code>gcloud beta ml</code> codepath is out-of-date. The following should work:</p>

<pre><code>gcloud beta ml-engine predict --model my_model --json-instances my_instance.json
</code></pre>

<p>(Note the presence of the keyword <code>beta</code>).</p>

<p>The reason for the 400 error when using <code>gcloud ml-engine predict</code> (note the absence of the keyword <code>beta</code>) is because your model is not a <a href=""https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/saved_model/README.md"" rel=""nofollow noreferrer"">SavedModel</a>. In that set of commands (non-beta), we only support SavedModel.</p>

<p>You can continue to use your existing models via the old endpoint (<code>gcloud beta ...</code>), but we expect to deprecate that functionality soon, so we recommend exporting new models as SavedModels and deploying them via <code>gcloud ml-engine</code>.</p>
","1399222",7
144,42958357,2,42938728,2017-03-22 17:10:29,3,"<p>It looks like the error is caused by some code that feeds the <code>""RESHAPE_PREDICT:0""</code> tensor (i.e. the output of the <code>tf.reshape()</code> op, <code>image</code>) rather than the <code>""PREDICT_PLACEHOLDER:0""</code> tensor (i.e. the input to the <code>tf.image.resize_images()</code> op, <code>input_features</code>).</p>

<p>Without the whole source to your trained model, it's hard to say exactly what changes are necessary, but it might be as simple as changing the definition of <code>inputs</code> to:</p>

<pre><code>inputs = {'image': input_features}
</code></pre>

<p>...so that the prediction service knows to feed values to that placeholder, rather than the fixed-shape output of <code>tf.reshape()</code>.</p>
","3574081",2
145,42980688,2,42766263,2017-03-23 15:38:46,1,"<p>The <em>new</em> web service does not store any information about the experiment or workspace that was deployed (not all <em>new</em> web services are deployed from an experiment).</p>

<p>Here are the options available to track the relationship between the experiment and a <em>new</em> web service.</p>

<h2>last deployment</h2>

<p>However, the experiment keeps track of the <strong>last</strong> <em>new</em> web service that was deployed from the experiment. each deployment to a <em>new</em> web service overwrites this value.</p>

<p>The value is stored in the experiment graph. One way to get the graph is to use the powershell module <a href=""http://aka.ms/amlps"" rel=""nofollow noreferrer"">amlps</a></p>

<p><code>Export-AmlExperimentGraph -ExperimentId &lt;Experiment Id&gt; -OutputFile e.json</code></p>

<p><strong>e.json</strong></p>

<pre><code>{
""ExperimentId"":""&lt;Experiment Id&gt;"",
// . . .
""WebService"":{
// . . .
""ArmWebServiceId"":""&lt;Arm Id&gt;""
},
// . . . 
}
</code></pre>

<h2>azure resource tags</h2>

<p>The tags feature for Azure resources is supported by the <em>new</em> web services. Setting a <code>tag</code> on the web service programmatically, with powershell or via the azure portal UX can be used to store a reference to the experiment on the <em>new</em> web service.</p>
","599231",0
146,43002175,2,43001719,2017-03-24 14:32:08,7,"<p>In your signature_def_map, use the key 'serving_default', which is defined in <a href=""https://www.tensorflow.org/api_docs/python/tf/saved_model/signature_constants"" rel=""noreferrer""><code>signature_constants</code></a> as <code>DEFAULT_SERVING_SIGNATURE_DEF_KEY</code>:</p>

<pre><code>b.add_meta_graph_and_variables(sess,
                               [tf.saved_model.tag_constants.SERVING],
                               signature_def_map={'serving_default': signature})
</code></pre>
","1399222",0
147,43058090,2,42932714,2017-03-27 23:26:14,4,"<p>tf.train.Saver() only produces a checkpoint.</p>

<p>Cloud ML Engine uses a SavedModel, produced from these APIs: <a href=""https://www.tensorflow.org/versions/master/api_docs/python/tf/saved_model?hl=bn"" rel=""nofollow noreferrer"">https://www.tensorflow.org/versions/master/api_docs/python/tf/saved_model?hl=bn</a></p>

<p>A saved model is a checkpoint + a serialized protobuf containing one or more graph definitions + a set of signatures declaring the inputs and outputs of the graph/model + additional asset files if applicable, so that all of these can be used at serving time.</p>

<p>I suggest looking at couple of examples:</p>

<ol>
<li><p>The census sample - <a href=""https://github.com/GoogleCloudPlatform/cloudml-samples/blob/master/census/tensorflowcore/trainer/task.py#L334"" rel=""nofollow noreferrer"">https://github.com/GoogleCloudPlatform/cloudml-samples/blob/master/census/tensorflowcore/trainer/task.py#L334</a></p></li>
<li><p>And my own sample/library code - <a href=""https://github.com/TensorLab/tensorfx/blob/master/src/training/_hooks.py#L208"" rel=""nofollow noreferrer"">https://github.com/TensorLab/tensorfx/blob/master/src/training/_hooks.py#L208</a> that calls into <a href=""https://github.com/TensorLab/tensorfx/blob/master/src/prediction/_model.py#L66"" rel=""nofollow noreferrer"">https://github.com/TensorLab/tensorfx/blob/master/src/prediction/_model.py#L66</a> to demonstrate how to use a checkpoint, load it into a session and then produce a savedmodel.</p></li>
</ol>

<p>Hope these pointers help adapt your existing code to produce a model to now produce a SavedModel.</p>

<p>I think you also asked another similar question to convert a previously exported model, and I'll link to it here for completeness for anyone else: <a href=""https://stackoverflow.com/questions/43001719/deploy-retrained-inception-savedmodel-to-google-cloud-ml-engine"">Deploy retrained inception SavedModel to google cloud ml engine</a></p>
","40999",0
148,43080708,2,43079274,2017-03-28 21:59:11,0,"<p>Yes. I've submitted a <a href=""https://github.com/GoogleCloudPlatform/cloudml-samples/pull/30"" rel=""nofollow noreferrer"">PR</a> (in the future, never hesitate to do so yourself)</p>
","1399222",0
149,43118788,2,43116934,2017-03-30 13:06:18,1,"<p>I have found a workaround: in <code>preprocess.py</code> I have replaced the import of the <code>util</code> package with the definition of <code>get_cloud_project()</code> that is contained in <code>util.py</code>.</p>

<p>I don't know if the issue was caused by the local package employed on a dataflow. I don't think this is the case because <code>get_cloud_project()</code> is not called inside the pipeline definition, but this is the first time I use dataflow.</p>

<p>If someone else knows if it is possibile to make the code work without modifying it, please tell me!</p>
","7310903",1
150,43126284,2,42844593,2017-03-30 18:58:43,0,"<p>I turned out that the problem was caused by a few NaN values in the second dataframe.
Adding <code>dataframe2 = dataframe2.dropna()</code> to the top of the script solved the problem.</p>
","4594063",0
151,43127500,2,43122520,2017-03-30 20:12:53,2,"<p>Not sure if this will help but in <a href=""https://github.com/GoogleCloudPlatform/cloudml-samples/blob/master/flowers/sample.sh"" rel=""nofollow noreferrer"">Google's sample code for flowers</a>, the error is avoided by appending the date and time to the job id as shown on line 22, e.g.,</p>

<pre><code>declare -r JOB_ID=""flowers_${USER}_$(date +%Y%m%d_%H%M%S)""
</code></pre>
","7262545",1
152,43151611,2,43150772,2017-04-01 00:55:36,2,"<p>This has been fixed and will be pushed in the next couple of days.</p>
","1399222",0
153,43210378,2,43204424,2017-04-04 14:39:22,1,"<p>By and large, your distributed TensorFlow program will be exactly that -- distributed TensorFlow, with minimal -- or even no -- cloud-specific changes. The best resource for distributed TensorFlow is <a href=""https://www.tensorflow.org/deploy/distributed"" rel=""nofollow noreferrer"">this tutorial</a> on tensorflow.org. The tutorial walks you through the low-level way of doing things.</p>

<p>There is also a higher-level API, currently in contrib (so API may change and will move out of contrib in a future version), that simplifies the amount of boilerplate code you have to write for distributed training. The official tutorial is <a href=""https://www.tensorflow.org/get_started/tflearn"" rel=""nofollow noreferrer"">here</a>.</p>

<p>Once you've understood the general TensorFlow bits (whether high-level or low-level APIs), there are some specific elements that must be present in your code to get it to run on CloudML Engine. In the case of the low-level TensorFlow APIs, you'll need to parse the TF_CONFIG environment variable to setup your ClusterSpec. This is exemplified in <a href=""https://github.com/GoogleCloudPlatform/cloudml-samples/tree/master/census/tensorflowcore"" rel=""nofollow noreferrer"">this example</a> (see specifically <a href=""https://github.com/GoogleCloudPlatform/cloudml-samples/blob/01ed847eea06300d278ffcf1214021b487d3d463/census/tensorflowcore/trainer/task.py#L383"" rel=""nofollow noreferrer"">this</a> block of code).</p>

<p>One advantage of the higher-level APIs, is that all of that parsing is already taken care of for you. Your code should just generally work. See <a href=""https://github.com/GoogleCloudPlatform/cloudml-samples/tree/master/census/estimator"" rel=""nofollow noreferrer"">this example</a>. The important piece is that you will need to use <a href=""https://github.com/agrawalnishant/tensorflow-1/blob/master/tensorflow/contrib/learn/python/learn/learn_runner.py"" rel=""nofollow noreferrer"">learn_runner.run()</a> (see <a href=""https://github.com/GoogleCloudPlatform/cloudml-samples/blob/b78bfc5e81dd33c4c95cd9f768ee10960f666764/census/estimator/trainer/task.py#L191"" rel=""nofollow noreferrer"">this</a> line), which will work locally and in the cloud to train your model.</p>

<p>Of course, there are other frameworks as well, e.g., <a href=""https://github.com/TensorLab/tensorfx/blob/master/README.md"" rel=""nofollow noreferrer"">TensorFX</a>.</p>

<p>After you've structured your code appropriately, then you simply select an appropriate <a href=""https://cloud.google.com/ml-engine/reference/rest/v1/projects.jobs#scaletier"" rel=""nofollow noreferrer"">scale tier</a> that has multiple machines when launching your training job. (See <a href=""https://stackoverflow.com/a/43209918/1399222"">Chuck Finley's</a> answer for an example)</p>

<p>Hope it helps!</p>
","1399222",6
154,43253000,2,43164560,2017-04-06 10:49:28,1,"<p>Maybe you could show where and how you declare the $OUTPUT_PATH? </p>

<p>Also the model directory, might be the directory within the $OUTPUT_PATH where you could find the model of that specific Job.</p>
","7813066",1
155,43259627,2,43249220,2017-04-06 15:34:15,0,"<p>One thing you can do is combining two data-sets together and selecting the appropriate fields using the R script. That would be an easy workaround.   </p>
","5383733",2
156,43280865,2,43278593,2017-04-07 14:37:48,1,"<p>You are almost correct on most points. To make sure we are talking in the same terms, a little bit of background:</p>

<p>A linear regression uses data on some outcome variable <code>y</code> and independent variables <code>x1, x2, ..</code> and tries to find the linear combination of <code>x1, x2, ..</code> that best predicts <code>y</code>. Once this ""best linear combination"" is established, you can assess the quality of the fit (i.e. quality of the model) in multiple ways. The six points you mention are all key metrics for the quality of a regression equation. </p>

<p>Running a regression gives you multiple ""ingredients"". For example, every observation will get a <em>predicted value</em> for the outcome variable. The difference between the observed value of <code>y</code> and the predicted value is called the residual or error. Residuals can be negative (if the <code>y</code> is overestimated) and positive (if <code>y</code> is underestimated). The closer the residuals are to zero, the better. But, what is ""close""? The metrics you present are supposed to give an insight in this.</p>

<ul>
<li><strong>Mean absolute error</strong>: takes the <em>absolute value</em>  of the residuals and takes the mean of that. </li>
<li><strong>Root Mean Square Error</strong>: is the standard deviation of your residuals. This will help you see, how large the <em>spread</em>  is of your residuals. The residuals are squared and therefore, high residuals will count in more than small residuals. A low RMSE is good. </li>
<li><p><strong>Relative Absolute Error</strong>: The absolute error as a fraction of the real value of the outcome variable <code>y</code>. In your case, the predictions are on average 75% higher/lower than the actual value of <code>y</code>.</p></li>
<li><p><strong>Relative Squared Error</strong>: The squared error (<code>residual^2</code>) as a fraction of the real value. </p></li>
<li><strong>Coefficient of Determination</strong>: Almost correct. This ranges between 0 and 1 and can be interpreted as the explanatory power of the independent variables in explaining <code>y</code>. In fact, in your case the independent variables can model 38,15% of the variation in <code>y</code>.  Also, if you have only one independent variable, this coefficient is equal to the squared correlation coefficient. </li>
</ul>

<p>Root Mean Squared Error and Coefficient of Determination are the most important metrics in nearly all situations. To be honest, I've never really seen the other metrics being reported.</p>
","6256482",1
157,43421161,2,43406111,2017-04-15 01:16:15,0,"<p>Your assumption is a reasonable rule of thumb. That said, Parag points to a paper that describes a model that can leverage GPUs in the parameter server, so it's not always the case that parameter servers are not able to leverage GPUs.</p>

<p>In general, you may want to try both for a short time and see if throughput improves.</p>

<p>If you have any question as to what ops are actually being assigned to your parameter server, you can <a href=""https://www.tensorflow.org/tutorials/using_gpu#logging_device_placement"" rel=""nofollow noreferrer"">log the device placement</a>. If it looks like ops are on the parameter server that can benefit from the GPU (and supposing they really should be there), then you can go ahead and try a GPU in the parameter server.</p>
","1399222",0
158,43452063,2,43323788,2017-04-17 13:03:32,0,"<p>So the log messages indicate that GPUs are available. To check whether GPUs are actually being used you can turn on <a href=""https://www.tensorflow.org/tutorials/using_gpu#logging_device_placement"" rel=""nofollow noreferrer"">logging of device placement</a> to see which OPs are assigned to GPUs.</p>

<p>The Cloud Compute console won't show any utilization metrics related to Cloud ML Engine. If you look at the Cloud Console UI for your jobs you will see memory and CPU graphs but not GPU graphs.</p>
","4392784",2
159,43453388,2,43453078,2017-04-17 14:25:23,3,"<p><code>scipy.misc.imresize</code> requires PIL to be installed, which you probably have installed locally (since it works).</p>

<p>To ensure your code runs correctly in the cloud, you need to ensure <code>pillow</code> is installed. If you have created your own <code>setup.py</code> include <code>pillow</code> in the list of requirements. If you have to create your own, create a <code>setup.py</code> like this:</p>

<pre><code>from setuptools import find_packages
from setuptools import setup

REQUIRED_PACKAGES = ['pillow']

setup(
    name='trainer',
    version='0.1',
    install_requires=REQUIRED_PACKAGES,
    packages=find_packages(),
    include_package_data=True,
    description='My trainer application package.'
)
</code></pre>

<p>(<a href=""https://cloud.google.com/ml-engine/docs/how-tos/packaging-trainer#to_include_additional_pypi_dependencies"" rel=""nofollow noreferrer"">source</a>, with one important modification, the <code>packages</code> attribute)</p>

<p>See the official CloudML Engine <a href=""https://cloud.google.com/ml-engine/docs/how-tos/packaging-trainer#to_include_additional_pypi_dependencies"" rel=""nofollow noreferrer"">documentation</a> for more information about recommended directory layout and packaging instructions.</p>
","1399222",3
160,43486561,2,43451568,2017-04-19 04:43:03,1,"<p>Yes. For this you should train separate models for each column you going to predict values. If the predicted value has an effect with the intensities of other columns use them as inputs for building the predictive model. </p>
","5383733",0
161,43549512,2,43548780,2017-04-21 18:17:45,0,"<p>The code that you submit is responsible for exporting the model. You can find an example on <a href=""https://stackoverflow.com/q/43001719"">this post</a>; please reference <a href=""https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/saved_model/README.md"" rel=""nofollow noreferrer"">SavedModel docs</a>.</p>

<p>The inputs and outputs of your model will of course be specific to your model, but for convenience (and slightly modified), here's the code from that post:</p>

<pre><code>### BUILD THE PREDICTION GRAPH
in_image = tf.placeholder(tf.uint8, shape=(None,))
out_classes = build_prediction_graph(in_image)

### DEFINE SAVED MODEL SIGNATURE
inputs = {'image_bytes': tf.saved_model.utils.build_tensor_info(in_image)}
outputs = {'prediction': tf.saved_model.utils.build_tensor_info(out_classes)}
signature = tf.saved_model.signature_def_utils.build_signature_def(
    inputs=inputs,
    outputs=outputs,
    method_name='tensorflow/serving/predict'
)

### SAVE OUT THE MODEL
b = saved_model_builder.SavedModelBuilder('new_export_dir')
b.add_meta_graph_and_variables(sess,
                               [tf.saved_model.tag_constants.SERVING],
                               signature_def_map={'serving_default': signature})
b.save()
</code></pre>
","1399222",1
162,43684055,2,43651296,2017-04-28 15:40:31,0,"<p>Problem is resolved. All I had to do is use tensorflow 1.1.0 instead default 1.0.1</p>
","7929246",2
163,43691982,2,43667018,2017-04-29 05:06:00,2,"<p>The format of the model you deployed to the CloudML Engine service is a <a href=""https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/saved_model/README.md"" rel=""nofollow noreferrer""><code>SavedModel</code></a>. Loading a <code>SavedModel</code> in Python is fairly simple using the <a href=""https://www.tensorflow.org/api_docs/python/tf/saved_model/loader"" rel=""nofollow noreferrer""><code>loader</code></a> module:</p>

<pre><code>import tensorflow as tf

with tf.Session(graph=tf.Graph()) as sess:
   tf.saved_model.loader.load(
       sess,
       [tf.saved_model.tag_constants.SERVING],
       path_to_model)
</code></pre>

<p>To perform inference, you're code is almost correct; you will need to make sure that you are feeding a batch to <code>session.run</code>, so just wrap <code>image_data</code> in a list:</p>

<pre><code># Feed the image_data as input to the graph and get first prediction
softmax_tensor = sess.graph.get_tensor_by_name('conv1/weights:0')

predictions = sess.run(softmax_tensor, \
                       {'DecodeJpeg/contents:0': [image_data]})

# Sort to show labels of first prediction in order of confidence
top_k = predictions[0].argsort()[-len(predictions[0]):][::-1]

for node_id in top_k:
    human_string = label_lines[node_id]
    score = predictions[0][node_id]
    print('%s (score = %.5f)' % (human_string, score))
</code></pre>

<p>(Note that, depending on your graph, wrapping your input_data in a list may increase the rank of your predictions tensor, and you would need to adjust the code accordingly).</p>
","1399222",2
164,41011154,2,41010551,2016-12-07 06:53:22,0,"<p>you will need to train your model with both active and inactive members.  I would split your dataset so there are examples of active and inactive members in both your training and your test set.  </p>

<p>Let's discuss why we split the data.  Remember that with supervised learning, you need data with labeled examples.  For example, let’s say I want to predict how much a house will cost based on its square footage and zip code.  To train my model, I need a dataset of existing houses with their square footage, zip codes, and prices, like this:</p>

<p>SquareFootage ZipCode Price <br/>
2000          48075   200,000 <br/>
3000 48075 300,000 <br/>
4000 48075 400,000 <br/>
5000 48075 500,000 <br/></p>

<p>In this example, square footage and zip code are my features (things that influence the thing you want to predict) and price is my label (the thing that you want to predict).  I could train a model on some data like the above, and then use the trained model to predict prices, given only a square footage and zip code.  </p>

<p>So, the reason I split the data is to provide most of the data to train the model (it will process the data to figure out correlations between the features and labels in the “train model” module), but we want to hold back some of that labeled data to test the model that we built.  Then, we can compare the price value that the trained model generates against the actual labeled price value in the test dataset (in the “score model” module) to see how well the model is performing.  (We can’t use the same data for both...the model is built using the training data, so it will perform pretty accurately with that; we hold back unused data to test.)  </p>

<p>So, for your example, I would try a random split so there are examples of both active and inactive members (that is your label - inactive or active) and you will also need to provide relevant features that influence activity.  </p>
","1535478",5
165,40933356,2,40907303,2016-12-02 13:29:15,6,"<p>So I got it working! With the help from this link: <a href=""https://stackoverflow.com/questions/40932669/deserializing-json-using-c-sharp-to-return-items"">Deserializing JSON using C# to return items</a>, which i posted to simplify where my issue was occurring now.</p>

<p>So what this code is doing is the following:</p>

<ol>
<li>Creating an Index in Azure called textanalytics.</li>
<li>Creating a JSON string of the text provided.</li>
<li>Retrieving the KeyPhrases and adding these to the Index created in point 1 above.</li>
</ol>

<p>Below is my entire code, in case it helps someone else:</p>

<p>(Please ensure that you add the relevant references from Nuget packages: Microsoft.Azure.Search and Newtonsoft.Json)</p>

<p>Program.cs(This is a console application):</p>

<pre><code>using Microsoft.Azure.Search;
using System;
using System.Configuration;
using System.IO;
using System.Threading;

namespace AzureSearchTextAnalytics
{
class Program
{
    static string searchServiceName = ""&lt;removed&gt;"";     // This is the Azure Search service name that you create in Azure
    static string searchServiceAPIKey = ""&lt;removed&gt;"";   // This is the Primary key that is provided after creating a Azure Search Service

    static string indexName = ""textanalytics"";
    static SearchServiceClient serviceClient = new SearchServiceClient(searchServiceName, new SearchCredentials(searchServiceAPIKey));
    static SearchIndexClient indexClient = serviceClient.Indexes.GetClient(indexName);

    static void Main()
    {
        MakeRequests();
        Console.WriteLine(""Hit ENTER to exit..."");
        Console.ReadLine();
    }

    static async void MakeRequests()
    {
        // Note, this will create a new Azure Search Index for the text and the key phrases
        Console.WriteLine(""Creating Azure Search index..."");
        AzureSearch.CreateIndex(serviceClient, indexName);

        // Apply the Machine Learning Text Extraction to retrieve only the key phrases
        Console.WriteLine(""Extracting key phrases from processed text... \r\n"");
        KeyPhraseResult keyPhraseResult = await TextExtraction.ProcessText();

        Console.WriteLine(""Found the following phrases... \r\n"");
        foreach (var phrase in keyPhraseResult.KeyPhrases)
            Console.WriteLine(phrase);

        // Take the resulting key phrases to a new Azure Search Index
        // It is highly recommended that you upload documents in batches rather 
        // individually like is done here
        Console.WriteLine(""Uploading extracted text to Azure Search...\r\n"");
        AzureSearch.UploadDocuments(indexClient, ""1"", keyPhraseResult);
        Console.WriteLine(""Wait 5 seconds for content to become searchable...\r\n"");
        Thread.Sleep(5000);

        // Execute a test search 
        Console.WriteLine(""Execute Search..."");
        AzureSearch.SearchDocuments(indexClient, ""Azure Search"");

        Console.WriteLine(""All done.  Press any key to continue."");
        Console.ReadLine();
    }
}
}
</code></pre>

<p>My TextExtractionHelper.cs:</p>

<pre><code>using System;
using System.Collections.Generic;
using System.Linq;
using System.Net.Http;
using System.Net.Http.Headers;
using System.Text;
using System.Threading.Tasks;
using System.Web;
using Newtonsoft.Json;
using System.Configuration; // get it from http://www.newtonsoft.com/json
using Newtonsoft.Json.Linq;

namespace AzureSearchTextAnalytics
{
/// &lt;/summary&gt;
public class TextExtraction
{
    static string azureMLTextAnalyticsKey = ""&lt;removed&gt;"";     // This key you will get when you have added TextAnalytics in Azure.
    private const string ServiceBaseUri = ""https://westus.api.cognitive.microsoft.com/""; //This you will get when you have added TextAnalytics in Azure
    public static async Task&lt;KeyPhraseResult&gt; ProcessText()
    {
        string filetext = ""Build great search experiences for your web and mobile apps. "" +
            ""Many applications use search as the primary interaction pattern for their users. When it comes to search, user expectations are high. They expect great relevance, suggestions, near-instantaneous responses, multiple languages, faceting, and more. Azure Search makes it easy to add powerful and sophisticated search capabilities to your website or application. The integrated Microsoft natural language stack, also used in Bing and Office, has been improved over 16 years of development. Quickly and easily tune search results, and construct rich, fine-tuned ranking models to tie search results to business goals. Reliable throughput and storage provide fast search indexing and querying to support time-sensitive search scenarios. "" +
            ""Reduce complexity with a fully managed service. "" +
            ""Azure Search removes the complexity of setting up and managing your own search index. This fully managed service helps you avoid the hassle of dealing with index corruption, service availability, scaling, and service updates. Create multiple indexes with no incremental cost per index. Easily scale up or down as the traffic and data volume of your application changes."";


        KeyPhraseResult keyPhraseResult = new KeyPhraseResult();
        using (var httpClient = new HttpClient())
        {

            httpClient.BaseAddress = new Uri(ServiceBaseUri);

            // Request headers.
            httpClient.DefaultRequestHeaders.Add(""Ocp-Apim-Subscription-Key"", azureMLTextAnalyticsKey);
            httpClient.DefaultRequestHeaders.Accept.Add(new MediaTypeWithQualityHeaderValue(""application/json""));

            byte[] byteData = Encoding.UTF8.GetBytes(""{\""documents\"":["" +
               ""{\""id\"":\""1\"",\""text\"":\"""" + filetext + ""\""},]}"");

            // Detect key phrases:
            var keyPhrasesRequest = ""text/analytics/v2.0/keyPhrases"";

            // get key phrases
            using (var getcontent = new ByteArrayContent(byteData))
            {
                getcontent.Headers.ContentType = new MediaTypeHeaderValue(""application/json"");
                var response = await httpClient.PostAsync(keyPhrasesRequest, getcontent);

                Task&lt;string&gt; contentTask = response.Content.ReadAsStringAsync();

                string content = contentTask.Result;

                if (!response.IsSuccessStatusCode)
                {
                    throw new Exception(""Call to get key phrases failed with HTTP status code: "" +
                                        response.StatusCode + "" and contents: "" + content);
                }


                var result = JsonConvert.DeserializeObject&lt;RootObject&gt;(content);

                keyPhraseResult.KeyPhrases = result.documents[0].keyPhrases;
            }

        }
        return keyPhraseResult;
    }
}

public class Documents
{
    public List&lt;string&gt; keyPhrases { get; set; }
    public string id { get; set; }
}

public class RootObject
{
    public List&lt;Documents&gt; documents { get; set; }
    public List&lt;object&gt; errors { get; set; }
}

/// &lt;summary&gt;
/// Class to hold result of Key Phrases call
/// &lt;/summary&gt;
public class KeyPhraseResult
{
    public List&lt;string&gt; KeyPhrases { get; set; }
}
}
</code></pre>

<p>AzureSearch.cs:</p>

<pre><code>using Microsoft.Azure.Search;
using Microsoft.Azure.Search.Models;
using System;
using System.Collections.Generic;
using System.Configuration;
using System.Linq;
using System.Text;
using System.Threading.Tasks;

namespace AzureSearchTextAnalytics
{
public class AzureSearch
{
    public static void CreateIndex(SearchServiceClient serviceClient, string indexName)
    {

        if (serviceClient.Indexes.Exists(indexName))
        {
            serviceClient.Indexes.Delete(indexName);
        }

        var definition = new Index()
        {
            Name = indexName,
            Fields = new[]
            {
                new Field(""fileId"", DataType.String)                       { IsKey = true },
                new Field(""fileText"", DataType.String)                      { IsSearchable = true, IsFilterable = false, IsSortable = false, IsFacetable = false },
                new Field(""keyPhrases"", DataType.Collection(DataType.String)) { IsSearchable = true, IsFilterable = true,  IsFacetable = true }
            }
        };

        serviceClient.Indexes.Create(definition);
    }

    public static void UploadDocuments(SearchIndexClient indexClient, string fileId, KeyPhraseResult keyPhraseResult)
    {
        List&lt;IndexAction&gt; indexOperations = new List&lt;IndexAction&gt;();
        var doc = new Document();
        doc.Add(""fileId"", fileId);
        doc.Add(""keyPhrases"", keyPhraseResult.KeyPhrases.ToList());
        indexOperations.Add(IndexAction.Upload(doc));

        try
        {
            indexClient.Documents.Index(new IndexBatch(indexOperations));
        }
        catch (IndexBatchException e)
        {
            // Sometimes when your Search service is under load, indexing will fail for some of the documents in
            // the batch. Depending on your application, you can take compensating actions like delaying and
            // retrying. For this simple demo, we just log the failed document keys and continue.
            Console.WriteLine(
            ""Failed to index some of the documents: {0}"",
                   String.Join("", "", e.IndexingResults.Where(r =&gt; !r.Succeeded).Select(r =&gt; r.Key)));
        }

    }


    public static void SearchDocuments(SearchIndexClient indexClient, string searchText)
    {
        // Search using the supplied searchText and output documents that match 
        try
        {
            var sp = new SearchParameters();

            DocumentSearchResult&lt;OCRTextIndex&gt; response = indexClient.Documents.Search&lt;OCRTextIndex&gt;(searchText, sp);
            foreach (SearchResult&lt;OCRTextIndex&gt; result in response.Results)
            {
                Console.WriteLine(""File ID: {0}"", result.Document.fileId);
                Console.WriteLine(""Key Phrases: {0}"", string.Join("","", result.Document.keyPhrases));

            }
        }
        catch (Exception e)
        {
            Console.WriteLine(""Failed search: {0}"", e.Message.ToString());
        }

    }

}
}
</code></pre>

<p>DataModel.cs</p>

<pre><code>using Microsoft.Azure.Search.Models;
using System;
using System.Collections.Generic;
using System.Linq;
using System.Text;
using System.Threading.Tasks;

namespace AzureSearchTextAnalytics
{
[SerializePropertyNamesAsCamelCase]
public class OCRTextIndex
{
    public string fileId { get; set; }

    public string[] keyPhrases { get; set; }

}
}
</code></pre>
","2298585",2
166,40837389,2,40714064,2016-11-28 04:51:43,0,"<ol>
<li>Create a web service from the AzureML experiment. </li>
<li>Access the web service using a program you written from C# or any language.</li>
<li>You can get the output of the web service as a JSON.</li>
<li>Use typical SQL ADD/UPDATE queries to update the table</li>
<li>When giving an input for the web service, fetch the data from the DB and pass as the JSON for it. </li>
</ol>
","5383733",0
167,40713966,2,40703961,2016-11-21 06:23:15,2,"<p>The algorithms used as modules in Azure ML Studio are not currently able to be used directly as code for Python programming.</p>

<p>That being said: you can attempt to publish the outputs of the out-of-the-box algorithms as web services, which can be consumed by Python code in the Azure ML Studio notebooks. You can also create your own algorithms and use them as custom Python or R modules.</p>
","4989676",1
168,40607406,2,40577607,2016-11-15 10:25:02,0,"<p>Keeping leading zeros when the data column is in integer format is not possible. Here's an alternative way of keeping the leading zeros. (The String data type is used instead of int)</p>

<ol>
<li>Add an special character('/' etc) before the number </li>
<li>Pipe the contentthrough 'Preprocess Text' module.</li>
<li>Make sure only to tick ""Remove Special Characters"" option.</li>
<li>Output would be in a string with your desired format.</li>
</ol>

<p><a href=""https://i.stack.imgur.com/H0IBg.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/H0IBg.png"" alt=""Remove Special Characters option""></a></p>

<p><a href=""https://i.stack.imgur.com/ML1YM.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ML1YM.png"" alt=""Output as a string""></a></p>
","5383733",0
169,40093660,2,40028165,2016-10-17 18:55:27,3,"<p>The input schema (names/types of required input) based on the location in the graph where you attach the ""Web Service Input"" module. To get the schema you want, you will need to find -- or if necessary, create -- a place in the experiment where the data has the column names/types you desire.</p>

<p>Consider this simple example experiment that predicts whether a field called ""income"" will be above or below $50k/year:</p>

<p><a href=""https://i.stack.imgur.com/nWaN2.png"" rel=""nofollow""><img src=""https://i.stack.imgur.com/nWaN2.png"" alt=""enter image description here""></a></p>

<p>When we click ""Set up web service"", the following graph is automatically generated:</p>

<p><a href=""https://i.stack.imgur.com/NMMpV.png"" rel=""nofollow""><img src=""https://i.stack.imgur.com/NMMpV.png"" alt=""enter image description here""></a></p>

<p>Since the input dataset and ""Web service input"" modules are connected to the same port, the web service schema will perfectly match the schema of the input dataset. This is unfortunate because the input dataset contains a column called ""income"", which is what our web service is supposed to predict -- this is equivalent to the problem that you are having.</p>

<p>To get around it, we need to create a place in our experiment graph where we've dropped the unneeded ""income"" field from the input dataset, and attach the ""Web service input"" module there:</p>

<p><a href=""https://i.stack.imgur.com/WPeSB.png"" rel=""nofollow""><img src=""https://i.stack.imgur.com/WPeSB.png"" alt=""enter image description here""></a></p>

<p>With this arrangement, the web service only requests the features actually needed to score the model. I'm sure you can use a similar method to create a predictive experiment with whatever input schema you need for your own work.</p>
","6722050",0
170,40037716,2,40018320,2016-10-14 07:46:39,1,"<p>When you talk about ""secure"", are you worried about secure transmission between AML and ADF, or secure storage of your DB query information? For the former, all communication between these two services will be done with HTTPS. For the latter, our production storage has its strict access control. Besides, we only log the count of the global parameters and never the values. I believe it's secure to pass your DB query as a global parameter to the AzureML web service.</p>
","5676168",1
171,40025404,2,39150834,2016-10-13 15:40:08,0,"<p>Tayehi, </p>

<p>Nice to meet you. I am the program manager in charge of the recommendations API.
2 things:</p>

<ol>
<li><p>If you get a 0.5 probability you are most likely getting ""default recommendations"". This usually means that you do not have enough training data or that there are not enough co-occurrences for the item you are testing in the data. To describe the extreme case, imagine an item A that only gets purchased with an item B only one or two times -- it would be hard to say with confidence (statistical significance) that someone that likes item A is also likely to like item B.</p></li>
<li><p>It looks like you are still using the old recommendations API. I would like to encourage you to use our newer version (the Recommendations API cognitive service). Please take a look at <a href=""https://azure.microsoft.com/en-us/documentation/articles/cognitive-services-migration-from-dm/to"" rel=""nofollow"">https://azure.microsoft.com/en-us/documentation/articles/cognitive-services-migration-from-dm/to</a> help you in this process.</p></li>
</ol>

<p>Thanks!
Luis Cabrera
Cortana Intelligence Applications.</p>
","6483167",0
172,39944927,2,39941622,2016-10-09 14:51:38,5,"<p>Personally, I think we could have marketed MRO a bit differently, without making such a big deal about parallelism/multithreading. Ah well.</p>

<p>R comes with an Rblas.dll/.so which implements the routines used for linear algebra computations. These routines are used in various places, but one common use case is for fitting regression models. With MRO, we replace the standard Rblas with one that uses the <a href=""https://software.intel.com/en-us/intel-mkl"" rel=""noreferrer"">Intel Math Kernel Library</a>. When you call a function like <code>lm</code> or <code>glm</code>, MRO will use multiple threads and optimized CPU instructions to fit the model, which can get you dramatic speedups over the standard implementation.</p>

<p>MRO isn't the only way you can get this sort of speedup; you can also compile/download other BLAS implementations that are similarly optimized. We just make it an easy one-step download.</p>

<p>Note that the MKL only affects code that involves linear algebra. It isn't a general-purpose speedup tool; any R code that doesn't do matrix computations won't see a performance improvement. In particular, it won't speed up any code that involves <em>explicit</em> parallelism, such as code using the parallel package, SNOW, or other cluster computing tools.</p>

<p>On the other hand, it won't <em>degrade</em> them either. You can still use packages like parallel, SNOW, etc to create compute clusters and distribute your code across multiple processes. MRO works just like regular CRAN R in this respect. (One thing you might want to do, though, if you're creating a cluster of nodes on the one machine, is reduce the number of MKL threads. Otherwise you risk contention between the nodes for CPU cores, which will degrade performance.)</p>

<p>Disclosure: I work for Microsoft.</p>
","474349",0
173,39468956,2,39228377,2016-09-13 11:31:31,1,"<p>@MuhammedKadirYücel, Based on my understanding, I think you want to send raw accelerometer data to Azure and store into some storage service for importing on Machine Learning service.</p>

<p>Per my experience, I think the best practice is that create a <a href=""https://azure.microsoft.com/en-us/documentation/services/event-hubs/"" rel=""nofollow"">EventHub</a> or <a href=""https://azure.microsoft.com/en-us/documentation/services/iot-hub/"" rel=""nofollow"">IoTHub</a> instance for receiving these raw accelerometer data. </p>

<p>Then create a <a href=""https://azure.microsoft.com/en-us/documentation/articles/stream-analytics-get-started-with-azure-stream-analytics-to-process-data-from-iot-devices/"" rel=""nofollow"">Stream Analytics</a> to transfer sensor data from EventHub or IoTHub to Azure Blob Storage. </p>

<p>Finally, you can import these data of the blob storage on Machine Learning service, please see <a href=""https://azure.microsoft.com/en-us/documentation/articles/machine-learning-import-data-from-online-sources/"" rel=""nofollow"">https://azure.microsoft.com/en-us/documentation/articles/machine-learning-import-data-from-online-sources/</a>.</p>
","4989676",0
174,39258814,2,39251701,2016-08-31 20:53:32,1,"<p>Thanks so much for publishing the example -- this really helped to understand the issue. I suspect that you want to modify the <code>gsub()</code> calls in your script by adding the argument ""<code>fixed=TRUE</code>"" to each. (The documentation for this function is <a href=""https://stat.ethz.ch/R-manual/R-devel/library/base/html/grep.html"" rel=""nofollow"">here</a>.)</p>

<p>What appears to have happened is that somewhere in your full dataset -- but not in the subsampled dataset -- there is some text that winds up being included in <code>df[i, ""names""]</code> as ""<code>(art.</code>"".  Your script pads this into ""<code>\\b(art.\\b</code>"". The <code>gsub()</code> function tries to interpret this as a regular expression instead of a simple string, then throws an error because it is not a valid regular expression: it contains an opening parenthesis but no closing parenthesis. I believe that you actually did not want <code>gsub()</code> to interpret the input as a regular expression in the first place, and specifying <code>gsub(..., fixed=TRUE)</code> will correct that.</p>

<p>I believe the reason why this error disappears when you add the sample/partition module is because, by chance, the problematic input value was dropped on subsampling. I do not think it is an issue of available resources on Azure ML. (Caveat: I cannot confirm the fix works yet; I made the suggested update and started running the experiment, but it has not yet completed successfully.)</p>
","6722050",6
175,38629138,2,38505336,2016-07-28 06:57:13,0,"<p>This issue has been resolved. Please let me know if this happens again</p>
","4589073",0
176,38607238,2,38500359,2016-07-27 08:05:18,0,"<p>I have ""solved"" thanks to the help of the AzureML support: it is a bug they are trying to solve right now.</p>

<p>The bug shows up when you have <strong>more R script modules</strong>, and the <strong>first has no a zip</strong> input module while the following have. </p>

<p><em>Workaround</em>: connect the zip input module to the first R script module too.
<a href=""https://i.stack.imgur.com/C4fV9.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/C4fV9.png"" alt=""enter image description here""></a></p>
","5097722",0
177,42030285,2,39376560,2017-02-03 17:58:47,1,"<p>Quick update for you. As of TensorFlow r0.12 there is now a native TensorFlow package for Windows. I have it running successfully on my Windows 10 laptop. See this <a href=""https://developers.googleblog.com/2016/11/tensorflow-0-12-adds-support-for-windows.html"" rel=""nofollow noreferrer"">blog post</a> from Google for more information.</p>
","3167182",0
178,41139254,2,41136102,2016-12-14 09:40:21,1,"<p>BES doesn't work with streams. The data needs to be stored somewhere for AML to access the data. Take a look at: <a href=""https://learn.microsoft.com/en-us/azure/machine-learning/machine-learning-consume-web-services#batch-execution-service-bes"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/machine-learning/machine-learning-consume-web-services#batch-execution-service-bes</a> for more information.</p>
","7252713",0
179,40845482,2,40844351,2016-11-28 13:27:37,0,"<p>Consider using rbenchmark or other benchmarking tools to get an idea of the runtime and complexity of your code. In general for loops tend to be slow.</p>

<p>It's very possible that the server has less resources available (ram, cpu) or that you have to wait in a que before you get served. Without any more code it's hard to comment further on this issue.</p>
","7210384",0
180,42270028,2,42228715,2017-02-16 09:36:45,2,"<p>So since there was no reply to my question, I made some research and came up with the following:</p>

<p>H2O cannot be ran in a straightforward manner in Azure machine learning embedded R scripts. A workaround the problem is to consider using an Azure created environment - specially for H2O. The options available are:</p>

<ol>
<li>Spinning up an H2O Artificial Intelligence Virtual Machine solution</li>
<li>Using an H2O application for HDInsight</li>
</ol>

<p>For more reading, you can go to: <a href=""http://docs.h2o.ai/h2o/latest-stable/h2o-docs/azure.html"" rel=""nofollow noreferrer"">http://docs.h2o.ai/h2o/latest-stable/h2o-docs/azure.html</a> </p>
","1783739",0
181,42106607,2,42010405,2017-02-08 06:58:00,1,"<p>You can try using Azure Data Factory to create a Machine Learning pipeline or use Azure ML Studio's Predictive Web Services.</p>

<ol>
<li><p>With Azure Data Factory
Follow <a href=""https://learn.microsoft.com/en-us/azure/data-factory/data-factory-azure-ml-batch-execution-activity"" rel=""nofollow noreferrer"">this link</a> for details. Azure Data Factory implementations would seem difficult at first but they do work great with Azure ML experiments. </p>

<p>Azure Data Factory can run your ML Experiment on a schedule or one-off at a specified time (I guess you can set only for UTC Timezone right now) and monitor it through a dashboard (which is pretty cool).</p>

<p>As an example you can look @ <a href=""https://github.com/Microsoft/azure-docs/blob/master/articles/data-factory/data-factory-azure-ml-batch-execution-activity.md"" rel=""nofollow noreferrer"">ML Batch Execution</a>. I used this in one of our implementations (we do have latency issues, but trying to solve that).</p></li>
<li><p>If you directly want to use the experiment in your console (assuming it is a web application), use create a Predictive Web service out of your ML Experiment, details <a href=""https://learn.microsoft.com/en-us/azure/machine-learning/machine-learning-walkthrough-5-publish-web-service"" rel=""nofollow noreferrer"">here</a></p></li>
</ol>

<p>I couldn't exactly understand your use case so I posted two alternatives that should help you. Hope this might lead you to a better solution/approach.</p>
","7171278",0
182,42062540,2,41902672,2017-02-06 07:38:02,0,"<p>The Batch request R command has a <code>saveBlobToFile</code> function. The problem is in the <code>saveBlobToFile</code> function that uses wrong encoding with <code>getUrl</code>.  <code>getUrl</code> function needs to specify the encodings explicitly. Do the following changes</p>

<pre><code>blobContent = getURL(blobUrl, .encoding=""UTF-8"")
</code></pre>

<p>where without <code>.encoding</code>, the output is <code>ISO8859-1('latin1')</code> or something inherited from your system.</p>
","164148",0
183,41084814,2,41080045,2016-12-11 09:40:33,1,"<p>It is not currently possible to test your query when you use a function to call out to Azure ML. The test query functionality runs in the web browser window so I guess they haven't implemented that feature yet. </p>

<p>I expect if you start the job it will actually work. However you may need to change <code>result.[Scored Labels]</code> to match the columns in the Azure ML API output by saying <code>result.Sentiment</code> and <code>result.Score</code></p>
","5070440",0
184,43761023,2,43576656,2017-05-03 13:08:46,0,"<p>Double-check that you've selected ""Allow duplicates and preserve column order in selection"" in column selection options, so it matches the columns in listed order.</p>

<p>Also, you could try Apply SQL Transformation module to join datasets.</p>
","5784983",0
185,43769251,2,43713402,2017-05-03 20:18:20,1,"<p>It was fixed by using tensorflow 1.1.0 instead of 1.0.0. Though, profiling information wasn't shown. </p>
","250560",0
186,43774067,2,43762127,2017-05-04 04:45:09,0,"<p>We worked with @sag and determined that the 500 error is a result of a timeout due to a long ""cold start"". If you haven't sent traffic to your model in a while, or if you send enough that we need to spin more instances up, you will hit a ""cold start"", where one or more instances are spun up. Currently, this can be a lengthy process that sometimes times out on our end and a 500 error may result.</p>

<p>These errors can safely be retried; we recommend using exponential backoff.</p>
","1399222",0
187,43821644,2,43452978,2017-05-06 14:30:33,1,"<p>To make the ""labels"" floats, you need to make sure the default value for the label column is a float. The following changes are needed:</p>

<pre><code>CSV_COLUMN_DEFAULTS = [[0], [''], [0], [''], [0], [''], [''], [''], [''], [''],
                       [0], [0], [0], [''], [0.0]]
label_tensor = features.pop(LABEL_COLUMN)
</code></pre>

<p>(You may want to consider <code>s/LABEL_COLUMN/INCOME_COLUMN/g</code>)</p>
","1399222",1
188,43846974,2,43805736,2017-05-08 11:53:52,0,"<p>I solved the problem with .Net API.
I created two new classes Inherits the Google API's classes.
Something like that:</p>

<pre><code>Imports Google.Apis.CloudMachineLearningEngine.v1.Data
Imports Newtonsoft.Json

Public Class myGoogleCloudMlV1PredictRequest
    Inherits GoogleCloudMlV1PredictRequest

    &lt;JsonProperty(""instances"")&gt;
    Public Property MyHttpBody As List(Of myGoogleApiHttpBody)
End Class



Imports Google.Apis.CloudMachineLearningEngine.v1.Data
Imports Newtonsoft.Json

Public Class myGoogleApiHttpBody
    Inherits GoogleApiHttpBody

    &lt;JsonProperty(""image_bytes"")&gt;
    Public Property MyData As image_byte

    &lt;JsonProperty(""key"")&gt;
    Public Property key As String

End Class
</code></pre>

<p>So, in my original code I change this part:</p>

<pre><code>    Dim myBase64 As String = GetBase64(""my_image_path_to_convert_into_a _base64_String"")
    Dim myJsonRequest As String = ""{""""instances"""" : [{""""key"""":""""0"""", """"image_bytes"""": {""""b64"""": """""" + myBase64 + """"""}}]}""

    Dim myRequest = New GoogleCloudMlV1PredictRequest With {
        .HttpBody = New GoogleApiHttpBody With {.Data = myJsonRequest,
                                               .ContentType = ""application/json""
                                            }
    }
</code></pre>

<p>For this one:</p>

<pre><code>Dim myBase64 As String = GetBase64(""my_image_path_to_convert_into_a _base64_String"")
Dim myRequest = New myGoogleCloudMlV1PredictRequest With {
            .MyHttpBody = New List(Of myGoogleApiHttpBody)()
        }

        Dim item As myGoogleApiHttpBody = New myGoogleApiHttpBody With {
                                                    .key = ""0"",
                                                    .MyData = New image_byte With {
                                                        .b64 = myBase64
                                                    }
                                                 }
        myRequest.MyHttpBody.Add(item)
</code></pre>

<p>And voilá, It's working!</p>

<p>Thanks for everyone!!</p>
","7600246",0
189,43957951,2,43952943,2017-05-13 21:00:12,3,"<p>The Cloud ML service doesn't execute with permissions sufficient to access Datastore. One way around this would be to upload credentials (e.g a json service account key file) for a service account with access to Cloud Datastore. You could then use that to obtain credentials capable of accessing Datastore.</p>
","4392784",0
190,44031966,2,44030608,2017-05-17 18:05:08,1,"<p>tl;dr the discovery.build may not be using the expected service account as it tries many authentication options</p>

<p>I'd suggest to be explicit instead of relying on the default behavior as in: <a href=""https://stackoverflow.com/questions/40917675/using-cloudml-prediction-api-in-production-without-gcloud/40919307#40919307"">Using CloudML prediction API in production without gcloud</a>.  Also, its possible your project IAM settings don't include the service account if you call:</p>

<pre><code>gcloud --project ""$PROJECT"" get-iam-policy 
</code></pre>

<p>Do you see the expected service account with roles/viewer or above? If not you need to grant it permission.  Its presence in the service accounts page only means you have that service account, not that it is allowed to do anything!</p>
","3818158",4
191,44303072,2,44285641,2017-06-01 09:04:30,1,"<p>I viewed the <code>pyxdameraulevenshtein</code> module page, there are two packages you can download which include a wheel file for MacOS and a source code tar file. I don't think you can directly use the both on Azure ML, because the MacOS one is just a share library <code>.so</code> file for darwin which is not compatible with Azure ML, and the other you need to first compile it.</p>

<p>So my suggestion is as below for using <code>pyxdameraulevenshtein</code>.</p>

<ol>
<li>First, compile the source code of <code>pyxdameraulevenshtein</code> to a DLL file on Windows, please refer to the document for Python <a href=""https://docs.python.org/2/extending/windows.html"" rel=""nofollow noreferrer"">2</a>/<a href=""https://docs.python.org/3/extending/windows.html"" rel=""nofollow noreferrer"">3</a> or search for doing this.</li>
<li>Write a Python script using the DLL you compiled to implement your needs, please refer to the SO thread <a href=""https://stackoverflow.com/questions/252417/how-can-i-use-a-dll-file-from-python"">How can I use a DLL file from Python?</a> for how to use DLL from Python and refer to the Azure offical <a href=""https://learn.microsoft.com/en-us/azure/machine-learning/machine-learning-execute-python-scripts"" rel=""nofollow noreferrer"">tutorial</a> to write your Python script</li>
<li>Package your Python script and DLL file as a zip file, then to upload the zip file to use it in <code>Execute Python script</code> model of Azure ML.</li>
</ol>

<p>Hope it helps.</p>
","4989676",2
192,44383310,2,44371692,2017-06-06 06:59:06,2,"<p>According to the <a href=""https://learn.microsoft.com/en-us/azure/machine-learning/machine-learning-execute-python-scripts#limitations"" rel=""nofollow noreferrer""><code>Limitations</code></a> and <a href=""https://msdn.microsoft.com/en-us/library/azure/dn955437.aspx#Anchor_3"" rel=""nofollow noreferrer""><code>Technical Notes</code></a> of <code>Execute Python Script</code> tutorial, the only way to add custom Python modules is via the zip file mechanism to package the modules and all dependencies.</p>

<p>For example to install <code>CVXPY</code>, as below.</p>

<ol>
<li>Download the wheel file of <a href=""http://www.lfd.uci.edu/~gohlke/pythonlibs/#cvxpy"" rel=""nofollow noreferrer""><code>CVXPY</code></a> and its dependencies like <a href=""http://www.lfd.uci.edu/~gohlke/pythonlibs/#cvxopt"" rel=""nofollow noreferrer""><code>CVXOPT</code></a>.</li>
<li>Decompress these wheel files, and package these files in the path <code>cvxpy</code> and <code>cvxopt</code>, etc as a zipped file with your script.</li>
<li>Upload the zip file as a dataset and use it as the script bundle.</li>
</ol>

<p>If you were using IPython, you also can try to install the Python Package via the code <code>!pip install cvxpy</code>.</p>

<p>And there are some similar SO threads which may be helpful for you, as below.</p>

<ol>
<li><a href=""https://stackoverflow.com/questions/44285641/azure-ml-python-with-script-bundle-cannot-import-module"">Azure ML Python with Script Bundle cannot import module</a></li>
<li><a href=""https://stackoverflow.com/questions/8663046/how-to-install-a-python-package-from-within-ipython"">How to install a Python package from within IPython?</a></li>
</ol>

<p>Hope it helps.</p>

<hr>

<p>Update:</p>

<p>For IPython interface of Azure ML, you move to the <code>NOTEBOOKS</code> tab to create a notebook via <code>ADD TO PROJECT</code> button at the bottom of the page, as the figure below.</p>

<p><a href=""https://i.stack.imgur.com/X2Asv.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/X2Asv.png"" alt=""enter image description here""></a></p>

<p>Or you can directly login to the website <code>https://notebooks.azure.com</code> to use it.</p>
","4989676",7
193,44401950,2,44401088,2017-06-07 01:09:32,27,"<p>Try executing the following command</p>

<p><code>gcloud auth application-default login</code></p>
","1399222",2
194,44416345,2,44416344,2017-06-07 15:03:41,0,"<p>The duplicate key exception is a red herring. <a href=""https://msdn.microsoft.com/en-us/library/azure/dn962112.aspx"" rel=""nofollow noreferrer"" title=""MSDN Module Error Code 0114"">Build error 0114</a> is a general error that occurs if there is a system exception while building the custom module. The real issue my module was compressed using the built in compress folder option in the Mac Finder. To fix this compress the file using the command line interface for <code>zip</code> in Terminal in the following very specific manner.</p>

<blockquote>
  <p>The following example:</p>
</blockquote>

<pre><code>cd ScoredDatasetMetadata/
zip ScoredDatasetMetadata *
mv ScoredDatasetMetadata.zip ../
</code></pre>

<p>Builds a zip file with the correct file structure.</p>
","965382",0
195,44424838,2,44423753,2017-06-08 00:34:34,2,"<p>Sorry that you are experiencing issues. Currently, the available debugging tools are Job logs, metrics and TensorBoard, but seems like all of these can't be used in your case. 
If possible, can you please send us your project number and job id to cloudml-feedback@google.com,so that we can take a close look?</p>
","7438427",1
196,44426118,2,44409066,2017-06-08 03:24:41,1,"<p>Following code should work:</p>

<pre><code>import pandas as pd
def azureml_main(dataframe1 = None, dataframe2 = None):
    result = pd.DataFrame({'mycol': [123]})
    return result,
</code></pre>
","5784983",0
197,44443380,2,44381879,2017-06-08 18:44:22,8,"<p>UPDATE: In version 1.3 the contrib estimators (tf.contrib.learn.DNNClassifier for example), were changed to inherit from the core estimator class tf.estimator.Estimator which unlike it's predecessor, hides the model function as a private class member, so you'll need to replace <code>estimator.model_fn</code> in the solution below with <code>estimator._model_fn</code>. </p>

<p>Josh's answer points you to the Flowers example, which is a good solution if you want to use a custom estimator. If you want to stick with a canned estimator, (e.g. the <code>tf.contrib.learn.DNNClassifiers</code>) you can wrap it in a custom estimator that adds support for keys. (Note: I think it's likely canned estimators will gain key support when they move into core).</p>

<pre><code>KEY = 'key'
def key_model_fn_gen(estimator):
    def _model_fn(features, labels, mode, params):
        key = features.pop(KEY, None)
        model_fn_ops = estimator.model_fn(
           features=features, labels=labels, mode=mode, params=params)
        if key:
            model_fn_ops.predictions[KEY] = key
            # This line makes it so the exported SavedModel will also require a key
            model_fn_ops.output_alternatives[None][1][KEY] = key
        return model_fn_ops
    return _model_fn

my_key_estimator = tf.contrib.learn.Estimator(
    model_fn=key_model_fn_gen(
        tf.contrib.learn.DNNClassifier(model_dir=model_dir...)
    ),
    model_dir=model_dir
)
</code></pre>

<p><code>my_key_estimator</code> can then be used exactly like your <code>DNNClassifier</code> would be used, except it will expect a feature with the name <code>'key'</code> from input_fns (prediction, evaluation and training). </p>

<p>EDIT2:
You will also need to add the corresponding input tensor to the prediction input function of your choice. For example, a new JSON serving input fn would look like:</p>

<pre><code>def json_serving_input_fn():
  inputs = # ... input_dict as before
  inputs[KEY] = tf.placeholder([None], dtype=tf.int64)
  features = # .. feature dict made from input_dict as before
  tf.contrib.learn.InputFnOps(features, None, inputs)
</code></pre>

<p>(slightly different between 1.2 and 1.3, as <code>tf.contrib.learn.InputFnOps</code> is replaced with <code>tf.estimator.export.ServingInputReceiver</code>, and padding tensors to rank 2 is no longer necessary in 1.3) </p>

<p>Then ML Engine will send a tensor named ""key"" with your prediction request, which will be passed to your model, and through with your predictions.</p>

<p>EDIT3: Modified <code>key_model_fn_gen</code> to support ignoring missing key values.
EDIT4: Added key for prediction</p>
","3597868",7
198,44473135,2,44462823,2017-06-10 12:05:59,3,"<p>It depends what your TensorFlow model contains. The coremltools do not support TensorFlow so you'll have to write your own converter. But this will only work if your model only contains things that are supported by the mlmodel format. You can download the spec for this format from the coremltools web page. Since it's possible to build compute graphs of arbitrary complexity in TensorFlow, it's not surprising that coremltools currently does not support it (since mlmodel itself only supports a limited number of model types).</p>
","7501629",4
199,44572725,2,44572056,2017-06-15 16:38:59,3,"<p>Note that every argument after the ""-- \"" line is a pass through to the tensorflow code and is therefore dependent on the individual sample code.</p>

<p>In this case, the ""--verbosity"" flag isn't supported by the sample you are running.  Looking at the <a href=""https://github.com/GoogleCloudPlatform/cloudml-samples/search?utf8=%E2%9C%93&amp;q=VERBOSITY&amp;type="" rel=""nofollow noreferrer"">samples repo</a>, it looks like the only sample that has that flag is the <a href=""https://github.com/GoogleCloudPlatform/cloudml-samples/blob/867832752380c89a47d6a7a9df44ca4dae862a5d/census/estimator/trainer/task.py"" rel=""nofollow noreferrer"">census estimator sample</a>.</p>
","5441818",1
200,44594935,2,44594332,2017-06-16 17:29:06,0,"<p>It looks like <code>img</code> is bytes, so try converting to string:</p>

<pre><code>img = img.decode('utf-8')
</code></pre>
","1399222",2
201,44621791,2,44593469,2017-06-19 03:20:01,3,"<p>Your description is not clear, I don't know what happended when you got error via <code>import humanfriendly</code> as you said. So I just can do for you that only post my steps about how to install <code>humanfriendly</code> Python package in Azure ML, as below.</p>
<ol>
<li><p>According to the section <a href=""https://msdn.microsoft.com/library/azure/cdb56f95-7f4c-404d-bde7-5bb972e6f232/#Anchor_3"" rel=""nofollow noreferrer""><code>Technical Notes</code></a> of the document &quot;Execute Python Script&quot;, I download the <code>humanfriendly</code> package from <a href=""https://pypi.python.org/pypi/humanfriendly"" rel=""nofollow noreferrer"">here</a>. I decompressed the package (the same for either <a href=""https://pypi.python.org/packages/8e/24/de8e894b8f27d726e10af64123f1de64fe13cb106b281965419d004feabf/humanfriendly-3.2-py2.py3-none-any.whl#md5=9a1a8389e398de5170bc68c0917d94ca"" rel=""nofollow noreferrer""><code>.whl</code></a> file or <a href=""https://pypi.python.org/packages/e3/98/5de3e2f2d9dad8d9709be88c1bb930c1ea91f84bea9b85b7c04d52104bd8/humanfriendly-3.2.tar.gz#md5=a8176d0c4e7e58505844c53b748a82da"" rel=""nofollow noreferrer""><code>.tar.gz</code></a> file), and package its <code>humanfriendly</code> directory as a zip file.</p>
</li>
<li><p>Then I click the botton <code>+New</code> and select the <code>DATASET</code> tab to Upload a new dataset from a local file, as the figures below.</p>
<p>Fig 1. Click the botton <code>+New</code>
<a href=""https://i.stack.imgur.com/JwgMV.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/JwgMV.png"" alt=""enter image description here"" /></a>
Fig 2. Select the tab <code>DATASET</code>
<a href=""https://i.stack.imgur.com/Apcyy.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Apcyy.png"" alt=""enter image description here"" /></a>
Fig 3. Upload a new dataset from a local file
<a href=""https://i.stack.imgur.com/pYmvq.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/pYmvq.png"" alt=""enter image description here"" /></a></p>
</li>
<li><p>Drag &amp; drop the dataset module <code>humanfriendly.zip</code> and a <code>Execute Python Script</code> module to connect them and write the Python code, as below.</p>
</li>
</ol>
<p><a href=""https://i.stack.imgur.com/TOLJe.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/TOLJe.png"" alt=""enter image description here"" /></a></p>
<p>Here is My testing code in the <code>Execute Python Script</code>.</p>
<pre><code>import humanfriendly

def azureml_main(dataframe1 = None, dataframe2 = None):
    user_input = '16G'
    num_bytes = humanfriendly.parse_size(user_input)
    print num_bytes
</code></pre>
<p>Finally, I ran the experiment successfully.</p>
<hr />
<p>Update: The file structure tree of my <code>humanfriendly</code> zip. I decompressed the wheel file and just package the <code>humanfriendly</code> directory.</p>
<pre><code>humanfriendly
├── data.csv
└── humanfriendly
    ├── cli.py
    ├── compat.py
    ├── __init__.py
    ├── prompts.py
    ├── sphinx.py
    ├── tables.py
    ├── terminal.py
    ├── tests.py
    ├── text.py
    └── usage.py
</code></pre>
","4989676",2
202,44665004,2,44639463,2017-06-21 00:28:02,2,"<p>SavedModel contains a <a href=""https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/protobuf/meta_graph.proto"" rel=""nofollow noreferrer"">MetaGraphDef</a> inside its <a href=""https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/saved_model/README.md"" rel=""nofollow noreferrer"">structure</a>.
To create a SavedModel from a GraphDef in python you may want to use builder as described in the link.</p>

<pre><code>export_dir = ...
...
builder = tf.saved_model.builder.SavedModelBuilder(export_dir)
with tf.Session(graph=tf.Graph()) as sess:
  ...
  builder.add_meta_graph_and_variables(sess,
                                       [tag_constants.TRAINING],
                                       signature_def_map=foo_signatures,
                                       assets_collection=foo_assets)
...
with tf.Session(graph=tf.Graph()) as sess:
  ...
  builder.add_meta_graph([""bar-tag"", ""baz-tag""])
...
builder.save()
</code></pre>
","4077291",3
203,44686789,2,44680769,2017-06-21 21:46:12,9,"<p>Embeddings are typically large enough that the only viable approach is using them to initialize a <code>tf.Variable</code> in your graph. This will allow you to take advantage of param servers in distributed, etc. </p>

<p>For this (and anything else), I would recommend you use the new ""core"" estimator, <a href=""https://www.tensorflow.org/api_docs/python/tf/estimator/Estimator"" rel=""nofollow noreferrer""><code>tf.estimator.Estimator</code></a> as this will make things much easier.</p>

<p>From the answer in the link you provided, and knowing that we want a variable not a constant, we can either take approach:</p>

<p>(2) Initialize the variable using a feed dict, or
 (3) Load the variable from a checkpoint</p>

<hr>

<p>I'll cover option (3) first since it's much easier, and better:</p>

<p>In your <code>model_fn</code>, simply initialize a variable using the <code>Tensor</code> returned by a <a href=""https://www.tensorflow.org/versions/r1.0/api_docs/python/tf/contrib/framework/load_variable"" rel=""nofollow noreferrer""><code>tf.contrib.framework.load_variable</code></a> call. This requires:</p>

<ol>
<li>That you have a valid TF checkpoint with your embeddings</li>
<li>You know the <em>fully qualified</em> name of the embeddings variable within the checkpoint.</li>
</ol>

<p>The code is pretty simple:</p>

<pre><code>def model_fn(mode, features, labels, hparams):
  embeddings = tf.Variable(tf.contrib.framework.load_variable(
      'gs://my-bucket/word2vec_checkpoints/',
      'a/fully/qualified/scope/embeddings'
  ))
  ....
  return tf.estimator.EstimatorSpec(...)
</code></pre>

<p>However this approach won't work for you if your embeddings weren't produced by another TF model, hence option (2).</p>

<hr>

<p>For (2), we need to use <a href=""https://www.tensorflow.org/api_docs/python/tf/train/Scaffold"" rel=""nofollow noreferrer""><code>tf.train.Scaffold</code></a> which is essentially a configuration object that holds all the options for starting a <code>tf.Session</code> (which estimator intentionally hides for lots of reasons).</p>

<p>You may specify a <code>Scaffold</code> in the <a href=""https://www.tensorflow.org/api_docs/python/tf/estimator/EstimatorSpec"" rel=""nofollow noreferrer""><code>tf.train.EstimatorSpec</code></a> you return in your <code>model_fn</code>. </p>

<p>We create a placeholder in our model_fn, and make it the 
initializer operation for our embedding variable, then pass an <code>init_feed_dict</code> via the <code>Scaffold</code>.  e.g.</p>

<pre><code>def model_fn(mode, features, labels, hparams):
  embed_ph = tf.placeholder(
      shape=[hparams.vocab_size, hparams.embedding_size], 
      dtype=tf.float32)
  embeddings = tf.Variable(embed_ph)
  # Define your model
  return tf.estimator.EstimatorSpec(
      ..., # normal EstimatorSpec args
      scaffold=tf.train.Scaffold(init_feed_dict={embed_ph: my_embedding_numpy_array})
  )
</code></pre>

<p>What's happening here is the <code>init_feed_dict</code> will populate the values of the <code>embed_ph</code> placeholder at runtime, which will then allow the <code>embeddings.initialization_op</code> (assignment of the placeholder), to run.</p>

<hr>
","3597868",3
204,44779433,2,44367367,2017-06-27 11:42:53,0,"<p>Azure Machine Learning allows at least 3 options to do that:</p>

<ul>
<li>Apply SQL Transformation <code>select *,case when rating&lt;6 then 0 else 1 end RatingB from t1</code></li>
<li>Execute Python Script <code>return dataframe1.rating[dataframe1.rating &lt; 6] = 0</code></li>
<li>Execute R Script <code>dataset1$rating[dataset1$rating &lt; 6] &lt;- 0</code></li>
</ul>
","565903",0
205,44789074,2,44788368,2017-06-27 20:12:33,0,"<p>There was an issue with Cloud ML Engine; the issue should be fixed now. Please try again and let us know if you are still having problems.</p>
","4392784",0
206,44911877,2,44909608,2017-07-04 18:01:14,0,"<p>Please take a look at the documentation and sample <a href=""https://cloud.google.com/ml-engine/docs/concepts/trainer-considerations"" rel=""nofollow noreferrer"">here</a>. You should set ClusterSpec using the environment variable TF_CONFIG; e.g.</p>

<pre><code>  tf_config = os.environ.get('TF_CONFIG')

  # If TF_CONFIG is not available run local
  if not tf_config:
    return run('', True, *args, **kwargs)

  tf_config_json = json.loads(tf_config)
  cluster = tf_config_json.get('cluster')
  ...
  cluster_spec = tf.train.ClusterSpec(cluster)
</code></pre>
","4392784",1
207,44928569,2,44915792,2017-07-05 14:08:38,3,"<p>As explained by other gcloud --job-dir expects the location to be in GCS. To go around that you can pass it as a folder directly to your module.</p>

<pre><code>gcloud ml-engine local train \
    --package-path trainer \
    --module-name trainer.task \
    -- \
    --train-files $TRAIN_FILE \
    --eval-files $EVAL_FILE \
    --job-dir $JOB_DIR \
    --train-steps $TRAIN_STEPS
</code></pre>
","3210627",3
208,44943544,2,44932098,2017-07-06 08:25:12,0,"<p>Please try to use the code below to convert as you want.</p>

<pre><code>import pandas as pd
import datetime as dt
df['TXN_DATE_2'] = pd.to_datetime(df['TXN_DATE']).dt.date
</code></pre>

<p>Hope it helps.</p>
","4989676",0
209,45063490,2,45023053,2017-07-12 16:49:11,5,"<p>Found my answer, hope this will be useful to someone else:</p>

<p><a href=""https://ml.googleapis.com/v1/projects/"" rel=""noreferrer"">https://ml.googleapis.com/v1/projects/</a>{projectname}/models/{modelname}/versions/{versionname}:predict</p>

<pre><code>{projectname} - the name of your project, all lowercase
{modelname} - the name of your model, all lowercase
{versionname} - the name of your version, all lowercase
</code></pre>
","2589312",1
210,45065843,2,45065842,2017-07-12 19:15:23,3,"<p>After checking the Google API and seeing requests coming through, I thought something might be wrong with the library. I've posted details on my own blog here: <a href=""http://tylerfindlay.com/net-core-googlecloudmlv1predictrequest-execture-method-returns-null-response/"" rel=""nofollow noreferrer"">.NET-Core GoogleCloudMlV1PredictRequest Execture Method Returns null Response</a></p>

<p>What's happening is the GoogleApiHttpBody doesn't serialize the ""instances"" object where the :predict endpoint is expecting it. I figured that out when I read the stream and saw this response:</p>

<pre><code>{""error"": ""&lt;strong&gt;Missing \""instances\"" field in request body&lt;/strong&gt;: {\""httpBody\"":{\""data\"":\""{\\\""instances\\\"":[{\\\""age\\\"":25,\\\""workclass\\\"":\\\"" Private\\\"",\\\""education\\\"":\\\"" 11th\\\"",\\\""education_num\\\"":7,\\\""marital_status\\\"":\\\"" Never - married\\\"",\\\""occupation\\\"":\\\"" Machine - op - inspct\\\"",\\\""relationship\\\"":\\\"" Own - child\\\"",\\\""race\\\"":\\\"" Black\\\"",\\\""gender\\\"":\\\"" Male\\\"",\\\""capital_gain\\\"":0,\\\""capital_loss\\\"":0,\\\""hours_per_week\\\"":40,\\\""native_country\\\"":\\\"" United - States\\\""}]}\""}}""}
</code></pre>

<p>So, I simple changed my code as follows, and now I get the predict result back correctly</p>

<pre><code>string credPath = @"".\appkey.json"";
var json = File.ReadAllText(credPath);
PersonalServiceAccountCred cr = JsonConvert.DeserializeObject(json);

// Create an explicit ServiceAccountCredential credential
var xCred = new ServiceAccountCredential(new ServiceAccountCredential.Initializer(cr.ClientEmail) {
    Scopes = new [] {
        CloudMachineLearningEngineService.Scope.CloudPlatform
    }
}.FromPrivateKey(cr.PrivateKey));

var service = new CloudMachineLearningEngineService(new BaseClientService.Initializer {
    HttpClientInitializer = xCred
});

ProjectsResource.PredictRequest req = new ProjectsResource.PredictRequest(service, new GoogleCloudMlV1PredictRequest(), ""projects/{project-name}/models/census/versions/v1"");

string requestPath = req.Service.BaseUri + 
CloudMachineLearningEngineService.Version + ""/"" + req.Name + "":"" + req.MethodName;
Task result = service.HttpClient.PostAsync(requestPath, new StringContent(""{\""instances\"": [{\""age\"": 25, \""workclass\"": \"" Private\"", \""education\"": \"" 11th\"", \""education_num\"": 7, \""marital_status\"": \"" Never - married\"", \""occupation\"": \"" Machine - op - inspct\"", \""relationship\"": \"" Own - child\"", \""race\"": \"" Black\"", \""gender\"": \"" Male\"", \""capital_gain\"": 0, \""capital_loss\"": 0, \""hours_per_week\"": 40, \""native_country\"": \"" United - States\""}]}""));
Task.WaitAll(result);

HttpResponseMessage responseMessage = result.Result;

Task responseStreamTask = responseMessage.Content.ReadAsStringAsync();
Task.WaitAll(responseStreamTask);

string responseText = responseStreamTask.Result;
</code></pre>
","2589312",2
211,45091578,2,45091099,2017-07-13 21:57:33,7,"<p>You will need to create a callback keras.callbacks.TensorBoard(..) in order to write out the logs. See <a href=""https://keras.io/callbacks/#tensorboard"" rel=""noreferrer"">Tensorboad callback</a>. You can supply GCS path as well (gs://path/to/my/logs) to the log_dir argument of the callback and then point Tensorboard to that location. You will add the callback as a list when calling model.fit_generator(...) or model.fit(...).  </p>

<pre><code>tb_logs = callbacks.TensorBoard(
            log_dir='gs://path/to/logs',
            histogram_freq=0,
            write_graph=True,
            embeddings_freq=0)

model.fit_generator(..., callbacks=[tb_logs])
</code></pre>
","6031363",1
212,45110533,2,45106910,2017-07-14 19:42:56,-1,"<p>Since the arguments you pass after the <code>-- \</code> line will be user arguments, how the program handles these arguments will largely depend on the trainer you defined. I would go back and modify the trainer program and make it either treat the directory differently or take multiple paths like this:</p>

<pre><code>gcloud ml-engine jobs submit training $JOB_NAME \
    --job-dir $OUTPUT_PATH \
    --runtime-version 1.2 \
    --module-name trainer.task \
    --package-path trainer/ \
    --region $REGION \
    --scale-tier STANDARD_1 \
    -- \
    --train-files $TRAIN_DATA \
    --eval-files $EVAL_DATA \
    --train-steps 1000 \
    --verbosity DEBUG  \
    --eval-steps 100
</code></pre>

<p>Some links that will be helpful for developing your own trainer: <a href=""https://cloud.google.com/ml-engine/docs/tutorials/samples"" rel=""nofollow noreferrer"">[1]</a> <a href=""https://cloud.google.com/ml-engine/docs/concepts/trainer-considerations"" rel=""nofollow noreferrer"">[2]</a></p>
","4970438",1
213,45177169,2,45051055,2017-07-18 21:12:02,1,"<p>Here's some more detail on the approach others have outlined above. Try replacing the code currently in the ""Execute Python Script"" module with the following:</p>

<pre><code>import pandas as pd
import os
def azureml_main(dataframe1=None, dataframe2=None):
    print(os.listdir('.'))
    return(pd.DataFrame([]))
</code></pre>

<p>After running the experiment, click on the module. There should be a ""View output log"" link now in the right-hand bar. I get something like the following:</p>

<pre><code>[Information]         Started in [C:\temp]
[Information]         Running in [C:\temp]
[Information]         Executing 4af67c05ba02417a980f6a16e84e61dc with inputs [] and generating outputs ['.maml.oport1']
[Information]         Extracting Script Bundle.zip to .\Script Bundle
[Information]         File Name                                             Modified             Size
[Information]         temp.csv                                       2016-05-06 13:16:56           52
[Information]         [ READING ] 0:00:00
[Information]         ['4af67c05ba02417a980f6a16e84e61dc.py', 'Script Bundle', 'Script Bundle.zip']
</code></pre>

<p>This tells me that the contents of my zip file have been extracted to the <code>C:\temp\Script Bundle</code> folder. In my case the zip file contained just one CSV file, <code>temp.csv</code>: your output would probably have four files. You may also have zipped a folder containing your four files, in which case the filepath would be one layer deeper. You can use the <code>os.listdir()</code> to explore your directory structure further if necessary.</p>

<p>Once you think you know the full filepaths for your CSV files, edit your Execute Python Script module's code to load them, e.g.:</p>

<pre><code>import pandas as pd
def azureml_main(dataframe1 = None, dataframe2 = None):
    df = pd.read_csv('C:/temp/Script Bundle/temp.csv')
    # ...load other files and merge into a single dataframe...
    return(df)
</code></pre>

<p>Hope that helps!</p>
","6722050",0
214,45201314,2,45200137,2017-07-19 21:26:30,1,"<p>Stackdriver logs (in general, not Cloud ML Engine logs, specifically) are not guaranteed to arrive immediately. Your logs are probably just delayed. Can you let us know if they don't show up within an hour? We apologize for the inconvenience.</p>
","1399222",1
215,45358113,2,45337137,2017-07-27 18:08:37,3,"<p>Thanks for the very detailed question. </p>

<p>To explain what happened here, in the first case Cloud ML Engine used an internal service account (the one that is added to your project with the <code>Cloud ML Service Agent</code> role). Due to some internal security considerations, that service account is restricted from accessing BigQuery, so hence the first 403 error that you saw.</p>

<p>Now, when you replaced machine credentials with your own service account using the <code>.json</code> credentials file, that restriction went away. However your service account didn't have all the access to the internal systems, such as the pubsub service used for Hyperparameter tuning mechanism internally. Hence the pubsub error in the second case.</p>

<p>There are a few possible solutions to this problem:</p>

<ul>
<li><p>on the Cloud ML Engine side, we're working on better BigQuery support out-of-the-box, although we don't have an ETA at this point.</p></li>
<li><p>your approach with a custom service account might work as a short-term solution as long as you don't use Hyperparameter tuning. However this is obviously fragile because it depends on the implementation details in Cloud ML Engine, so I wouldn't recommend relying on this long-term</p></li>
<li><p>finally, consider exporting data from BigQuery to GCS first and using GCS to read training data. This scenario is well-supported in Cloud ML Engine. Besides you'll get performance gains on large datasets compared to reading BigQuery directly: the current implementation of <a href=""https://github.com/tensorflow/tensorflow/blob/838a19df11b53656d081447a74b7fb29aa18b5ac/tensorflow/contrib/cloud/python/ops/bigquery_reader_ops.py#L26"" rel=""nofollow noreferrer""><code>BigQueryReader</code></a> in TensorFlow has suboptimal perf characteristics, which we're also working to improve. </p></li>
</ul>
","3818582",0
216,45381837,2,45375031,2017-07-28 20:23:36,1,"<p>The model your exported accepts the input like following for prediction if you use gcloud to submit your requests for <code>gcloud ml-engine local predict</code> as well as batch prediction.</p>

<pre><code>{""inputs"": [[[242, 240, 239], [242, 240, 239], [242, 240, 239], [242, 240, 239], [242, 240, 23]]]}
{""inputs"": [[[232, 242, 219], [242, 240, 239], [242, 240, 239], [242, 242, 239], [242, 240, 123]]]}
...
</code></pre>

<p>If you're sending the requests directly to the service (i.e., not using <code>gcloud</code>), the body of the request would look like:</p>

<pre><code>{""instances"": [{""inputs"": [[[242, 240, 239], [242, 240, 239], [242, 240, 239], [242, 240, 239], [242, 240, 23]]]}]}
{""instances"": [{""inputs"": [[[232, 242, 219], [242, 240, 239], [242, 240, 239], [242, 242, 239], [242, 240, 123]]]}]}
</code></pre>

<p>The input tensor name should be ""inputs"" because it is <a href=""https://github.com/derekjchow/models/blob/master/object_detection/exporter.py#L262"" rel=""nofollow noreferrer"">what we've specified</a> in the signature.inputs.The value of each JSON object is a 3-D array as you can tell from <a href=""https://github.com/derekjchow/models/blob/master/object_detection/exporter.py#L117"" rel=""nofollow noreferrer"">here</a>. The outer dimension is None to support batched-input. No ""instances"" is needed (unless you use the http API directly). Note that you cannot specify ""key"" in the input unless you modify the graph to include an extra placeholder and output it untouched using tf.identity. </p>

<p>Also as mentioned in <a href=""https://github.com/tensorflow/models/issues/1811"" rel=""nofollow noreferrer"">the github issue</a>, the online service may not work due to the large memory the model requires. We are working on that.</p>
","6896656",7
217,45721373,2,45720464,2017-08-16 19:18:07,3,"<p>The metric function in your case can be implemented using <code>tf.metrics</code> using an moving average of the loss:</p>

<pre><code>def metric_fn(labels, predict):
   loss = tf.contrib.legacy_seq2seq.sequence_loss(logits, labels, weights)
   mean, op = tf.metrics.mean(loss)
   return mean, op
</code></pre>
","8143158",2
218,45988076,2,45986578,2017-08-31 18:39:09,1,"<p>The short answer is that <code>tf.estimator</code> is currently mostly built around Data-parallel training (2).</p>

<p>You get Model-parallel training simply by using <code>with tf.device()</code> statements in your code.</p>

<p>You could try to use <a href=""https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/training/sync_replicas_optimizer.py#L41"" rel=""nofollow noreferrer"">SyncReplicasOptimizer</a> and probably accomplish synchronous training (1). </p>

<p>All of the above applies generally to <code>tf.estimator</code>; nothing is different for CloudML Engine.</p>
","1399222",3
219,45994422,2,45989971,2017-09-01 06:30:15,11,"<p>The previous answer did a good job at explaining the performance bottlenecks. Let me explain about ""epochs"" and how TensorFlow processes datasets.</p>

<p>The way that distributed training works in TensorFlow is that each worker independently iterates through the entire dataset. It is a common misconception that the training set is partitioned amongst the workers, but this is not the case.</p>

<p>In a typical setup with queues (see <a href=""https://www.tensorflow.org/programmers_guide/threading_and_queues"" rel=""noreferrer"">this tutorial</a>), what happens is each worker creates it's own queue. That queue gets filled with a list of all the filenames of all the training files (typically the list is shuffled and everytime the queue is exhausted, it gets repopulated and reshuffled). Each file is read in instance-by-instance, and the data is parsed, preprocessed, and then fed into another queue where the instances are shuffled and batched. Once the last instance of any file is read, the next filename is popped off the filename queue. If there are no more files to pop, an ""epoch"" has completed.</p>

<p>The important point here is that all of these queues are by default <em>local</em> -- not shared. So every worker is independently repeating the same work -- creating queues with all the files and iterating through the entire dataset. A full epoch, then, is roughly equal to the number of instances in the full dataset * the number of workers. (I'm not sure about your standard_1 result, but the result on CUSTOM means you have your master + 5 workers = 6 workers * 650K examples * (1 batch / 128 examples) = 31K steps).</p>

<p>FYI the use of epochs are discouraged for parameterizing distributed training because it's too confusing and there may even be issues with it in general. Just stick with max_steps.</p>

<p>Note that, as a consequence of TensorFlow's design, ""batch size"" means the batch size <em>of each worker</em>. But each worker is going to be sending updates to the parameter servers at roughly the same rate, so over a time period roughly equivalent to the time need to process one ""batch"", the number of updates that happen to the parameters are roughly <code>batch_size</code> * <code>num_workers</code>. This is what we call the <em>effective</em> batch size. This in turn has a few practical consequences:</p>

<ol>
<li>You tend to use smaller batch_sizes, especially if you have a large number of workers, so that the <em>effective</em> batch size stays sane.</li>
<li>As you increase the number of workers, your effective batch size increase and hence you need to decrease your learning rate, at least when using ""vanilla"" stochastic gradient descent. Optimizers with adaptive learning rates (such as Adagrad, Adam, etc.) tend to be robust to the initial learning rates, but if you add enough workers you still may need to adjust the learning rate.</li>
</ol>

<p>You may wonder why TensorFlow handles training data in this fashion. It's because in distributed systems, you can't rely on machines being the same speeds, or even being reliable at all. If you partition the training data into disjoint sets that go to each worker and then one or more machines are slow relative to the other or the network goes down on one, etc., your training process will see the data from the ""fast""/reliable workers more frequently than the ""slow""/unreliable workers. That biases the results towards those instances (or in extreme cases, ignores it all together).</p>
","1399222",10
220,46002534,2,46001982,2017-09-01 14:32:33,2,"<p>MAIN QUESTION:</p>

<p>The 2 codes your placed actually are 2 different parts of the training, Line 282/294 in my options is so called ""pre-processing"" part, for it's parse raw input data into Tensors, this operations not suitable for GPU accelerating, so it will be sufficient if allocated on CPU.</p>

<p>Line 152/152 is part of the training model for it's processing the raw feature into different type of features.</p>

<ol>
<li><p>'cpu:0' means the operations of this section will be allocated on CPU, but not bind to specified core. The operations allocated on CPU will run in multi-threads and use multi-cores.</p></li>
<li><p>If your running machine has GPUs, the TensorFlow will prefer allocating the operations on GPUs if the device is not specified.</p></li>
</ol>
","4922660",8
221,46004831,2,46002510,2017-09-01 16:52:48,0,"<p>Short answer: You should not pass the decryption key into the training job.  Instead see <a href=""https://cloud.google.com/kms/docs/store-secrets"" rel=""nofollow noreferrer"">https://cloud.google.com/kms/docs/store-secrets</a></p>

<p>Long answer: While you could technically make the decryption key a flag that gets passed through the Training Job definition, this would expose it to anyone with access to List Training Jobs.  You should instead place the key in the Google Cloud Key Management Service and give the service account running the ML training job permission to fetch the key from there.</p>

<p>You can determine the service account that runs the training job by following the procedure listed at <a href=""https://cloud.google.com/ml-engine/docs/how-tos/working-with-data#using_a_cloud_storage_bucket_from_a_different_project"" rel=""nofollow noreferrer"">https://cloud.google.com/ml-engine/docs/how-tos/working-with-data#using_a_cloud_storage_bucket_from_a_different_project</a></p>

<p>Edit: Also note what Alexey says in the comment below; Tensorflow won't currently be able to read and decrypt the files directly from GCS, you'll need to copy them to local disk on every worker with the keys supplied to <code>gsutil cp</code>.</p>
","5441818",3
222,46070325,2,44881303,2017-09-06 08:28:58,1,"<p>one way is to use a Python code block in AzureML, something like this:</p>

<pre><code>import pandas as p
import pymongo as m

def azureml_main():
    c = m.MongoClient(host='host_IP')
    a = p.DataFrame(c.database_names())
    return a
</code></pre>
","471372",0
223,46099403,2,46095110,2017-09-07 14:51:25,0,"<p>There are likely two problems.</p>

<p>First, it looks as though the graph that you are exporting is expecting tf.Example protos as input, i.e. has a parse_example(...) op in it. The Boston sample does not appear to be adding that op, so I suspect that is part of your modifications.</p>

<p>Before showing the code you want for the input_fn, we need to talk about the second problem: versioning. Estimators existed in previous versions of TensorFlow under tensorflow.contrib. However, various parts have migrated into tensorflow.estimator with successive TensorFlow versions and the APIs have changed as they've moved.</p>

<p>CloudML Engine currently (as of 07 Sep 2017) only supports TF 1.0 and 1.2, so I'll provide a solution that works with 1.2. This is based on the <a href=""https://github.com/GoogleCloudPlatform/cloudml-samples/tree/master/census/estimator/trainer"" rel=""nofollow noreferrer"">census sample</a>. This is the input_fn you need in order to use CSV data, although I generally recommend exporting models that are independent of input format:</p>

<pre><code># Provides the data types for the various columns.
FEATURE_DEFAULTS=[[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0], [0.0]]

def predict_input_fn(rows_string_tensor):
  # Takes a rank-1 tensor and converts it into rank-2 tensor
  # Example if the data is ['csv,line,1', 'csv,line,2', ..] to
  # [['csv,line,1'], ['csv,line,2']] which after parsing will result in a
  # tuple of tensors: [['csv'], ['csv']], [['line'], ['line']], [[1], [2]]
  row_columns = tf.expand_dims(rows_string_tensor, -1)
  columns = tf.decode_csv(row_columns, record_defaults=FEATURE_DEFAULTS)
  features = dict(zip(FEATURES, columns))

  return tf.contrib.learn.InputFnOps(features, None, {'csv_row': csv_row})
</code></pre>

<p>And you'll need an export strategy like this:</p>

<pre><code>saved_model_export_utils.make_export_strategy(
    predict_input_fn,
    exports_to_keep=1,
    default_output_alternative_key=None,
)
</code></pre>

<p>which you'll pass as a list of size 1 to the constructor of <a href=""https://github.com/tensorflow/tensorflow/blob/18f36927160d05b941c056f10dc7f9aecaa05e23/tensorflow/contrib/learn/python/learn/experiment.py#L141"" rel=""nofollow noreferrer""><code>tf.contrib.learn.Experiment</code></a>.</p>
","1399222",2
224,46160031,2,46120660,2017-09-11 16:11:30,2,"<p>From what I can tell from the <a href=""https://github.com/tensorflow/tensorflow/blob/702d595822e9e5f5232b8140c6296683612c33a9/tensorflow/python/keras/_impl/keras/preprocessing/text.py#L112"" rel=""nofollow noreferrer"">source code</a>, it appears that even TensorFlow's Keras-compatible library is doing Tokenization <em>in Python</em>, i.e., not as part of the TensorFlow graph.</p>

<p>At this time, CloudML Engine only supports serving TensorFlow models where all of the logic is encoded in a TensorFlow graph. That means you'll have to do the tokenization client side and pass the results onto the server for prediction. This would involve coding the <em>client</em> to deserialize the <code>Tokenizer</code> and call <code>tokenizer.texts_to_sequences</code> for the inputs for which predictions are desired. </p>

<p>We recognize that this is not always ideal (a non-starter for non-Python clients and inconvenient, at least, even for Python clients) and are actively investigating solutions for allowing arbitrary Python code to be run as part of prediction.</p>
","1399222",0
225,46181604,2,46177828,2017-09-12 16:36:57,0,"<p>Besides the issue you mentioned, there are actual multiple additional issues I foresee you running into:</p>

<ul>
<li>You are using <code>tf.estimator.DnnRegressor</code> which was introduced in TensorFlow 1.3. CloudML Engine only officially supports TF 1.2.</li>
<li>You are normalizing the features in the panda dataframe, and that won't happen at serving time (unless you do it client side). This introduces skew and you'll get poor prediction results.</li>
</ul>

<p>So let's start by using <code>tf.contrib.learn.DNNRegressor</code>, which only requires minor changes:</p>

<pre><code>regressor = tf.estimator.DNNRegressor(
    feature_columns=feature_columns,
    hidden_units=[40, 30, 20],
    model_dir=""model1"",
    optimizer='RMSProp'
)
regressor.fit(input_fn=train_input_fn, steps=5)
regressor.export_savedmodel(""test"",json_serving_input_fn)
</code></pre>

<p>Note the <code>fit</code> instead of <code>train</code>.</p>

<p>(<strong>NB:</strong> your <code>json_serving_inputfn</code> is actually already written for TF 1.2 and is incompatible with TF 1.3. Which is good for now).</p>

<p>Now, the root cause of the error that you are seeing is that the column/features <code>ad_provider</code> is not in the list of inputs and features (but you do have <code>ad_provider_indicator</code>). This is because you are iterating through <code>feature_columns</code> and not through the original input column list. The way to address that is by iterating over the actual inputs instead of the feature columns; however, we'll need to know the types, too (simplified with just a few columns):</p>

<pre><code>CSV_COLUMNS = [""ad_provider"", ""gold"", ""video_success""] 
FEATURES = [""ad_provider"", ""gold""] 
TYPES = [tf.string, tf.float32] 
LABEL = ""video_success"" 

def json_serving_input_fn(): 
  """"""Build the serving inputs."""""" 
  inputs = {} 
  for feat, dtype in zip(FEATURES, TYPES): 
    inputs[feat] = tf.placeholder(shape=[None], dtype=dtype) 

  features = {
    key: tf.expand_dims(tensor, -1)
    for key, tensor in inputs.items()
  }
  return tf.contrib.learn.InputFnOps(features, None, inputs)
</code></pre>

<p>Finally, to normalize your data, you'll probably want to do that in the graph. You could try using <a href=""https://github.com/tensorflow/transform"" rel=""nofollow noreferrer""><code>tf.transform</code></a>, or, alternatively, write a custom estimator that does the transformation, delegating the actual model implementation DNNRegressor.</p>
","1399222",1
226,46208255,2,46193201,2017-09-13 23:27:37,1,"<p>You can request for more ML unit quota (as well as other quota) by following the steps described here: <a href=""https://cloud.google.com/ml-engine/quotas#requesting_a_quota_increase"" rel=""nofollow noreferrer"">https://cloud.google.com/ml-engine/quotas#requesting_a_quota_increase</a></p>

<p>The most direct link is: <a href=""https://docs.google.com/a/google.com/forms/d/1f-tmYYl8uiV6KcWFLsNKwWnTvk4KwQYsCzZHcb3g2YM/viewform"" rel=""nofollow noreferrer"">Cloud Machine Learning Engine Quota Request form</a></p>
","3191940",1
227,46346667,2,46340959,2017-09-21 14:31:06,1,"<p>You can author custom R Modules. </p>

<p>Here is some documentation: 
<a href=""https://blogs.technet.microsoft.com/machinelearning/2015/04/23/build-your-own-r-modules-in-azure-ml/"" rel=""nofollow noreferrer"">https://blogs.technet.microsoft.com/machinelearning/2015/04/23/build-your-own-r-modules-in-azure-ml/</a>
<a href=""https://learn.microsoft.com/en-us/azure/machine-learning/machine-learning-custom-r-modules"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/machine-learning/machine-learning-custom-r-modules</a></p>
","8376649",1
228,46498781,2,46486796,2017-09-30 01:38:47,0,"<p>Can you verify that you're exporting the model with tensorflow 1.2?</p>

<p>gcloud ml-engine local predict doesn't have the --runtime-version flag, so if you have TF 1.3 installed and exported your model with that, then local predict would work using TF 1.3, but there may be incompatibilities in the model when trying to use TF 1.2 on the service.</p>
","8698478",0
229,46712599,2,46690216,2017-10-12 14:47:02,1,"<p>Update: this issue should now be fixed with the most recent version of gcloud. Can you give it a try and see if it works for you? First do:<br>
<code>gcloud components update</code></p>

<p><s>What's happening is that gcloud is (silently) requiring py2.7, which is causing your import error.  This is a bug that we will fix soon.  (It's particularly problematic for Windows, since TF doesn't support a 2.7 install for windows). </s>  We'll update here when it's fixed.</p>

<p>In the meantime, the best option is probably to test locally by just running your python script directly (unless you are trying to test distributed training locally).</p>

<p>If you are trying to test distributed training locally, then your best temporary option is probably to use Docker and the <a href=""https://www.tensorflow.org/install/install_linux#installing_with_docker"" rel=""nofollow noreferrer"">TensorFlow docker container</a>.</p>
","992563",1
230,46800986,2,46799261,2017-10-18 00:12:49,3,"<p>The output of your <code>exact_match_fn</code> is an op which can be used for evaluation. If you want the average over a batch, change your <code>reduce_min</code> to just reduce over the relevant axis.</p>

<p>E.g. if you <code>y_true</code>/<code>y_logits</code> each have shape <code>(batch_size, n)</code></p>

<pre><code>def exact_match_fn(y_true, y_logits, threshold):
    #pred = tf.equal(tf.round(y_logits), tf.round(y_true))
    predictions = tf.to_float(tf.greater_equal(y_logits, threshold))
    pred_match = tf.equal(predictions, tf.round(y_true))
    exact_match = tf.reduce_min(tf.to_float(pred_match), axis=1)
    return exact_match


def exact_match_prop_fn(*args):
    return tf.reduce_mean(exact_match_fn(*args))
</code></pre>

<p>This will give you the average over a batch. If you want the average over an entire dataset, I'd just collect the matches (or <code>correct</code> and <code>total</code> counts) and evaluate outside of the session/tensorflow, but <code>streaming_mean</code> probably does just that, not sure.</p>
","3098092",1
231,46803428,2,46801674,2017-10-18 05:35:58,1,"<p>Looks like this was <em>actually</em> a Cloud ML bug. It has been fixed! Thank you for the super fast turnaround.</p>
","5479078",2
232,46812526,2,46797468,2017-10-18 14:25:07,1,"<p>There is an issue with using custom NET# and parameter sweeps together: it switches over to using the default fully connected topology. </p>

<p>Unfortunately, the workaround is to train the model for each parameter value separately.  </p>

<p>-Roope  </p>
","5784983",0
233,46935400,2,45753090,2017-10-25 14:42:24,0,"<p>Deleting and creating the endpoint again fixed it.</p>

<p>Microsoft...</p>
","7037695",0
234,46936284,2,46900593,2017-10-25 15:22:20,0,"<p>Azure Machine Learning supports Hive but not over ADLS. </p>
","534681",0
235,47492332,2,47488544,2017-11-26 02:00:20,0,"<p>In Azure ML Studio, Python scripts (and R scripts for that matter) run in a sandbox so cannot access resources over the network. See <a href=""https://learn.microsoft.com/en-us/azure/machine-learning/studio/execute-python-scripts?#limitations"" rel=""nofollow noreferrer"">limitations</a>:</p>
<blockquote>
<p>The Execute Python Script currently has the following limitations:</p>
<ol>
<li>Sandboxed execution. The Python runtime is currently sandboxed and, as
a result, does not allow access to the network...</li>
</ol>
</blockquote>
<p>So if you want to read from a blob use the separate Import Data module and if you want to write to a blob use the separate Export Data module.</p>
","5070440",0
236,47518752,2,47460981,2017-11-27 19:50:51,1,"<p>Ashvin [MSFT]</p>

<p>Sorry to hear that you were facing issues. We checked logs on our side using the info you provided in the screenshot. The cluster setup failed because there weren't enough cores to fit AzureML and system components in the cluster. You specified agent-vm-size of D1v2 which has 1 CPU core. By default we create 2 agents so total cores were 2. To resolve, can you please try creating a new cluster without specifying agent size? Then AzureML will create 2 agents of D3v2 which is 8 cores total. This should fit the AzureML and system components and leave some room for you to deploy your services. </p>

<p>If you wish a bigger cluster you could specify agent-count along with agent-vm-size to appropriately size your cluster but please have minimum total of 8 cores with each individual VM >= 2 cores to ensure cluster works smoothly. Hope this helps.</p>

<p>We are working on our side to add error handling to ensure request fails with clear error message. </p>
","9016659",0
237,47557224,2,47097272,2017-11-29 16:16:35,0,"<p>It's not possible as far as I'm aware :(</p>

<p>However, the general rule is that you should flatten a relational graph into a single array of values.  Remember, though, that you should have one array of values per main entity, it looks to me like your main entity in your example is the one with the Visits in.</p>

<p>Effectively, you'd be saying that all diagnoses are a property of Visit, but because there's potentially more than one, you'd have to have properties such as Diagnosis1, Diagnosis2, Diagnosis3 ...etc.</p>
","1073157",0
238,47651977,2,47625056,2017-12-05 11:00:55,7,"<p>I'm not sure about Amazon ML but SageMaker uses the docker containers listed here for the built-in training: <a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-algo-docker-registry-paths.html"" rel=""noreferrer"">https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-algo-docker-registry-paths.html</a></p>

<p>So, in general, anything you can do with Amazon ML you should be able to do with SageMaker (although Amazon ML has a pretty sweet schema editor).</p>

<p>You can check out each of those containers to dive deep on how it all works.</p>

<p>You can find an exhaustive list of available algorithms in SageMaker here:
<a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/algos.html"" rel=""noreferrer"">https://docs.aws.amazon.com/sagemaker/latest/dg/algos.html</a></p>

<p>For now, as of December 2017, these algorithms are all available:</p>

<ul>
<li><a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/linear-learner.html"" rel=""noreferrer"">Linear Learner</a></li>
<li><a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/fact-machines.html"" rel=""noreferrer"">Factorization Machines</a></li>
<li><a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/xgboost.html"" rel=""noreferrer"">XGBoost Algorithm</a></li>
<li><a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/image-classification.html"" rel=""noreferrer"">Image Classification Algorithm</a></li>
<li><a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/seq-2-seq.html"" rel=""noreferrer"">Amazon SageMaker Sequence2Sequence</a></li>
<li><a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/k-means.html"" rel=""noreferrer"">K-Means Algorithm</a></li>
<li><a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/pca.html"" rel=""noreferrer"">Principal Component Analysis (PCA)</a></li>
<li><a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/lda.html"" rel=""noreferrer"">Latent Dirichlet Allocation (LDA)</a></li>
<li><a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/ntm.html"" rel=""noreferrer"">Neural Topic Model (NTM)</a></li>
</ul>

<p>The general SageMaker SDK interface to these algorithms looks something like this:</p>

<pre><code>from sagemaker import KMeans
kmeans = KMeans(role=""SageMakerRole"",
                train_instance_count=2,
                train_instance_type='ml.c4.8xlarge',
                data_location=""s3://training_data/"",
                output_path=""s3://model_artifacts/"",
                k=10)
</code></pre>

<p>The libraries here: <a href=""https://github.com/awslabs/amazon-sagemaker-examples"" rel=""noreferrer"">https://github.com/awslabs/amazon-sagemaker-examples</a>
and here: <a href=""https://github.com/aws/sagemaker-python-sdk"" rel=""noreferrer"">https://github.com/aws/sagemaker-python-sdk</a> are particularly useful for playing with SageMaker.</p>

<p>You can also make use of Spark with SageMaker the Spark library here: <a href=""https://github.com/aws/sagemaker-spark"" rel=""noreferrer"">https://github.com/aws/sagemaker-spark</a></p>
","240004",1
239,47840630,2,47838705,2017-12-15 22:21:40,5,"<p>Amazon SageMaker is a very new service (December 2017).</p>

<p>You will need to update your boto library to use it:</p>

<pre><code>sudo pip install boto --upgrade
sudo pip install boto3 --upgrade
sudo pip install awscli --upgrade
</code></pre>
","174777",1
240,47984264,2,47840209,2017-12-26 22:15:23,3,"<p>Rather than using curl, it's recommended that you use the SageMaker Runtime client to send data and get back inferences from a SageMaker Endpoint:</p>

<p><a href=""http://docs.aws.amazon.com/sagemaker/latest/dg/API_runtime_InvokeEndpoint.html"" rel=""nofollow noreferrer"">http://docs.aws.amazon.com/sagemaker/latest/dg/API_runtime_InvokeEndpoint.html</a></p>
","4941254",0
241,48267500,2,48267427,2018-01-15 16:55:53,2,"<p>Have you set the backend?</p>

<pre><code>%matplotlib inline
</code></pre>

<p>Worth reading about what this does for a notebook here too
<a href=""https://stackoverflow.com/questions/43027980/purpose-of-matplotlib-inline/43028034"">Purpose of &quot;%matplotlib inline&quot;</a></p>
","8412492",0
242,48483174,2,48438202,2018-01-28 04:31:53,4,"<p>When you are using any of the AWS SDK (including the one for Amazon SageMaker), you need to configure the credentials of your AWS account on the machine that you are using to run your code. If you are using your local machine, you can use the AWS CLI flow. You can find detailed instructions on the Python SDK page: <a href=""https://aws.amazon.com/developers/getting-started/python/"" rel=""nofollow noreferrer"">https://aws.amazon.com/developers/getting-started/python/</a> </p>

<p>Please note that when you are deploying the code to a different machine, you will have to make sure that you are giving the EC2, ECS, Lambda or any other target a role that will allow the call to this specific endpoint. While in your local machine it can be OK to give you admin rights or other permissive permissions, when you are deploying to a remote instance, you should restrict the permissions as much as possible. </p>

<pre><code>{
    ""Version"": ""2012-10-17"",
    ""Statement"": [
        {
            ""Sid"": ""VisualEditor0"",
            ""Effect"": ""Allow"",
            ""Action"": ""sagemaker:InvokeEndpoint"",
            ""Resource"": ""arn:aws:sagemaker:*:1234567890:endpoint/&lt;my-endpoint-name&gt;""
        }
    ]
}
</code></pre>
","179529",2
243,48603753,2,48507471,2018-02-04 01:12:42,2,"<p>Distributed training requires having a way to sync the results of the training between the training workers. Most of the traditional libraries, such as scikit-learn are designed to work with a single worker, and can't just be used in a distributed environment. Amazon SageMaker is distributing the data across the workers, but it is up to you to make sure that the algorithm can benefit from the multiple workers. Some algorithms, such as Random Forest, are easier to take advantage of the distribution, as each worker can build a different part of the forest, but other algorithms need more help. </p>

<p>Spark MLLib has distributed implementations of popular algorithms such as k-means, logistic regression, or PCA, but these implementations are not good enough for some cases. Most of them were too slow and some even crushed when a lot of data was used for the training. The Amazon SageMaker team reimplemented many of these algorithms from scratch to benefit from the scale and economics of the cloud (20 hours of one instance costs the same as 1 hour of 20 instances, just 20 times faster). Many of these algorithms are now more stable and much faster beyond the linear scalability. See more details here: <a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/algos.html"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/sagemaker/latest/dg/algos.html</a></p>

<p>For the deep learning frameworks (TensorFlow and MXNet) SageMaker is using the built-in parameters server that each one is using, but it is taking the heavy lifting of the building the cluster and configuring the instances to communicate with it. </p>
","179529",5
244,48985220,2,48365866,2018-02-26 09:32:35,1,"<p>Later solved it by <code>Encoding.ASCII.GetBytes</code>as in below code.</p>

<pre><code> byte[] bytes = System.IO.File.ReadAllBytes(@""EXCEL_FILE_PATH"");
    string listA = """";
    while (!reader.EndOfStream)
        {
            var line = reader.ReadLine();
            listA = listA + line + ""\n"";
        }
    byte[] bytes = Encoding.ASCII.GetBytes(listA);
    request.Body = new MemoryStream(bytes);
    InvokeEndpointResponse response = sagemakerRunTimeClient.InvokeEndpoint(request);
    string predictions = Encoding.UTF8.GetString(response.Body.ToArray());
</code></pre>
","1412010",0
245,49002443,2,48990264,2018-02-27 06:27:30,1,"<p>Here is a <a href=""https://blogs.technet.microsoft.com/machinelearning/2016/10/26/speeding-up-azure-ml-web-services-containing-r-or-python-modules/?"" rel=""nofollow noreferrer"">clever trick</a> for running initialization steps only the first time and not on every subsequent call.</p>
<p>My understanding is that you would wrap the first 3 statements (that is through line 11) of your script in the following <code>if</code> statement:</p>
<pre><code>if (!is.element(&quot;my_env&quot;, search()))
</code></pre>
<p>The <code>if</code> statement would also contain the initialization of the <code>my_env</code> variable as shown in the example used in that blog:</p>
<p><a href=""https://i.stack.imgur.com/S445n.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/S445n.png"" alt=""R optimization example"" /></a></p>
","5070440",0
246,49056630,2,49056593,2018-03-01 18:45:28,2,"<p>Yes, there is a open source online gernator on the net (<a href=""http://jsonutils.com/"" rel=""nofollow noreferrer"">http://jsonutils.com/</a>). Copy paste your result will give you that:</p>

<pre><code> public class Value
    {
        public IList&lt;string&gt; ColumnNames { get; set; }
        public IList&lt;string&gt; ColumnTypes { get; set; }
        public IList&lt;IList&lt;string&gt;&gt; Values { get; set; }
    }

    public class WSOutput
    {
        public string type { get; set; }
        public Value value { get; set; }
    }

    public class Results
    {
        public WSOutput WSOutput { get; set; }
    }

    public class Example
    {
        public Results Results { get; set; }
    }
</code></pre>
","1163423",1
247,49099320,2,49064183,2018-03-04 19:11:02,3,"<p>You can have multiple Production Variants behind an Amazon SageMaker endpoint. Each production variant has an initial variant weight and based on the ratio of each variant weight to the total sum of weights, SageMaker can distribute the calls to each of the models. For example, if you have only one production variant with a weight of 1, all traffic will go to this variant. If you add another production variant with an initial weight of 2, the new variant will get 2/3 of the traffic and the first variant will get 1/3. </p>

<p>You can see more details on ProductionVariant on Amazon SageMaker documentations here: <a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/API_ProductionVariant.html"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/sagemaker/latest/dg/API_ProductionVariant.html</a> </p>

<p>You can provide an array of ProductionVariants when you ""Create Endpoint Configuration"": <a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/API_CreateEndpointConfig.html"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/sagemaker/latest/dg/API_CreateEndpointConfig.html</a> , and you can update the variants with ""Update Endpoint Weights and Capacities"" call: <a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/API_UpdateEndpointWeightsAndCapacities.html"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/sagemaker/latest/dg/API_UpdateEndpointWeightsAndCapacities.html</a></p>
","179529",0
248,49124471,2,49006174,2018-03-06 06:14:55,1,"<p>Your python script should implement a few methods like train, model_fn, transform_fn, input_fn etc. SagaMaker would call appropriate method when needed. </p>

<p><a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/mxnet-training-inference-code-template.html"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/sagemaker/latest/dg/mxnet-training-inference-code-template.html</a></p>
","6530653",0
249,49600781,2,31130629,2018-04-01 17:33:08,0,"<p>Looks like it isn't possible to set this timeout based on <a href=""https://feedback.azure.com/forums/257792-machine-learning/suggestions/6472562-configurable-timeout-for-experiments-and-web-servi"" rel=""nofollow noreferrer"">a feature request that is still marked as ""planned"" as of 4/1/2018</a>.</p>

<p>The recommendation from <a href=""https://social.msdn.microsoft.com/Forums/sqlserver/en-US/cb4ee96d-c2ca-4c65-b02f-0ccb26181f7f/timeout-in-web-service?forum=MachineLearning"" rel=""nofollow noreferrer"">MSDN forums from 2017</a> is to use the Batch Execution Service, which starts the machine learning experiment and then asynchronously asks whether it's done.</p>

<p>Here's a code snippet from the Azure ML Web Services Management Sample Code (all comments are from their sample code):</p>

<pre><code>        using (HttpClient client = new HttpClient())
        {
            var request = new BatchExecutionRequest()
            {

                Outputs = new Dictionary&lt;string, AzureBlobDataReference&gt; () {
                    {
                        ""output"",
                        new AzureBlobDataReference()
                        {
                            ConnectionString = storageConnectionString,
                            RelativeLocation = string.Format(""{0}/outputresults.file_extension"", StorageContainerName) /*Replace this with the location you would like to use for your output file, and valid file extension (usually .csv for scoring results, or .ilearner for trained models)*/
                        }
                    },
                },    

                GlobalParameters = new Dictionary&lt;string, string&gt;() {
                }
            };

            client.DefaultRequestHeaders.Authorization = new AuthenticationHeaderValue(""Bearer"", apiKey);

            // WARNING: The 'await' statement below can result in a deadlock
            // if you are calling this code from the UI thread of an ASP.Net application.
            // One way to address this would be to call ConfigureAwait(false)
            // so that the execution does not attempt to resume on the original context.
            // For instance, replace code such as:
            //      result = await DoSomeTask()
            // with the following:
            //      result = await DoSomeTask().ConfigureAwait(false)

            Console.WriteLine(""Submitting the job..."");

            // submit the job
            var response = await client.PostAsJsonAsync(BaseUrl + ""?api-version=2.0"", request);

            if (!response.IsSuccessStatusCode)
            {
                await WriteFailedResponse(response);
                return;
            }

            string jobId = await response.Content.ReadAsAsync&lt;string&gt;();
            Console.WriteLine(string.Format(""Job ID: {0}"", jobId));

            // start the job
            Console.WriteLine(""Starting the job..."");
            response = await client.PostAsync(BaseUrl + ""/"" + jobId + ""/start?api-version=2.0"", null);
            if (!response.IsSuccessStatusCode)
            {
                await WriteFailedResponse(response);
                return;
            }

            string jobLocation = BaseUrl + ""/"" + jobId + ""?api-version=2.0"";
            Stopwatch watch = Stopwatch.StartNew();
            bool done = false;
            while (!done)
            {
                Console.WriteLine(""Checking the job status..."");
                response = await client.GetAsync(jobLocation);
                if (!response.IsSuccessStatusCode)
                {
                    await WriteFailedResponse(response);
                    return;
                }

                BatchScoreStatus status = await response.Content.ReadAsAsync&lt;BatchScoreStatus&gt;();
                if (watch.ElapsedMilliseconds &gt; TimeOutInMilliseconds)
                {
                    done = true;
                    Console.WriteLine(string.Format(""Timed out. Deleting job {0} ..."", jobId));
                    await client.DeleteAsync(jobLocation);
                }
                switch (status.StatusCode) {
                    case BatchScoreStatusCode.NotStarted:
                        Console.WriteLine(string.Format(""Job {0} not yet started..."", jobId));
                        break;
                    case BatchScoreStatusCode.Running:
                        Console.WriteLine(string.Format(""Job {0} running..."", jobId));
                        break;
                    case BatchScoreStatusCode.Failed:
                        Console.WriteLine(string.Format(""Job {0} failed!"", jobId));
                        Console.WriteLine(string.Format(""Error details: {0}"", status.Details));
                        done = true;
                        break;
                    case BatchScoreStatusCode.Cancelled:
                        Console.WriteLine(string.Format(""Job {0} cancelled!"", jobId));
                        done = true;
                        break;
                    case BatchScoreStatusCode.Finished:
                        done = true;
                        Console.WriteLine(string.Format(""Job {0} finished!"", jobId));
                        ProcessResults(status);
                        break;
                }

                if (!done) {
                    Thread.Sleep(1000); // Wait one second
                }
            }
        }
</code></pre>
","2585099",0
250,49676109,2,49665241,2018-04-05 15:23:55,10,"<p>When creating you model, you can specify the requirements.txt as an environment variable. </p>

<p>For Eg. </p>

<pre><code>env = {
    'SAGEMAKER_REQUIREMENTS': 'requirements.txt', # path relative to `source_dir` below.
}
sagemaker_model = TensorFlowModel(model_data = 's3://mybucket/modelTarFile,
                                  role = role,
                                  entry_point = 'entry.py',
                                  code_location = 's3://mybucket/runtime-code/',
                                  source_dir = 'src',
                                  env = env,
                                  name = 'model_name',
                                  sagemaker_session = sagemaker_session,
                                 )
</code></pre>

<p>This would ensure that the requirements file is run after the docker container is created, before running any code on it. </p>
","6530653",0
251,49677404,2,49675637,2018-04-05 16:37:35,13,"<p>Once the endpoint is created, you can invoke it as any other restful service, with credentials and payload. </p>

<p>I am guessing, there could be two places where might be stuck. 
One could be, sending an actual PostMan Request with all the headers and everything. 
Newer version of Postman has AWS Signature as one of the Authorization types. You can use that to invoke the service. There are no other spacial headers required. Note that there is a bug in Postman still open (<a href=""https://github.com/postmanlabs/postman-app-support/issues/1663"" rel=""noreferrer"">issue-1663</a>) that only affects if you are a AWS federated account. Individual accounts should not be affected by this issue. </p>

<p>Or, you could be stuck at the actual payload. When you invoke the SageMaker endpoint, the payload is passed as is to the model. If you want to preprocess the input before feeding it to the model, you'd have to implement an input_fn method and specify that when instantiating the model. </p>

<p>You might also be able to invoke SageMaker endpoint using AWS SDK boto3 as follows </p>

<pre><code>import boto3
runtime= boto3.client('runtime.sagemaker')

payload = getImageData()


result  = runtime.invoke_endpoint(
    EndpointName='my_endpoint_name',
    Body=payload,
    ContentType='image/jpeg'
)
</code></pre>

<p>Hope this helps.</p>
","6530653",0
252,49677567,2,49673042,2018-04-05 16:47:04,2,"<p>Yes you can. You can implement the output_fn to ""brick wall"" your output. SageMaker would call the output_fn after the model returns the value to do any post-processing of the result. 
This can be done by creating a separate python file, specify the output_fn method there. 
Provide this python file when instantiating your Estimator. 
something like </p>

<pre><code>sess = sagemaker.Session()

linear = 
sagemaker.estimator.Estimator(containers[boto3.Session().region_name],
role, 
train_instance_count=1, 
train_instance_type='ml.c4.xlarge',
output_path='s3://{}/{}/output'.format(bucket, prefix),
 sagemaker_session=sess)
linear.set_hyperparameters(feature_dim=5,
mini_batch_size=100,
predictor_type='regressor',
epochs=10,
num_models=32,
loss='absolute_loss', 
</code></pre>

<blockquote>
  <p>entry_point = 'entry.py'</p>
</blockquote>

<p>)</p>

<pre><code>linear.fit({'train': s3_train_data, 'validation': s3_validation_data})
</code></pre>

<p>Your entry.py could look something like </p>

<pre><code>def output_fn(data, accepts):
    """"""
    Args:
        data: A result from TensorFlow Serving
        accepts: The Amazon SageMaker InvokeEndpoint Accept value. The content type the response object should be
            serialized to.
    Returns:
        object: The serialized object that will be send to back to the client.

    """"""    
</code></pre>

<blockquote>
  <p>Implement the logic to ""brick wall"" here.</p>
</blockquote>

<pre><code>    return data.outputs['outputs'].string_val
</code></pre>
","6530653",0
253,49691185,2,49689216,2018-04-06 10:55:41,1,"<p>A work-around for the asked question:</p>

<pre><code>req = requests.request('POST', 'http://httpbin.org/get')
req.body = b''
req.method = ''
print(auth.get_aws_request_headers(req,
                                   aws_access_key=auth.aws_access_key,
                                   aws_secret_access_key=auth.aws_secret_access_key,
                                   aws_token=auth.aws_token))
</code></pre>

<p>The problem is not solved, though. And now I wonder what the first argument of <code>auth.get_aws_request_headers</code> is.</p>
","562769",2
254,49721459,2,49582307,2018-04-08 18:45:15,20,"<p>Amazon SageMaker is a set of API that can help various machine learning and data science tasks. These API can be invoked from various sources, such as CLI, <a href=""https://aws.amazon.com/tools/"" rel=""noreferrer"">SDK</a> or specifically from schedule AWS Lambda functions (see here for documentation: <a href=""https://docs.aws.amazon.com/lambda/latest/dg/with-scheduled-events.html"" rel=""noreferrer"">https://docs.aws.amazon.com/lambda/latest/dg/with-scheduled-events.html</a> )</p>

<p>The main parts of Amazon SageMaker are notebook instances, training and tuning jobs, and model hosting for real-time predictions. Each one has different types of schedules that you might want to have. The most popular are:</p>

<ul>
<li><strong>Stopping and Starting Notebook Instances</strong> - Since the notebook instances are used for interactive ML models development, you don't really need them running during the nights or weekends. You can schedule a Lambda function to call the <a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/API_StopNotebookInstance.html"" rel=""noreferrer"">stop-notebook-instance</a> API at the end of the working day (8PM, for example), and the <a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/API_StartNotebookInstance.html"" rel=""noreferrer"">start-notebook-instance</a> API in the morning. Please note that you can also run crontab on the notebook instances (after opening the local terminal from the Jupyter interface).</li>
<li><strong>Refreshing an ML Model</strong> - Automating the re-training of models, on new data that is flowing into the system all the time, is a common issue that with SageMaker is easier to solve. Calling <a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/API_CreateTrainingJob.html"" rel=""noreferrer"">create-training-job</a> API from a scheduled Lambda function (or even from a <a href=""https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/WhatIsCloudWatchEvents.html"" rel=""noreferrer"">CloudWatch Event</a> that is monitoring the performance of the existing models), pointing to the S3 bucket where the old and new data resides, can <a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/API_CreateModel.html"" rel=""noreferrer"">create a refreshed model</a> that you can now deploy into an <a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/API_UpdateEndpointWeightsAndCapacities.html"" rel=""noreferrer"">A/B testing environment</a> .</li>
</ul>

<p>----- UPDATE (thanks to @snat2100 comment) -----</p>

<ul>
<li><strong>Creating and Deleting Real-time Endpoints</strong> - If your realtime endpoints are not needed 24/7 (for example, serving internal company users working during workdays and hours), you can also <a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/API_CreateEndpoint.html"" rel=""noreferrer"">create the endpoints</a> in the morning and <a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/API_DeleteEndpoint.html"" rel=""noreferrer"">delete them</a> at night. </li>
</ul>
","179529",2
255,49809088,2,49438903,2018-04-13 03:56:54,1,"<p>Ah i have had a similar problem to you where I needed to download something off S3 to use in the input_fn for inference. In my case it was a dictionary.</p>

<p>Three options:</p>

<ol>
<li>use your config.yml approach, and download and import the s3 file from within your entrypoint file before any function declarations. This would make it available to the input_fn </li>
<li>Keep using the hyperparameter approach, download and import the vectorizer in <code>serving_input_fn</code> and make it available via a global variable so that <code>input_fn</code> has access to it.</li>
<li>Download the file from s3 before training and include it in the source_dir directly.</li>
</ol>

<p>Option 3 would only work if you didnt need to make changes to the vectorizer seperately after initial training.</p>

<p>Whatever you do, don't download the file directly in input_fn. I made that mistake and the performance is terrible as each invoking of the endpoint would result in the s3 file being downloaded.</p>
","3709072",0
256,49826308,2,49826004,2018-04-13 23:21:40,1,"<p>Even if you have all the access to the bucket, you need to provide access key and secret in order to put some object in bucket if it is private. Or if you make bucket access public to all then you can push object to bucket without any problem.</p>
","4302725",0
257,49833875,2,49818134,2018-04-14 16:51:26,1,"<p>The comment @gvee mentioned may be the best to use going forward though it is in beta.</p>

<p>However, to answer your question, use the <code>Install-Module -Name AzureML</code> <a href=""https://www.powershellgallery.com/packages/AzureML/1.0.1"" rel=""nofollow noreferrer"">command</a> to get access to the Azure ML commands.</p>

<p><a href=""https://i.stack.imgur.com/ZerMp.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ZerMp.png"" alt=""enter image description here""></a></p>
","186013",0
258,50088925,2,50068941,2018-04-29 16:33:54,3,"<p>Posting this here in case anyone else has this issue.</p>

<p>After a bunch of trial and error I managed to solve my issue by writing my serving input function like this:</p>

<pre><code>FEATURES = ['amount_normalized', 'x_month', 'y_month']
def serving_input_fn(hyperparameters):
    feature_spec = {
        key : tf.FixedLenFeature(shape=[], dtype = tf.float32) \
          for key in FEATURES
    }
    return tf.estimator.export.build_parsing_serving_input_receiver_fn(feature_spec)()
</code></pre>

<p>I can then call my deployed model by passing in a hash:</p>

<pre><code>deployed_model.predict({""amount_normalized"": 2.3, ""x_month"": 0.2, ""y_month"": -0.3})
</code></pre>
","4760261",0
259,50105061,2,50078161,2018-04-30 17:04:20,1,"<p>This was caused by the training dataset. The dataset had characters in the humidity and temperature columns. This led to the model expecting characters but operating on floating point numbers. I cleaned the dataset and ensured that there are only floats in the temperature and humidity columns. Then I used this training data for the model and phew!!!! Everything's working now. </p>
","1426185",0
260,50109294,2,50032795,2018-04-30 23:01:07,0,"<p>looks like the sagemaker notebook wizard has you create a role that has limited s3 access. If I add this and the default <code>AmazonSageMakerFullAccess</code> the user is properly restricted. <a href=""https://i.stack.imgur.com/9BjRD.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/9BjRD.png"" alt=""Amazon make sagemaker role""></a></p>

<p><a href=""https://i.stack.imgur.com/IM7WW.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/IM7WW.png"" alt=""choose iam roles""></a></p>
","630752",2
261,50115155,2,50113374,2018-05-01 10:46:03,1,"<p><a href=""http://apprize.info/microsoft/azure_1/9.html"" rel=""nofollow noreferrer"">This</a> page may have better descriptions on it.</p>

<p>Basically, traits are the features the algorithm will learn about each user related to each item. For example, in the <a href=""https://gallery.azure.ai/Experiment/Recommender-Restaurant-ratings-2"" rel=""nofollow noreferrer"">restaurant ratings recommender</a> traits could include a user's birth year, if they're a student or working professional, martial status, etc.</p>

<p>Hope that helps!</p>
","186013",2
262,50134084,2,50133418,2018-05-02 11:54:13,0,"<p>I can give some advices but every project is different and use what works best for you.</p>

<p>Is it a one timer data exploration or something you need to crunch on a frequent schedule?  In order to do it frequently, spark might be the right tool. Spark is awesome at transforming/featurizing/cleaning/preprocessing your data into something more usable for tensorflow (usually into sparse format). The important thing here is to keep your gpus busy and to achieve that, you need to preprocess as much as you can before using tf. S3 is a good storage if you do not have small files.</p>

<p>For tensorflow to be happy, most of the time you need to densify your feature vectors. By that, you take a minibatch of records and transform sparse feature vectors into dense vectors. Only then you can send it to tf. This is because gpus are pretty bad at working with sparse data and some operations like convolutions do not even support sparse inputs. (all that can change anytime since it is an active field of research)</p>
","1607185",2
263,50135037,2,50133056,2018-05-02 12:47:42,0,"<p>The ""Apply SQL Transformation"" module should be able to do it. For example, I have a dataset with an <em>age</em> column and here's the SQL to create another column called <em>double_age</em>:</p>

<pre><code>select age, age * 2 as double_age from t1;
</code></pre>

<p>Which produces a dataset with just the <em>age</em> and <em>double_age</em> columns:</p>

<p><a href=""https://i.stack.imgur.com/uZblo.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/uZblo.png"" alt=""enter image description here""></a></p>
","186013",0
264,50160838,2,50158647,2018-05-03 17:30:55,1,"<p>Google cloud has similar options and they give $300 credit to developers. 
Since google is the creator of tensorflow, I am guessing their cloud would be the one most up to date the latest. Try it out. </p>

<p><a href=""https://cloud.google.com/ml-engine/docs/pricing"" rel=""nofollow noreferrer"">https://cloud.google.com/ml-engine/docs/pricing</a></p>
","6530653",0
265,50166136,2,50165669,2018-05-04 02:05:46,1,"<p>Sure, go ahead and upload.</p>

<p>You are running an application (Jupyter) on your own EC2 instance. What you do on that is up to you, as long as it doesn't violate Terms of Service (gambling, illegal activities, etc).</p>

<p>Worst case, you might run out of disk space.</p>
","174777",4
266,50202932,2,49320679,2018-05-06 18:02:33,2,"<ol>
<li><p>The AML Experimentation is one of our many new ML offerings, including data preparation, experimentation, model management, and operationalization. Workbench is a PREVIEW product that provides a GUI for some of these services. But it is just a installer/wrapper for the CLI that is needed to run. The services are Spark and Python based. Other Python frameworks will work, and you can get a little hacky to call Java/Scala from Python. Not really sure what you mean by an ""Azure ML Service"", perhaps you are referring to the operationalization service I mentioned above. This will quickly let you create new Python based APIs using Docker containers, and will connect with the model management account to keep track of the linage between your models and your services. All services here are still in preview and may breaking change before GA release. </p></li>
<li><p>Azure ML Studio is an older product that is perhaps simpler for some(myself an engineer not a data scientist). It offers a drag and drop experience, but is limited in it's data size to about 10G. This product is GA. </p></li>
<li><p>It is, but you need smaller data sizes, and the job flow is not spark based. I use this to do rapid PoC's. Also you will less control over the scalability of your scoring (batch or real time), because it is PaaS, compared to the newer service which is more IaaS. I would recommend looking at the new service instead of studio for most use cases. </p></li>
<li><p>The web services are completely based on Docker. Needing docker for experimentation is more about running things locally, which I myself rarely do. But, for the real time service, everything you package is placed into a docker container so it can be deployed to an ACS cluster. </p></li>
</ol>
","1341806",0
267,50202958,2,49586005,2018-05-06 18:04:52,2,"<p>Workbench is a preview product and issues may occur. Please try and get a newer exe and try again. It also seems like you have azure powershell issues here which I would have expected to be taken care of by the installer, but perhaps you can try and install azure powershell first. </p>
","1341806",3
268,50235492,2,50219664,2018-05-08 13:59:56,0,"<p>For anyone looking for your ""Don't Overthink It"" moment of the day:</p>

<p>I needed to provide TWO output blob file references:</p>

<pre><code>var request = new BatchExecutionRequest()
            {
                Inputs = new Dictionary&lt;string, AzureBlobDataReference&gt;() {
                    {
                        ""input1"",
                        new AzureBlobDataReference()
                        {
                            ConnectionString = _connectionString,
                            RelativeLocation = $""{_containerName}/{experimentId}/{tenantId}/{trainingDataFileName}.csv""
                        }
                    },
                },

                Outputs = new Dictionary&lt;string, AzureBlobDataReference&gt;() {
                    {
                        ""output1"",
                        new AzureBlobDataReference()
                        {
                            ConnectionString = _connectionString,
                            RelativeLocation = $""{_containerName}/{experimentId}/{tenantId}/{outputFileNameCsv}.csv""
                        }
                    },
                    {
                        ""output2"",
                        new AzureBlobDataReference()
                        {
                            ConnectionString = _connectionString,
                            RelativeLocation = $""{_containerName}/{experimentId}/{tenantId}/{outputFileNameIlearner}.ilearner""
                        }
                    },
                },

                GlobalParameters = new Dictionary&lt;string, string&gt;()
                {
                }
            };
</code></pre>

<p>There's an old saying in American English about not making assumptions, and I assumed the second output was an optional parameter used in batch operations. Since I'm not actually looking for more than one result from each call, I thought I was safe to remove the second output parameter.</p>

<p>TL/DR: Keep all the parameters the webservice portal's ""Consume"" tab generates, and make sure the first one is a .csv file reference.</p>
","7053468",0
269,50247421,2,50209284,2018-05-09 07:06:58,0,"<p>As far as I know, the model could run in <strong>Azure Machine Learning Studio</strong>.It seems that you are unable to download it, the model could do nothing outside of Azure ML. </p>

<p><a href=""https://stackoverflow.com/questions/41236871/how-to-download-the-trained-models-from-azure-machine-studio"">Here</a> is a similar post for you to refer, I have also tried @Ahmet's 
method, but result is like @mrjrdnthms says.</p>
","9455659",0
270,50300373,2,50271174,2018-05-11 21:15:55,1,"<p>Amazon SageMaker is a set of different services for data scientists. You are using the notebook service that is used for developing ML models in an interactive way. The hosting service in SageMaker is creating an endpoint based on a trained model. You can call this endpoint with invoke-endpoint API call for real time inference. </p>

<p>It seems that you are looking for a different type of hosting that is more suitable for serving HTML media rich pages, and doesn’t fit into the hosting model of SageMaker. A combination of EC2 instances, with pre-built AMI or installation scripts, Congnito for authentication, S3 and EBS for object and block storage, and similar building blocks should give you a scalable and cost effective solution. </p>
","179529",4
271,50312335,2,50303607,2018-05-13 02:52:10,2,"<p>You should create an IAM role first that defines what should be permitted (mainly calling the invoke-endpoint API call for SageMaker runtime). Then you should create an IAM user, add the above role to that user, and then generate credentials that you can use in your Postman to call the service. Here you can find some details about the IAM role for SageMaker that you can use in this process: <a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/using-identity-based-policies.html"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/sagemaker/latest/dg/using-identity-based-policies.html</a></p>

<p>A popular option to achieve external access to a SageMaker endpoint, is to create an API Gateway that calls a Lambda Function that is then calling the invoke-endpoint API. This chain is giving you various options such as different authentication options for the users and API keys as part of API-GW, processing the user input and inference output using API-GW and Lambda code, and giving the permission to call the SageMaker endpoint to the Lambda function. This chain removes the need for the credentials creation, update and distribution.  </p>
","179529",1
272,50356165,2,50352412,2018-05-15 17:31:40,2,"<p>I am not sure if you have masked the actual access id and key or this is what you are running.</p>
<pre><code>session = boto3.session.Session(aws_access_key_id='SECRET',aws_secret_access_key='SECRET',region_name='eu-west-1')
client = boto3.client('sagemaker',region_name='eu-west-1',aws_access_key_id='SECRET',aws_secret_access_key='SECRET')
</code></pre>
<p>I am hoping you are providing the actual aws_access_key_id and aws_secret_access_key in the above lines of code.</p>
<p>Another way of specifying the same and not hardcoding in the code is to create a credentials file in your profile directory i.e.</p>
<p>in Mac    ~/.aws/</p>
<p>and in Windows <code>&quot;%UserProfile%\.aws&quot;</code></p>
<p>the file is a plain text file with a name &quot;credentials&quot; (without the quotes).
file contains</p>
<pre><code>[default]
aws_access_key_id=XXXXXXXXXXXXXX
aws_secret_access_key=YYYYYYYYYYYYYYYYYYYYYYYYYYY
</code></pre>
<p>AWS CLI would pick it up from the above location and use it. You can also use non-default profiles and pass on the profile with</p>
<pre><code>os.environ[&quot;AWS_PROFILE&quot;] = &quot;profile-name&quot;
</code></pre>
<p>Hope this helps.</p>
","6530653",0
273,50413288,2,50334735,2018-05-18 14:14:47,0,"<p>Solving my own problem. I decided to use Apache Avro instead of BeanIO. Spark allow to serialize using Avro (c.f. Spark-Avro). This seems to work however it did not fit my use case has I was trying to serialize an array of numbers.</p>
","4761371",0
274,50413344,2,50281188,2018-05-18 14:17:53,1,"<p>Answering my own question. It was an issue in the algorithm configuration. I reduced mini batch size and it worked fine.</p>
","4761371",0
275,50415409,2,50414639,2018-05-18 16:11:16,5,"<p>How do you intend to use your bot? Azure Bots work by connecting them to existing channels like Skype, Facebook Messenger, SMS, etc or making REST calls from a custom application.</p>

<p>However you can also reach your bot directly from: <code>https://webchat.botframework.com/embed/YOUR_BOT_ID?t=YOUR_TOKEN_HERE</code></p>

<p>You can embed it on any web page with this HTML tag:</p>

<pre><code>&lt;iframe src=""https://webchat.botframework.com/embed/YOUR_BOT_ID?t=YOUR_TOKEN_HERE""&gt;&lt;/iframe&gt;
</code></pre>

<p>Please note that both of these methods expose your token and would allow other developers to add your bot to their pages as well.</p>

<p>Bot ID is the name of your bot and you can get the token from the portal by going to your bot and choosing ""Channel"" blade and then clicking the ""Get bot embed codes"" link.</p>

<p>Edit: I went ahead and wrote a blog post on this topic <a href=""https://medium.com/@joelatwar/how-to-embed-your-azure-web-app-bot-in-any-web-page-120dfda91fdc"" rel=""nofollow noreferrer"">https://medium.com/@joelatwar/how-to-embed-your-azure-web-app-bot-in-any-web-page-120dfda91fdc</a></p>
","8826629",2
276,50423946,2,49438358,2018-05-19 09:52:31,1,"<p>The solution is </p>

<p>pip install ""<a href=""https://azuremldownloads.blob.core.windows.net/wheels/latest/azureml.logging-1.0.79-py3-none-any.whl?sv=2016-05-31&amp;si=ro-2017&amp;sr=c&amp;sig=xnUdTm0B%2F%2FfknhTaRInBXyu2QTTt8wA3OsXwGVgU%2BJk%3D"" rel=""nofollow noreferrer"">https://azuremldownloads.blob.core.windows.net/wheels/latest/azureml.logging-1.0.79-py3-none-any.whl?sv=2016-05-31&amp;si=ro-2017&amp;sr=c&amp;sig=xnUdTm0B%2F%2FfknhTaRInBXyu2QTTt8wA3OsXwGVgU%2BJk%3D</a>""</p>

<p>I found the blob url here. It's inside docker container dependencies.</p>

<p><a href=""https://github.com/Azure/LearnAI-Bootcamp/blob/master/lab03.3_manage_conda_envs_in_aml/0_README.md"" rel=""nofollow noreferrer"">https://github.com/Azure/LearnAI-Bootcamp/blob/master/lab03.3_manage_conda_envs_in_aml/0_README.md</a></p>
","1484950",0
277,50428761,2,50418501,2018-05-19 19:29:38,3,"<p>Amazon SageMaker is a set of multiple components that you can choose which ones to use. </p>

<p>The built-in algorithms are designed for (infinite) scale, which means that you can have huge datasets and be able to build a model with them quickly and with low cost. Once you have large datasets you usually don't need to use techniques such as cross-validation, and the recommendation is to have a clear split between training data and validation data. Each of these parts will be defined with an input channel when you are submitting a training job.  </p>

<p>If you have a small amount of data and you want to train on all of it and use cross-validation to allow it, you can use a different part of the service (interactive notebook instance). You can bring your own algorithm or even container image to be used in the development, training or hosting. You can have any python code based on any machine learning library or framework, including scikit-learn, R, TensorFlow, MXNet etc. In your code, you can define cross-validation based on the training data that you copy from S3 to the worker instances. </p>
","179529",3
278,50440865,2,50439489,2018-05-21 01:22:38,1,"<p>I have reproduced your issue. Try to go to your project -> EDIT ->remove the <strong>ASSETS</strong> of your project. Then the delete button will be able.</p>

<p>You could follow the screenshot.</p>

<ol>
<li>The <strong>DELETE</strong> button is disable.</li>
</ol>

<p><a href=""https://i.stack.imgur.com/E850F.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/E850F.png"" alt=""enter image description here""></a></p>

<p>2.Go to <strong>EDIT</strong> and remove the <strong>ASSETS</strong>.</p>

<p><a href=""https://i.stack.imgur.com/PbEZC.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/PbEZC.png"" alt=""enter image description here""></a></p>

<p><a href=""https://i.stack.imgur.com/2kBgO.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/2kBgO.png"" alt=""enter image description here""></a></p>

<p>3.Then the <strong>DELETE</strong> button will be able</p>

<p><a href=""https://i.stack.imgur.com/08lOW.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/08lOW.png"" alt=""enter image description here""></a></p>
","9455659",0
279,50475779,2,50473170,2018-05-22 20:13:04,1,"<p>From <a href=""https://stackoverflow.com/questions/41236871/how-to-download-the-trained-models-from-azure-machine-studio"">this answer</a> you won't be able to save the model locally if you do everything within Azure ML Studio.</p>

<p>If you create the model using Python or R and execute it within Azure ML Studio, then you can save it from the library that you use.</p>
","186013",0
280,50584617,2,50576929,2018-05-29 12:37:49,3,"<p>It can be done! :)</p>

<p>You would just use the ""Group Categorical Values"" module. Choose the column that has the data you want to group, and you can set the values like the following:</p>

<p><a href=""https://i.stack.imgur.com/UGhrR.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/UGhrR.png"" alt=""enter image description here""></a></p>

<p>What's going on here is that the default, which will get used if the other levels aren't caught, is set to ""yes"". Then when any values are ""no"", or ""maybe"", it gets grouped into a category of ""no"".</p>

<p>However, this will error unless you make that column a categorical type, so you would need to use the ""Edit Metadata"" module to do that.</p>

<p><a href=""https://i.stack.imgur.com/45s2Q.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/45s2Q.png"" alt=""enter image description here""></a></p>

<p>The example I used is <a href=""https://gallery.cortanaintelligence.com/Experiment/Replace-Values-in-Dataset"" rel=""nofollow noreferrer"">published to the gallery</a>, if you need to reference it.</p>

<p>If you need more info, just let me know.</p>
","186013",2
281,50646458,2,50514817,2018-06-01 15:11:30,0,"<p>Never mind. I got rid of the SelectColumnsTransform altogether (departing from the example experiment), instead using a R script in the training experiment to save the names of the columns selected, then another R script in the predictive experiment to load those names and remove any other feature columns.</p>
","9697327",0
282,50684081,2,50669991,2018-06-04 15:31:01,4,"<p>I solved the problem. We must set a permission at SageMaker Execution Role as following:</p>

<pre><code>{
""Version"": ""2012-10-17"",
""Statement"": [
    {
        ""Effect"": ""Allow"",
        ""Action"": [
            ""ecr:*""            ],
        ""Resource"": ""*""
    }
]}
</code></pre>
","6901690",2
283,50883011,2,50872338,2018-06-15 21:41:54,1,"<p>We could share the entire app by <a href=""https://learn.microsoft.com/en-us/azure/cognitive-services/luis/luis-how-to-manage-versions"" rel=""noreferrer"">cloning the app</a>. And the only way to use specific entities would be to delete the others.</p>
<blockquote>
<p>If we'd like to have one Regex definition at one place could we eventually share such definition among multiple LUIS models?</p>
<p>Right now we proceed with Ctrl+C and Ctrl+V</p>
</blockquote>
<p>This is the only way to do it right now.</p>
","5508654",0
284,51052851,2,50985138,2018-06-27 00:04:50,5,"<p>Your arguments when initializing the HyperparameterTuner object are in the wrong order. The constructor has the following signature:</p>

<pre><code>HyperparameterTuner(estimator, 
                    objective_metric_name, 
                    hyperparameter_ranges, 
                    metric_definitions=None, 
                    strategy='Bayesian', 
                    objective_type='Maximize', 
                    max_jobs=1, 
                    max_parallel_jobs=1, 
                    tags=None, 
                    base_tuning_job_name=None)
</code></pre>

<p>so in this case, your <code>objective_type</code> is in the wrong position. See <a href=""https://sagemaker.readthedocs.io/en/latest/tuner.html#sagemaker.tuner.HyperparameterTuner"" rel=""nofollow noreferrer"">the docs</a> for more details.</p>
","8207229",2
285,51065201,2,51064366,2018-06-27 14:19:18,1,"<p>Apparently I had to install the <code>wheel</code> module inside my virtual environment. I deleted the virtual environment, re-created it, and then installed the <code>wheel</code> module:</p>

<pre><code>pip install wheel
</code></pre>

<p>after that <code>pip install mlflow</code>, as well as <code>mlflow ui</code> worked successfully.</p>
","236007",0
286,51086481,2,51086377,2018-06-28 15:20:30,1,"<p>You could save the workbook as a (set of) CSV file(s) and upload them separately.</p>

<p>A CSV file, a '<a href=""https://en.wikipedia.org/wiki/Comma-separated_values"" rel=""nofollow noreferrer"">Comma Separated Values</a>' file, is exactly that. A flat file with some values separated by a comma. If you load an Excel file it will mess up since there's way more information in an Excel file than just values separated by comma's. Have a look at <code>File</code> -> <code>Save as</code> -> <code>Save as type</code> where you can select 'CSV (comma delimited) (*.csv)'</p>

<p><em>Disclaimer: no, it's not always a comma...</em>  </p>

<blockquote>
  <p>In addition, the term ""CSV"" also denotes some closely related delimiter-separated formats that use different field delimiters. These include tab-separated values and space-separated values. A delimiter that is not present in the field data (such as tab) keeps the format parsing simple. These alternate delimiter-separated files are often even given a .csv extension despite the use of a non-comma field separator.</p>
</blockquote>

<p><strong>Edit</strong><br>
So apparently Excel files <em>are</em> supported: <a href=""https://learn.microsoft.com/en-us/azure/machine-learning/desktop-workbench/data-prep-appendix2-supported-data-sources"" rel=""nofollow noreferrer"">Supported data sources for Azure Machine Learning data preparation</a>  </p>

<p><em>Excel (.xls/.xlsx)</em><br>
Read an Excel file one sheet at a time by specifying sheet name or number.</p>

<p>But also, only UTF-8 is supported: <a href=""https://learn.microsoft.com/en-us/azure/machine-learning/studio-module-reference/import-data#bkmk_Notes"" rel=""nofollow noreferrer"">Import Data - Technical notes</a></p>

<blockquote>
  <p>Azure Machine Learning requires UTF-8 encoding. If the data you are importing uses a different encoding, or was exported from a data source that uses a different default encoding, various problems might appear in the text.</p>
</blockquote>
","1945525",3
287,51086609,2,50983316,2018-06-28 15:27:11,2,"<p>You can use boto3 sdk for python to start training on lambda then you need to trigger the lambda when csv is update.</p>

<blockquote>
  <p><a href=""http://boto3.readthedocs.io/en/latest/reference/services/sagemaker.html"" rel=""nofollow noreferrer"">http://boto3.readthedocs.io/en/latest/reference/services/sagemaker.html</a></p>
</blockquote>

<p>Example python code</p>

<blockquote>
  <p><a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/ex1-train-model-create-training-job.html"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/sagemaker/latest/dg/ex1-train-model-create-training-job.html</a></p>
</blockquote>

<p>Addition: You dont need to use lambda you just start/cronjob the python script any kind of instance which has python and aws sdk in it.</p>
","5894758",0
288,51099116,2,51088145,2018-06-29 09:57:58,1,"<p>I think the code they give may be pretty old at this point.</p>

<p>The <a href=""https://pypi.org/project/azure-storage-blob/#history"" rel=""nofollow noreferrer"">latest version</a> of <code>azure.storage.blob</code> is 1.3. So perhaps a <code>pip install azure.storage.blob --update</code> or simply uninstalling and reinstalling would help.</p>

<p>Once you got the latest version, try using the <code>create_blob_from_text</code> method to load the file to your storage container.</p>

<pre><code>from azure.storage.blob import BlockBlobService

blobService = BlockBlobService(account_name=""accountName"", account_key=""accountKey)

blobService.create_blob_from_text(""containerName"", ""fileName"", csv_file)
</code></pre>

<p>Hope that works to help lead you down the right path, but if not we can work through it. :)</p>
","186013",2
289,51141667,2,50993858,2018-07-02 18:12:50,0,"<p>Using the ""Filter Based Feature Selection"" module and then removing all columns besides those found significant and the identifiers seemed to fix the issue. </p>
","5112620",0
290,51141771,2,51117133,2018-07-02 18:19:26,12,"<p>The supported way to do this for Sagemaker notebook instances is with <strong>Lifecycle Configurations</strong>.</p>

<p>You can create an <strong>onStart</strong> lifecycle hook that can install the required packages into the respective Conda environments each time your notebook instance starts.</p>

<p>Please see the following blog post for more details</p>

<p><a href=""https://aws.amazon.com/blogs/machine-learning/customize-your-amazon-sagemaker-notebook-instances-with-lifecycle-configurations-and-the-option-to-disable-internet-access/"" rel=""noreferrer"">https://aws.amazon.com/blogs/machine-learning/customize-your-amazon-sagemaker-notebook-instances-with-lifecycle-configurations-and-the-option-to-disable-internet-access/</a></p>
","8899112",1
291,51144455,2,51110274,2018-07-02 22:32:04,4,"<p>If you use local mode with the SageMaker Python SDK, you can train using local data:</p>

<pre><code>from sagemaker.mxnet import MXNet

mxnet_estimator = MXNet('train.py',
                        train_instance_type='local',
                        train_instance_count=1)

mxnet_estimator.fit('file:///tmp/my_training_data')
</code></pre>

<p>However, this only works if you are training a model locally, not on SageMaker. If you want to train on SageMaker, then yes, you do need to use S3.</p>

<p>For more about local mode: <a href=""https://github.com/aws/sagemaker-python-sdk#local-mode"" rel=""noreferrer"">https://github.com/aws/sagemaker-python-sdk#local-mode</a></p>
","9074534",0
292,51144800,2,51137973,2018-07-02 23:23:37,2,"<p>Through discussion in the comments, it may be that the ""Endpoint URL"" just needed to be updated, but I'll go over all of the inputs in case anyone else needs a reference to it.</p>

<ul>
<li>Endpoint URL - Can use the URI in the CosmosDB ""Overview"" pane in the Azure Portal</li>
<li>Database ID - The name of the database to connect to</li>
<li>DocumentDB Key - The primary password from the ""Connection Strings"" pane in the Azure Portal</li>
<li>Collection ID - The name of the collection to read data from</li>
</ul>

<p>And, for reference, here's what my data explorer looks like in CosmosDB (database ID then collection ID):</p>

<p><a href=""https://i.stack.imgur.com/7Z4Q7.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/7Z4Q7.png"" alt=""enter image description here""></a></p>

<p>And the settings in Azure ML Studio to import the data:
<a href=""https://i.stack.imgur.com/LheXz.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/LheXz.png"" alt=""enter image description here""></a></p>
","186013",0
293,51149875,2,51149079,2018-07-03 08:24:40,0,"<p>Unfortunately, no. However, the various wrappers around it are.</p>
","8518433",0
294,51257922,2,51238413,2018-07-10 05:41:16,0,"<p>It was happening because of the Proxy (although I have configured the Proxy on the Workbench). When I am connected to internet directly, everything works fine (Able to create project, train, compare models etc). However the Workbench should return meaningful error instead of hanging or simply waiting while creating the project.</p>
","406896",0
295,51273498,2,50732094,2018-07-10 20:34:02,5,"<p>Thank you for using Amazon SageMaker.</p>

<p>The issue here is Pandas 0.23.0 changed the location of a core class named DataError and SparkMagic has not been updated to require DataError from correct namespace.</p>

<p>The workaround for this issue is to downgrade Pandas version in SageMaker Notebook Instance with <code>pip install pandas==0.22.0</code>.</p>

<p>You can get more information in this open github issue <a href=""https://github.com/jupyter-incubator/sparkmagic/issues/458"" rel=""noreferrer"">https://github.com/jupyter-incubator/sparkmagic/issues/458</a>.</p>

<p>Let us know if there is any other way we can be of assistance.</p>

<p>Thanks,<br>
Neelam</p>
","4570570",0
296,51277035,2,51272746,2018-07-11 03:58:25,0,"<p>Your question doesn't describe problem correctly, but if you are looking for installation commands. then please see below,</p>

<p>install pyspark first.</p>

<pre><code>pip install pyspark
</code></pre>

<p>To install MMLSpark on an existing HDInsight Spark Cluster, you can execute a script action on the cluster head and worker nodes. For instructions on running script actions, see this guide.</p>

<p>The script action url is: <a href=""https://mmlspark.azureedge.net/buildartifacts/0.13/install-mmlspark.sh."" rel=""nofollow noreferrer"">https://mmlspark.azureedge.net/buildartifacts/0.13/install-mmlspark.sh.</a></p>

<p>If you're using the Azure Portal to run the script action, go to Script actions → Submit new in the Overview section of your cluster blade. In the Bash script URI field, input the script action URL provided above. Mark the rest of the options as shown on the screenshot to the right.</p>

<p>Submit, and the cluster should finish configuring within 10 minutes or so.</p>

<p>from Original Docs:- 
<a href=""https://github.com/Azure/mmlspark"" rel=""nofollow noreferrer"">https://github.com/Azure/mmlspark</a></p>
","7906629",2
297,51297274,2,50049928,2018-07-12 04:22:35,2,"<p>The RecordIO format is designed to pack a large number of images into a single file, so I don't think it would work well for predicting single images.</p>

<p>When it comes to prediction, you definitely don't have to copy images to a notebook instance or to S3. You just have to load them from anywhere and inline them in your prediction requests.</p>

<p><strong>If you want HTTP-based prediction, here are your options:</strong></p>

<p>1) Use the SageMaker SDK Predictor.predict() API on any machine (as long as it has proper AWS credentials) <a href=""https://github.com/aws/sagemaker-python-sdk"" rel=""nofollow noreferrer"">https://github.com/aws/sagemaker-python-sdk</a></p>

<p>2) Use the AWS Python SDK (aka boto3) API invoke_endpoint() on any machine (as long as it has proper AWS credentials)</p>

<p>You can even build a simple service to perform pre-processing or post-processing with Lambda. Here's an example: <a href=""https://medium.com/@julsimon/using-chalice-to-serve-sagemaker-predictions-a2015c02b033"" rel=""nofollow noreferrer"">https://medium.com/@julsimon/using-chalice-to-serve-sagemaker-predictions-a2015c02b033</a></p>

<p><strong>If you want batch prediction:</strong>
 the simplest way is to retrieve the trained model from SageMaker, write a few lines of ad-hoc MXNet code to load it and run all your predictions. Here's an example: <a href=""https://mxnet.incubator.apache.org/tutorials/python/predict_image.html"" rel=""nofollow noreferrer"">https://mxnet.incubator.apache.org/tutorials/python/predict_image.html</a></p>

<p>Hope this helps.</p>
","4686192",1
298,51297369,2,51288629,2018-07-12 04:34:59,1,"<p>I'm the author of this post :)</p>

<p>FM will ""simply"" fill the missing values in the recommendation matrix. What you could do is batch-predict all movies for all users, sort the results by descending score and store the top 10 results for each user in a cache, why not. That would make it easy to retrieve results in real-time from any kind of app.  I suppose you would also retrain periodically to account for new user recos.</p>

<p>Hope this helps.</p>
","4686192",1
299,51311703,2,51293471,2018-07-12 17:48:25,5,"<p>Ok, the prefix you set in the <code>S3Uri</code> matters here. Based on your screenshot I think your bucket looks something like this (in tree form):</p>

<pre><code>s3://bucket
└── LABEL-inputdata
    ├── train.csv
    └── validation.csv
</code></pre>

<p>Based on your <code>InputDataConfig</code> above, SageMaker has to download it to folders on the filesystem for the <code>xgboost</code> training algorithm to run. It does so based on the channel names and on the <code>S3Uri</code> prefix you provided. The prefix is chopped off to determine the name of the folder/file to download to. So, in your example, the <code>train</code> channel gets downloaded as:</p>

<pre><code>/opt/ml/input/data/train/.csv
</code></pre>

<p>Finally, the <code>xgboost</code> implementation sees the <code>.csv</code> file as a hidden file and complains about it.</p>

<p>To get it to work you could rearrange your data in s3 like so...</p>

<pre><code>s3:bucket
└── LABEL-inputdata
    ├── train
    │   └── data.csv
    └── validation
        └── data.csv
</code></pre>

<p>.. and change your input data config to:</p>

<pre><code>   ""InputDataConfig"": [
        {
            ""ChannelName"": ""train"",
            ""DataSource"": {
                ""S3DataSource"": {
                    ""S3DataType"": ""S3Prefix"",
                    ""S3Uri"": ""s3://{}/{}-inputdata/train/"".format(s3_utils.bucket, LABEL)
                }
            },
            ""ContentType"": ""csv"",
            ""CompressionType"": ""None""
        },
        {
            ""ChannelName"": ""validation"",
            ""DataSource"": {
                ""S3DataSource"": {
                    ""S3DataType"": ""S3Prefix"",
                    ""S3Uri"": ""s3://{}/{}-inputdata/validation/"".format(s3_utils.bucket, LABEL)
                }
            },
            ""ContentType"": ""csv"",
            ""CompressionType"": ""None""
        }
</code></pre>
","1502599",0
300,51313052,2,50508217,2018-07-12 19:23:35,0,"<p>Yes, it is possible now.</p>

<p>You can set the predictor_type hyper-parameter to <code>multiclass_classifier</code>.</p>

<p>See the documentation here: <a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/ll_hyperparameters.html"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/sagemaker/latest/dg/ll_hyperparameters.html</a>.</p>
","3231594",0
301,51314348,2,51305956,2018-07-12 20:57:45,1,"<p>My assumption is you have trained the model using Sagemaker XGBoost built-in algorithm. You would like to use that model and do the predictions in your own hosting environment (not Sagemaker hosting).</p>

<p><code>pickle.load(file)</code> reads a pickled object from the open file object file and <code>pickle.loads(bytes_object)</code> reads a pickled object from a bytes object and returns the deserialized object. Since you have the S3 object already downloaded (into memory) as bytes, you can use <code>pickle.loads</code> without using <code>open</code></p>

<pre><code>xgb_model = pkl.loads(obj['Body'].read())
</code></pre>
","6069517",5
302,51317676,2,51183169,2018-07-13 04:30:53,1,"<p>There is a new built-in algorithm released with Amazon Sagemaker today for object detection. Based on the <a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/object-detection.html"" rel=""nofollow noreferrer"">documentation</a>, Amazon SageMaker Object Detection uses the Single Shot multibox Detector (SSD) algorithm. The response from the inference contains an array consists of a predicted class label of the object detected, associated confidence score and bounding box co-ordinates.</p>
","6069517",2
303,51341104,2,50675708,2018-07-14 16:02:52,1,"<p>The call to <strong>net.predict</strong> is working fine. </p>

<p>It seems that you are using the SageMaker Python SDK <strong>predict_fn</strong> for hosting. After the <strong>predict_fn</strong> is invoked, the MXNet container will try to serialize your prediction to JSON before sending it back to the client. You can see code that does that here: <a href=""https://github.com/aws/sagemaker-mxnet-container/blob/master/src/mxnet_container/serve/transformer.py#L132"" rel=""nofollow noreferrer"">https://github.com/aws/sagemaker-mxnet-container/blob/master/src/mxnet_container/serve/transformer.py#L132</a></p>

<p>The container is failing to serialize because <strong>net.predict</strong> does not return a serializable object. You can solve this issue by returning a list instead:</p>

<pre><code>return net.predict(data.asnumpy().tolist()).asnumpy().tolist()
</code></pre>

<p>Another alternative is to use a <strong>transform_fn</strong> instead of <strong>prediction_fn</strong> so you can handle the output serialization yourself. You can see an example of a <strong>transform_fn</strong> here <a href=""https://github.com/aws/sagemaker-python-sdk/blob/e93eff66626c0ab1f292048451c4c3ac7c39a121/examples/cli/host/script.py#L41"" rel=""nofollow noreferrer"">https://github.com/aws/sagemaker-python-sdk/blob/e93eff66626c0ab1f292048451c4c3ac7c39a121/examples/cli/host/script.py#L41</a></p>
","3361753",0
304,51370707,2,51365850,2018-07-16 22:05:53,1,"<p>This <a href=""https://github.com/awslabs/amazon-sagemaker-examples/blob/d5681a07611ae29567355b60b2f22500b561218b/advanced_functionality/xgboost_bring_your_own_model/xgboost_bring_your_own_model.ipynb"" rel=""nofollow noreferrer"">example notebook</a> is good starting point showing how to use a pre-existing scikit-learn xgboost model with the Amazon SageMaker to create a hosted endpoint for that model.</p>
","6069517",1
305,51383721,2,51335594,2018-07-17 14:17:19,0,"<p><a href=""https://github.com/databricks/mlflow"" rel=""nofollow noreferrer"">mlflow documentation</a> already says that </p>

<blockquote>
  <p>Note 2: We <strong>do not currently support running MLflow on Windows</strong>.
  Despite this, we would appreciate any contributions to make MLflow
  work better on Windows.</p>
</blockquote>

<p>You're hitting <code>fcntl</code> problem: it's not available on MS Windows platform because it's a ""wrapper"" around the <a href=""http://man7.org/linux/man-pages/man2/fcntl.2.html"" rel=""nofollow noreferrer"">fcntl function</a> that's available on POSIX-compatible systems. (See <a href=""https://stackoverflow.com/a/1422436/236007"">https://stackoverflow.com/a/1422436/236007</a> for more details.)</p>

<p>Solving this requires modifying the source code of mlflow accordingly. </p>
","236007",2
306,51402839,2,51391639,2018-07-18 13:07:32,1,"<p>While you need to to specify a s3 ""folder"" as input, this folder can contain only a dummy file. 
Also if you bring your own docker container for training like in <a href=""https://github.com/awslabs/amazon-sagemaker-examples/tree/master/advanced_functionality/scikit_bring_your_own"" rel=""nofollow noreferrer"">this example</a>, you can do pretty much everthing in it. So you could do your daily query inside your docker container, because they have access to the internet. </p>

<p>Also inside this container you have access to all the other aws services. Your access is defined by the role you're passing to your training job.</p>
","9305450",0
307,51431050,2,51430645,2018-07-19 20:14:57,1,"<p>You need to split the column with:</p>

<pre><code>dataframe1['nominal;data;curs;cdx'].str.split(';',expand=True)
</code></pre>

<p>Then change the headers with:</p>

<pre><code>dataframe1.columns = 'nominal;data;curs;cdx'.split(';')
</code></pre>
","2271478",0
308,51449269,2,50441181,2018-07-20 19:32:06,0,"<p>Today, SageMaker Notebook Instances are managed EC2 instances but users still have full control over the the Notebook Instance as root. You have full capabilities to install missing libraries through the Jupyter terminal. </p>

<p>To access a terminal, open your Notebook Instance to the home page and click the drop-down on the top right: “New” -> “Terminal”. 
Note: By default, conda installs to the root environment. </p>

<p>The following are instructions you can follow <a href=""https://conda.io/docs/user-guide/tasks/manage-environments.html"" rel=""nofollow noreferrer"">https://conda.io/docs/user-guide/tasks/manage-environments.html</a> on how to install libraries in the particular conda environment. </p>

<p>In general you will need following commands, </p>

<pre><code>conda env list 
</code></pre>

<p>which list all of your conda environments </p>

<pre><code>source activate &lt;conda environment name&gt; 
</code></pre>

<p>e.g. source activate python3 </p>

<pre><code>conda list | grep &lt;package&gt; 
</code></pre>

<p>e.g. conda list | grep numpy 
list what are the current package versions </p>

<pre><code>pip install numpy 
</code></pre>

<p>Or </p>

<pre><code>conda install numpy 
</code></pre>

<p>Note: Periodically the SageMaker team releases new versions of libraries onto the Notebook Instances. To get the new libraries, you can stop and start your Notebook Instance. </p>

<p>If you have recommendations on libraries you would like to see by default, you can create a forum post under <a href=""https://forums.aws.amazon.com/forum.jspa?forumID=285"" rel=""nofollow noreferrer"">https://forums.aws.amazon.com/forum.jspa?forumID=285</a> . Alternatively, you can bootstrap your Notebook Instances with Lifecycle Configurations to install custom libraries. More details here: <a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/API_CreateNotebookInstanceLifecycleConfig.html"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/sagemaker/latest/dg/API_CreateNotebookInstanceLifecycleConfig.html</a></p>
","9181901",0
309,51451308,2,51450610,2018-07-20 22:51:44,3,"<p>Thanks for sharing complete example. The issue is with the escaping in <code>SERDEPROPERTIES</code>. After modifying <code>createTable</code> as below it works</p>

<pre><code>createTable = \
""""""CREATE EXTERNAL TABLE testtable (
    `id` string,
    `customerid` string, 
    `ip` string,
    `messagefilename` string
)
ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.OpenCSVSerde'
WITH SERDEPROPERTIES (
  'separatorChar' = ',', 
  'quoteChar' = '\\\""', 
  'escapeChar' = '\\\\' )
STORED AS TEXTFILE
LOCATION 's3://bucket_name/results/csv/'
TBLPROPERTIES (""skip.header.line.count""=""1"");""""""
</code></pre>
","6069517",1
310,51510952,2,51488308,2018-07-25 04:43:38,1,"<p>(Making comment to the original question as answer)</p>

<p>It looks like a jupyter kernel issue. I had a similar issue and I used <code>Sparkmagic (pyspark)</code> kernel instead of <code>Sparkmagic (pyspark3)</code> and it is working fine. Follow instructions on this <a href=""https://aws.amazon.com/blogs/machine-learning/build-amazon-sagemaker-notebooks-backed-by-spark-in-amazon-emr/"" rel=""nofollow noreferrer"">blog</a> and see if it helps.</p>
","6069517",0
311,51518906,2,51517103,2018-07-25 12:27:51,0,"<p>you have two options:</p>

<p>1) Start a virtual machine on AWS (known as an Amazon EC2 instance). You can pick from many different instance types, including GPU instances. You'll have full administrative access on this machine, meaning that you can copy you TF model to it and predict just like you would on your own machine. </p>

<p>More details on getting started with EC2 here: <a href=""https://aws.amazon.com/ec2/getting-started/"" rel=""nofollow noreferrer"">https://aws.amazon.com/ec2/getting-started/</a> </p>

<p>I would also recommend using the Deep Learning Amazon Machine Image, which bundles all the popular ML/DL tools as well as the NVIDIA environment for GPU training/prediction : <a href=""https://aws.amazon.com/machine-learning/amis/"" rel=""nofollow noreferrer"">https://aws.amazon.com/machine-learning/amis/</a></p>

<p>2) If you don't want to manage virtual machines, I'd recommend looking at Amazon SageMaker. You'll be able to import your TF model and to deploy it on fully-managed infrastructure for prediction. </p>

<p>Here's a sample notebook showing you how to bring your own TF model to SageMaker: <a href=""https://github.com/awslabs/amazon-sagemaker-examples/blob/master/advanced_functionality/tensorflow_iris_byom/tensorflow_BYOM_iris.ipynb"" rel=""nofollow noreferrer"">https://github.com/awslabs/amazon-sagemaker-examples/blob/master/advanced_functionality/tensorflow_iris_byom/tensorflow_BYOM_iris.ipynb</a></p>

<p>Hope this helps.</p>
","4686192",1
312,51540985,2,51198775,2018-07-26 14:13:52,1,"<p>You need to take the model.pkl file, zip it, and upload it into Azure Machine Learning Studio as a new dataset. Then add the python module and connect it to your newly generated zip.</p>

<p>You can now use it inside the AML Studio experiment. To use the model add the following code in your python module:</p>

<pre><code>import pandas as pd
import sys
import pickle

def azureml_main(dataframe1 = None, dataframe2 = None):
    sys.path.insert(0,"".\Script Bundle"")
    model = pickle.load(open("".\Script Bundle\model.pkl"", 'rb'))
    pred = model.predict(dataframe1)
    return pd.DataFrame([pred[0]]),
</code></pre>

<p><a href=""https://blogs.technet.microsoft.com/uktechnet/2018/04/25/deploying-externally-generated-pythonr-models-as-web-services-using-azure-machine-learning-studio/"" rel=""nofollow noreferrer"">You may find this post useful</a></p>
","9929041",1
313,51566507,2,51549048,2018-07-27 22:56:32,2,"<p>You can follow below steps to convert your notebook to slides on AWS Sagemaker (tried on sagemaker notebook instance) without installing any extensions.</p>

<p><strong>Step 1:</strong> Follow this <a href=""https://medium.com/@mjspeck/presenting-code-using-jupyter-notebook-slides-a8a3c3b59d67"" rel=""nofollow noreferrer"">article</a> to chose which cells in your notebook can be presented or skipped.
  - Go to View → Cell Toolbar → Slideshow
  - A light gray bar will appear above each cell with a scroll down window on the top right
  - Select type of slide each cell should be - regular slide, sub-slide, skip, notes</p>

<p><strong>Step 2:</strong> Go to Sagemaker notebook home page and open terminal</p>

<p><a href=""https://i.stack.imgur.com/kDl3d.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/kDl3d.png"" alt=""enter image description here""></a></p>

<p><strong>Step 3:</strong> Change directory in the instance where your notebook exists</p>

<p><a href=""https://i.stack.imgur.com/rA1lZ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/rA1lZ.png"" alt=""enter image description here""></a></p>

<p><strong>Step 4:</strong> Clone <code>reveal.js</code> in the directory where notebook exists from <a href=""https://github.com/hakimel/reveal.js"" rel=""nofollow noreferrer"">github</a>. <code>reveal.js</code> is used for rendering HTML file as presentation.</p>

<p><a href=""https://i.stack.imgur.com/dillF.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/dillF.png"" alt=""enter image description here""></a></p>

<p><strong>Step 5:</strong> Run the below command (same as in your question) to convert the notebook to slides without serving them (since there is no browser on the Sagemaker instance). This will just convert notebook to slides html.</p>

<pre><code>jupyter nbconvert Image-classification-fulltraining.ipynb --to slides
[NbConvertApp] Converting notebook Image-classification-fulltraining.ipynb to slides
[NbConvertApp] Writing 346423 bytes to Image-classification-fulltraining.slides.html
</code></pre>

<p><strong>Step 6:</strong> Now open the html file from Sagemaker notebook file browser </p>

<p><a href=""https://i.stack.imgur.com/fykyl.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/fykyl.png"" alt=""enter image description here""></a></p>

<p>Now you can see the notebook rendered as slides based on how setup each cell in your notebook in Step 1</p>

<p><a href=""https://i.stack.imgur.com/9PLUA.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/9PLUA.png"" alt=""enter image description here""></a></p>

<p>Hope it helps.</p>
","6069517",2
314,51622468,2,51622144,2018-07-31 21:38:01,1,"<p>You're right; I strongly suspect that you have one or more features in your X data that is nearly perfectly correlated with the Y data. Usually this is bad, because those variables don't explain Y but are either explained by Y or jointly determined with Y. To troubleshoot this, consider performing a linear regression of Y on X and then using simple p values or AIC/BIC to determine which X variables are the least relevant. Drop these and repeat the process until your R^2 begins to drop seriously (though it will drop a little each time). The remaining variables will be the most relevant in prediction, and hopefully you'll be able to identify from that subset which variables are so tightly correlated with Y.</p>
","10107760",5
315,51628484,2,49016896,2018-08-01 08:17:45,3,"<p>Instead of removing the ControlNo column from the dataset, you can use the <a href=""https://learn.microsoft.com/en-us/azure/machine-learning/studio-module-reference/edit-metadata"" rel=""nofollow noreferrer"">Edit Metadata</a> module to clear its ""Feature"" flag - just select the column and set <strong>Fields</strong> to <strong>Clear feature</strong>. </p>

<p><a href=""https://i.stack.imgur.com/EUy9A.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/EUy9A.png"" alt=""Edit Metadata settings""></a></p>

<p>This will cause the Azure ML Studio algorithms to ignore it during training, and you'll be able to return it as part of your output. </p>
","155697",0
316,50876324,2,50870166,2018-06-15 13:09:37,1,"<p>Following the suggestion from @Jon in the comment section as well as a suggestion on <a href=""https://social.msdn.microsoft.com/Forums/windowsdesktop/en-US/84a33ecc-1db2-4d11-83d2-3e96f0bcfaa7/why-open-in-a-new-notebook-is-invalid?forum=MachineLearning"" rel=""nofollow noreferrer"">microsoft.com</a>, I added a <strong>Convert to CSV Module</strong> at the end. After running the experiment, <strong>Open in a new Notebook</strong> is available when you right clik the <strong>Convert to CSV Module</strong>:</p>

<p><a href=""https://i.stack.imgur.com/Bz7nJ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Bz7nJ.png"" alt=""enter image description here""></a></p>

<p>What you get by clicking <kbd>Python 3</kbd> is this:</p>

<p><a href=""https://i.stack.imgur.com/ywbHz.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ywbHz.png"" alt=""enter image description here""></a></p>

<p>The functionality is certainly not as magnificent as I was hoping, but it's still pretty cool. If anyone knows <em>anything</em> about other possibilites or plans for future development, please don't hesitate to contribute with an answer!</p>
","3437787",3
317,50336893,2,50334563,2018-05-14 18:36:11,1,"<p>I think it depends on what workspace you're in. If you're in the free one then you get the screen that you already get, but if you create a workspace in the Azure portal and use that one, then you will get a screen like below.</p>

<p><a href=""https://i.stack.imgur.com/drRpa.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/drRpa.png"" alt=""enter image description here""></a></p>

<p>To create a new workspace, in the Azure Portal, create a new ""Machine Learning Studio Workspace"" and when you go to Azure ML Studio select the new workspace from the top right.</p>
","186013",4
318,50333288,2,50302810,2018-05-14 14:52:02,5,"<p>You don't need much. Just an AWS Account with the correlated permissions on your role.
Inside the AWS SageMaker Console you can just run an AWS Notebook Instance with one click. There is Sklearn preinstalled and you can use it out of the box. No special container needed.</p>

<p>As minimum you just need your AWS Account with the correlated permissions to create EC2 Instances and read / write from your S3. Thats all, just try it. :)</p>

<p>Use this as a starting point: <a href=""https://aws.amazon.com/blogs/aws/sagemaker/"" rel=""noreferrer"">Amazon SageMaker – Accelerating Machine Learning</a></p>

<p><a href=""https://i.stack.imgur.com/98gRb.png"" rel=""noreferrer"">You can also access it via the Jupyter Terminal</a></p>
","1964223",1
319,50055668,2,50035628,2018-04-27 05:17:04,1,"<p>So running <code>az ml service create realtime -h</code> provides information about the <code>-d</code> flag.</p>

<p><code>-d : Files and directories required by the service. Multiple dependencies can be specified with additional -d arguments.</code></p>

<p>Please try using this flag and provide the additional python file that you would like to call too from your <code>score.py</code></p>
","1341806",0
320,50006839,2,50003050,2018-04-24 16:37:20,0,"<p>Looks like you don't have access to the resource </p>

<pre><code>arn:aws:sagemaker:eu-west-1:307504647302:training-job/decision-trees-sample-2018-04-24-13-13-38-281
</code></pre>

<p>Can you check if the resource url is correct and the proper permissions are set in the security group. </p>
","6530653",5
321,49985763,2,49977679,2018-04-23 16:25:22,8,"<p>It is exactly as the error say, the variable <code>bucket</code> is not defined. 
you might want to do something like </p>

<pre><code>bucket = &lt;name of already created bucket in s3&gt;
</code></pre>

<p>before you call </p>

<pre><code>s3.Bucket(bucket).put_object(Key=key, Body=data)
</code></pre>
","6530653",1
322,49549520,2,49548422,2018-03-29 06:37:55,4,"<p>If your IAM roles are setup correctly, then you need to download the file to the Sagemaker instance first and then work on it. Here's how:</p>

<pre><code># Import roles
import sagemaker
role = sagemaker.get_execution_role()

# Download file locally
s3 = boto3.resource('s3')
s3.Bucket(bucket).download_file('your_training_s3_file.rec', 'training.rec')

#Access locally
train = mx.io.ImageRecordIter(path_imgrec=‘training.rec’ …… )
</code></pre>
","4061061",0
323,49390914,2,49374476,2018-03-20 17:43:01,6,"<p>If you've gotten that response, your request is successful. The output should be in the output file you specified - output.json :)</p>
","9175625",0
324,49370397,2,49365900,2018-03-19 18:53:49,1,"<p>It looks like you’re passing in the actual contents of the file as the file name?</p>

<p>I think you’ll need to download the object from S3 to a tmp file and pass the path to that file into restore.</p>

<p>Try using the method here: <a href=""http://boto3.readthedocs.io/en/latest/reference/services/s3.html#S3.Object.download_file"" rel=""nofollow noreferrer"">http://boto3.readthedocs.io/en/latest/reference/services/s3.html#S3.Object.download_file</a></p>

<p>Update:
I went through the code here: <a href=""https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/learn/python/learn/preprocessing/text.py"" rel=""nofollow noreferrer"">https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/learn/python/learn/preprocessing/text.py</a> and it looks like this just saves a pickle so you can really easily just import pickle and call the following:</p>

<pre><code>import pickle
obj = s3.Object(BUCKET_NAME, KEY).get()['Body']
vocab_processor = pickle.loads(obj.read())
</code></pre>

<p>Hopefully that works?</p>
","240004",3
325,49330074,2,49112204,2018-03-16 21:58:07,0,"<p>I think you're almost there! The Apply SQL Transformation module still outputs a result dataset, so try switching your statements around:</p>

<pre><code>update t1 
set [Scored Labels] = 0
where [Scored Labels] &lt; 0;

select * from t1; 
</code></pre>

<p>See how that works. :)</p>
","186013",0
326,49276402,2,49130977,2018-03-14 11:19:15,1,"<p>This is only based on my experience with Azure ML, but I think I can help with your questions.</p>

<blockquote>
  <p>When web job hits our predictive webservice, will the trained ML model be run again?</p>
</blockquote>

<p>Yes, in the sense that it will call the <code>predict</code> (or similar) method on the model on the new data. For instance, in <code>scikit-learn</code> you would train your model using the <code>fit</code> method. Once the model is in production, only the <code>predict</code> method would be called.</p>

<p>It will also run the whole workflow you have set up to be deployed as the web service. As an example below is a workflow I've played around with before. Each time the web service is run with new data, this whole thing will be run. This is like creating a Pipeline in <code>scikit-learn</code>.</p>

<p><a href=""https://i.stack.imgur.com/YMFZb.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/YMFZb.png"" alt=""Azure ML Workflow""></a></p>

<blockquote>
  <p>Do we need to convert the UTC timezone for new incoming test data( which we want to predict) into CST/CDT timezone, as TimeStamp does matter for our prediction?</p>
</blockquote>

<p>I would say yes, you would need to convert to the timezone that was used when training in the model. This can be done by adding a step in your workflow then when you call the web service it will do the necessary converting for you before making a prediction.</p>

<blockquote>
  <p>What happens in backend when predictive webservice API is called?</p>
</blockquote>

<p>I'm not sure if anyone knows for sure other than the folks at Microsoft, but for sure it will run the workflow you have set up.</p>

<hr>

<p>I know it's not much, but I hope this helps or at least gets you on the right track for what you need.</p>
","186013",2
327,49214427,2,49103679,2018-03-10 21:57:41,3,"<p>Following SRC's advice, I was able to get it to work by following the instructions in this <a href=""https://stackoverflow.com/questions/47190614/how-to-load-a-trained-mxnet-model"">question</a> and this <a href=""http://mxnet.incubator.apache.org/tutorials/python/predict_image.html"" rel=""nofollow noreferrer"">doc</a> which describe how to load a MXnet model.</p>

<p>I loaded the model like so:</p>

<pre><code>lenet_model = mx.mod.Module.load('model_directory/image-classification',5)
image_l = 64
image_w = 64
lenet_model.bind(for_training=False, data_shapes=[('data',(1,3,image_l,image_w))],label_shapes=lenet_model._label_shapes)
</code></pre>

<p>Then predicted using the slightly modified helper functions in the previously linked documentation:</p>

<pre><code>import mxnet as mx
import matplotlib.pyplot as plot
import cv2
import numpy as np
from mxnet.io import DataBatch

def get_image(url, show=False):
    # download and show the image
    fname = mx.test_utils.download(url)
    img = cv2.cvtColor(cv2.imread(fname), cv2.COLOR_BGR2RGB)
    if img is None:
         return None
    if show:
         plt.imshow(img)
         plt.axis('off')
    # convert into format (batch, RGB, width, height)
    img = cv2.resize(img, (64, 64))
    img = np.swapaxes(img, 0, 2)
    img = np.swapaxes(img, 1, 2)
    img = img[np.newaxis, :]
    return img

def predict(url, labels):
    img = get_image(url, show=True)
    # compute the predict probabilities
    lenet_model.forward(DataBatch([mx.nd.array(img)]))
    prob = lenet_model.get_outputs()[0].asnumpy()

    # print the top-5
    prob = np.squeeze(prob)
    a = np.argsort(prob)[::-1]

    for i in a[0:5]:
       print('probability=%f, class=%s' %(prob[i], labels[i]))
</code></pre>

<p>Finally I called the prediction with this code:</p>

<pre><code>labels = ['a','b','c', 'd','e', 'f']
predict('https://eximagesite/img_tst_a.jpg', labels )
</code></pre>
","5302826",2
328,49202490,2,49168673,2018-03-09 21:36:14,1,"<p>Amazon SageMaker is a combination of multiple services that each is independent of the others. You can use the notebook instances if you want to develop your models in the familiar Jupyter environment. But if just need to train a model, you can use the training jobs without opening a notebook instance. </p>

<p>There a few ways to launch a training job:</p>

<ul>
<li>Use the high-level SDK for Python that is similar to the way that you start a training step in your python code</li>
</ul>

<p><code>kmeans.fit(kmeans.record_set(train_set[0]))</code></p>

<p>Here is the link to the python library: <a href=""https://github.com/aws/sagemaker-python-sdk"" rel=""nofollow noreferrer"">https://github.com/aws/sagemaker-python-sdk</a></p>

<ul>
<li>Use the low-level API to Create-Training-Job, and you can do that using various SDK (Java, Python, JavaScript, C#...) or the CLI. </li>
</ul>

<p><code>sagemaker = boto3.client('sagemaker')
 sagemaker.create_training_job(**create_training_params)</code></p>

<p>Here is a link to the documentation on these options: <a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/ex1-train-model-create-training-job.html"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/sagemaker/latest/dg/ex1-train-model-create-training-job.html</a> </p>

<ul>
<li>Use Spark interface to launch it using a similar interface to creating an MLLib training job</li>
</ul>

<p><code>val estimator = new KMeansSageMakerEstimator(
  sagemakerRole = IAMRole(roleArn),
  trainingInstanceType = ""ml.p2.xlarge"",
  trainingInstanceCount = 1,
  endpointInstanceType = ""ml.c4.xlarge"",
  endpointInitialInstanceCount = 1)
  .setK(10).setFeatureDim(784)</code></p>

<p><code>val model = estimator.fit(trainingData)</code></p>

<p>Here is a link to the spark-sagemaker library: <a href=""https://github.com/aws/sagemaker-spark"" rel=""nofollow noreferrer"">https://github.com/aws/sagemaker-spark</a></p>

<ul>
<li>Create a training job in the Amazon SageMaker console using the wizard there: <a href=""https://console.aws.amazon.com/sagemaker/home?region=us-east-1#/jobs"" rel=""nofollow noreferrer"">https://console.aws.amazon.com/sagemaker/home?region=us-east-1#/jobs</a></li>
</ul>

<p>Please note that there a few options also to train models, either using the built-in algorithms such as K-Means, Linear Learner or XGBoost (see here for the complete list: <a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/algos.html"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/sagemaker/latest/dg/algos.html</a>). But you can also bring your own models for pre-baked Docker images such as TensorFlow (<a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/tf.html"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/sagemaker/latest/dg/tf.html</a>) or MXNet (<a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/mxnet.html"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/sagemaker/latest/dg/mxnet.html</a>), your own Docker image (<a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/your-algorithms-training-algo.html"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/sagemaker/latest/dg/your-algorithms-training-algo.html</a>).  </p>
","179529",0
329,48883665,2,48879595,2018-02-20 10:57:07,0,"<p>I got it working. Changed the folder inside .zip file to <code>""cassandra""</code> (just like cassandra package). </p>

<p>And in the Python script, i added </p>

<pre><code>from cassandra import *
</code></pre>
","7719284",0
330,48849691,2,48158545,2018-02-18 08:10:29,1,"<p>Yes, there is a way and it is simple. What you need is an excel add-in. You need not create any other account.</p>

<p>You can either read <a href=""https://learn.microsoft.com/en-us/azure/machine-learning/studio/excel-add-in-for-web-services"" rel=""nofollow noreferrer"">Excel Add-in for Azure Machine Learning web services doc</a> or you can watch <a href=""https://www.youtube.com/watch?v=ju1CzDjiOMQ"" rel=""nofollow noreferrer"">Azure ML Excel Add-in video</a>. </p>

<p>If you search for <a href=""https://www.google.co.in/search?q=excel%20add%20in%20for%20azure%20ml&amp;client=firefox-b-ab&amp;dcr=0&amp;source=lnms&amp;tbm=vid&amp;sa=X&amp;ved=0ahUKEwinqP3a_67ZAhXBr48KHdiYAXUQ_AUICigB&amp;biw=1280&amp;bih=616"" rel=""nofollow noreferrer"">videos on excel add in for azure ml</a>, you get other useful videos too. </p>

<p>I hope this is the solution you are looking for.</p>
","5343790",0
331,48813008,2,48539564,2018-02-15 17:26:35,1,"<p>I have found out where this is - for some reason ""checkpoints"" seems to be a reserved word - changing the word to ""checks"" allowed me to write the folder.  Hope this helps someone!</p>
","1058210",0
332,48687026,2,48650152,2018-02-08 13:46:57,1,"<p>You could look at the Sample notebooks, how to upload the data S3 bucket 
There have many ways. I am just giving you hints to answer. 
And you forgot create a boto3 session to access the S3 bucket </p>

<p><strong>It is one of the ways to do it.</strong> </p>

<pre><code>import os 
import urllib.request
import boto3

def download(url):
    filename = url.split(""/"")[-1]
    if not os.path.exists(filename):
        urllib.request.urlretrieve(url, filename)


def upload_to_s3(channel, file):
    s3 = boto3.resource('s3')
    data = open(file, ""rb"")
    key = channel + '/' + file
    s3.Bucket(bucket).put_object(Key=key, Body=data)


# caltech-256
download('http://data.mxnet.io/data/caltech-256/caltech-256-60-train.rec')
upload_to_s3('train', 'caltech-256-60-train.rec')
download('http://data.mxnet.io/data/caltech-256/caltech-256-60-val.rec')
upload_to_s3('validation', 'caltech-256-60-val.rec')
</code></pre>

<p>link : <a href=""https://buildcustom.notebook.us-east-2.sagemaker.aws/notebooks/sample-notebooks/introduction_to_amazon_algorithms/imageclassification_caltech/Image-classification-fulltraining.ipynb"" rel=""nofollow noreferrer"">https://buildcustom.notebook.us-east-2.sagemaker.aws/notebooks/sample-notebooks/introduction_to_amazon_algorithms/imageclassification_caltech/Image-classification-fulltraining.ipynb</a></p>

<p><strong>Another way to do it.</strong> </p>

<pre><code>bucket = '&lt;your_s3_bucket_name_here&gt;'# enter your s3 bucket where you will copy data and model artifacts
prefix = 'sagemaker/breast_cancer_prediction' # place to upload training files within the bucket
# do some processing then prepare to push the data. 

f = io.BytesIO()
smac.write_numpy_to_dense_tensor(f, train_X.astype('float32'), train_y.astype('float32'))
f.seek(0)

boto3.Session().resource('s3').Bucket(bucket).Object(os.path.join(prefix, 'train', train_file)).upload_fileobj(f)
</code></pre>

<p>Link : <a href=""https://buildcustom.notebook.us-east-2.sagemaker.aws/notebooks/sample-notebooks/introduction_to_applying_machine_learning/breast_cancer_prediction/Breast%20Cancer%20Prediction.ipynb"" rel=""nofollow noreferrer"">https://buildcustom.notebook.us-east-2.sagemaker.aws/notebooks/sample-notebooks/introduction_to_applying_machine_learning/breast_cancer_prediction/Breast%20Cancer%20Prediction.ipynb</a></p>

<p>Youtube link : <a href=""https://www.youtube.com/watch?v=-YiHPIGyFGo"" rel=""nofollow noreferrer"">https://www.youtube.com/watch?v=-YiHPIGyFGo</a> - how to pull the data in S3 bucket.</p>
","6555647",0
333,48378719,2,48197524,2018-01-22 09:53:28,1,"<p>you could use ""<strong>Execute Python Script</strong>"" or ""<strong>Execute R Script</strong>"" to archive that. Or just use ""<strong>Apply SQL Transformation</strong>"" -> <code>SELECT * FROM tbl1 where column1 IS NULL AND column2 IS NULL</code>.... </p>

<p>Greetings,
Stefan</p>
","9250720",0
334,48375741,2,48310237,2018-01-22 06:28:10,0,"<p>When you are calling the training job you should specify the output directory:</p>

<pre><code>#Bucket location where results of model training are saved.
model_artifacts_location = 's3://&lt;bucket-name&gt;/artifacts'

m = MXNet(entry_point='lstm_trainer.py',
          role=role,
          output_path=model_artifacts_location,
          ...)
</code></pre>

<p>If you don't specify the output directory the function will use a default location, that it might not have the permissions to create or write to.</p>
","179529",0
335,48267804,2,48264656,2018-01-15 17:16:02,12,"<p>If you have a look <a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-dg.pdf"" rel=""noreferrer"">here</a> it seems you can specify this in the <em>InputDataConfig</em>. Search for ""S3DataSource"" (<a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/API_S3DataSource.html"" rel=""noreferrer"">ref</a>) in the document. The first hit is even in Python, on page 25/26.</p>
","2087854",0
336,48130011,2,47847736,2018-01-06 17:35:39,1,"<p>Seems like AWS SageMaker team answered the question, 
<a href=""https://forums.aws.amazon.com/thread.jspa?threadID=270493&amp;tstart=0"" rel=""nofollow noreferrer"">https://forums.aws.amazon.com/thread.jspa?threadID=270493&amp;tstart=0</a></p>
","9181901",0
337,48087578,2,48087407,2018-01-04 01:43:00,2,"<p><code>NULL</code> is indeed a value; entries containing NULLs are <em>not</em> missing, hence they are neither cleaned with the 'Clean Missing Data' operator nor reported as missing.</p>
","4685471",0
338,47984882,2,47946790,2017-12-26 23:55:00,3,"<p>Looks like this question was also answered on the <a href=""https://forums.aws.amazon.com/thread.jspa?threadID=270054&amp;tstart=0"" rel=""nofollow noreferrer"">AWS Forums</a>.</p>
<p>The IAM Role referenced by</p>
<blockquote>
<p>role = get_execution_role()</p>
</blockquote>
<p>needs to have a policy attached to it that grants S3:GetObject permission on the S3 bucket holding your training data.</p>
<hr />
<p>Note that as of at least October 28, 2022, the linked forum post now re-directs to a page which states (among other things):</p>
<blockquote>
<p>The thread you are trying to access has outdated guidance, hence we have archived it.</p>
</blockquote>
<p>Please keep this in mind as it is possible that this answer no longer works, or that it at some point in the future will no longer work.</p>
","4941254",0
339,47945017,2,47778076,2017-12-22 17:07:27,1,"<p>Hi I had the same problem 2 days ago with the function <code>pull()</code>, always of the package <code>dplyr</code>.
The problem is that the both version of R (CRAN R 3.1.0 and Microsoft R open 3.2.2) supported by Azure Machine Learning Studio, does not support the version <code>0.7.4</code> of package <code>dplyr</code>.
If you read the <a href=""https://cran.r-project.org/web/packages/dplyr/dplyr.pdf"" rel=""nofollow noreferrer"">documentation</a> related to the package <code>dplyr</code> you can see that the package is installable only for R versions >= 3.1.2.</p>

<p>Then you must wait for the R version used by Azure Machine Learning Studio be updated, or find an alternative solution to your function.</p>
","7831752",0
340,47879940,2,47735839,2017-12-19 04:14:26,0,"<p>The execution time on AzureML Studio depends on the pricing tier. The free version does one node execution at time while the standard pricing tier do the execute multiple execution at one time. </p>
","5383733",0
341,47602126,2,31863977,2017-12-01 22:04:43,0,"<p>Look for <strong>Project Passau</strong>. Here's one of the references, in <a href=""https://visualstudiomagazine.com/articles/2014/09/01/azure-machine-learning-studio.aspx"" rel=""nofollow noreferrer"">Visual Studio Magazine</a> or another one <a href=""https://blogs.msdn.microsoft.com/mspowerutilities/2014/07/08/harnessing-the-power-of-big-data-with-cloud-predictive-analytics-introducing-azure-machine-learning/"" rel=""nofollow noreferrer"">here</a>.</p>
","674700",1
342,46993016,2,46965383,2017-10-28 18:22:47,0,"<p>To summarize the comments -- Applying a binary mask to the loss function as described in the post seems to be the appropriate way to mask the loss function. However, there may be other unintended consequences from reducing the effetive batch size of C that would discourage this approach.</p>
","4086968",0
343,46977493,2,46963846,2017-10-27 14:32:20,2,"<p>We discovered a bug in our system that could have caused this. The fix was deployed last night. Can you please try again and let us know if you still encounter this issue?</p>
","1153282",0
344,46916413,2,46898565,2017-10-24 17:18:19,1,"<p>It's not supported by CMLE Online Prediction.
There is a list of supported GCP services here: <a href=""https://cloud.google.com/docs/authentication/api-keys"" rel=""nofollow noreferrer"">https://cloud.google.com/docs/authentication/api-keys</a></p>
","6900734",1
345,46832298,2,46823434,2017-10-19 14:27:11,1,"<p>The Census sample <a href=""https://github.com/GoogleCloudPlatform/cloudml-samples/blob/master/census/estimator/trainer/model.py#L197"" rel=""nofollow noreferrer"">shows</a> how to setup the serving_input_fn for CSV. Adjusted for your example:</p>

<pre><code>CSV_COLUMNS = ['feat1', 'feat2', 'feat3', 'feat4', 'feat5']
CSV_COLUMN_DEFAULTS = [[0.0],[0.0],[0.0],[0.0],[0.0]] 

def parse_csv(rows_string_tensor):
  """"""Takes the string input tensor and returns a dict of rank-2 tensors.""""""

  # Takes a rank-1 tensor and converts it into rank-2 tensor
  # Example if the data is ['csv,line,1', 'csv,line,2', ..] to
  # [['csv,line,1'], ['csv,line,2']] which after parsing will result in a
  # tuple of tensors: [['csv'], ['csv']], [['line'], ['line']], [[1], [2]]
  row_columns = tf.expand_dims(rows_string_tensor, -1)
  columns = tf.decode_csv(row_columns, record_defaults=CSV_COLUMN_DEFAULTS)
  features = dict(zip(CSV_COLUMNS, columns))

  return features

def csv_serving_input_fn():
  """"""Build the serving inputs.""""""
  csv_row = tf.placeholder(
      shape=[None],
      dtype=tf.string
  )
  features = parse_csv(csv_row)
  return tf.contrib.learn.InputFnOps(features, None, {'csv_row': csv_row})

# No need for fcols/fspec
est.export_savedmodel(MODEL, serving_input_fn)
</code></pre>

<p>TensorFlow 1.4 will simplify at least some of this.</p>

<p>Also, consider using JSON, as that is the more standard approach for serving. Happy to provide details upon request.</p>
","1399222",5
346,46679864,2,46671944,2017-10-11 04:28:10,0,"<p>please see the time-series anomaly detection module. it should do what you need:</p>

<p><a href=""https://msdn.microsoft.com/library/azure/96b98cc0-50df-46ff-bc18-c0665d69f3e3?f=255&amp;MSPPError=-2147217396"" rel=""nofollow noreferrer"">https://msdn.microsoft.com/library/azure/96b98cc0-50df-46ff-bc18-c0665d69f3e3?f=255&amp;MSPPError=-2147217396</a></p>
","6191408",1
347,46634044,2,46615264,2017-10-08 17:35:12,1,"<p>You can use CUSTOM tier with only a single master node, and no workers/parameter servers. Those are optional parameters. </p>

<p>Then <code>complex_model_m_gpu</code> has 4 GPUs, and <code>complex_model_l_gpu</code> has 8. </p>
","6933420",0
348,46543840,2,46543320,2017-10-03 11:45:39,1,"<p>The problem was due to skipping a step in <a href=""https://cloud.google.com/ml/docs/how-tos/getting-set-up"" rel=""nofollow noreferrer"">here</a>.<br>
Specifically - use the following commands:</p>

<pre><code>gcloud config set project [your-project-id]
gcloud auth application-default login
</code></pre>
","817452",0
349,46538868,2,46523924,2017-10-03 07:03:59,2,"<p>For installing python module on Azure ML Studio, there is a section <a href=""https://msdn.microsoft.com/library/azure/cdb56f95-7f4c-404d-bde7-5bb972e6f232/#Anchor_3"" rel=""nofollow noreferrer""><code>Technical Notes</code></a> of the offical document <code>Execute Python Script</code> which introduces it.</p>

<p>The general steps as below.</p>

<ol>
<li>Create a Python project via <code>virtualenv</code> and active it.</li>
<li>Install all packages you want via <code>pip</code> on the virtual Python environment, and then</li>
<li>Package all files and directorys under the path <code>Lib\site-packages</code> of your project as a zip file.</li>
<li>Upload the zip package into your Azure ML WorkSpace as a dataSet.</li>
<li>Follow the offical <a href=""https://learn.microsoft.com/en-us/azure/machine-learning/studio/execute-python-scripts#importing-existing-python-script-modules"" rel=""nofollow noreferrer"">document</a> to import Python Module for your <code>Execute Python Script</code>.</li>
</ol>

<p>For more details, you can refer to the other similar SO thread <a href=""https://stackoverflow.com/questions/46222606/updating-pandas-to-version-0-19-in-azure-ml-studio/46232963#46232963"">Updating pandas to version 0.19 in Azure ML Studio</a>, it even introduced how to update the version of Python packages installed by Azure.</p>

<p>Hope it helps.</p>
","4989676",1
350,46535860,2,46379932,2017-10-03 01:13:53,0,"<p>There actually was a bug when running this on the Cloud ML Engine because the checkpoints are disabled for now on GCS (Keras can't natively write checkpoints to GCS). See this <a href=""https://github.com/GoogleCloudPlatform/cloudml-samples/pull/85"" rel=""nofollow noreferrer"">PR</a> for the immediate fix for the issue you are facing. Also take a look at <a href=""https://github.com/GoogleCloudPlatform/cloudml-samples/pull/78"" rel=""nofollow noreferrer"">pending PR</a> which fixes the checkpoint issue and makes files available on GCS (Workaround for the inability to do GCS writes for Keras). </p>
","6031363",2
351,46518418,2,46500756,2017-10-02 01:19:53,0,"<p>access to the workspace is controlled by the workspace owner from the settings page inside of the Azure ML workspace. the owners/contributors etc. listed in Azure portal does NOT grant you access to the workspace. </p>
","6191408",1
352,46232963,2,46222606,2017-09-15 06:21:53,6,"<p>I offer the below steps for you to show how to update the version of pandas  library in <code>Execute Python Script</code>.</p>

<p><strong><em>Step 1</em></strong> : Use the <code>virtualenv</code> component to create an independent python runtime environment in your system.Please install it first with command <code>pip install virtualenv</code> if you don't have it.</p>

<p>If you installed it successfully ,you could see it in your python/Scripts file.</p>

<p><a href=""https://i.stack.imgur.com/ZFI2t.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/ZFI2t.png"" alt=""enter image description here""></a></p>

<p><strong><em>Step2</em></strong> : Run the commad to create independent python runtime environment.</p>

<p><a href=""https://i.stack.imgur.com/nzDqz.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/nzDqz.png"" alt=""enter image description here""></a></p>

<p><strong><em>Step 3</em></strong> : Then go into the created directory's Scripts folder and activate it (this step is important , don't miss it)</p>

<p>Please don't close this command window and use <code>pip install pandas==0.19</code> to download external libraries in this command window.</p>

<p><a href=""https://i.stack.imgur.com/Wj857.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/Wj857.png"" alt=""enter image description here""></a></p>

<p><strong><em>Step 4</em></strong> : Compress all of the files in the Lib/site-packages folder into a zip package (I'm calling it pandas - package here)</p>

<p><a href=""https://i.stack.imgur.com/Ch9Oo.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/Ch9Oo.png"" alt=""enter image description here""></a></p>

<p><strong><em>Step 5</em></strong> ：Upload the zip package into the Azure Machine Learning WorkSpace DataSet.</p>

<p><a href=""https://i.stack.imgur.com/efRkK.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/efRkK.png"" alt=""enter image description here""></a></p>

<p>specific steps please refer to the <a href=""https://msdn.microsoft.com/library/azure/cdb56f95-7f4c-404d-bde7-5bb972e6f232/"" rel=""noreferrer"">Technical Notes</a>.</p>

<p>After success, you will see the uploaded package in the DataSet List</p>

<p><a href=""https://i.stack.imgur.com/ngGCu.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/ngGCu.png"" alt=""enter image description here""></a></p>

<p><strong><em>Step 6</em></strong> ： Before the defination of method <code>azureml_main</code> in the Execute Python Script module, you need to remove the old <code>pandas</code> modules &amp; its dependencies, then to import <code>pandas</code> again, as the code below.</p>

<pre><code>import sys
import pandas as pd
print(pd.__version__)
del sys.modules['pandas']
del sys.modules['numpy']
del sys.modules['pytz']
del sys.modules['six']
del sys.modules['dateutil']
sys.path.insert(0, '.\\Script Bundle')
for td in [m for m in sys.modules if m.startswith('pandas.') or m.startswith('numpy.') or m.startswith('pytz.') or m.startswith('dateutil.') or m.startswith('six.')]:
    del sys.modules[td]
import pandas as pd
print(pd.__version__)
# The entry point function can contain up to two input arguments:
#   Param&lt;dataframe1&gt;: a pandas.DataFrame
#   Param&lt;dataframe2&gt;: a pandas.DataFrame
def azureml_main(dataframe1 = None, dataframe2 = None):
</code></pre>

<p>Then you can see the result from logs as below, first print the old version <code>0.14.0</code>, then print the new version <code>0.19.0</code> from the uploaded zip file.</p>

<pre><code>[Information]         0.14.0
[Information]         0.19.0
</code></pre>

<p>You could also refer to these threads: <a href=""https://stackoverflow.com/questions/45749479/access-blob-file-using-time-stamp-in-azure/45814318#45814318"">Access blob file using time stamp in Azure</a> and <a href=""https://stackoverflow.com/questions/12669546/reload-with-reset"">reload with reset</a>.</p>

<p>Hope it helps you.</p>
","8198946",0
353,46133533,2,46132850,2017-09-09 17:53:27,1,"<p>As said before, Permutation Feature Importance do the trick. Attach the Permutation Feature Importance block do the train block, click on the output port, and select visualize to get results of the module. The figure above shows the list of features sorted in descending order of their permutation importance scores. 
<a href=""https://i.stack.imgur.com/NQRsK.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/NQRsK.png"" alt=""Importance Score output""></a></p>

<p>An advice: be careful when interpreting results of permutation score when you have high correlated features.</p>

<p>For more info, see: 
<a href=""https://standupdata.com/category/permutation-feature-importance/"" rel=""nofollow noreferrer"">https://standupdata.com/category/permutation-feature-importance/</a> <a href=""https://gallery.cortanaintelligence.com/Experiment/Permutation-Feature-Importance-5"" rel=""nofollow noreferrer"">https://gallery.cortanaintelligence.com/Experiment/Permutation-Feature-Importance-5</a></p>
","6457024",0
354,45947553,2,45889273,2017-08-29 20:06:52,2,"<p>Because the <code>summary</code> node in the graph is just a node. It still needs to be evaluated (outputting a protobuf string), and that string still needs to be written to a file. It's not evaluated in training mode because it's not upstream of the <code>train_op</code> in your graph, and even if it were evaluated, it wouldn't be written to a file unless you specified a <a href=""https://www.tensorflow.org/api_docs/python/tf/train/SummarySaverHook"" rel=""nofollow noreferrer""><code>tf.train.SummarySaverHook</code></a> as one of you <code>training_chief_hooks</code> in your <code>EstimatorSpec</code>. Because the <code>Estimator</code> class doesn't assume you want any extra evaluation during training, normally evaluation is only done during the EVAL phase, and you just increase <code>min_eval_frequency</code> or <code>checkpoint_frequency</code> to get more evaluation datapoints.</p>

<p>If you <em>really really</em> want to log a summary during training here's how you'd do it:</p>

<pre><code>def model_fn(mode, features, labels, params):
  ...
  if mode == Modes.TRAIN:
    # loss is already written out during training, don't duplicate the summary op
    loss = tf.contrib.legacy_seq2seq.sequence_loss(logits, outputs, weights)
    sequence_accuracy = sequence_accuracy(targets, predictions,weights)
    seq_sum_op = tf.summary.scalar('sequence_accuracy', sequence_accuracy)
    with tf.control_depencencies([seq_sum_op]):
       train_op = optimizer.minimize(loss)

    return tf.estimator.EstimatorSpec(
      loss=loss,
      mode=mode,
      train_op=train_op,
      training_chief_hooks=[tf.train.SummarySaverHook(
          save_steps=100,
          output_dir='./summaries',
          summary_op=seq_sum_op
      )]
    )
</code></pre>

<p>But it's better to just increase your eval frequency and make an <code>eval_metric_ops</code> for accuracy with <code>tf.metrics.streaming_accuracy</code></p>
","3597868",9
355,45908414,2,45576092,2017-08-27 19:08:42,0,"<p>Here are two links that might help:</p>

<ol>
<li><a href=""https://github.com/Azure/Machine-Learning-Operationalization"" rel=""nofollow noreferrer"">AzureML Operationalization</a> </li>
<li><a href=""https://github.com/Azure/Machine-Learning-Operationalization/blob/master/samples/python/tutorials/realtime/digit_classification.ipynb"" rel=""nofollow noreferrer"">Example notebook</a> that shows how to publish Python model as a web service. You would do a similar thing, only you would pickle the dictionary of your models instead. </li>
</ol>

<p>Note that this functionality is currently in preview mode.</p>
","1615450",1
356,45901289,2,45892583,2017-08-27 02:47:10,2,"<p>It looks like Google Cloud ML Engine only supports serving models produced using <code>tensorflow 1.2.0</code> and below as of now. See here: <a href=""https://cloud.google.com/ml-engine/docs/concepts/runtime-version-list"" rel=""nofollow noreferrer"">https://cloud.google.com/ml-engine/docs/concepts/runtime-version-list</a></p>

<p>Use <code>--runtime-version 1.2</code> if possible. If you are using a feature specific to <code>tensorflow 1.3</code>, you will need to host your model using <code>Flask</code> on Google App Engine until ML Engine support for <code>tensorflow 1.3</code> arrives.</p>
","4736556",0
357,45890363,2,45887073,2017-08-25 22:58:18,1,"<p>I believe what you are looking for can be found in the <a href=""https://cloud.google.com/ml-engine/docs/how-tos/online-predict#requesting_predictions"" rel=""nofollow noreferrer"">official documentation</a> under the section ""Requesting predictions"" (be sure to click on the Python tab).</p>

<p>For your convenience:</p>

<pre><code>def predict_json(project, model, instances, version=None):
    """"""Send json data to a deployed model for prediction.

    Args:
        project (str): project where the Cloud ML Engine Model is deployed.
        model (str): model name.
        instances ([Mapping[str: Any]]): Keys should be the names of Tensors
            your deployed model expects as inputs. Values should be datatypes
            convertible to Tensors, or (potentially nested) lists of datatypes
            convertible to tensors.
        version: str, version of the model to target.
    Returns:
        Mapping[str: any]: dictionary of prediction results defined by the
            model.
    """"""
    # Create the ML Engine service object.
    # To authenticate set the environment variable
    # GOOGLE_APPLICATION_CREDENTIALS=&lt;path_to_service_account_file&gt;
    service = googleapiclient.discovery.build('ml', 'v1')
    name = 'projects/{}/models/{}'.format(project, model)

    if version is not None:
        name += '/versions/{}'.format(version)

    response = service.projects().predict(
        name=name,
        body={'instances': instances}
    ).execute()

    if 'error' in response:
        raise RuntimeError(response['error'])

    return response['predictions']
</code></pre>
","1399222",2
358,45888389,2,45868971,2017-08-25 19:38:45,1,"<p>The division between what happens in the <code>input_fn</code> and what happens in the <code>model_fn</code> is entirely determined by what behavior you want at inference time. As a general rule of thumb:</p>

<ul>
<li>If you need to perform a transformation on both the training input and the prediction input, put it in the <code>model_fn</code></li>
<li>If you need to preform a transformation only on either the training input or prediction input, put it in the corresponding <code>input_fn</code> (serving or training/eval)</li>
</ul>

<p>This is only a rule for convenience, it will work either way. But often you want to put as much preprocessing outside the graph as possible for training/eval, so you don't duplicate the compute time of the preprocessing when you train for multiple epochs, or try a new model architecture. However, you then want to put as much of that preprocessing <em>inside</em> the graph as possible for inference, since it will (generally) be more efficient from a latency perspective than proxying.</p>

<p>Hope this clears things up.</p>
","3597868",3
359,45847177,2,45243527,2017-08-23 18:40:24,0,"<p>I just went through this process myself for the first time 2 weeks ago. What I'd recommend is using this <a href=""https://cloud.google.com/ml-engine/docs/how-tos/getting-started-training-prediction"" rel=""nofollow noreferrer"">tutorial</a> (created by the kind folks at Google).</p>

<blockquote>
  <ul>
  <li><p><a href=""https://cloud.google.com/ml-engine/docs/how-tos/getting-started-training-prediction#develop_and_validate_your_trainer_locally"" rel=""nofollow noreferrer"">Develop and validate your trainer locally</a></p></li>
  <li><p><a href=""https://cloud.google.com/ml-engine/docs/how-tos/getting-started-training-prediction#cloud-train-single"" rel=""nofollow noreferrer"">Run a single-instance trainer in the cloud</a>  </p></li>
  <li><p><a href=""https://cloud.google.com/ml-engine/docs/how-tos/getting-started-training-prediction#deploy_a_model_to_support_prediction"" rel=""nofollow noreferrer"">Deploy a model to support prediction</a> </p></li>
  <li><p><a href=""https://cloud.google.com/ml-engine/docs/how-tos/getting-started-training-prediction#send_a_prediction_request_to_a_deployed_model"" rel=""nofollow noreferrer"">Send a prediction request to a deployed model</a></p></li>
  </ul>
</blockquote>

<p>I don't remember running into any big issues, but let me know if you hit any road blocks and I might be able to help you out.</p>

<p>To change the prediction input from json to csv in the example from the above linked tutorial, you'll notice that the default given is 'JSON', but this can be changed to 'CSV' (<a href=""https://github.com/GoogleCloudPlatform/cloudml-samples/blob/d6b35be29db2a7861a9f5f12a2a9d82e61a31c0f/census/estimator/trainer/task.py#L168"" rel=""nofollow noreferrer"">source</a>):</p>

<pre><code>parser.add_argument(
      '--export-format',
      help='The input format of the exported SavedModel binary',
      choices=['JSON', 'CSV', 'EXAMPLE'],
      default='JSON'
  )
</code></pre>

<p>This means you can specify <code>--export-format 'CSV'</code> when you create the model. For example:</p>

<pre><code>python trainer/task.py \
--train-files ~/Documents/data/adult.data.csv \
--eval-files ~/Documents/data/adult.test.csv \
--job-dir ~/Documents/models/census/v1 \
--train-steps 100 \
--verbosity 'DEBUG' \
--export-format 'CSV'
</code></pre>
","3524960",0
360,45842707,2,45840665,2017-08-23 14:35:30,6,"<p>It generally means your regularization technique and/or activation function is forcing activations to zero. You haven't shared details of your model, but this is common when using dropout, especially with relu activation functions.</p>

<p>Models with lots of zero activations tend to generalize better and therefore give better accuracy.</p>

<p>If you want more details, here's a <a href=""http://jmlr.org/papers/v15/srivastava14a.html"" rel=""nofollow noreferrer"">JMLR paper on dropout</a>.</p>

<p>I do have to note that having activations go to zero is sometimes bad, at least for ReLU activation functions. Basically, they can irreverisbly ""die"". So if you are seeing poor model quality beware. More information <a href=""https://datascience.stackexchange.com/questions/5706/what-is-the-dying-relu-problem-in-neural-networks"">here</a>.</p>
","1399222",2
361,45840109,2,45828620,2017-08-23 12:44:07,4,"<p>There is currently no way to see GPU utilization with Cloud ML Engine.</p>

<p>TensorFlow has a feature called timelines which can be used to obtain profile data. Here's a <a href=""https://medium.com/towards-data-science/howto-profile-tensorflow-1a49fb18073d"" rel=""nofollow noreferrer"">blog post</a> describing how to use it.</p>
","4392784",2
362,45813644,2,45796489,2017-08-22 09:21:27,0,"<p>this error is basically saying the apiKey you provided is invalid to perform the update resource operation. Here is some posts for your reference: <a href=""https://social.msdn.microsoft.com/Forums/azure/en-US/3bb77e37-8860-43c6-bcaa-d6ebd70617b8/retrain-predictive-web-service-programmatically-when-do-not-have-access-to-managementazuremlnet?forum=MachineLearning"" rel=""nofollow noreferrer"">https://social.msdn.microsoft.com/Forums/azure/en-US/3bb77e37-8860-43c6-bcaa-d6ebd70617b8/retrain-predictive-web-service-programmatically-when-do-not-have-access-to-managementazuremlnet?forum=MachineLearning</a></p>

<p>Please also be noted that if you modified your linked service in ADF, remember to re-deploy the pipeline as well to reflect your change in time.</p>
","7661337",1
363,45812333,2,45009184,2017-08-22 08:14:14,2,"<p>Have you installed Azure PowerShell Installer on your local machine?
<strong><a href=""https://github.com/Azure/azure-powershell/releases"" rel=""nofollow noreferrer"">Click here</a></strong> for more info.</p>

<p>Download the latest <strong>Azure PowerShell Installer (4.3.1)</strong>, then install on your local machine. Then retry using Azure PowerShell module and commands.</p>

<p>I installed mine last May, using Azure PowerShell 4.0.1, and the command Get-AmlWorkspace is working.</p>

<pre><code># Set local folder location
Set-Location -Path ""C:\Insert here the location of AzureMLPS.dll""

# Unblock and import Azure Powershell Module (leverages config.json file)
Unblock-File .\AzureMLPS.dll
Import-Module .\AzureMLPS.dll

# Get Azure ML Workspace info
Get-AmlWorkspace
</code></pre>

<p>The output on my side looks like this:
<a href=""https://i.stack.imgur.com/mEGeT.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/mEGeT.png"" alt=""enter image description here""></a></p>
","7777617",0
364,45809077,2,45783285,2017-08-22 04:48:11,1,"<blockquote>
  <p>are any additional code changes needed to create a custom tier of any size?</p>
</blockquote>

<p>No; no changes are needed to the MNIST sample to get it to work with different number or type of worker. To use a <code>tf.estimator.Estimator</code> on CloudML engine, you must have your program invoke <code>learn_runner.run</code>, as <a href=""https://github.com/GoogleCloudPlatform/cloudml-dist-mnist-example/blob/79f07aef969995f0e4445311b9771735fbd7173b/trainer/task.py#L118"" rel=""nofollow noreferrer"">exemplified</a> in the samples. When you do so, the framework reads in the <a href=""https://cloud.google.com/ml-engine/docs/concepts/trainer-considerations#use_tf_config"" rel=""nofollow noreferrer""><code>TF_CONFIG</code></a> environment variables and populates a <a href=""https://www.tensorflow.org/api_docs/python/tf/estimator/RunConfig"" rel=""nofollow noreferrer""><code>RunConfig</code></a> object with the relevant information such as the <a href=""https://www.tensorflow.org/api_docs/python/tf/train/ClusterSpec"" rel=""nofollow noreferrer""><code>ClusterSpec</code></a>. It will automatically do the right thing on Parameter Server nodes and it will use the provided Estimator to start training and evaluation.</p>

<p>Most of the magic happens because <code>tf.estimator.Estimator</code> automatically uses a device setter that distributes ops correctly. That device setter uses the cluster information from the <code>RunConfig</code> object whose constructor, by default, uses TF_CONFIG to do its magic (e.g. <a href=""https://github.com/tensorflow/tensorflow/blob/593dc8e5d65f4db93e8f5fced772abb3531a9752/tensorflow/python/estimator/estimator.py#L790"" rel=""nofollow noreferrer"">here</a>). You can see where the device setter is being used <a href=""https://github.com/tensorflow/tensorflow/blob/593dc8e5d65f4db93e8f5fced772abb3531a9752/tensorflow/python/estimator/estimator.py#L627"" rel=""nofollow noreferrer"">here</a>.</p>

<p>This all means that you can just change your <code>config.yaml</code> by adding/removing workers and/or changing their types and things should generally just work.</p>

<p>For sample code using a custom model_fn, see the <a href=""https://github.com/GoogleCloudPlatform/cloudml-samples/blob/master/census/customestimator/trainer/model.py"" rel=""nofollow noreferrer"">census/customestimator</a> example.</p>

<p>That said, please note that as you add workers, you are increasing your effective batch size (this is true regardless of whether or not you are using <code>tf.estimator</code>). That is, if your <code>batch_size</code> was 50 and you were using 10 workers, that means each worker is processing batches of size 50, for an effective batch size of 10*50=500. Then if you increase the number of workers to 20, your effective batch size becomes 20*50=1000. You may find that you may need to decrease your learning rate accordingly (linear seems to generally work well; <a href=""https://arxiv.org/abs/1706.02677"" rel=""nofollow noreferrer"">ref</a>).</p>

<blockquote>
  <p>I poked around some of the other ML Engine samples and found that
  reddit_tft uses distributed training, but they appear to have defined
  their own runconfig.cluster_spec within their trainer package:
  task.pyeven though they are also using the Estimator API. So, is there
  any additional configuration needed?</p>
</blockquote>

<p>No additional configuration needed. The reddit_tft sample does instantiate its own <code>RunConfig</code>, however, the constructor of <code>RunConfig</code> grabs any properties not explicitly set during instantiation by using <code>TF_CONFIG</code>. And it does so only as a convenience to figure out how many Parameter Servers and workers there are.</p>

<blockquote>
  <p>Does any of this change if the config.yaml specifies using GPUs?</p>
</blockquote>

<p>You should not need to change anything to use <code>tf.estimator.Estimator</code> with GPUs, other than possibly needing to manually assign ops to the GPU (but that's not specific to CloudML Engine); see <a href=""https://www.tensorflow.org/tutorials/using_gpu"" rel=""nofollow noreferrer"">this article</a> for more info. I will look into clarifying the documentation.</p>
","1399222",2
365,45804013,2,45582412,2017-08-21 19:04:16,1,"<p>The correct network definition for 35-column length input with given kernels and strides would be following:</p>

<pre><code>const { T = true; F = false; }

input Data [35];

hidden C1 [7, 15]
  from Data convolve {
    InputShape  = [35];
    KernelShape = [7];
    Stride      = [2];
    MapCount = 7;
  }

hidden C2 [14, 7, 5]
   from C1 convolve {
     InputShape  = [ 7, 15];
     KernelShape = [ 1,  7];
     Stride      = [ 1,  2];
     Sharing     = [ F,  T];
     MapCount = 14;
  }

hidden H3 [100]
  from C2 all;

output Result [1] linear
  from H3 all;
</code></pre>

<p>First, the C1 = [7,15]. The first dimension is simply the MapCount. For the second dimension, the kernel shape defines the length of the ""window"" that's used to scan the input columns, and the stride defines how much it moves at each step. So the kernel windows would cover columns 1-7, 3-9, 5-11,...,29-35, yielding the second dimension of 15 when you tally the windows.</p>

<p>Next, the C2 = [14,7,5]. The first dimension is again the MapCount. For the second and third dimension, the 1-by-7 kernel ""window"" has to cover the input size of 7-by-15, using steps of 1 and 2 along corresponding dimensions. </p>

<p>Note that you could specify C2 hidden layer shape of [98,5] or even [490], if you wanted to flatten the outputs. </p>
","5784983",0
366,45776607,2,45776128,2017-08-19 21:53:36,9,"<p>GPUs are in high demand in <code>us-central1</code>. I suggest running your job in <code>us-east1</code>, if possible, in the near term until more GPUs become available.</p>
","1399222",0
367,45749576,2,45558337,2017-08-18 06:06:36,1,"<p>As much as I want to replicate your Azure ML experiment, I do not have enough data. But what I've done are as follows:</p>

<p><a href=""https://i.stack.imgur.com/XNaeg.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/XNaeg.png"" alt=""enter image description here""></a></p>

<p>I copied your sample data, and then multiplied it by 4 times (<strong>Add Rows x 2</strong>).
Then <strong>Split Data</strong> (70%/30%), random seed 7 (for reproducible results).
The <strong>Boosted Decision Tree Regression</strong> has default parameters.
On <strong>Tune Model Hyperparameters</strong>, I selected <strong><em>AMOUNT_SOLD</em></strong> as the label column.
Then <strong>Score Model</strong> and <strong>Evaluate Model</strong>.</p>

<p><a href=""https://i.stack.imgur.com/aIJlk.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/aIJlk.png"" alt=""enter image description here""></a></p>

<p>Accuracy / Coefficient of Determination was pretty good.</p>

<p>After that, to deploy this as a web service, you must setup first a Predictive Experiment from your Training Experiment. <code>Setup Web Service &gt; Predictive Experiment</code> You experiment will move like magic.</p>

<p><a href=""https://i.stack.imgur.com/gTEOl.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/gTEOl.png"" alt=""enter image description here""></a></p>

<p>The <strong>Web Service Input</strong> module will be placed by default at the top of the experiment. I <strong>moved it and connected at the right side of Score Model</strong>, so that when you are inputting the parameters of your web service, it <em>will be predicted using your Trained Model</em>.</p>

<p>After the Score Model module, I placed a <strong>Select Columns in Dataset</strong> module and selected only the column named <strong>Scored Labels</strong>. This column contains the model's predictions. Then I used <strong>Edit Metadata</strong> module to rename the Scored Labels column, before passing it to the <strong>Web Service Output</strong> module.</p>

<p>Your experiment is now ready to deploy as a web service.</p>

<p>To predict new values, I tested the web service using the current date details as input. (<strong>Although the DATE_REF input must be 20170818</strong> :D )</p>

<p><a href=""https://i.stack.imgur.com/fPm65.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/fPm65.png"" alt=""enter image description here""></a></p>

<p>And then the output looks like this:</p>

<p><a href=""https://i.stack.imgur.com/R6N4B.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/R6N4B.png"" alt=""enter image description here""></a></p>

<p>Your web service can now predict new values.</p>
","7777617",0
368,45735203,2,45328657,2017-08-17 12:31:05,1,"<p>You can use <strong>Azure PowerShell</strong> for automating this task, and use <strong>Windows Task Scheduler</strong> to schedule this script to run automatically.</p>

<p>For Azure PowerShell,</p>

<p>You may visit <a href=""https://github.com/hning86/azuremlps"" rel=""nofollow noreferrer""><strong>this page</strong></a> to setup an Azure PowerShell script. It's a long journey, but it's worth it. Make sure to <strong><em>follow the prerequisites to be installed on your local PC (Azure-PowerShell v4.0.1)</em></strong>.</p>

<p>For Windows Task Scheduler,</p>

<p>Visit <a href=""https://www.metalogix.com/help/Content%20Matrix%20Console/SharePoint%20Edition/002_HowTo/004_SharePointActions/012_SchedulingPowerShell.htm"" rel=""nofollow noreferrer""><strong>this link</strong></a> to schedule your created Azure PowerShell script to run at a scheduled/repeated time.</p>
","7777617",0
369,45722714,2,45662282,2017-08-16 20:45:37,2,"<p>Checkpoints happen with a certain frequency. If a new checkpoint has not occurred by the time a new evaluation is scheduled to occur, you'll get the message ""Skipping evaluation due to same checkpoint..."". This is because evaluation needs to work off of frozen weights in a separate <code>tf.Session</code> to avoid having weights change during evaluation, and the only way to communicate these weights between sessions is with a checkpoint. So if you want to evaluate more often and you are getting that message, increase your checkpoint frequency. You can do this by adding a flag that populates <a href=""https://www.tensorflow.org/api_docs/python/tf/contrib/learn/RunConfig"" rel=""nofollow noreferrer""><code>tf.contrib.learn.RunConfig#save_checkpoints_steps</code></a>.</p>
","3597868",0
370,45713241,2,45705252,2017-08-16 12:11:57,0,"<p>It looks like the solution to my problem is to use <a href=""https://www.tensorflow.org/api_docs/python/tf/contrib/legacy_seq2seq/sequence_loss_by_example"" rel=""nofollow noreferrer"">sequence_loss_by_example</a></p>
","4086968",3
371,45642669,2,45641474,2017-08-11 19:39:54,2,"<p>You have multiple options. I think the most straightforward is to store <code>labels.txt</code> in a GCS location.</p>

<p>However, if you prefer, you can also package the file up in your <code>setup.py</code>. There are multiple ways to do this, so I'll refer you to the <a href=""http://setuptools.readthedocs.io/en/latest/setuptools.html#including-data-files"" rel=""nofollow noreferrer"">official setuptools documentation</a>.</p>

<p>Let me walk through a quick example:</p>

<p>Create a <code>setup.py</code> in the directory below your training package (often called <code>trainer</code> in CloudML Engine's samples, so I will proceed as if you're code is structured the same as the samples, including using <code>trainer</code> as the package). The following is based on the <a href=""https://cloud.google.com/ml-engine/docs/how-tos/packaging-trainer"" rel=""nofollow noreferrer"">docs</a> you referenced with one important change, namely, the <code>package_data</code> argument instead of <code>include_package_data</code>:</p>

<pre><code>from setuptools import find_packages
from setuptools import setup

setup(
    name='my_model',
    version='0.1',
    install_requires=REQUIRED_PACKAGES,
    packages=find_packages(),
    package_data={'trainer': ['labels.txt']},
    description='My trainer application package.'
)
</code></pre>

<p>If you run <code>python setup.py sdist</code>, you can see that <code>trainer/labels.txt</code> was copied into the tarball.</p>

<p>Then in your code, you can access the file like this:</p>

<pre><code>from pkg_resources import Requirement, resource_filename
resource_filename(Requirement.parse('trainer'),'labels.txt')
</code></pre>

<p>Note that to run this code locally, you're going to have to install your package: <code>python setup.py install [--user]</code>.</p>

<p>And that's the primary reason I think storing the file on GCS might be easier.</p>
","1399222",0
372,45604793,2,45600567,2017-08-10 04:57:49,1,"<p>Update:
The job failed due to out-of-memory. Try to use larger machine instead please.</p>

<p>In addition to rhaertel80's answer, it will be also helpful if you can share the project number and job id with us via cloudml-feedback@google.com.</p>
","7438427",0
373,45603557,2,45603305,2017-08-10 02:38:24,1,"<p>The trainer application accesses other files via TensorFlow's <a href=""https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/lib/io/file_io.py"" rel=""nofollow noreferrer"">file_io</a> module.  <a href=""https://stackoverflow.com/a/42799952/1399222"">This post</a> has a few tips, but if you want to open a file:</p>

<pre><code>with file_io.FileIO(""gs://my_bucket/myfile"") as f:
  f.read()
</code></pre>

<p>There is also a <code>copy(..)</code> function and <code>read_file_to_string(...)</code>, if those fit your needs better.</p>
","1399222",1
374,45552187,2,45550629,2017-08-07 17:06:41,0,"<p>All of your hyperparameters have exactly one possible value, so the first Hyperparameter trial exhausted the parameter space and there wasn't anything new to try for a second trial.</p>

<p>Of course, this should not be communicated as an Internal Error, so I'll make sure that gets fixed.</p>
","5441818",1
375,45516178,2,45511942,2017-08-04 22:12:48,0,"<p>did I answer your question in <a href=""https://stackoverflow.com/questions/45375031/tensorflow-serving-prediction-not-working-with-object-detection-pets-example/45381837#45381837"">tensorflow serving prediction not working with object detection pets example</a>? The input of batch prediction should not include '{""instances: }'. </p>
","6896656",1
376,45240523,2,45234155,2017-07-21 14:43:41,2,"<p>To do this, you will need to add the Cython to the list of required packages in your <code>setup.py</code>. Instructions can be found <a href=""https://cloud.google.com/ml-engine/docs/how-tos/packaging-trainer#to_include_additional_pypi_dependencies"" rel=""nofollow noreferrer"">here</a>.</p>

<p>Here is a sample <code>setup.py</code>, that would reside in the parent directory of the directory you pass as <code>--package-path</code> to <code>gcloud</code>.</p>

<pre><code>from setuptools import find_packages
from setuptools import setup

REQUIRED_PACKAGES = ['Cython&gt;=0.26']

setup(
    name='trainer',
    version='0.1',
    install_requires=REQUIRED_PACKAGES,
    packages=find_packages(),
    include_package_data=True,
    description='My trainer application package.'
)
</code></pre>
","1399222",6
377,50983632,2,50924494,2018-06-22 08:18:26,1,"<p>I solved this issue, The problem was using absolute path for <code>entry_point</code>. 
<br>
when you use a <code>source_dir</code> parameter the path to the <code>entry_point</code> should be relative to the <code>source_dir</code></p>
","5275376",0
378,50973526,2,50954802,2018-06-21 16:34:37,0,"<p>Figured it out. In my ""Remove Duplicate Rows"" module up the chain a bit I was only removing duplicates by UserID instead of UserID <em>and</em> ItemID. This still left quite a bit of rows but I'm assuming it messed with the stratification. </p>
","5112620",0
379,51660654,2,51560452,2018-08-02 19:11:15,1,"<p>At the core, the mathematical methodology between the two is nearly identical, but there are some differences in how they are implemented within Kinesis and SageMaker that should help drive your decision. </p>

<p>Kinesis RandomCutForest:</p>

<ul>
<li>Streaming version of the algorithm which is great for near-real-time updates to the model.</li>
<li>Supports time decay of older records, shingling of the input data, and if you are using multiple dimensions, anomaly attribution that helps you understand the effect of each of the dimensions.  </li>
<li>So, in case your logs are being stored in CloudWatch, by using subscription filters (and Lambda if needed) you can get them preprocessed and sent to Kinesis with little effort.  </li>
</ul>

<p>SageMaker RandomCutForest:</p>

<ul>
<li>Batch version of the algorithm, great for large datasets (typically stored in S3) or where there's no need to update the model frequently.  </li>
<li>Similar to Kinesis, supports near-real-time scoring of incoming data points via inference endpoint, but new data points do not change the underlying model.  </li>
<li>Supports hyper parameter optimization, which identifies the best set of parameters for your model (such as number of samples, number of trees etc.)  </li>
<li>Scaling up instances for both training and scoring is straightforward, and the available SageMaker Notebooks can help you preprocess and prepare your data for training.  </li>
<li>So, if your dataset is large and you don't have a need for dynamic updates to your model, SageMaker solution should be preferred solution for you.  </li>
</ul>

<p>Hope this answers your question.</p>
","3894732",0
380,51684303,2,51668053,2018-08-04 09:35:26,3,"<p>It looks like the Python script just needs a little updating. :)</p>

<p>This should work since you get <code>dataframe1</code> automatically as a <code>pandas</code> data frame.</p>

<pre><code>import pandas as pd

def azureml_main(dataframe1 = None, dataframe2 = None):
  dataframe1.age = pd.to_numeric(dataframe1.age, errors=""coerce"")

  return dataframe1
</code></pre>
","186013",0
381,51745715,2,51722555,2018-08-08 11:34:33,0,"<p>In AWS Lambda, only the sdk that have a sable version are available.
The sdk of the SageMaker service is not stable yet, so functions related to HyperParameterTuningJob are not in the version of the sdk included in AWS Lambda.</p>

<p>To use theses functions, you need to install the latest version of the sdk on local on your machine (with npm install aws-sdk). 
Then zip the node_modules folder and your script (called index.js), then upload this zip folder into the AWS lambda.</p>
","10190872",1
382,51790507,2,51698373,2018-08-10 16:21:59,4,"<p>The first problem is that you're using 'http' for your CURL request. Virtually all AWS services strictly use 'https' as their protocol, SageMaker included. <a href=""https://docs.aws.amazon.com/general/latest/gr/rande.html"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/general/latest/gr/rande.html</a>. I'm going to assume this was a typo though.</p>

<p>You can check the verbose output of the AWS CLI by passing the '--debug' argument to your call. I re-ran a similar experiment with my favorite duck.jpg image:</p>

<pre><code>aws --debug sagemaker-runtime invoke-endpoint --endpoint-name MyEndpoint --body image=@/duck.jpg --content-type multipart/form-data  &gt;(cat)
</code></pre>

<p>Looking at the output, I see:</p>

<pre><code>2018-08-10 08:42:20,870 - MainThread - botocore.endpoint - DEBUG - Making request for OperationModel(name=InvokeEndpoint) (verify_ssl=True) with params: {'body': 'image=@/duck.jpg', 'url': u'https://sagemaker.us-west-2.amazonaws.com/endpoints/MyEndpoint/invocations', 'headers': {u'Content-Type': 'multipart/form-data', 'User-Agent': 'aws-cli/1.15.14 Python/2.7.10 Darwin/16.7.0 botocore/1.10.14'}, 'context': {'auth_type': None, 'client_region': 'us-west-2', 'has_streaming_input': True, 'client_config': &lt;botocore.config.Config object at 0x109a58ed0&gt;}, 'query_string': {}, 'url_path': u'/endpoints/MyEndpoint/invocations', 'method': u'POST'}
</code></pre>

<p>It looks like the AWS CLI is using the string literal '@/duck.jpg', not the file contents.</p>

<p>Trying again with curl and the ""--verbose"" flag:</p>

<pre><code>curl --verbose -X POST -F ""image=@/duck.jpg"" https://sagemaker.us-west-2.amazonaws.com/endpoints/MyEndpoint/invocations
</code></pre>

<p>I see the following:</p>

<pre><code>Content-Length: 63097
</code></pre>

<p>Much better. The '@' operator is a CURL specific feature. The AWS CLI does have a way to pass files though: </p>

<pre><code>--body fileb:///duck.jpg
</code></pre>

<p>There is also a 'file' for non-binary files such as JSON. Unfortunately you cannot have the prefix. That is, you cannot say:</p>

<pre><code> --body image=fileb:///duck.jpg
</code></pre>

<p>You can prepend the string 'image=' to your file with a command such as the following. (You'll probably need to be more clever if your images are really big; this is really inefficient.)</p>

<pre><code> echo -e ""image=$(cat /duck.jpg)"" &gt; duck_with_prefix
</code></pre>

<p>Your final command would then be:</p>

<pre><code> aws sagemaker-runtime invoke-endpoint --endpoint-name MyEndpoint --body fileb:///duck_with_prefix --content-type multipart/form-data  &gt;(cat)
</code></pre>

<p>Another note: Using raw curl with AWS services is extremely difficult due to the AWS Auth signing requirements - <a href=""https://docs.aws.amazon.com/AmazonS3/latest/API/sig-v4-authenticating-requests.html"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/AmazonS3/latest/API/sig-v4-authenticating-requests.html</a> </p>

<p>It can be done, but you'll likely be more productive by using the AWS CLI or a pre-existing tool such as Postman - <a href=""https://docs.aws.amazon.com/apigateway/latest/developerguide/how-to-use-postman-to-call-api.html"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/apigateway/latest/developerguide/how-to-use-postman-to-call-api.html</a> </p>
","407160",3
383,51792877,2,51790720,2018-08-10 19:20:24,1,"<p>You don't need to create <code>/opt/ml</code>, SageMaker will do it for you when it launches your training job.</p>

<p>The contents of the <code>/opt/ml</code> directory are determined by the parameters you pass to the CreateTrainingJob API call. The scikit example notebook you linked to describes this (look at the <strong>Running your container</strong> sections). You can find more info about this in the <a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/ex1-train-model-create-training-job.html"" rel=""nofollow noreferrer"">Create a Training Job</a> section of the main SageMaker documentation.</p>

<hr>
","3920238",0
384,51793118,2,51792005,2018-08-10 19:39:04,2,"<p>It looks like you are trying to maximize this metric, <a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/deepar-tuning.html"" rel=""nofollow noreferrer"">test:RMSE can only be minimized</a> by SageMaker HyperParameter Tuning. </p>

<p>To achieve this in the SageMaker Python SDK, create your HyperparameterTuner with objective_type='Minimize'. You can see the signature of the init method <a href=""https://github.com/aws/sagemaker-python-sdk/blob/master/src/sagemaker/tuner.py#L158"" rel=""nofollow noreferrer"">here</a>.</p>

<p>Here is the change you should make to your call to HyperparameterTuner:</p>

<pre class=""lang-py prettyprint-override""><code>my_tuner = HyperparameterTuner(estimator=estimator,
                               objective_metric_name=""test:RMSE"",
                               objective_type='Minimize',
                               hyperparameter_ranges=hyperparams,
                               max_jobs=20,
                               max_parallel_jobs=2)
</code></pre>
","449081",0
385,51806501,2,51803032,2018-08-12 06:59:37,1,"<p>So you need to troubleshoot. Here are a few things to check:   </p>

<p>0) Make sure the bucket is in the SageMaker region.</p>

<p>1) Include the string ""sagemaker"" in your bucket name (e.g., <em>my_bucketName_here-sagemaker</em>, SageMaker has out of the box access to buckets named this way.</p>

<p>2) Try using the SageMaker S3 <a href=""https://github.com/aws/sagemaker-python-sdk/blob/master/src/sagemaker/session.py"" rel=""nofollow noreferrer"">default_bucket()</a>:</p>

<pre><code>import sagemaker
s = sagemaker.Session()
s.upload_data(path='somefile.csv', bucket=s.default_bucket(), key_prefix='data/train')
</code></pre>

<p>3) Open terminal on the Notebook instance, to try to list your bucket using AWS CLI in bash:</p>

<pre><code>aws iam get-user
aws s3 ls my_bucketName_here
</code></pre>

<p>Finally, pasting the bucket's access and resource policy in your question could help others to answer you.</p>
","121956",4
386,51819406,2,51817494,2018-08-13 09:39:42,3,"<p>Yes this is possible but it will need some overhead:
You can pass your own docker images for training and inference to sagemaker.</p>

<p>Inside this containers you can do anything you want including return your <code>my_square</code> function. Keep in mind that you have to write your own flask microservice including proxy and wsgi server(if needed).</p>

<p>In my opinion <a href=""https://github.com/awslabs/amazon-sagemaker-examples/blob/master/advanced_functionality/scikit_bring_your_own/scikit_bring_your_own.ipynb"" rel=""nofollow noreferrer"">this example</a> is the most helpfull one.</p>
","9305450",0
387,51828794,2,51710241,2018-08-13 18:50:28,1,"<p>Tom, XGBoost does not support RecordIO format. It only supports csv and libsvm. Also, the algorithm itself doesn’t natively support multi-label. But there are a couple of ways around it: <a href=""https://stackoverflow.com/questions/40916939/xg-boost-for-multilabel-classification"">Xg boost for multilabel classification?</a></p>

<p>Random Cut Forest does not support multiple labels either. If more than one label is provided it picks up the first only.</p>
","10208961",2
388,51846290,2,51725489,2018-08-14 16:49:55,1,"<p>At this time there is no direct integration with AWS SageMaker and QuickSight, however you can use utilize SageMaker's batch transform jobs to convert data outside of QuickSight and then import this information into QuickSight for visualization. The output format for SageMaker's batch transform jobs is S3, which is a supported input data source for QuickSight.</p>

<ul>
<li><a href=""https://aws.amazon.com/about-aws/whats-new/2018/07/amazon-sagemaker-supports-high-throughput-batch-transform-jobs-for-non-real-time-inferencing/"" rel=""nofollow noreferrer"">https://aws.amazon.com/about-aws/whats-new/2018/07/amazon-sagemaker-supports-high-throughput-batch-transform-jobs-for-non-real-time-inferencing/</a></li>
</ul>

<p>Depending on how fancy you want to be, you can also integrate calls to AWS services such as AWS Lambda or AWS SageMaker as a user-defined function (UDF) within your datastore. Here are a few resources that may help:</p>

<ul>
<li><a href=""https://docs.aws.amazon.com/redshift/latest/dg/user-defined-functions.html"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/redshift/latest/dg/user-defined-functions.html</a></li>
<li><a href=""https://aws.amazon.com/blogs/big-data/from-sql-to-microservices-integrating-aws-lambda-with-relational-databases/"" rel=""nofollow noreferrer"">https://aws.amazon.com/blogs/big-data/from-sql-to-microservices-integrating-aws-lambda-with-relational-databases/</a></li>
</ul>

<p>Calculated fields will probably not help you in this regard - calculated fields are restricted to a relatively small set of operations, and none of these operations support calls to external sources.</p>

<ul>
<li><a href=""https://docs.aws.amazon.com/quicksight/latest/user/calculated-field-reference.html"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/quicksight/latest/user/calculated-field-reference.html</a> </li>
</ul>
","407160",0
389,51851577,2,51702359,2018-08-15 00:59:57,0,"<p>I found an answer for this. In fact, I cannot see the result in the outcome of the scoring model but when I linked it to a <em>select column in the dataset</em> module, I see the predicted columns there.</p>
","8373037",0
390,51880661,2,51780562,2018-08-16 15:31:53,4,"<p>I have tried with you sample data and here is my quick and dirty solution:
1) Add any symbol (I've added the '#') in front of each date
2) Load it to AML Studio (it is now considered as a string feature)
3) Add a Python/R component to remove the '#' symbol and explicitly convert the column to string (as.string(columnname) or str(columnname))</p>

<p>Hope this helps</p>
","9929041",1
391,51904108,2,51888996,2018-08-17 23:57:46,1,"<p>Thanks javadba for your answer!</p>

<p>I am not very well adversed in Machine Learning or TensorFlow, so please correct me. However, it looks like you were able to integrate with SageMaker, but the predictions aren't what you are expecting.</p>

<p>Ultimately, SageMaker runs your <a href=""https://github.com/aws/sagemaker-python-sdk/blob/master/src/sagemaker/tensorflow/README.rst#preparing-the-tensorflow-training-script"" rel=""nofollow noreferrer"">EstimatorSpec</a> with <a href=""https://github.com/aws/sagemaker-tensorflow-container/blob/master/src/tf_container/trainer.py#L73"" rel=""nofollow noreferrer"">train_and_evaluate</a> for training and uses TensorFlow Serving for your predictions. It doesn't have any other hidden functionalities, so the results you get from your KMeans predictions using the TensorFlow estimator is going to be independent of SageMaker. It might be affected by how you define your serving_input_fn and output_fn however.</p>

<p>When you run this same estimator outside of the SageMaker ecosystem using the same setup, do you get predictions in the format you're expecting?</p>

<p>The SageMaker TensorFlow experience is open sourced here and shows what is possible and isn't as of now.
<a href=""https://github.com/aws/sagemaker-tensorflow-container"" rel=""nofollow noreferrer"">https://github.com/aws/sagemaker-tensorflow-container</a></p>
","9090582",1
392,51954073,2,51858379,2018-08-21 17:43:00,7,"<p>You can load S3 Data into AWS SageMaker Notebook by using the sample code below. Do make sure the Amazon SageMaker role has policy attached to it to have access to S3. </p>

<p>[1] <a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-roles.html"" rel=""noreferrer"">https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-roles.html</a> </p>

<pre><code>import boto3 
import botocore 
import pandas as pd 
from sagemaker import get_execution_role 

role = get_execution_role() 

bucket = 'Your_bucket_name' 
data_key = your_data_file.csv' 
data_location = 's3://{}/{}'.format(bucket, data_key) 

pd.read_csv(data_location) 
</code></pre>
","10256065",1
393,52002052,2,51968742,2018-08-24 10:02:05,2,"<p>The problem relied on the data format as suspected. In my case all I had to do is send the data as a json serialized string array and use <code>ContentType = application/json</code> because the python function running on the endpoint which is responsible for sending the data to the predictor was only accepting json strings. </p>

<p>Another way to solve this issues is to modify the python function which is responsible for the input handling to accept all content types and modify the data in a way that the predictor will understand.</p>

<p>example of working code for my case:</p>

<pre><code>        var data = new string[] { ""this movie was extremely good ."", ""the plot was very boring ."" };
        var serializedData = JsonConvert.SerializeObject(data);

        var credentials = new Amazon.Runtime.BasicAWSCredentials("""","""");
        var awsClient = new AmazonSageMakerRuntimeClient(credentials, RegionEndpoint.EUCentral1);
        var request = new Amazon.SageMakerRuntime.Model.InvokeEndpointRequest
        {
            EndpointName = ""endpoint"",
            ContentType = ""application/json"",
            Body = new MemoryStream(Encoding.ASCII.GetBytes(serializedData)),
        };

        var response = awsClient.InvokeEndpoint(request);
        var predictions = Encoding.UTF8.GetString(response.Body.ToArray());
</code></pre>
","8065092",0
394,52003888,2,52003180,2018-08-24 11:51:59,3,"<p>To start with, your are in a <em>binary</em> classification setting, not in a multi-class one (we normally use this term when number of classes > 2).</p>

<blockquote>
  <p>If scored probabilities output for my model is 0.6 and scored labels = 1, how sure is the model of the scored labels 1? </p>
</blockquote>

<p>In <em>practice</em>, the scored probabilities are routinely interpreted as the <em>confidence</em> of the model; so, in this example, we would say that your model has 60% confidence that the particular sample belongs to class 1 (and, complementary, 40% confidence that it belongs to class 0).</p>

<blockquote>
  <p>And how sure can I be that actual outcome will be a 1?</p>
</blockquote>

<p>If you don't have any alternate means of computing such outcomes yourself (e.g. a different model), I cannot see how this question is different from your previous one.</p>

<blockquote>
  <p>Can I safely assume that a scored probabilities of 0.80 = 80% chance of outcome? </p>
</blockquote>

<p>This is the kind of statement that would drive a professional statistician mad; nevertheless, the clarifications above regarding the confidence should be enough for your purposes (they are enough indeed for ML practitioners).</p>

<p>My answer in <a href=""https://stackoverflow.com/questions/51367755/predict-classes-or-class-probabilities/51423325#51423325"">Predict classes or class probabilities?</a> should also be helpful.</p>
","4685471",0
395,52004682,2,51589403,2018-08-24 12:37:13,2,"<p>The version of TensorFlow is determined by the TensorFlow Serving Docker image you use. If you are using the Kubeflow ksonnet prototype for TFServing the parameter <a href=""https://github.com/kubeflow/kubeflow/blob/master/kubeflow/tf-serving/tf-serving.libsonnet#L31"" rel=""nofollow noreferrer"">modelServerImage</a> can be used to set the image.</p>
","4392784",0
396,52010762,2,52010761,2018-08-24 19:39:35,1,"<p>After ruling out all possibility of passing in the wrong file, our data scientist took a closer look at the experiment itself. He discovered that it was defaulting to one hardcoded .ilearner path he had been using in development.</p>

<p>At one point in time, he had created webservice parameters to override this value (hence why I had them defined in my webservice call), but they had been removed during one of the redesigns of the experiment with anyone noticing, because the webservice will apparently accept superfluous arguments.</p>

<p><strong>The webservice was accepting my global parameters</strong>, and apparently even validating them. But since they weren't wired to anything inside <strong>the experiment the passed .ilearner file info was never applied to anything</strong>--the hardcoded .ilearner was being applied no matter what.</p>

<p>We were all very surprised there was no exception thrown about passing in parameters to the webservice that weren't actually defined. Had <em>that</em> happened, we would have gotten to the bottom of it much more quickly.</p>

<p>tl/dr: The experiment wasn't properly configured to accept an .ilearner file path (or Account Name, or Account Key) as a parameter, and the webservice was happily accepting and ignoring the parameter arguments without raising any alarm since it had the hardcoded value to run with.</p>
","7053468",0
397,52055099,2,52032535,2018-08-28 09:59:03,0,"<blockquote>
  <p>1) Does the primary key automatically expire after a while or by signing out from ML studio?</p>
</blockquote>

<p>I could not find any limit of the primary key in the office docs. Per my test, my primary key does not expire more than two hours or sign out from ML studio.</p>

<blockquote>
  <p>2) What is the application of the second key in ML Studio APIs? Where do we need the second key?</p>
</blockquote>

<p>The second key is the same usage of the primary key, like a backup of the primary key. Also, the primary key equals the API key in the ML studio.</p>
","9455659",0
398,52066669,2,52053776,2018-08-28 21:51:17,1,"<blockquote>
  <p>For this to be useful, you need to get the variant assignment data back and join with some internal data to figure out the best performing variant. </p>
</blockquote>

<p>The InvokeEndpoint response includes the ""InvokedProductionVariant"", in order to support the kind of analysis you describe. Details can be found in the API documentation: <a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/API_runtime_InvokeEndpoint.html#API_runtime_InvokeEndpoint_ResponseSyntax"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/sagemaker/latest/dg/API_runtime_InvokeEndpoint.html#API_runtime_InvokeEndpoint_ResponseSyntax</a></p>

<blockquote>
  <p>How is this assignment done? Is it purely random? </p>
</blockquote>

<p>Traffic is distributed randomly while remaining proportional to the weight of the production variant.</p>
","10286863",0
399,52075215,2,52055933,2018-08-29 10:27:45,2,"<p>So it turns out the <a href=""https://github.com/Azure/Azure-MachineLearning-ClientLibrary-Python"" rel=""nofollow noreferrer"">Python SDK</a> has an <code>update_from_dataframe</code> method on it that can be used to update a dataset that has been uploaded to Azure ML Studio. If you're unable to use a new CSV and need to update an existing data set, then this should do the trick.</p>
","186013",5
400,52075964,2,52070950,2018-08-29 11:06:41,1,"<p>The output of <code>TfidfVectorizer</code> is a scipy sparse matrix, not a simple numpy array.</p>
<p>So either use a different function like:</p>
<blockquote>
<p><a href=""https://github.com/aws/sagemaker-python-sdk/blob/master/src/sagemaker/amazon/common.py#L113"" rel=""nofollow noreferrer"">write_spmatrix_to_sparse_tensor</a></p>
<p>&quot;&quot;&quot;Writes a scipy sparse matrix to a sparse tensor&quot;&quot;&quot;</p>
</blockquote>
<p>See <a href=""https://github.com/aws/sagemaker-python-sdk/issues/27"" rel=""nofollow noreferrer"">this issue</a> for more details.</p>
<p><strong>OR</strong> first convert the output of <code>TfidfVectorizer</code> to a dense numpy array and then use your above code</p>
<pre><code>xtrain_tfidf =  tfidf_vect.transform(X_train).toarray()   
buf = io.BytesIO()
smac.write_numpy_to_dense_tensor(buf, xtrain_tfidf, y_train)
...
...
</code></pre>
","3374996",1
401,52158345,2,52157855,2018-09-04 03:20:16,2,"<p>Yes you're missing something here and that is to use the correct namespace. Use:</p>

<pre><code>$ kubectl -n kubeflow-admin get all
</code></pre>
","396567",2
402,52282756,2,52244963,2018-09-11 19:13:14,5,"<p>The payload variable is a Pandas' DataFrame, while <a href=""https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/sagemaker-runtime.html#SageMakerRuntime.Client.invoke_endpoint"" rel=""noreferrer"">invoke_endpoint()</a> expects  <code>Body=b'bytes'|file</code>.</p>

<p>Try something like this (coding blind):</p>

<pre><code>response = runtime.invoke_endpoint(EndpointName=r_endpoint,
                                   ContentType='text/csv',
                                   Body=open('payload.csv'))
</code></pre>

<p>More on the <a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/cdf-inference.html"" rel=""noreferrer"">expected formats here</a>. 
Make sure the file doesn't include a header.</p>

<p>Alternatively, convert your DataFrame to bytes, <a href=""https://stackoverflow.com/questions/34666860/converting-pandas-dataframe-to-bytes"">like in this example</a>, and pass those bytes instead of passing a DataFrame.</p>
","121956",1
403,52333511,2,52317237,2018-09-14 13:56:57,2,"<blockquote>
  <p>I am looking for an easy work environment where I can quickly test my models, exactly. And it won't be only me working on it, it's a team effort. </p>
</blockquote>

<p>Since you are working as a team I would recommend to use sagemaker with custom docker images. That way you have complete freedom over your algorithm. The docker images are stored in ecr. Here you can upload many versions of the same image and tag them to keep control of the different versions(which you build from a git repo).</p>

<p>Sagemaker also gives the execution role to inside the docker image. So you still have full access to other aws resources (if the execution role has the right permissions)</p>

<p><a href=""https://github.com/awslabs/amazon-sagemaker-examples/blob/master/advanced_functionality/scikit_bring_your_own/scikit_bring_your_own.ipynb"" rel=""nofollow noreferrer"">https://github.com/awslabs/amazon-sagemaker-examples/blob/master/advanced_functionality/scikit_bring_your_own/scikit_bring_your_own.ipynb</a>
In my opinion this is a good example to start because it shows how sagemaker is interacting with your image.</p>

<p><strong>Some notes on other solutions:</strong></p>

<p>The problem of every other solution you posted is you want to build and execute on the same machine. Sure you can do this but keep in mind, that gpu instances are expensive and therefore you might only switch to the cloud when the code is ready to run.</p>

<p><strong>Some other notes</strong></p>

<ul>
<li><p>Jupyter Notebooks in general are not made for collaborative programming. I think they want to change this with jupyter lab but this is still in development and sagemaker only use the notebook at the moment.</p></li>
<li><p>EC2 is cheaper as sagemaker but you have  to do more work. Especially if you want to run your model as docker images. Also with sagemaker you can easily  build an endpoint for model inference which would be even more complex to realize with ec2.</p></li>
<li><p>Cloud 9 I never used this service and but on first glance it seems good to develop on, but the question remains if you want to do this on a gpu machine. Because you're using ec2 as instance you have the same advantage/disadvantage.</p></li>
</ul>
","9305450",0
404,52362807,2,52354671,2018-09-17 07:32:55,3,"<p>All logs are available in Amazon Cloudwatch. You can query CloudWatch programmatically or via an API to parse the logs.</p>

<p>Are you using built-in algorithms or a Framework like MXNet or TensorFlow? For TensorFlow you can monitor your job with <a href=""https://github.com/awslabs/amazon-sagemaker-examples/blob/master/sagemaker-python-sdk/tensorflow_resnet_cifar10_with_tensorboard/tensorflow_resnet_cifar10_with_tensorboard.ipynb"" rel=""nofollow noreferrer"">TensorBoard</a>.</p>

<p>Additionally, you can see high level job status using the <a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/API_DescribeTrainingJob.html"" rel=""nofollow noreferrer"">describe training job</a> API call:</p>

<pre><code>import sagemaker
sm_client = sagemaker.Session().sagemaker_client
print(sm_client.describe_training_job(TrainingJobName='You job name here'))
</code></pre>
","121956",0
405,52369384,2,52278613,2018-09-17 13:52:18,2,"<p>I have installed the latest version of the forecast package and here are the steps I followed during the installation. </p>

<ol>
<li>Download latest version of CRAN</li>
<li>Be sure that tsCV is working locally</li>
<li>Zip all the dependencies + forecast package</li>
<li>Zip all the generated zips together and upload it to the AMLStudio</li>
<li>Run the following code:</li>
</ol>

<blockquote>
<pre><code>install.packages(""src/glue.zip"", lib = ""."", repos = NULL, verbose = TRUE)
install.packages(""src/stringi.zip"", lib = ""."", repos = NULL, verbose = TRUE)
install.packages(""src/assertthat.zip"", lib = ""."", repos = NULL, verbose = TRUE)
install.packages(""src/fansi.zip"", lib = ""."", repos = NULL, verbose = TRUE)
install.packages(""src/utf8.zip"", lib = ""."", repos = NULL, verbose = TRUE)
install.packages(""src/stringr.zip"", lib = ""."", repos = NULL, verbose = TRUE)
install.packages(""src/labeling.zip"", lib = ""."", repos = NULL, verbose = TRUE)
install.packages(""src/munsell.zip"", lib = ""."", repos = NULL, verbose = TRUE)
install.packages(""src/R6.zip"", lib = ""."", repos = NULL, verbose = TRUE)
install.packages(""src/RColorBrewer.zip"", lib = ""."", repos = NULL, verbose = TRUE)
install.packages(""src/cli.zip"", lib = ""."", repos = NULL, verbose = TRUE)
install.packages(""src/crayon.zip"", lib = ""."", repos = NULL, verbose = TRUE)
install.packages(""src/pillar.zip"", lib = ""."", repos = NULL, verbose = TRUE)
install.packages(""src/xts.zip"", lib = ""."", repos = NULL, verbose = TRUE)
install.packages(""src/TTR.zip"", lib = ""."", repos = NULL, verbose = TRUE)
install.packages(""src/curl.zip"", lib = ""."", repos = NULL, verbose = TRUE)
install.packages(""src/digest.zip"", lib = ""."", repos = NULL, verbose = TRUE)
install.packages(""src/gtable.zip"", lib = ""."", repos = NULL, verbose = TRUE)
install.packages(""src/lazyeval.zip"", lib = ""."", repos = NULL, verbose = TRUE)
install.packages(""src/plyr.zip"", lib = ""."", repos = NULL, verbose = TRUE)
install.packages(""src/reshape2.zip"", lib = ""."", repos = NULL, verbose = TRUE)
install.packages(""src/rlang.zip"", lib = ""."", repos = NULL, verbose = TRUE)
install.packages(""src/scales.zip"", lib = ""."", repos = NULL, verbose = TRUE)
install.packages(""src/tibble.zip"", lib = ""."", repos = NULL, verbose = TRUE)
install.packages(""src/viridisLite.zip"", lib = ""."", repos = NULL, verbose = TRUE)
install.packages(""src/withr.zip"", lib = ""."", repos = NULL, verbose = TRUE)
install.packages(""src/quadprog.zip"", lib = ""."", repos = NULL, verbose = TRUE)
install.packages(""src/quantmod.zip"", lib = ""."", repos = NULL, verbose = TRUE)
install.packages(""src/colorspace.zip"", lib = ""."", repos = NULL, verbose = TRUE)
install.packages(""src/fracdiff.zip"", lib = ""."", repos = NULL, verbose = TRUE)
install.packages(""src/ggplot2.zip"", lib = ""."", repos = NULL, verbose = TRUE)
install.packages(""src/lmtest.zip"", lib = ""."", repos = NULL, verbose = TRUE)
install.packages(""src/magrittr.zip"", lib = ""."", repos = NULL, verbose = TRUE)
install.packages(""src/Rcpp.zip"", lib = ""."", repos = NULL, verbose = TRUE)
install.packages(""src/timeDate.zip"", lib = ""."", repos = NULL, verbose = TRUE)
install.packages(""src/tseries.zip"", lib = ""."", repos = NULL, verbose = TRUE)
install.packages(""src/urca.zip"", lib = ""."", repos = NULL, verbose = TRUE)
install.packages(""src/uroot.zip"", lib = ""."", repos = NULL, verbose = TRUE)
install.packages(""src/zoo.zip"", lib = ""."", repos = NULL, verbose = TRUE)
install.packages(""src/RcppArmadillo.zip"", lib = ""."", repos = NULL, verbose = TRUE)
install.packages(""src/forecast.zip"", lib = ""."", repos = NULL, verbose = TRUE)

library(forecast, lib.loc=""."", verbose=TRUE)
far2 &lt;- function(x, h){forecast(Arima(x, order=c(2,0,0)), h=h)}
e &lt;- tsCV(lynx, far2, h=1)
</code></pre>
</blockquote>

<p><a href=""https://drive.google.com/open?id=10Bj0RGCmRFrRECLQrVc26nbx3T-bNSL6"" rel=""nofollow noreferrer"">Here is the zip I have generated:</a></p>

<p><a href=""https://i.stack.imgur.com/bbowH.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/bbowH.png"" alt=""My experiment""></a></p>
","9929041",3
406,52383850,2,52294404,2018-09-18 09:53:44,0,"<p>I found one way to execute R on Azure functions.
the solutions is copy R-Portable in
<a href=""https://sourceforge.net/projects/rportable/"" rel=""nofollow noreferrer"">https://sourceforge.net/projects/rportable/</a>
unzip it using powershell and create a process on function code. In my case i used the code:</p>

<pre><code>System.Diagnostics.Process process = new System.Diagnostics.Process();
            process.StartInfo.WorkingDirectory = @""D:\home\site\tools\R-Portable\App\R-Portable\bin\"";
            process.StartInfo.FileName = @""D:\home\site\tools\R-Portable\App\R-Portable\bin\Rscript.exe"";
            process.StartInfo.Arguments = ""-e \""print('Hello world')\"""";
            process.StartInfo.UseShellExecute = false;
            process.StartInfo.RedirectStandardOutput = true;
            process.StartInfo.RedirectStandardError = true;
            process.Start();
            string outputt = process.StandardOutput.ReadToEnd();
            string err = process.StandardError.ReadToEnd();
            process.WaitForExit();
</code></pre>

<p>On your script you can access csv files or write, and after on function read and return that file.</p>
","9194464",0
407,52396536,2,52359397,2018-09-19 01:03:03,1,"<p>Allow me please to answer my own question. Although not a 100% what I was hoping for, there's certainly support for this in the platform which is great to see: <a href=""https://docs.aws.amazon.com/marketplace/latest/userguide/saas-products.html"" rel=""nofollow noreferrer"">Software-as-a-Service-Based Products</a></p>
","10368417",0
408,52396607,2,52360540,2018-09-19 01:14:42,0,"<p>I'd like to answer my own question - there's support for this in the platform, which is great. Place to start <a href=""https://learn.microsoft.com/en-us/azure/marketplace/marketplace-saas-applications-technical-publishing-guide"" rel=""nofollow noreferrer"">SaaS applications Offer Publishing Guide</a></p>

<p>P.S. Hopefully, they'll make it even more integrated with the payment/licensing system - e.g. for certain prod license types limit the max number of simultaneous WS calls and max CPU cores allowed, and implement enhanced trial licenses time/functional limits support.</p>
","10368417",0
409,52411769,2,52404879,2018-09-19 18:03:30,1,"<p>Preprocessing implies that this is something you might want to decouple from the model execution and run separately, possibly on a schedule or in response to new data flowing in.</p>

<p>If so, you'll probably want to do the preprocessing outside of SageMaker. You could orchestrate it using <a href=""https://aws.amazon.com/glue/"" rel=""nofollow noreferrer"">Glue</a>, or you could write a custom job and run it through <a href=""https://aws.amazon.com/batch/"" rel=""nofollow noreferrer"">AWS Batch</a> or alternatively on an EMR cluster.</p>

<p>That way, your Keras notebook can load the already preprocessed data, train and test through SageMaker.</p>

<p>With a little care, you should be able to perform at least some of the heavy lifting incrementally in the preprocessing step, saving both time and cost downstream in the Deep Learning pipeline.</p>
","57068",0
410,52489197,2,52100549,2018-09-25 01:00:58,1,"<p><strong>training_dir</strong> points to your training channel, i.e. <em>/opt/ml/input/data/training</em>. You can hardcode this location inside your <strong>estimation_fn</strong>.</p>

<p>When training starts, SageMaker makes the data for the channel available in the <em>/opt/ml/input/data/<strong>channel_name</em></strong> directory in the Docker container.</p>

<p>You can find more information here <a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/your-algorithms-training-algo.html#your-algorithms-training-algo-running-container"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/sagemaker/latest/dg/your-algorithms-training-algo.html#your-algorithms-training-algo-running-container</a>.</p>
","3361753",0
411,52507501,2,52370381,2018-09-25 22:32:47,0,"<p>CrashLoopBackOff error normally means that the init() function of your score.py file has a problem, for example, finding or loading the model. It could also mean you are using a library that hasn't been imported.
Azure ML just announced an update to the preview with an updated Python SDK (<a href=""https://learn.microsoft.com/en-us/azure/machine-learning/service/quickstart-get-started"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/machine-learning/service/quickstart-get-started</a>). 
There are tutorials and notebooks that show the process in more details with examples. I would start there.</p>

<p><a href=""https://learn.microsoft.com/en-us/azure/machine-learning/service/tutorial-deploy-models-with-aml"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/machine-learning/service/tutorial-deploy-models-with-aml</a></p>
","4905427",1
412,52532207,2,52532078,2018-09-27 08:01:48,4,"<p>you should be able to find this from the Azure portal. Open the storage account, drill down into blobs, then your container. Use properties for the context menu, the URL should be the path ?</p>

<p><a href=""https://i.stack.imgur.com/r5hxi.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/r5hxi.jpg"" alt=""enter image description here""></a></p>
","8622323",1
413,52549559,2,52537861,2018-09-28 06:28:18,1,"<p>As you suspect, setting a column as <code>feature</code> does have an effect, and it's actually quite important - when training a model, the algorithms will only take into account columns with the <code>feature</code> flag, effectively ignoring the others. </p>

<p>For example, if you have a dataset with columns <code>Feature1</code>, <code>Feature2</code>, and <code>Label</code> and you want to try out just <code>Feature1</code>, you would apply <code>clear feature</code> to the <code>Feature2</code> column (while making sure that <code>Feature1</code> has the <code>feature</code> label set, of course).</p>
","155697",2
414,52588553,2,52588354,2018-10-01 09:53:33,4,"<p>By printing the result type by <code>print(type(result))</code> you can see its a dictionary. now you can see the key name is ""score"" instead of ""predicted_label"" that you are giving to pred. Hence replace it with</p>

<pre><code>pred = int(result['predictions'][0]['score'])
</code></pre>

<p>I think this solves your problem.</p>

<p>here is my lambda function:</p>

<pre><code>import os
import io
import boto3
import json
import csv

# grab environment variables
ENDPOINT_NAME = os.environ['ENDPOINT_NAME']
runtime= boto3.client('runtime.sagemaker')

def lambda_handler(event, context):
   print(""Received event: "" + json.dumps(event, indent=2))

   data = json.loads(json.dumps(event))
   payload = data['data']
   print(payload)

   response = runtime.invoke_endpoint(EndpointName=ENDPOINT_NAME,
                                      ContentType='text/csv',
                                      Body=payload)
   #print(response)
   print(type(response))
   for key,value in response.items():
       print(key,value)
   result = json.loads(response['Body'].read().decode())
   print(type(result))
   print(result['predictions'])
   pred = int(result['predictions'][0]['score'])
   print(pred)
   predicted_label = 'M' if pred == 1 else 'B'

   return predicted_label
</code></pre>
","7652810",2
415,52606794,2,52575449,2018-10-02 10:45:26,0,"<p>We tried to minimize the dependency. PAI doesn’t need dns. You can add dns service to k8s deployment yourself.</p>
","8878518",0
416,52643287,2,52632388,2018-10-04 09:22:37,3,"<p>Yes, it is possible, and yes, the official documentation is not much of help.
However, I wrote an <a href=""https://gnomezgrave.com/2018/07/05/using-a-custom-model-for-ml-inference-with-amazon-sagemaker"" rel=""nofollow noreferrer"">article on that</a>, and I hope it will help you.</p>

<p>Let me know if you need more details. Cheers!</p>
","1538258",1
417,52644212,2,52603929,2018-10-04 10:15:58,0,"<p>Well, finally I have found a way to bypass this limitation. From this <a href=""https://learn.microsoft.com/en-us/azure/machine-learning/studio/use-data-from-an-on-premises-sql-server"" rel=""nofollow noreferrer"">documentation</a> I have found that: </p>

<blockquote>
  <p>The IR does not need to be on the same machine as the data source. But staying closer to the data source reduces the time for the gateway to connect to the data source. We recommend that you install the IR on a machine that's different from the one that hosts the on-premises data source so that the gateway and data source don't compete for resources.</p>
</blockquote>

<p>So the  logic is pretty simple. You provide access to your local server to another machine on vpn and install your gateway there. Important: I have set up the firewall rules on the server before, to be able to establish the connection remotely.</p>
","9929041",0
418,52748079,2,52705769,2018-10-10 20:12:16,3,"<p>When you use the Tune Model Hyperparameters module without a validation dataset, this means, when you use only the 2nd input data port, the module works in cross-validation mode. So the best-parameters model is found by doing cross-validation over the provided dataset, and to do this, the dataset is splitted in k-folds. By default, the module splits the data in 10 folds. In case you want to split the data in a different number of folds, you can connect a Partition and Sample module at the 2nd input, selecting Assign to Folds and indicating the number of folds desired. In many cases k=5 is a reasonable option.</p>
","10330359",0
419,52810390,2,52807787,2018-10-15 05:48:10,1,"<p>They're available under the <code>./Script Bundle</code> directory. For example, if you were to load a pickled model from the zip file, you'd write something along these lines:</p>

<pre><code>import pandas as pd
import pickle

def azureml_main(dataframe1 = None, dataframe2 = None):

    model = pickle.load(open(""./Script Bundle/model.pkl"", ""rb""))
    ...
</code></pre>
","155697",1
420,52821433,2,52762367,2018-10-15 16:57:52,1,"<p>Your comment is correct - you can re-create an Endpoint given an existing EndpointConfiguration. This can be done via the console, the AWS CLI, or the SageMaker boto client.</p>

<ul>
<li><a href=""https://docs.aws.amazon.com/cli/latest/reference/sagemaker/create-endpoint.html"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/cli/latest/reference/sagemaker/create-endpoint.html</a></li>
<li><a href=""https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/sagemaker.html#SageMaker.Client.create_endpoint"" rel=""nofollow noreferrer"">https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/sagemaker.html#SageMaker.Client.create_endpoint</a></li>
</ul>
","407160",0
421,52823515,2,51533650,2018-10-15 19:27:34,1,"<p>When you train a model with <code>Estimators</code>, it <a href=""https://sagemaker.readthedocs.io/en/latest/estimators.html"" rel=""nofollow noreferrer"">defaults to 30 GB of storage</a>, which may not be enough. You can use the <code>train_volume_size</code> param on the constructor to increase this value. Try with a large-ish number (like 100GB) and see how big your model is. In subsequent jobs, you can tune down the value to something closer to what you actually need.</p>

<p>Storage costs <a href=""https://aws.amazon.com/sagemaker/pricing/"" rel=""nofollow noreferrer"">$0.14 per GB-month of provisioned storage</a>. Partial usage is prorated, so giving yourself some extra room is a cheap insurance policy against running out of storage.</p>
","2601671",2
422,52850290,2,52844431,2018-10-17 08:19:37,1,"<p>I don't think you can fix that - the Python script environment in Azure ML Studio is rather locked down, you can't really configure it (except for choosing from a small selection of Anaconda/Python versions). </p>

<p>You might be better off using the new Azure ML service, which allows you considerably more configuration options (including using GPUs and the like). </p>
","155697",2
423,52853464,2,52664415,2018-10-17 11:10:04,2,"<p>Ml Batch Execution- to call retraining experiment and get a .ilearner file as output.
ML Update Resource- Use the above .ilearner as input and call patch endpoint of predictive web service to Update resource.</p>
","10397951",0
424,52864930,2,52684987,2018-10-17 23:26:48,1,"<p>SageMaker local mode is designed to pick up whatever credentials are available in your boto3 session, and pass them into the docker container as environment variables. </p>

<p>However, the version of the sagemaker sdk that you are using (1.11.1 and earlier) will ignore the credentials if they include a token, because that usually indicates short-lived credentials that won't remain valid long enough for a training job to complete or endpoint to be useful.</p>

<p>If you are using temporary credentials, try replacing them with permanent ones, or running from an ec2 instance (or SageMaker notebook!) that has an appropriate instance role assigned.</p>

<p>Also, the sagemaker sdk's handling of credentials changed in v1.11.2 and later -- temporary credentials will be passed to local mode containers, but with a warning message. So you could just upgrade to a newer version and try again (<code>pip install -U sagemaker</code>). </p>

<p>Also, try upgrading <code>boto3</code> can change, so try using the latest version.</p>
","3920238",2
425,52866406,2,52847777,2018-10-18 03:06:33,0,"<p>Assuming you are referring to <a href=""https://github.com/awslabs/amazon-sagemaker-examples/blob/master/introduction_to_amazon_algorithms/imageclassification_caltech/Image-classification-transfer-learning-highlevel.ipynb"" rel=""nofollow noreferrer"">https://github.com/awslabs/amazon-sagemaker-examples/blob/master/introduction_to_amazon_algorithms/imageclassification_caltech/Image-classification-transfer-learning-highlevel.ipynb</a> - you have a typo, it's <code>precision_dtype</code>, not <code>precission_dtype</code>.</p>
","1502599",0
426,52896212,2,52876202,2018-10-19 16:10:56,1,"<p>I'm not sure if I understood your question correctly. However, if you want to feed multiple images to the model at once, you can create a multi-dimensional array of images (byte arrays) to feed the model.</p>

<p>The code would look something like this.</p>

<pre><code>import numpy as np
...

#  predict_images_list is a Python list of byte arrays
predict_images = np.stack(predict_images_list)

with graph.as_default():
    #  results is an list of typical results you'd get.
    results = object_detector.predict(predict_images)
</code></pre>

<p>But, I'm not sure if it's a good idea to feed 2000 images at once. Better to batch them in 20-30 images at a time and predict. </p>
","1538258",1
427,52944464,2,52940677,2018-10-23 08:27:49,1,"<p>Pull our data from S3 for example:</p>

<pre><code>import boto3
import io
import pandas as pd


# Set below parameters
bucket = '&lt;bucket name&gt;'
key = 'data/training/iris.csv'
endpointName = 'decision-trees'

# Pull our data from S3
s3 = boto3.client('s3')
f = s3.get_object(Bucket=bucket, Key=key)

# Make a dataframe
shape = pd.read_csv(io.BytesIO(f['Body'].read()), header=None)
</code></pre>
","10535783",1
428,52953000,2,52819122,2018-10-23 15:41:31,0,"<p>Currently, the score.py needs to be in current working directory, but dependency scripts - the <em>dependencies</em> argument to  <em>ContainerImage.image_configuration</em> - can be in a subfolder.</p>

<p>Therefore, you should be able to use folder structure like this:</p>

<pre><code>./score.py 
./myscripts/train.py 
./myscripts/common.py
</code></pre>

<p>Note that the relative folder structure is preserved during web service deployment; if you reference the common file in subfolder from your score.py, that reference should be valid within deployed image.</p>
","5784983",3
429,53020100,2,52871630,2018-10-27 08:34:28,4,"<p>I'm one of the dvc developers. Similar error has affected dvc running on cygwin. We've released a fix for it in <code>0.20.0</code>. Please upgrade.</p>
","2628602",0
430,53059130,2,52857309,2018-10-30 07:15:11,0,"<p>You can certainly match multiple files with the same prefix, so your first attempt could have worked as long as you organize your files in your S3 bucket to suit. For e.g. the prefix: <code>s3://mybucket/foo/</code> will match the files <code>s3://mybucket/foo/bar/data1.txt</code> and <code>s3://mybucket/foo/baz/data2.txt</code></p>

<p>However, if there is a third file in your bucket called <code>s3://mybucket/foo/qux/data3.txt</code> that you <em>don't</em> want matched (while still matching the first two) there is no way to do achieve that with a single prefix. In these cases a manifest would work. So, in the above example, the manifest would simply be:</p>

<pre><code>[
  {""prefix"": ""s3://mybucket/foo/""},
  ""bar/data1.txt"",
  ""baz/data2.txt""
]
</code></pre>

<p><em>(and yes, this is valid json - it is an array whose first element is an object with an attribute called <code>prefix</code> and all subsequent elements are strings).</em></p>

<p>Please double check your manifest (you didn't actually post it so I can't do that for you) and make sure it conforms to the above syntax.</p>

<p>If you're still stuck please open up a thread on the AWS sagemaker forums - <a href=""https://forums.aws.amazon.com/forum.jspa?forumID=285"" rel=""nofollow noreferrer"">https://forums.aws.amazon.com/forum.jspa?forumID=285</a> and after you do that we can setup a PM to try and get to the bottom of this (never post your AWS account id in a public forum like StackOverflow or even in AWS forums).</p>
","1502599",0
431,53129356,2,53087851,2018-11-03 07:32:59,2,"<p>SageMaker team member here.</p>

<p>The problem here is that the training job in question was setup with S3DataType=ManifestFile. In this case SageMaker expects to be able to download a single manifest file from the location specified by the S3Uri, if the file does not exist in S3 we get a 404 which is what we're sending back as the error here.</p>

<p>See here for documentation on S3DataType/S3Uri and manifests: <a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/API_S3DataSource.html#SageMaker-Type-S3DataSource-S3DataType"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/sagemaker/latest/dg/API_S3DataSource.html#SageMaker-Type-S3DataSource-S3DataType</a></p>

<p>We will work to make this error message a bit more user-friendly, thanks for calling this out!</p>
","1502599",2
432,53165954,2,53165953,2018-11-06 04:57:19,1,"<p>The issue was resolved by calling _make_predict_function() method in the model load phase.</p>

<pre><code>@classmethod
def get_model(cls):
    if cls.model == None:
        cls.model = load_model('/opt/ml/bitcoin_model.h5')
        cls.model._make_predict_function()
    return cls.model
</code></pre>

<p>Bug Reference : <a href=""https://github.com/keras-team/keras/issues/6462"" rel=""nofollow noreferrer"">https://github.com/keras-team/keras/issues/6462</a></p>
","6164611",0
433,53170257,2,53143396,2018-11-06 10:41:58,3,"<p>Microsoft AI is a first party solution Microsoft built, Databricks is based off of Apache Spark that we will manage for you in Azure.  One of the things we are trying to do in Azure is meet the customer where they are most comfortable.  In a way it's similar to how we have Service Fabric (Microsoft Service) and Azure Kubernetes Service (AKS).  Both allow you to run microservices but one is a service we developed and the other is a managed Open Source Project we support.  </p>

<p>When to use each is more of a question of preference and skillset. </p>
","4800789",2
434,53234247,2,53105741,2018-11-09 22:48:11,2,"<p>Do you mean logging out of AWS console or your laptop? Your training job should still be running on the notebook instance whether you have notebook open or not. Notebook instance will always be active until you manually stop it[1]. You can always access the notebook instance again by opening the notebook through console.</p>

<p>[1]<a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/API_StopNotebookInstance.html"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/sagemaker/latest/dg/API_StopNotebookInstance.html</a></p>
","4842651",1
435,53266856,2,53125108,2018-11-12 17:02:25,1,"<p>It's not exactly that you want, but looks like <a href=""https://en.wikipedia.org/wiki/Version_control"" rel=""nofollow noreferrer"">VCS</a> can fit your needs. You can use Github(if you already use it) or CodeCommit(free privat repos) Details and additional ways like <code>sync</code> target <code>dir</code> with <code>S3</code> bucket - <a href=""https://aws.amazon.com/blogs/machine-learning/how-to-use-common-workflows-on-amazon-sagemaker-notebook-instances/"" rel=""nofollow noreferrer"">https://aws.amazon.com/blogs/machine-learning/how-to-use-common-workflows-on-amazon-sagemaker-notebook-instances/</a></p>
","4249707",0
436,53285949,2,53259647,2018-11-13 16:55:19,3,"<p>Looks like <code>pip install</code> in your case executed ""outside"" of virtualenv  </p>

<p>try to change from:  </p>

<p><code>source /home/ec2-user/anaconda3/bin/activate tensorflow_p36</code>  </p>

<p>to: </p>

<p><code>source /home/ec2-user/anaconda3/bin/activate tensorflow_p36 &amp;&amp; pip install pandas tensorflow-gpu --upgrade</code>  </p>

<p>and delete redundant lines</p>
","4249707",0
437,53347423,2,53224172,2018-11-17 01:36:42,2,"<p>Thank you for using Amazon SageMaker! </p>

<p>You can find keras.json file located in /home/ec2-user/.keras . </p>

<p>Thanks,<br>
Neelam </p>
","4570570",0
438,53453394,2,53182436,2018-11-23 22:07:36,3,"<p>you can now specify metrics(metricName, Regex) that you want to track by using AWS management console or Amazon SageMaker Python SDK APIs. After the model training starts, Amazon SageMaker will automatically monitor and stream the specified metrics in real time to the Amazon CloudWatch console for visualizing time-series curves. </p>

<p>Ref: 
<a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/API_MetricDefinition.html"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/sagemaker/latest/dg/API_MetricDefinition.html</a></p>
","10697340",0
439,53453401,2,53423061,2018-11-23 22:08:24,8,"<p>If you're running the example code on a SageMaker notebook instance, you can use the execution_role which has the <code>AmazonSageMakerFullAccess</code> attached.</p>
<pre><code>from sagemaker import get_execution_role
sagemaker_session = sagemaker.Session()
role = get_execution_role()
</code></pre>
<p>And you can pass this role when initializing <code>tf_estimator</code>.
You can check out the example <a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/automatic-model-tuning-ex-role.html"" rel=""nofollow noreferrer"">here</a> for using <code>execution_role</code> with S3 on notebook instance.</p>
","4842651",1
440,53476451,2,49604773,2018-11-26 07:28:51,2,"<p>It should be added that Azure Machine Learning Workbench <a href=""https://learn.microsoft.com/en-us/azure/machine-learning/service/overview-what-happened-to-workbench"" rel=""nofollow noreferrer"">is deprecated since september 2018</a> and has been replaced by the <a href=""https://learn.microsoft.com/en-us/azure/machine-learning/service/"" rel=""nofollow noreferrer"">Azure Machine Learning services</a>, which was made generally available in december 2018. The core functionality is still intact, but some major changes to point out about the architecture are:</p>

<ul>
<li>A simplified Azure resources model</li>
<li>New portal UI to manage your experiments and compute targets</li>
<li>A new, more comprehensive Python SDK</li>
<li>A new expanded Azure CLI extension for machine learning</li>
</ul>
","6361775",1
441,53488618,2,53280902,2018-11-26 20:32:53,-1,"<p>read_pickle() likes the full path more than a relative path from where it was run. This fixed my issue.</p>
","5208232",0
442,53489851,2,53488870,2018-11-26 22:07:03,3,"<p>This might not be the best answer for your problem, but this is what I am using for a multi-gpu model with Tensorflow backend. First i initialize using: </p>

<pre><code>def setup_multi_gpus():
    """"""
    Setup multi GPU usage

    Example usage:
    model = Sequential()
    ...
    multi_model = multi_gpu_model(model, gpus=num_gpu)
    multi_model.fit()

    About memory usage:
    https://stackoverflow.com/questions/34199233/how-to-prevent-tensorflow-from-allocating-the-totality-of-a-gpu-memory
    """"""
    import tensorflow as tf
    from keras.utils.training_utils import multi_gpu_model
    from tensorflow.python.client import device_lib

    # IMPORTANT: Tells tf to not occupy a specific amount of memory
    from keras.backend.tensorflow_backend import set_session  
    config = tf.ConfigProto()  
    config.gpu_options.allow_growth = True  # dynamically grow the memory used on the GPU  
    sess = tf.Session(config=config)  
    set_session(sess)  # set this TensorFlow session as the default session for Keras.


    # getting the number of GPUs 
    def get_available_gpus():
       local_device_protos = device_lib.list_local_devices()
       return [x.name for x in local_device_protos if x.device_type    == 'GPU']

    num_gpu = len(get_available_gpus())
    print('Amount of GPUs available: %s' % num_gpu)

    return num_gpu
</code></pre>

<p>Then i call</p>

<pre><code># Setup multi GPU usage
num_gpu = setup_multi_gpus()
</code></pre>

<p>and create a model.</p>

<pre><code>...
</code></pre>

<p>After which you're able to make it a multi GPU model.</p>

<pre><code>multi_model = multi_gpu_model(model, gpus=num_gpu)
multi_model.compile...
multi_model.fit...
</code></pre>

<p>The only thing here that is different from what you are doing is the way Tensorflow is initializing the GPU's. I can't imagine it being the problem, but it might be worth trying out. </p>

<p>Good luck! </p>

<p>Edit: I noticed sequence to sequence not being able to work with multi GPU. Is that the type of model you are trying to train?</p>
","3482325",3
443,53534178,2,53533434,2018-11-29 07:51:11,0,"<p>I have found the issue. The batchtransform select the folder as input and the s3 source should be S3Prefix instead of manifest.</p>
","4985990",0
444,53555742,2,51933366,2018-11-30 10:32:45,1,"<p>The situation has changed : Git is now <a href=""https://aws.amazon.com/blogs/machine-learning/amazon-sagemaker-notebooks-now-support-git-integration-for-increased-persistence-collaboration-and-reproducibility/"" rel=""nofollow noreferrer"">available</a> in SageMaker</p>
","1058203",0
445,53588176,2,53588040,2018-12-03 05:52:05,5,"<p>you have to include azureml-train-automl package. and you have to do this:</p>

<p>import azureml.train.automl</p>
","6191408",2
446,53607200,2,53595157,2018-12-04 06:51:45,7,"<p>I resolved the issue by changing the instance type:</p>

<pre><code>kmeans_predictor = kmeans.deploy(initial_instance_count=1,
                                 instance_type='ml.t2.medium')
</code></pre>
","2553300",0
447,53635827,2,53586515,2018-12-05 15:40:16,0,"<p>I think I have found the answer to my problem. I have set up another k8s cluster and deployed the container there as well. They are working fine and the certificate issues does not happen. When investigating more I have noticed that they were some issues with DNS resolution on the first k8s cluster. In fact the containers with certificate issues could not ping google.com for example.
I fixed the DNS issue by not relying on core-dns and setting the DNS configuration in the deployment.yaml file. I am not sure to understand why exactly but this seems to have fixed the certificate issue.</p>
","4761371",0
448,53706796,2,53700018,2018-12-10 13:34:16,4,"<p>This is a feature of Deployment Manager, which is used to create the cluster.
If you create any resource using DM, but edit or delete it manually (=elsewhere in the console), the record of it remains unchanged in the DM. </p>

<p>To fix your issue, navigate to <a href=""https://console.cloud.google.com/dm/deployments"" rel=""nofollow noreferrer"">Deployment Manager in your GCP Console</a> and delete the relevant deployment. Then you will be able to re-install KubeFlow without this error. </p>
","6933420",0
449,53717457,2,53408927,2018-12-11 04:39:07,1,"<p>The error looks to be coming from a GRPC client closing the connection before the server is able to respond. (There looks to be an existing feature request for the sagemaker tensorflow container on <a href=""https://github.com/aws/sagemaker-tensorflow-container/issues/46"" rel=""nofollow noreferrer"">https://github.com/aws/sagemaker-tensorflow-container/issues/46</a> to make this timeout configurable)</p>

<p>You could try out a few things with the sagemaker Transformer to limit the size of each individual request so that it fits within the timeout:</p>

<ul>
<li>Set a <code>max_payload</code> to a smaller value, say 2-3 MB (<a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/API_CreateTransformJob.html#SageMaker-CreateTransformJob-request-MaxPayloadInMB"" rel=""nofollow noreferrer"">the default is 6 MB</a>)</li>
<li>If your instance metrics indicate it has compute / memory resources to spare, try <code>max_concurrent_transforms</code> > 1 to make use of multiple workers</li>
<li>Split up your csv file into multiple input files. With a bigger dataset, you could also increase the instance count to fan out processing</li>
</ul>
","10392672",0
450,53723588,2,53703191,2018-12-11 11:49:52,2,"<p>The documentation could be a little more clear on this part, I agree. The <code>new-image</code> is an image object that you should pass into the <code>update()</code> function. If you just created the image you might already have the object in a variable, then just pass it. If not, then you can obtain it from your workspace using</p>

<pre class=""lang-py prettyprint-override""><code>from azureml.core.image.image import Image
new_image = Image(ws, image_name)
</code></pre>

<p>where <code>ws</code> is your workspace object and <code>image_name</code> is a string with the name of the image you want to obtain. Then you go on calling <code>update()</code> as</p>

<pre class=""lang-py prettyprint-override""><code>from azureml.core.webservice import Webservice

service_name = 'aci-mnist-3'

# Retrieve existing service
service = Webservice(name = service_name, workspace = ws)

# Update the image used by the service
service.update(image = new_image) # Note that dash isn't supported in variable names

print(service.state)
</code></pre>

<p>You can find more information in the <a href=""https://learn.microsoft.com/en-us/python/api/overview/azure/ml/intro?view=azure-ml-py"" rel=""nofollow noreferrer"">SDK documentation</a></p>

<p>EDIT:
Both the <code>Image</code> and the <code>Webservice</code> classes above are abstract parent classes.</p>

<p>For the <code>Image</code> object, you should really use one of these classes, depending on your case:</p>

<ul>
<li><code>ContainerImage</code></li>
<li><code>UnknownImage</code></li>
</ul>

<p>(see <a href=""https://learn.microsoft.com/en-us/python/api/azureml-core/azureml.core.image?view=azure-ml-py"" rel=""nofollow noreferrer"">Image package</a> in the documentation).</p>

<p>For the <code>Webservice</code> object, you should use one of these classes, depending on your case:</p>

<ul>
<li><code>AciWebservice</code></li>
<li><code>AksWebservice</code></li>
<li><code>UnknownWebservice</code></li>
</ul>

<p>(see <a href=""https://learn.microsoft.com/en-us/python/api/azureml-core/azureml.core.webservice?view=azure-ml-py"" rel=""nofollow noreferrer"">Webservice package</a> in the documentation).</p>
","6361775",1
451,53725466,2,53701951,2018-12-11 13:47:05,3,"<p>The deployment created by <code>kfctl.sh</code> in ""Deploy Kubeflow on GKE using the command line"" also creates a load balancer resource for the ingress into the cluster and secures it using Cloud Identity-Aware Proxy (IAP).</p>

<p>To allow access to the resource for new users, go to:</p>

<p>Google Cloud Console > IAM &amp; Admin > Identity-Aware Proxy</p>

<p>Select the desired resource and click ""Add Member"".</p>

<p>Fill in the user in the ""Access Denied"" page and select ""Cloud IAP > IAP-Secured Web App User"" for role.</p>

<p>Once the policy change is propagated, the user will be able to access the URL successfully.</p>
","2224168",0
452,53733743,2,53636589,2018-12-11 23:11:17,6,"<p>This message mean SageMaker tried to launch the instance but EC2 was not having enough capacity of this instance hence after waiting for some time(in this case 1 hour) SageMaker gave up and failed the training job.</p>

<p>For more information about capacity issue from ec2, please visit: 
<a href=""https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/troubleshooting-launch.html#troubleshooting-launch-capacity"" rel=""noreferrer"">troubleshooting-launch-capacity</a></p>

<p>To solve this, you can either try running jobs with different instance type as suggested in failure reason or wait a few minutes and then submit your request again as suggested by EC2.</p>
","1472212",0
453,53734837,2,53717284,2018-12-12 01:31:48,0,"<p><strong>FIXED</strong></p>

<p><strong>Problem:</strong></p>

<p>Some R codes don't work if input data is limited (e.g 1-2 rows only)</p>

<p><strong>Solution:</strong></p>

<p>Load data by <code>Batch</code> instead of <code>REQUEST/RESPONSE</code></p>
","6541079",0
454,53754067,2,53753367,2018-12-13 02:05:44,1,"<p>In Azure ML Studio, the versioning is available as Run History: <a href=""https://learn.microsoft.com/en-us/azure/machine-learning/studio/manage-experiment-iterations"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/machine-learning/studio/manage-experiment-iterations</a>
Regards,
Jaya</p>
","8376649",0
455,53789373,2,53789057,2018-12-15 03:53:43,1,"<p>Predicting the most likely room, which they would walk in. :</p>

<p>This falls under the classification problem. The output falls under a set of categories, in this case it is different rooms.</p>

<p>Predicting the Occpancy of each room :
As mentioned by @poorna is a regression problem. </p>

<p>Two ways you can look at this problem, </p>

<ol>
<li><p>Multi- target regression problem with occupancy of each room as one target and past occupancies of all rooms as input. </p></li>
<li><p>Independent forecast problem for each room with past occupancies of corresponding room as input.</p></li>
</ol>

<p>For learning the basics of machine learning, you can go through this <a href=""https://scikit-learn.org/stable/tutorial/basic/tutorial.html"" rel=""nofollow noreferrer"">link</a></p>
","6347629",3
456,53799724,2,52415136,2018-12-16 05:36:40,3,"<p>The recommended way to do this (as of 12/16/2018) would be to use the newly- launched Git integration for SageMaker Notebook Instances.</p>

<ol>
<li>Create a Git repository for your notebooks</li>
<li>Commit and push changes from Notebook Instance #1 to your Git repo</li>
<li>Start Notebook Instance #2 using the same Git repo</li>
</ol>

<p>This way your notebooks are persisted in the Git repo rather than on the  instance, and the Git repo can be shared by multiple instances. </p>

<p><a href=""https://aws.amazon.com/blogs/machine-learning/amazon-sagemaker-notebooks-now-support-git-integration-for-increased-persistence-collaboration-and-reproducibility/"" rel=""nofollow noreferrer"">https://aws.amazon.com/blogs/machine-learning/amazon-sagemaker-notebooks-now-support-git-integration-for-increased-persistence-collaboration-and-reproducibility/</a></p>
","8899112",0
457,53813096,2,53791461,2018-12-17 10:18:09,0,"<p>I emailed the Auto ML helpdesk and they solved the problem.</p>

<p>Quote from them: </p>

<blockquote>
  <p>We have a bug where the AutoML inferencing fails because the pandas
  version is 0.22.0 which doesn’t have some API support.</p>
</blockquote>

<p>I upgraded pandas on my hosted notebook to version 0.23.4, and after this the model unpickles and works successfully</p>
","139200",0
458,53819136,2,51853249,2018-12-17 16:20:24,0,"<p>Upon further examination (of the log files), it appears the issue does not lie with the .lst file itself, but with the image files it was referencing (which now leaves me wondering why AWS doesn't just say that instead of saying the .lst file is corrupt). I'm going through the image files one-by-one to verify they are correct, hopefully that will solve the problem.</p>
","2174028",0
459,53847753,2,53717800,2018-12-19 09:02:47,5,"<p>You shouldn't need to install anything else. Keras relies on TensorFlow for GPU detection and configuration.</p>

<p>The only thing worth noting is how to use multiple GPUs during training. I'd recommend passing 'gpu_count' as an hyper parameter, and setting things up like so:</p>

<pre><code>from keras.utils import multi_gpu_model
model = Sequential()
model.add(...)
...
if gpu_count &gt; 1:
    model = multi_gpu_model(model, gpus=gpu_count)
model.compile(...)
</code></pre>
","4686192",0
460,53853913,2,53797757,2018-12-19 15:02:37,1,"<p>The key was in the SageMaker python SDK. There you can find a function that transforms a scipy sparse matrix to a sparse tensor (<code>write_spmatrix_to_sparse_tensor</code>).</p>

<p>The complete code that solved the problem without having to incur into a dense matrix is the following:</p>

<pre><code>from sagemaker.amazon.common import write_spmatrix_to_sparse_tensor

tfidf_matrix = tfidf_vectorizer.fit_transform('your_train_data') # output: sparse scipy matrix
sagemaker_bucket = 'your-bucket' 
data_key = 'kmeans_lowlevel/data'
data_location = f""s3://{sagemaker_bucket}/{data_key}""
buf = io.BytesIO()
write_spmatrix_to_sparse_tensor(buf, tfidf_matrix)
buf.seek(0)
boto3.resource('s3').Bucket(sagemaker_bucket).Object(data_key).upload_fileobj(buf)
</code></pre>

<p>After doing this, in the <code>create_training_params</code> configuration you'll have to feed the S3Uri field with the data location you have provided to store the sparse matrix in S3:</p>

<pre><code>create_training_params = \
{
    ... # all other params

    ""InputDataConfig"": [
        {
            ""ChannelName"": ""train"",
            ""DataSource"": {
                ""S3DataSource"": {
                    ""S3DataType"": ""S3Prefix"",
                    ""S3Uri"": data_location, # YOUR_DATA_LOCATION_GOES_HERE
                    ""S3DataDistributionType"": ""FullyReplicated""
                }
            },
            ""CompressionType"": ""None"",
            ""RecordWrapperType"": ""None""
        }
    ]
}
</code></pre>
","708366",0
461,53891090,2,53809556,2018-12-21 21:43:22,0,"<p>If you are using Amazon SageMaker you can use the SageMaker python library that is implementing the most useful commands for data scientists, including the upload of files to S3. It is already installed on your SageMaker notebook instance by default. </p>

<pre><code>import sagemaker
sess = sagemaker.Session()

# Uploading the local file to S3
sess.upload_data(path='local-file.txt', bucket=bucket_name, key_prefix='input')    
</code></pre>
","179529",0
462,53892198,2,53891907,2018-12-22 00:40:31,1,"<p>Just for the sake of trying, this is how I'd implement this. Maybe it will help you.</p>

<pre><code>df['RUL'] = df.loc[:, ['engine', 'cycle']].groupby('engine').transform('max')
df['RUL'] = df['RUL'] - df['cycle']
</code></pre>
","7340948",0
463,53910009,2,53908529,2018-12-24 06:38:18,3,"<p>It's probably because the name if your python file is the same as a module name you are trying import. In this case, rename the file to something other than <code>azureml.py</code>.</p>
","3679377",1
464,53921063,2,53872444,2018-12-25 09:31:15,1,"<p>I tried to reproduce your issue on my Azure Jupyter Notebook, but failed. There was no any issue for me without doing your two steps <code>!pip install --upgrade pip</code> &amp; <code>!pip install -I Cython==0.28.5</code> which I think not matter.</p>

<p>Please run some codes below to check your import package <code>pandas</code> whether be correct.</p>

<ol>
<li>Run <code>print(pandas.__dict__)</code> to check whether has the description of <code>read_parquet</code> function in the output.</li>
<li>Run <code>print(pandas.__file__)</code> to check whether you imported a different <code>pandas</code> package.</li>
<li>Run <code>import sys; print(sys.path)</code> to check the order of paths whether there is a same named file or directory under these paths.</li>
</ol>

<p>If there is a same file or directory named <code>pandas</code>, you just need to rename it and restart your <code>ipynb</code> to re-run. It's a common issue which you can refer to these SO threads <a href=""https://stackoverflow.com/questions/35341363/attributeerror-module-object-has-no-attribute-reader"">AttributeError: &#39;module&#39; object has no attribute &#39;reader&#39;</a> and <a href=""https://stackoverflow.com/questions/36250353/importing-installed-package-from-script-raises-attributeerror-module-has-no-at"">Importing installed package from script raises &quot;AttributeError: module has no attribute&quot; or &quot;ImportError: cannot import name&quot;</a>.</p>

<p>In Other cases, please update your post for more details to let me know.</p>

<hr>

<p>The latest <code>pandas</code> version should be <code>0.23.4</code>, not <code>0.24.0</code>.</p>

<p>I tried to find out the earliest version of <code>pandas</code> which support the <code>read_parquet</code> feature via search the function name <code>read_parquet</code> in the documents of different version from <code>0.19.2</code> to <code>0.23.3</code>. Then, I found <code>pandas</code> supports <code>read_parquet</code> feature after the version <code>0.21.1</code>, as below.</p>

<p><a href=""https://i.stack.imgur.com/a6Jl9.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/a6Jl9.png"" alt=""enter image description here""></a></p>

<p>The new features shown in the <a href=""http://pandas.pydata.org/pandas-docs/version/0.21/whatsnew.html"" rel=""nofollow noreferrer""><code>What's New</code></a> of version <code>0.21.1</code>
<a href=""https://i.stack.imgur.com/cuSOe.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/cuSOe.png"" alt=""enter image description here""></a></p>

<p>According to your <code>EDIT 2</code> description, it seems that you are using Python 3.4 in Azure Jupyter Notebook. Not all <code>pandas</code> versions support Python 3.4 version.</p>

<p>The versions <a href=""http://pandas.pydata.org/pandas-docs/version/0.21/install.html#python-version-support"" rel=""nofollow noreferrer""><code>0.21.1</code></a> &amp; <a href=""http://pandas.pydata.org/pandas-docs/version/0.22/install.html#python-version-support"" rel=""nofollow noreferrer""><code>0.22.0</code></a> offically support Python 2.7,3.5, and 3.6, as below.
<a href=""https://i.stack.imgur.com/fM9RT.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/fM9RT.png"" alt=""enter image description here""></a></p>

<p>And the <a href=""https://pypi.org/project/pandas/"" rel=""nofollow noreferrer"">PyPI page for <code>pandas</code></a> also requires the Python version as below.</p>

<p><a href=""https://i.stack.imgur.com/6613J.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/6613J.png"" alt=""enter image description here""></a></p>

<p>So you can try to install the <code>pandas</code> versions <code>0.21.1</code> &amp; <code>0.22.0</code> in the current notebook of Python 3.4. if failed, please create a new notebook in Python <code>2.7</code> or <code>&gt;=3.5</code> to install <code>pandas</code> version <code>&gt;= 0.21.1</code> to use the function <code>read_parquet</code>.</p>
","4989676",5
465,53985934,2,53952110,2018-12-31 09:44:01,0,"<p>Indeed as you said. I searched for the keywords <code>AudioConfig</code> &amp; <code>FromWavFileInput</code> on GitHub repo <a href=""https://github.com/Azure-Samples/cognitive-services-speech-sdk"" rel=""nofollow noreferrer""><code>Azure-Samples/cognitive-services-speech-sdk</code></a>, there is not any Python codes about it except for Java, C#, and <a href=""https://github.com/Azure-Samples/cognitive-services-speech-sdk/blob/3131ab75577116fe9359242d6d86321808601e19/samples/cpp/windows/console/samples/speech_recognition_samples.cpp#L116"" rel=""nofollow noreferrer"">C++</a>.</p>

<p>So per my experience, there are two workaround ways to do it.</p>

<ol>
<li>Wrap the C++ codes as a <a href=""https://docs.python.org/3/c-api/index.html"" rel=""nofollow noreferrer"">Python extension module</a>, or communicate with C++/Java codes.</li>
<li>Directly using <a href=""https://learn.microsoft.com/en-us/azure/cognitive-services/speech-service/rest-apis"" rel=""nofollow noreferrer"">Speech service REST APIs</a> with <a href=""http://docs.python-requests.org/en/master/"" rel=""nofollow noreferrer""><code>requests</code></a>, it's simple for Python and Azure Speech Service.</li>
</ol>
","4989676",0
466,54004317,2,54003442,2019-01-02 10:00:09,1,"<p>I would suggest to stop using <code>Azure Machine Learning Studio</code> and switch to ""real"" Azure ML with <code>Azure Machine Learning Services</code>, where you will have much more control on your compute needs.</p>

<p>Azure ML Studio roadmap is really limited and the purpose of this solution was to help people coming to Machine Learning. If you have a real use-case, use Azure Machine Learning Services.</p>
","3136339",3
467,54034372,2,54034172,2019-01-04 06:59:03,1,"<p>Are you using <a href=""https://www.tensorflow.org/guide/estimators"" rel=""nofollow noreferrer"">TensorFlow estimator APIs</a> in your script? If yes, I think you should run the script by wrapping it in <code>sagemaker.tensorflow.TensorFlow</code> class as described <a href=""https://github.com/aws/sagemaker-python-sdk/blob/master/src/sagemaker/tensorflow/README.rst#training-with-tensorflow-estimator"" rel=""nofollow noreferrer"">in the documentation here</a>. If you run training that way, parallelization and communication between instances should work out-of-the-box.</p>

<p>But note that scaling will not be linear when you increase the number of instances. Communicating between instances takes time and there could be non-parallelizable bottlenecks in your script like loading data to memory.</p>
","10561443",4
468,54036391,2,54026623,2019-01-04 09:43:01,2,"<p>The script is expecting 'bucket' to be bucket = Session().default_bucket() or your own. Have you tried setting bucket equal to your personal bucket?</p>
","4476294",0
469,54079366,2,53849650,2019-01-07 17:52:22,0,"<p>I'm a member of the Ground Truth service team. We do not support multiple objects as part of the bounding boxes labeling task. We recognize this as a feature request and will look to prioritize for future releases.</p>
","10880486",1
470,54096125,2,54003052,2019-01-08 16:41:37,14,"<p>I was able to create a PyTorch <code>Dataset</code> backed by S3 data using <code>boto3</code>. Here's the snippet if anyone is interested.</p>

<pre><code>class ImageDataset(Dataset):
    def __init__(self, path='./images', transform=None):
        self.path = path
        self.s3 = boto3.resource('s3')
        self.bucket = self.s3.Bucket(path)
        self.files = [obj.key for obj in self.bucket.objects.all()]
        self.transform = transform
        if transform is None:
            self.transform = transforms.Compose([
                transforms.Resize((128, 128)),
                transforms.ToTensor(),
                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
            ])

    def __len__(self):
        return len(files)

    def __getitem__(self, idx):
        img_name = self.files[idx]

        # we may infer the label from the filename
        dash_idx = img_name.rfind('-')
        dot_idx = img_name.rfind('.')
        label = int(img_name[dash_idx + 1:dot_idx])

        # we need to download the file from S3 to a temporary file locally
        # we need to create the local file name
        obj = self.bucket.Object(img_name)
        tmp = tempfile.NamedTemporaryFile()
        tmp_name = '{}.jpg'.format(tmp.name)

        # now we can actually download from S3 to a local place
        with open(tmp_name, 'wb') as f:
            obj.download_fileobj(f)
            f.flush()
            f.close()
            image = Image.open(tmp_name)

        if self.transform:
            image = self.transform(image)

        return image, label
</code></pre>
","2175052",2
471,54201141,2,54200444,2019-01-15 14:42:54,0,"<p>You can run <code>docker image prune</code> to clean up unused images or <code>docker system prune</code> 
to cleanup all docker unused resources.</p>

<p>Also you can configure <a href=""https://kubernetes.io/docs/concepts/cluster-administration/kubelet-garbage-collection/"" rel=""nofollow noreferrer"">Garbage Collection feature</a> of Kubernetes</p>
","10706312",0
472,54206403,2,54193723,2019-01-15 20:32:38,1,"<p>Your understanding is correct. The endpoint resizes images based on the parameter <code>image_shape</code>. To answer your questions:</p>

<ol>
<li>As long as the scale of objects (i.e., expansion of pixels) in the resized images are similar between training and prediction data, the trained model should work.</li>
<li>Cropping is one option. Another method is to train separate models for large and small images as David suggested.</li>
</ol>
","10130097",1
473,54221824,2,54154455,2019-01-16 16:55:53,1,"<p>This worked:</p>

<pre class=""lang-py prettyprint-override""><code>    input_text = app.current_request.raw_body
    d = parse_qs(input_text)
    lst = d[b'user_input'][0].decode()
    res = sagemaker.invoke_endpoint(
                    EndpointName='&lt;name-of-SageMaker-Endpoint&gt;',
                    Body=lst,
                    ContentType='text/csv',
                    Accept='Accept'
                )
</code></pre>
","10865390",0
474,54223416,2,54172907,2019-01-16 18:44:23,1,"<p>The problem was with the MFA autharization. 
When I invoked the model from inside the model, the MFA was passed. 
But when I tried to invoke the model from my machine, the MFA was not passed, so the access was denied.</p>

<p>I created special user without MFA to debug the model, and that solved my problem.</p>
","1117615",0
475,54223608,2,53770876,2019-01-16 18:59:52,3,"<p>This works:</p>

<pre><code>import numpy as np
import boto3

client = boto3.client('sagemaker-runtime')
endpoint_name = 'DEMO-XGBoostEndpoint-2018-12-12-22-07-28'
test_vector = [3.60606061e+00, 
               3.91395664e+00, 
               1.34200000e+03, 
               4.56100000e+03,
               2.00000000e+02, 
               2.00000000e+02]) 

body = ',',join([str(item) for item in test_vector])
response = client.invoke_endpoint(EndpointName=endpoint_name,
                               ContentType='text/csv',
                               Body=body)
</code></pre>
","1117615",0
476,54230572,2,54184145,2019-01-17 06:59:37,1,"<p>Your conda update does not refer to a specific virtualenv, while your notebook probably does. Therefore you dont see an update on the notebook virtualenv.</p>
","121956",5
477,54242871,2,53697587,2019-01-17 19:20:19,3,"<p>as showed in your log, one of input channels was named as <code>val</code>. The correct channel name for validation data should be <code>validation</code>. More details on input configuration can be found here: <a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/image-classification.html"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/sagemaker/latest/dg/image-classification.html</a></p>
","10130097",0
478,54250455,2,54224934,2019-01-18 08:53:59,2,"<p>I'll try to answer your question on a high level. Disclaimer: I'm not at an expert across the full stack of what you describe, and I would welcome corrections or additions from people who are. </p>

<p>I'll go over the different components from bottom to top:</p>

<p><strong>TensorFlow Serving</strong> is a library for deploying and hosting TensorFlow models as model servers that accept requests with input data and return model predictions. The idea is to train models with TensorFlow, export them to the SavedModel format and serve them with TF Serving. You can set up a TF Server to accept requests via HTTP and/or RPC. One advantage of RPC is that the request message is compressed, which can be useful when sending large payloads, for instance with image data.</p>

<p><strong>Flask</strong> is a python framework for writing web applications. It's much more general-purpose than TF Serving and is widely used to build web services, for instance in microservice architectures. </p>

<p>Now, the combination of Flask and TensorFlow serving should make sense. You could write a Flask web application that exposes an API to the user and calls a TF model hosted with TF Serving under the hood. The user uses the API to transmit some data (<strong>1</strong>), the Flask app perhaps transform the data (for example, wrap it in numpy arrays), calls the TF Server to get a model prediction (<strong>2</strong>)(<strong>3</strong>), perhaps transforms the prediction (for example convert a predicted probability that is larger than 0.5 to a class label of 1), and returns the prediction to the user (<strong>4</strong>). You could visualize this as follows:</p>

<p><a href=""https://i.stack.imgur.com/67EXW.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/67EXW.png"" alt=""enter image description here""></a></p>

<p><strong>Gunicorn</strong> is a Web Server Gateway Interface (WSGI) that is commonly used to host Flask applications in production systems. As the name says, it's the interface between a web server and a web application. When you are developing a Flask app, you can run it locally to test it. In production, gunicorn will run the app for you.</p>

<p>TF Serving will host your model as a functional application. Therefore, you do not need gunicorn to run the TF Server application for you. </p>

<p><strong>Nginx</strong> is the actual web server, which will host your application, handle requests from the outside and pass them to the application server (gunicorn). Nginx cannot talk directly to Flask applications, which is why gunicorn is there. </p>

<p><a href=""https://serverfault.com/questions/331256/why-do-i-need-nginx-and-something-like-gunicorn"">This answer</a> might be helpful as well. </p>

<p>Finally, if you are working on a cloud platform, the web server part will probably be handled for you, so you will either need to write the Flask app and host it with gunicorn, or setup the TF Serving server. </p>
","5495381",0
479,54261434,2,54216721,2019-01-18 21:06:16,3,"<p>You probably created your endpoint like this:</p>

<p>predictor = estimator.deploy(initial_instance_count=1,
                             instance_type='ml.m4.xlarge')</p>

<p>If you set initial_instance_count to more than 1, SageMaker automatically assigns that number of instances to your endpoint. Prediction requests will automatically load-balanced, there's nothing else to to.</p>

<p>If needed, you can also set up auto scaling to handle traffic variation: <a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/endpoint-auto-scaling.html"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/sagemaker/latest/dg/endpoint-auto-scaling.html</a></p>
","4686192",1
480,54261491,2,54049025,2019-01-18 21:11:36,1,"<p>You have two options:</p>

<p>1) run Jupyter notebooks on EMR: <a href=""https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-managed-notebooks.html"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-managed-notebooks.html</a></p>

<p>2) run Jupyter notebooks on SageMaker: <a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/gs.html"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/sagemaker/latest/dg/gs.html</a></p>

<p>Both support PySpark, so you should be able to run SQL queries on whatever backend your data lives in.</p>
","4686192",0
481,54269383,2,54268970,2019-01-19 17:02:45,1,"<p>Probably your labels vector needs to be of shape <code>(batch_size, 1)</code> instead of just <code>(batch_size,)</code>. </p>

<p><strong>Note:</strong> Since you are using <code>sparse_categorical_crossentropy</code> as loss function instead of <code>categorical_crossentropy</code>, it is correct to not one-hot encode the labels. </p>
","10921263",2
482,54315801,2,54254830,2019-01-22 20:13:31,9,"<p>Local compute cannot be used with ML Pipelines. Please see this <a href=""https://learn.microsoft.com/en-us/azure/machine-learning/service/how-to-set-up-training-targets#supported-compute-targets"" rel=""noreferrer"">article</a>.</p>
","10952233",3
483,54324717,2,54319471,2019-01-23 10:08:22,0,"<p>I think one could think about 2 scenarios:</p>

<p>1) if you need very low latency, you can fill up the matrix indeed, i.e. compute all recos for all users, and store it in a key/value backend queried by your app. You can definitely predict multiple users at a time, using the one-hot encoded technique above.</p>

<p>2) predict on-demand by invoking the endpoint directly from the app. This is quite simpler, at the cost of a little latency.</p>

<p>Hope this helps.</p>
","4686192",0
484,54328689,2,54314876,2019-01-23 13:43:41,5,"<p>There's a source_dir parameter which will ""lift"" a directory of files to the container and put it on your import path.</p>

<p>You're entrypoint script should be put there to and referenced from that location.</p>
","10883664",1
485,54328765,2,54295445,2019-01-23 13:48:10,3,"<p>I've talked to AWS support about this (see <a href=""https://stackoverflow.com/questions/54090270/more-efficient-way-to-send-a-request-than-json-to-deployed-tensorflow-model-in-s"">More efficient way to send a request than JSON to deployed tensorflow model in Sagemaker?</a>).</p>

<p>They suggest that it is possible to pass in a custom input_fn that will be used by the serving container where one can unpack a compressed format (such as protobuf).</p>

<p>I'll be testing this soon and hopefully this stuff works since it would add a lot of flexibility to the input processing.</p>
","10883664",5
486,54336588,2,54334462,2019-01-23 22:20:26,5,"<p>It seems that you are running a training job using one of the SageMaker frameworks. Given that, you can use the ""local mode"" feature of SageMaker, which will run your training job (specifically the container) locally in your notebook instance. That way, you can iterate on your script until it works. Then you can move on to the remote training cluster to train the model against the whole dataset if needed. To use local mode, you just set the instance type to ""local"". More details about local mode can be found at <a href=""https://github.com/aws/sagemaker-python-sdk#sagemaker-python-sdk-overview"" rel=""noreferrer"">https://github.com/aws/sagemaker-python-sdk#sagemaker-python-sdk-overview</a> and the blog post: <a href=""https://aws.amazon.com/blogs/machine-learning/use-the-amazon-sagemaker-local-mode-to-train-on-your-notebook-instance/"" rel=""noreferrer"">https://aws.amazon.com/blogs/machine-learning/use-the-amazon-sagemaker-local-mode-to-train-on-your-notebook-instance/</a></p>
","2038642",0
487,54415960,2,53731005,2019-01-29 07:35:19,2,"<p>The cloudshell is indeed probably too small to run this so you need to run the cloudshell in boosted mode<a href=""https://i.stack.imgur.com/hDtqZ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/hDtqZ.png"" alt=""How to enable boost mode""></a></p>
","9525142",0
488,54416736,2,54408673,2019-01-29 08:30:03,2,"<p>the same sequence works for me locally : 'aws ecr get-login', 'docker login', 'docker pull'. </p>

<p>Does your local IAM user have sufficient credentials to pull from ECR? The 'AmazonEC2ContainerRegistryReadOnly' policy should be enough: <a href=""https://docs.aws.amazon.com/AmazonECR/latest/userguide/ecr_managed_policies.html"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/AmazonECR/latest/userguide/ecr_managed_policies.html</a></p>

<p>Alternatively, you can grab the container from Github and build it: <a href=""https://github.com/aws/sagemaker-tensorflow-container"" rel=""nofollow noreferrer"">https://github.com/aws/sagemaker-tensorflow-container</a></p>
","4686192",3
489,54428809,2,54143127,2019-01-29 20:05:29,3,"<p>In the case you are using Kubeflow, I would suggest to use the kubeflow pipelines.</p>

<p>For the preprocessing you could use an image that is build on top of the standard pipeline dataflow image <code>gcr.io/ml-pipeline/ml-pipeline-dataflow-tft:latest</code> where you simply copy your dataflow code and run it: </p>

<pre><code>FROM gcr.io/ml-pipeline/ml-pipeline-dataflow-tft:latest
RUN mkdir /{folder}
COPY run_dataflow_pipeline.py /{folder}
ENTRYPOINT [""python"", ""/{folder}/run_dataflow_pipeline.py""]
</code></pre>

<p>See this <a href=""https://github.com/Fematich/mlengine-boilerplate/blob/master/examples/flowers/preprocess.py"" rel=""nofollow noreferrer"">boilerplate</a> for the dataflow code that does exactly this. The idea is that you write the TF records to Google Cloud Storage (GCS).</p>

<p>Subsequently you could use Google Cloud's ML engine for the actual training. In this case you can start also from the image <code>google/cloud-sdk:latest</code> and basically copy over the required files with probably a bash script that will be run to execute the gcloud commands to start the training job.</p>

<pre><code>FROM google/cloud-sdk:latest
RUN mkdir -p /{src} &amp;&amp; \
    cd /{src} 
COPY train.sh ./
ENTRYPOINT [""bash"", ""./train.sh""]
</code></pre>

<p>An elegant way to pass on the storage location of your TF records into your model is to use TF.data:</p>

<pre><code># Construct a TFRecordDataset
train_records = [os.path.join('gs://{BUCKET_NAME}/', f.name) for f in
                 bucket.list_blobs(prefix='data/TFR/train')]
validation_records = [os.path.join('gs://{BUCKET_NAME}/', f.name) for f in
                      bucket.list_blobs(prefix='data/TFR/validation')]

ds_train = tf.data.TFRecordDataset(train_records, num_parallel_reads=4).map(decode)
ds_val = tf.data.TFRecordDataset(validation_records,num_parallel_reads=4).map(decode)

# potential additional steps for performance: 
# https://www.tensorflow.org/guide/performance/datasets)

# Train the model
model.fit(ds_train,
          validation_data=ds_val,
          ...,
          verbose=2)
</code></pre>

<p>Check out this <a href=""https://towardsdatascience.com/how-to-create-and-deploy-a-kubeflow-machine-learning-pipeline-part-1-efea7a4b650f"" rel=""nofollow noreferrer"">blog post</a> for an actual implementation of a similar (more complex) kubeflow pipeline</p>
","9525142",0
490,54432762,2,54432761,2019-01-30 02:58:11,2,"<p>I found the answer via <a href=""https://translate.google.com/translate?hl=en&amp;sl=ja&amp;u=https://dev.classmethod.jp/machine-learning/sagemaker-tuning-stack/&amp;prev=search"" rel=""nofollow noreferrer"">this post translated from Japanese</a>.</p>

<p>When starting hyperparameter tuning jobs using the built-in algorithms in the Python SDK, <strong>you need to explicitly pass <code>include_cls_metadata=False</code></strong> as a keyword argument to <code>tuner.fit()</code> like this:</p>

<p><code>tuner.fit(inputs=data_channels, logs=True, include_cls_metadata=False)</code></p>
","30632",0
491,54449776,2,54393158,2019-01-30 21:25:25,1,"<p>With script mode the default serving method is the TensorFlow Serving-based one:
<a href=""https://github.com/aws/sagemaker-python-sdk/blob/master/src/sagemaker/tensorflow/estimator.py#L393"" rel=""nofollow noreferrer"">https://github.com/aws/sagemaker-python-sdk/blob/master/src/sagemaker/tensorflow/estimator.py#L393</a>
Custom script is not allowed with the TFS based container. You can use serving_input_receiver_fn to specify how the input data is processed as described here: <a href=""https://www.tensorflow.org/guide/saved_model"" rel=""nofollow noreferrer"">https://www.tensorflow.org/guide/saved_model</a></p>

<p>As for modifying the ngnix.conf, there are no supported ways of doing that. Depends on what you want to change in the config file you can hack the sagemaker-python-sdk to pass in different values for these environment variables: <a href=""https://github.com/aws/sagemaker-tensorflow-serving-container/blob/3fd736aac4b0d97df5edaea48d37c49a1688ad6e/container/sagemaker/serve.py#L29"" rel=""nofollow noreferrer"">https://github.com/aws/sagemaker-tensorflow-serving-container/blob/3fd736aac4b0d97df5edaea48d37c49a1688ad6e/container/sagemaker/serve.py#L29</a></p>

<p>Here is where you can override the environment variables: <a href=""https://github.com/aws/sagemaker-python-sdk/blob/master/src/sagemaker/tensorflow/serving.py#L130"" rel=""nofollow noreferrer"">https://github.com/aws/sagemaker-python-sdk/blob/master/src/sagemaker/tensorflow/serving.py#L130</a></p>
","10481182",0
492,54467283,2,54465049,2019-01-31 18:44:45,2,"<p>Would've added this as a comment but I don't have enough rep yet.</p>

<p>A few clarifying questions so that I can have some more context:</p>

<p><em>How exactly are you achieving 1TB of RAM?</em></p>

<ol>
<li><a href=""https://aws.amazon.com/ec2/instance-types/p2/"" rel=""nofollow noreferrer""><code>p2.xlarge</code></a> servers have 61GB of RAM, and <a href=""https://aws.amazon.com/ec2/instance-types/p3/"" rel=""nofollow noreferrer""><code>p3.2xlarge</code></a> servers have 61GB memory + 16GB onboard the Tesla V100 GPU. </li>
</ol>

<p><em>How are you storing, resizing, and ingesting the images into the SageMaker algorithm?</em></p>

<ol start=""2"">
<li>The memory error seems suspect considering it still occurs when downsizing images to 24x24. If you are resizing your original images (450 images at 2000x3000 resolution) as in-memory objects and aren't performing the transformations in-place (ie: not creating new images), you may have a substantial bit of memory pre-allocated, causing the SageMaker training algorithm to throw an OOM error.</li>
</ol>
","10885720",5
493,54524071,2,54521080,2019-02-04 20:46:42,5,"<p>The different services will all provide different levels of abstraction for Optical Character Recognition (OCR) depending on what parts of the pipeline you are most comfortable with working with, and what you prefer to have abstracted.</p>

<p>Here are a few options:</p>

<ul>
<li><p><strong>Rekognition</strong> will provide out of the box OCR with the <a href=""https://docs.aws.amazon.com/rekognition/latest/dg/text-detecting-text-procedure.html"" rel=""noreferrer"">DetectText</a> feature. However, it seems you will need to perform some sort of pre-processing on your images in your current case in order to get better results. This can be done through any method of your choice (Lambda, EC2, etc).</p></li>
<li><p><strong>SageMaker</strong> is a tool that will enable you to easily train and deploy your own models (of any type). You have two primary options with SageMaker:</p>

<ol>
<li><p><em>Do-it-yourself option</em>: If you're looking to go the route of labeling your own data, gathering a sizable training set, and training your own OCR model, this is possible by training and deploying your own model via SageMaker.</p></li>
<li><p><em>Existing OCR algorithm</em>: There are many algorithms out there that all have different potential tradeoffs for OCR. One example would be <a href=""https://github.com/tesseract-ocr/tesseract"" rel=""noreferrer"">Tesseract</a>. Using this, you can more closely couple your pre-processing step to the text detection.</p></li>
</ol></li>
<li><p><a href=""https://aws.amazon.com/textract/"" rel=""noreferrer""><strong>Amazon Textract</strong></a> (In preview) is a purpose-built dedicated OCR service that may offer better performance depending on what your images look like and the settings you choose. </p></li>
</ul>

<p>I would personally recommend looking into <a href=""https://docparser.com/blog/improve-ocr-accuracy/"" rel=""noreferrer"">pre-processing for OCR</a> to see if it improves Rekognition accuracy before moving onto the other options. Even if it doesn't improve Rekognition's accuracy, it will still be valuable for most of the other options!</p>
","10885720",1
494,54545259,2,53213596,2019-02-06 00:55:31,2,"<p>The version 0.24.3 of dvc correct this problem.</p>
","4435126",0
495,54557946,2,54557783,2019-02-06 16:13:03,1,"<p>I would recommend looking into TF-IDF approaches if the documents are of a technical nature. TF-IDFs look at the frequencies of terms (TF) in a document and multiply it with the inverse document frequency (IDF), a measure of the scarcity of the term in the overall corpus. The thinking there is: A word that you use often, but is very scarcely used in the overall corpus, is likely to make it an important term for the meaning of the document. A similarity measure (such as Cosine similarity) is then applied to the TFIDF to find documents with a similar profile in terms of TFIDF scores (i.e. a similar over-usage of the relatively unique terms)</p>

<p>If the texts are less technical in nature, you could take a look at Word Embedding approaches such as Document2Vec - basically they use trained sets with multi-dimensional vectors. These multi-dimensional vectors try to capture the meaning of a word, which means you are not dependent on the same keywords being used (which is the case with TFIDF).</p>

<p>Existing implementations are around (especially Python based), but Azure can probably facilitate these technologies as well (c.f. HDInsight <a href=""https://learn.microsoft.com/en-us/azure/architecture/data-guide/technology-choices/natural-language-processing"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/architecture/data-guide/technology-choices/natural-language-processing</a>). You can also look up ElasticSearch that does some of these things out of the box.</p>
","7967438",3
496,54593827,2,54557022,2019-02-08 13:49:45,0,"<p>I found an <a href=""https://github.com/kubeflow/pipelines/blob/cab950c06345b97a702a2891e61d3bb626c59710/samples/notebooks/Lightweight%20Python%20components%20-%20basics.ipynb"" rel=""nofollow noreferrer"">example notebook</a>.</p>

<p>At the bottom of the notebook, Kubeflow pipelines can be run on code using Kubeflow pipeline python sdk.</p>
","723405",0
497,54616314,2,54171261,2019-02-10 12:22:10,1,"<p>Thank you again for your help. All of which were valid in helping me get further. Having received a response on the AWS forum pages, I finally got it working.</p>

<p>I understood that my JSON was slightly different to the augmented manifest training guide. Having gone back to basics, I created another labelling job, but used the 'Bounding Box' type as opposed to the 'Custom - Bounding box template'. My output matched what was expected. This ran with no errors!</p>

<p>As my purpose was to have multiple labels, I was able to edit the files and mapping of my output manifests, which also worked!</p>

<p>i.e.</p>

<pre><code>{""source-ref"":""s3://xxxxx/Blackbird_15.JPG"",""ValidateBird"":{""annotations"":[{""class_id"":0,""width"":2023,""top"":665,""height"":1421,""left"":1312}],""image_size"":[{""width"":3872,""depth"":3,""height"":2592}]},""ValidateBird-metadata"":{""job-name"":""labeling-job/validatebird"",""class-map"":{""0"":""Blackbird""},""human-annotated"":""yes"",""objects"":[{""confidence"":0.09}],""creation-date"":""2019-02-09T14:23:51.174131"",""type"":""groundtruth/object-detection""}}
{""source-ref"":""s3://xxxx/Pigeon_19.JPG"",""ValidateBird"":{""annotations"":[{""class_id"":2,""width"":784,""top"":634,""height"":1657,""left"":1306}],""image_size"":[{""width"":3872,""depth"":3,""height"":2592}]},""ValidateBird-metadata"":{""job-name"":""labeling-job/validatebird"",""class-map"":{""2"":""Pigeon""},""human-annotated"":""yes"",""objects"":[{""confidence"":0.09}],""creation-date"":""2019-02-09T14:23:51.074809"",""type"":""groundtruth/object-detection""}} 
</code></pre>

<p>The original mapping was 0:'Bird' for all images through the labelling job.</p>
","10908571",7
498,54624707,2,54622191,2019-02-11 05:57:43,2,"<p>thanks to Matthew I looked into the permissions of the notebook itself, not just the user using Sagemaker.</p>

<p>The policies on the notebook look like this and I can download from the aws open data datasets!</p>

<p><a href=""https://i.stack.imgur.com/s0jwV.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/s0jwV.png"" alt=""notebook settings""></a></p>

<p><a href=""https://i.stack.imgur.com/OqEHi.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/OqEHi.png"" alt=""notebook permissions""></a></p>
","630752",0
499,54626658,2,54558832,2019-02-11 08:40:48,4,"<p>you definitely don't have to create an API in API Gateway. You can invoke the endpoint directly using the invoke_endpoint() API, passing the endpoint name, the content type, and the payload.</p>

<p>For example:</p>

<pre><code>import boto3

endpoint_name = &lt;INSERT_ENDPOINT_NAME&gt;
runtime = boto3.Session().client(service_name='sagemaker-runtime',region_name='us-east-1')

response = runtime.invoke_endpoint(EndpointName=endpoint_name, ContentType='application/x-image', Body=payload)
print(response['Body'].read())
</code></pre>

<p>More examples here using a Lambda function: <a href=""https://medium.com/@julsimon/using-chalice-to-serve-sagemaker-predictions-a2015c02b033"" rel=""nofollow noreferrer"">https://medium.com/@julsimon/using-chalice-to-serve-sagemaker-predictions-a2015c02b033</a></p>
","4686192",2
500,54638360,2,54485769,2019-02-11 20:10:25,2,"<p>Unfortunately there are no TF + Python 3 + EI serving images at this time. If you would like to use TF + EI, you'll need to make sure your code is compatible with Python 2.</p>

<p>Edit: after I originally wrote this, support for TF + Python 3 + EI has been released. At the time of this writing, I believe TF 1.12.0, 1.13.1, and 1.14.0 all have Python 3 + EI support. For the full list, see <a href=""https://github.com/aws/sagemaker-python-sdk#tensorflow-sagemaker-estimators"" rel=""nofollow noreferrer"">https://github.com/aws/sagemaker-python-sdk#tensorflow-sagemaker-estimators</a>.</p>
","9074534",0
501,54638400,2,54462105,2019-02-11 20:12:56,3,"<p>I am from Amazon SageMaker Ground Truth team and happy to assist you in your experiment. Just to be clear our understanding, are you running TF model in SageMaker using TF estimator in your own container (<a href=""https://github.com/aws/sagemaker-python-sdk/blob/master/src/sagemaker/tensorflow/README.rst"" rel=""nofollow noreferrer"">https://github.com/aws/sagemaker-python-sdk/blob/master/src/sagemaker/tensorflow/README.rst</a>)? </p>
","10768172",2
502,54661274,2,54629890,2019-02-13 01:44:56,4,"<p>If your Lambda function is scheduled, then you won't need an API Gateway. But if the predict action will be triggered by a user, by an application, for example, you will need.</p>

<p>When you call the invoke endpoint, actually you are calling a SageMaker endpoint, which is not the same as an API Gateway endpoint. </p>

<p>A common architecture with SageMaker is:</p>

<ol>
<li>API Gateway with receives a request then calls an authorizer, then
invoke your Lambda; </li>
<li>A Lambda with does some parsing in your input data, then calls your SageMaker prediction endpoint, then, handles the result and returns to your application.</li>
</ol>

<p>By the situation you describe, I can't say if your task is some academic stuff or a production one.</p>

<p>So, how you can save the data as a CSV file from your Lambda? </p>

<p>I believe you can just parse the output, then just upload the file to S3. Here you will parse manually or with a lib, with boto3 you can upload the file. The output of your model depends on your implementation on SageMaker image. So, if you need the response data in another format, maybe you will need to use a <a href=""https://github.com/awslabs/amazon-sagemaker-examples/tree/master/advanced_functionality/scikit_bring_your_own"" rel=""nofollow noreferrer"">custom image</a>. I normally use a custom image, which I can define how I want to handle my data on requests/responses.</p>

<p>In terms of a production task, I certainly recommend you check Batch transform jobs from SageMaker. You can provide an input file (the S3 path) and also a destination file (another S3 path). The SageMaker will run the batch predictions and will persist a file with the results. Also, you won't need to deploy your model to an endpoint, when this job run, will create an instance of your endpoint, download your data to predict, do the predictions, upload the output, and shut down the instance. You only need a trained model.</p>

<p>Here some info about Batch transform jobs:</p>

<p><a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/how-it-works-batch.html"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/sagemaker/latest/dg/how-it-works-batch.html</a></p>

<p><a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/ex1-batch-transform.html"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/sagemaker/latest/dg/ex1-batch-transform.html</a></p>

<p>I hope it helps, let me know if need more info.</p>

<p>Regards.</p>
","5741449",3
503,54713932,2,54671841,2019-02-15 17:03:41,3,"<p>You are not hitting your endpoint, but the endpoint of AWS SageMaker runtime. This endpoint is checking all the permissions to access your hosted model, and only if the credentials and requirements are met, the request is forwarded to your instances and models. </p>

<p>Therefore, you can't prevent this URL from being accessible from the Internet, but at the same time, you don't need to protect it or pay for it. AWS has a high level of security on these endpoints, and I don't think that you have a more secure way to protect these endpoints. </p>
","179529",4
504,54728825,2,54715601,2019-02-16 23:54:22,0,"<p>As Bruno has said you will have to use a container somewhere, but you can use an existing container to run your own custom tensorflow code.</p>

<p>There is a good example <a href=""https://github.com/awslabs/amazon-sagemaker-examples/blob/master/sagemaker-python-sdk/tensorflow_script_mode_quickstart/tensorflow_script_mode_quickstart.ipynb"" rel=""nofollow noreferrer"">in the sagemaker github</a> for how to do this.</p>

<p>The way this works is you modify your code to have an entry point which takes argparse command line arguments, and then you point a 'Sagemaker Tensorflow estimator' to the entry point. Then when you call fit on the sagemaker estimator it will download the tensorflow container and run your custom code in there.</p>

<p>So you start off with your own custom code that looks something like this</p>

<pre><code># my_custom_code.py
import tensorflow as tf
import numpy as np

def build_net():
    # single fully connected
    image_place = tf.placeholder(tf.float32, [None, 28*28])
    label_place = tf.placeholder(tf.int32, [None,])
    net = tf.layers.dense(image_place, units=1024, activation=tf.nn.relu)
    net = tf.layers.dense(net, units=10, activation=None)
    return image_place, label_place, net


def process_data():
    # load
    (x_train, y_train), (_, _) = tf.keras.datasets.mnist.load_data()

    # center
    x_train = x_train / 255.0
    m = x_train.mean()
    x_train = x_train - m

    # convert to right types
    x_train = x_train.astype(np.float32)
    y_train = y_train.astype(np.int32)

    # reshape so flat
    x_train = np.reshape(x_train, [-1, 28*28])
    return x_train, y_train


def train_model(init_learn, epochs):
    image_p, label_p, logit = build_net()
    x_train, y_train = process_data()

    loss = tf.nn.softmax_cross_entropy_with_logits_v2(
        logits=logit,
        labels=label_p)
    optimiser = tf.train.AdamOptimizer(init_learn)
    train_step = optimiser.minimize(loss)

    with tf.Session() as sess:
        sess.run(tf.global_variables_initializer())
        for _ in range(epochs):
            sess.run(train_step, feed_dict={image_p: x_train, label_p: y_train})


if __name__ == '__main__':
    train_model(0.001, 10)
</code></pre>

<p>To make it work with sagemaker we need to create a command line entry point, which will allow sagemaker to run it in the container it will download for us eventually.</p>

<pre><code># entry.py

import argparse
from my_custom_code import train_model

if __name__ == '__main__':
    parser = argparse.ArgumentParser(
        formatter_class=argparse.ArgumentDefaultsHelpFormatter)
    parser.add_argument(
        '--model_dir',
        type=str)
    parser.add_argument(
        '--init_learn',
        type=float)
    parser.add_argument(
        '--epochs',
        type=int)
    args = parser.parse_args()
    train_model(args.init_learn, args.epochs)
</code></pre>

<p>Apart from specifying the arguments my function needs to take, we also need to provide a <code>model_dir</code> argument. This is always required, and is an S3 location which is where an model artifacts will be saved when the training job completes. Note that you don't need to specify what this value is (though you can) as Sagemaker will provide a default location in S3 for you.</p>

<p>So we have modified our code, now we need to actually run it on Sagemaker. Go to the AWS console and fire up a small instance from Sagemaker. Download your custom code to the instance, and then create a jupyter notebook as follows:</p>

<pre><code># sagemaker_run.ipyb
import sagemaker
from sagemaker.tensorflow import TensorFlow

hyperparameters = {
    'epochs': 10,
    'init_learn': 0.001}

role = sagemaker.get_execution_role()
source_dir = '/path/to/folder/with/my/code/on/instance'
estimator = TensorFlow(
    entry_point='entry.py',
    source_dir=source_dir,
    train_instance_type='ml.t2.medium',
    train_instance_count=1,
    hyperparameters=hyperparameters,
    role=role,
    py_version='py3',
    framework_version='1.12.0',
    script_mode=True)

estimator.fit()
</code></pre>

<p>Running the above will:</p>

<ul>
<li>Spin up an ml.t2.medium instance</li>
<li>Download the tensorflow 1.12.0 container to the instance</li>
<li>Download any data we specify in fit to the newly created instance in fit (in this case nothing)</li>
<li>Run our code on the instance</li>
<li>upload the model artifacts to model_dir</li>
</ul>

<p>And that is pretty much it. There is of course a lot not mentioned here but you can:</p>

<ul>
<li>Download training/testing data from s3</li>
<li>Save checkpoint files, and tensorboard files during training and upload them to s3</li>
</ul>

<p>The best resource I found was the example I shared but here are all the things I was looking at to get this working:</p>

<ul>
<li><a href=""https://github.com/awslabs/amazon-sagemaker-examples/blob/master/sagemaker-python-sdk/tensorflow_script_mode_quickstart/tensorflow_script_mode_quickstart.ipynb"" rel=""nofollow noreferrer"">example code again</a></li>
<li><a href=""https://github.com/aws/sagemaker-python-sdk/blob/master/src/sagemaker/tensorflow/README.rst"" rel=""nofollow noreferrer"">documentation</a></li>
<li><a href=""https://github.com/aws/sagemaker-containers#list-of-provided-environment-variables-by-sagemaker-containers"" rel=""nofollow noreferrer"">explanation of environment variables</a></li>
</ul>
","1803124",0
505,54742893,2,54706312,2019-02-18 08:12:16,0,"<p>According to the offical tutorial <a href=""https://learn.microsoft.com/en-us/azure/machine-learning/studio-module-reference/export-to-azure-blob-storage"" rel=""nofollow noreferrer""><code>Export to Azure Blob Storage</code></a>, there are two authentication types for exporting data to Azure Blob Storage: SAS and Account. The description for them as below.</p>

<blockquote>
  <ol start=""4"">
  <li><p>For <strong>Authentication type</strong>, choose <strong>Public (SAS URL)</strong> if you know that the storage supports access via a SAS URL.</p>
  
  <p>A SAS URL is a special type of URL that can be generated by using an Azure storage utility, and is available for only a limited time. It contains all the information that is needed for authentication and download.</p>
  
  <p>For <strong>URI</strong>, type or paste the full URI that defines the account and the public blob.</p></li>
  <li><p>For private accounts, choose <strong>Account</strong>, and provide the account name and the account key, so that the experiment can write to the storage account.</p>
  
  <ul>
  <li><p><strong>Account name</strong>: Type or paste the name of the account where you want to save the data. For example, if the full URL of the storage account is <a href=""http://myshared.blob.core.windows.net"" rel=""nofollow noreferrer"">http://myshared.blob.core.windows.net</a>, you would type myshared.</p></li>
  <li><p><strong>Account key</strong>: Paste the storage access key that is associated with the account.</p></li>
  </ul></li>
  </ol>
</blockquote>

<p>I try to use a simple module combination as the figure and Python code below to test the issue you got.</p>

<p><a href=""https://i.stack.imgur.com/9XXPV.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/9XXPV.png"" alt=""enter image description here""></a></p>

<pre><code>import pandas as pd

def azureml_main(dataframe1 = None, dataframe2 = None):
    dataframe1 = pd.DataFrame(data={'col1': [1, 2], 'col2': [3, 4]})
    return dataframe1,
</code></pre>

<p>When I tried to use the authentication type <code>Account</code> of my Blob Storage V2 account, I got the same issue as yours which the error code is <a href=""https://learn.microsoft.com/en-us/azure/machine-learning/studio-module-reference/errors/error-0151"" rel=""nofollow noreferrer"">Error 0151</a> as below via click the <code>View error log</code> Button under the link of <code>View output log</code>.</p>

<p><a href=""https://i.stack.imgur.com/TFQgO.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/TFQgO.png"" alt=""enter image description here""></a></p>

<blockquote>
  <p><strong>Error 0151</strong></p>
  
  <p>There was an error writing to cloud storage. Please check the URL.</p>
  
  <p>This error in Azure Machine Learning occurs when the module tries to write data to cloud storage but the URL is unavailable or invalid.</p>
  
  <p><strong>Resolution</strong>
  Check the URL and verify that it is writable.</p>
  
  <p><strong>Exception Messages</strong></p>
  
  <ul>
  <li>Error writing to cloud storage (possibly a bad url).</li>
  <li>Error writing to cloud storage: {0}. Please check the url.</li>
  </ul>
</blockquote>

<p>Based on the error description above, the error should be caused by the blob url with SAS incorrectly generated by the <code>Export Data</code> module code with account information. May I think the code is old and not compatible with the new V2 storage API or API version information. You can report it to <code>feedback.azure.com</code>.</p>

<p>However, I switched to use <code>SAS</code> authentication type to type a blob url with a SAS query string of my container which I generated via <a href=""https://azure.microsoft.com/en-us/features/storage-explorer/"" rel=""nofollow noreferrer"">Azure Storage Explorer</a> tool as below, it works fine.</p>

<p>Fig 1: Right click on the container of your Blob Storage account, and click the <code>Get Shared Access Signature</code></p>

<p><a href=""https://i.stack.imgur.com/1fyFL.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/1fyFL.png"" alt=""enter image description here""></a></p>

<p>Fig 2: Enable the permission <code>Write</code> (recommended to use UTC timezone) and click <code>Create</code> button</p>

<p><a href=""https://i.stack.imgur.com/IsbQQ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/IsbQQ.png"" alt=""enter image description here""></a></p>

<p>Fig 3: Copy the <code>Query string</code> value, and build a blob url with a container SAS query string like <code>https://&lt;account name&gt;.blob.core.windows.net/&lt;container name&gt;/&lt;blob name&gt;&lt;query string&gt;</code></p>

<p><a href=""https://i.stack.imgur.com/RGnxn.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/RGnxn.png"" alt=""enter image description here""></a></p>

<p><strong><em>Note: The blob must be not exist in the container, otherwise an <a href=""https://learn.microsoft.com/en-us/azure/machine-learning/studio-module-reference/errors/error-0057"" rel=""nofollow noreferrer"">Error 0057</a> will be caused.</em></strong></p>
","4989676",1
506,54787238,2,54757598,2019-02-20 13:15:17,1,"<p>Good question :).</p>

<p>Looking at the current version of the API, it doesn't look like you can add the description using <code>Run.register_model</code>, as confirmed <a href=""https://learn.microsoft.com/en-us/python/api/azureml-core/azureml.core.run.run?view=azure-ml-py#register-model-model-name--model-path-none--tags-none--properties-none----kwargs-"" rel=""nofollow noreferrer"">by the docs</a>. </p>

<p>You can go around this however by registering the model using the <code>Model.register</code> method which, fortunately, includes an argument for <code>description</code> as detailed <a href=""https://learn.microsoft.com/en-us/python/api/azureml-core/azureml.core.model.model?view=azure-ml-py#register-workspace--model-path--model-name--tags-none--properties-none--description-none-"" rel=""nofollow noreferrer"">here</a>. In your case, you also need to <a href=""https://learn.microsoft.com/en-us/python/api/azureml-core/azureml.core.run(class)?view=azure-ml-py#download-file-name--output-file-path-none-"" rel=""nofollow noreferrer"">download the files</a> first.</p>

<p>In short, use something like:</p>

<pre class=""lang-python prettyprint-override""><code>best_run.download_file('outputs/rf.pkl', output_file_path='./rf.pkl')

Model.register(workspace=ws, model_path='./rf.pkl', model_name=""pumps_rf"", description=""There are many models like it, but this one is mine."")
</code></pre>
","155697",3
507,54798034,2,54797698,2019-02-21 01:54:42,2,"<p>It looks like AWS is currently in the process of supporting model deletion via API with <a href=""https://github.com/aws/sagemaker-python-sdk/pull/647"" rel=""nofollow noreferrer"" title=""sagemaker-python-sdk/pull/647"">this</a> pull request. </p>

<p>For the time being Amazon's only <a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/ex1-cleanup.html"" rel=""nofollow noreferrer"" title=""docs.aws.amazon.com/sagemaker"">recommendation</a> is to delete everything via the console. </p>

<p>If this is critical to your system you can probably manage everything via Cloud Formation and create/delete services containing your Sagemaker models and endpoints.</p>
","11093207",0
508,54817936,2,54653359,2019-02-21 23:40:34,1,"<p>You can also consider using <a href=""https://en.wikipedia.org/wiki/Nohup"" rel=""nofollow noreferrer"">nohup</a> in Lifecycle config to execute your script in background, so you don't get blocked by 5-min limit. </p>

<p>Let us know if there's anything else you need assistance of.</p>

<p>Thanks,</p>

<p>Han</p>
","4842651",0
509,54826654,2,54825390,2019-02-22 12:00:36,0,"<p>Yes you can Using <code>S3</code> event notification for object-created and call a <code>lambda</code> for creating endpoint for <code>sagemaker</code>.</p>

<p>This example shows how to make <code>object-created event trigger lambda</code></p>

<p><a href=""https://docs.aws.amazon.com/lambda/latest/dg/with-s3.html"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/lambda/latest/dg/with-s3.html</a></p>

<p>You can use <code>python sdk</code> to create endpoint for <code>sagemaker</code></p>

<p><a href=""https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/sagemaker.html#SageMaker.Client.create_endpoint"" rel=""nofollow noreferrer"">https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/sagemaker.html#SageMaker.Client.create_endpoint</a></p>

<p>But it might be slow for creating endpoint so you may be need to wait.</p>
","5894758",1
510,54857320,2,54556538,2019-02-24 22:48:38,0,"<p>I haven't tried scheduling pipelines yet, but hopefully this may help.</p>

<p>From the Kubeflow Pipelines UI, create an experiment for your pipeline. On the experiment page for your pipeline, there is an option to <strong>Create recurring run</strong>. Follow the instructions on that form to schedule runs for your pipeline.</p>
","74510",3
511,54890950,2,54638364,2019-02-26 17:22:56,4,"<p>You can import <code>sagemaker_tensorflow</code> from the training script as follows:</p>

<pre><code>from sagemaker_tensorflow import PipeModeDataset
from tensorflow.contrib.data import map_and_batch

channel = 'my-pipe-channel-name'

ds = PipeModeDataset(channel)
ds = ds.repeat(EPOCHS)
ds = ds.prefetch(PREFETCH_SIZE)
ds = ds.apply(map_and_batch(parse, batch_size=BATCH_SIZE,
                            num_parallel_batches=NUM_PARALLEL_BATCHES))
</code></pre>

<p>You can find the full example here: <a href=""https://github.com/awslabs/amazon-sagemaker-examples/blob/master/sagemaker-python-sdk/tensorflow_pipemode_example/pipemode.py"" rel=""nofollow noreferrer"">https://github.com/awslabs/amazon-sagemaker-examples/blob/master/sagemaker-python-sdk/tensorflow_pipemode_example/pipemode.py</a></p>

<p>You can find documentation about sagemaker_tensorflow here <a href=""https://github.com/aws/sagemaker-tensorflow-extensions#using-the-pipemodedataset"" rel=""nofollow noreferrer"">https://github.com/aws/sagemaker-tensorflow-extensions#using-the-pipemodedataset</a></p>
","3361753",4
512,54899702,2,54759647,2019-02-27 06:55:38,0,"<p>It was a mistake from my end. I was using a different input image array format for the model. I was sending an image tensor instead of encoded image string tensor.</p>
","user9460641",0
513,54931417,2,54931270,2019-02-28 17:45:26,45,"<p>You can do that by opening a terminal on sagemaker. Navigate to the path where your folder is. Run the command to zip it</p>

<pre><code>zip -r -X archive_name.zip folder_to_compress
</code></pre>

<p>You will find the zipped folder. You can then select it and download it.</p>
","5429010",1
514,54952503,2,54924835,2019-03-01 21:23:45,1,"<p><a href=""https://github.com/awslabs/amazon-sagemaker-examples/blob/master/scientific_details_of_algorithms/lda_topic_modeling/LDA-Science.ipynb"" rel=""nofollow noreferrer"">This SageMaker notebook</a>, which dives into the scientific details of LDA, also demonstrates how to inspect the model artifacts. Specifically, how to obtain the estimates for the Dirichlet prior <code>alpha</code> and the topic-word distribution matrix <code>beta</code>. You can find the instructions in the section titled <em>""Inspecting the Trained Model""</em>. For convenience, I will reproduce the relevant code here:</p>

<pre class=""lang-py prettyprint-override""><code>import tarfile
import mxnet as mx

# extract the tarball
tarflie_fname = FILENAME_PREFIX + 'model.tar.gz' # wherever the tarball is located
with tarfile.open(tarfile_fname) as tar:
    tar.extractall()

# obtain the model file (should be the only file starting with ""model_"")
model_list = [
    fname
    for fname in os.listdir(FILENAME_PREFIX)
    if fname.startswith('model_')
]
model_fname = model_list[0]

# load the contents of the model file into MXNet arrays
alpha, beta = mx.ndarray.load(model_fname)
</code></pre>

<p>That should get you the model data. Note that the topics, which are stored as rows of <code>beta</code>, are not presented in any particular order.</p>
","645494",11
515,54974421,2,54760032,2019-03-03 22:30:07,2,"<p>I haven't used Cloud Source Repositories (so I haven't tested this), but it seems like you would need to do one of two things:</p>
<ol>
<li><em>pip install</em> from the repo's URL, if it has a public URL. For example, something like the following:</li>
</ol>
<pre class=""lang-bash prettyprint-override""><code>!pip install https://source.developers.google.com/p/[PROJECT_ID]/r/[REPO_NAME]
</code></pre>
<ol start=""2"">
<li><em>pip install</em> from a local clone of your repository. For example:</li>
</ol>
<pre class=""lang-bash prettyprint-override""><code>gcloud source repos clone [REPOSITORY_NAME] --project=[PROJECT_NAME] 
!pip install -e [PATH_TO_CLONED_REPO]
</code></pre>
","74510",0
516,55022751,2,55020361,2019-03-06 12:05:50,4,"<p>You only need to set the <code>GOOGLE_APPLICATION_CREDENTIALS</code> env variable when <a href=""https://cloud.google.com/docs/authentication/production#providing_credentials_to_your_application"" rel=""nofollow noreferrer"">using the client libraries</a>.</p>

<p>As you are using the <code>gcloud</code> CLI change this line:</p>

<p><code>RUN export GOOGLE_APPLICATION_CREDENTIALS=/workdir/ml6-sandbox-cdc8cb4bcae2.json</code></p>

<p>to</p>

<pre><code>gcloud auth activate-service-account yourServiceAccount --key-file=/workdir/ml6-sandbox-cdc8cb4bcae2.json
</code></pre>

<p>This will log your service account as the the active account used by <code>gcloud</code>.</p>

<p>Also, this service account needs to be granted with the proper <a href=""https://cloud.google.com/ml-engine/docs/tensorflow/access-control"" rel=""nofollow noreferrer"">roles</a>. </p>
","7757976",0
517,55044766,2,55020390,2019-03-07 13:17:35,0,"<p>With some help of the AWS support service we were able to find the problem.
The docker image I used to run my code on was, as I said tensorflow/tensorflow:latest-gpu-py3 (available on <a href=""https://github.com/aws/sagemaker-tensorflow-container"" rel=""nofollow noreferrer"">https://github.com/aws/sagemaker-tensorflow-container</a>)</p>

<p>the ""latest"" tag refers to version 1.12.0 at this time. The problem was not my own, but with this version of the docker image. </p>

<p>If I base my docker image on tensorflow/tensorflow:1.10.1-gpu-py3, it runs as it should and uses the GPU fully. </p>

<p>Apparently the default runtime is set to ""nvidia"" in the docker/deamon.json on all GPU instances of AWS sagemaker.</p>
","2443088",0
518,55047303,2,54799512,2019-03-07 15:24:59,0,"<p>Step 1 : Create 2 web services with Azure ML Studio ( One for the training model and one for the predictive model)</p>

<p>Step 2: Create endpoint through the web service with the link Manage Endpoint on Azure ML Studio for each web service</p>

<p>Step 3: Create 2 new connections on Azure Data Factory / Find Azure ML (on compute tab) and copy the Endpoint key and API Key that you will find under the Consume tab in the endpoint configuration (the one that you created on step 2) Endpoint Key = Batch Requests Key and API Key = Primary Key </p>

<p>Set Disable Update Resource for the training model endpoint
Set Enable Update Resource for the predictive model endpoint ( Update Resource End Point = Patch key )</p>

<p>Step 4 : Create a pipeline with 2 activities ( ML Batch Execution and ML Update Resource)
Set the AML Linked service for the ML batch Execution with the connection that has  disable Update Resource</p>

<p>Set the AML Linked service for the ML Update Resource with the connection that has  Enable Update Resource</p>

<p>Step 5 : Set the Web Service Inputs and Outputs</p>
","9501913",0
519,55052209,2,54796762,2019-03-07 20:22:05,2,"<p>In my initial code, when creating the image, I was not using:</p>

<pre><code>image.wait_for_creation(show_output=True)
</code></pre>

<p>As a consequence, I was calling <code>CreateImage</code> and <code>DeployImage</code> before the image was created which errored out. Can't believe it was that simple.. </p>

<p>UPDATED IMAGE CREATION SNIPPET:</p>

<pre><code># Register the image from the image configuration
# to Azure Container Registry
image = ContainerImage.create(name = Config.IMAGE_NAME, 
                              models = [rf, le, ohc],
                              image_config = image_config,
                              workspace = ws)

image.wait_for_creation(show_output=True)
</code></pre>
","3962124",0
520,55105707,2,55070396,2019-03-11 15:54:55,1,"<p>Please look at this notebook for an example of distributed training with TF: <a href=""https://github.com/awslabs/amazon-sagemaker-examples/blob/master/sagemaker-python-sdk/tensorflow_distributed_mnist/tensorflow_distributed_mnist.ipynb"" rel=""nofollow noreferrer"">https://github.com/awslabs/amazon-sagemaker-examples/blob/master/sagemaker-python-sdk/tensorflow_distributed_mnist/tensorflow_distributed_mnist.ipynb</a></p>

<p>""Each instance will predict a batch of the dataset, calculate loss and minimize the optimizer. One entire loop of this process is called training step.</p>

<p>A global step is a global variable shared between the instances. It's necessary for distributed training, so the optimizer will keep track of the number of training steps between runs:</p>

<p>train_op = optimizer.minimize(loss, tf.train.get_or_create_global_step())
That is the only required change for distributed training!""</p>
","4686192",0
521,55111178,2,54992434,2019-03-11 22:11:47,6,"<p>I am from the engineering team and happy to help you here. I think the issue is not related to the manifest as it looks correct to me. The error suggests that you may haven't provided a correct lambda ARN for pre or post labeling task. Please see this doc for more details: <a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/sms-custom-templates-step3.html"" rel=""noreferrer"">https://docs.aws.amazon.com/sagemaker/latest/dg/sms-custom-templates-step3.html</a></p>

<p>I can also help further if you can send me details on how you starting the job and what parameters you are sending.  </p>
","10768172",1
522,55114480,2,55112494,2019-03-12 05:02:06,3,"<p>I was finally able to learn that Conda has a package which can install it for you. I was able to get it installed by running the command:</p>

<pre><code>!conda install python-graphviz --yes
</code></pre>

<p>Note the <code>--yes</code> is only needed if the installation needs to verify adding/changing other packages since the Jupyter notebook is not interactive once it is running.</p>
","1947251",1
523,55154085,2,55067802,2019-03-14 02:31:54,0,"<p>Firstly, Python shell jobs would not launch a Hadooo Cluster in the backend as it does not give you a Spark environment for your jobs.
Secondly, since PyGreSQL is not written in Pure Python, it will not work with Glue's native environment (Glue Spark Job, Dev endpoint etc)
Thirdly, Python Shell has additional support for certain package built-in.</p>

<p>Thus, I don't see a point of using DevEndpoint for Python Shell jobs.</p>
","4592183",0
524,55158002,2,55151159,2019-03-14 08:32:47,1,"<p>I wouldn't try to reverse the normalization, but instead join the normalized version of the dataset with the original version after you apply <code>Score Model</code>.</p>

<p>The <code>Join Data</code> module should work if you have a key column, otherwise just use <code>Add Columns</code>. </p>
","155697",3
525,55270524,2,55257580,2019-03-20 21:32:54,1,"<p>As you've noticed the 'embedded' lambda version of the aws-sdk lags behind. It's actually on <code>2.290.0</code> (you can see the full details on the environment here: <a href=""https://docs.aws.amazon.com/lambda/latest/dg/current-supported-versions.html"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/lambda/latest/dg/current-supported-versions.html</a>)</p>

<p>You can see here: <a href=""https://github.com/aws/aws-sdk-js/blame/master/clients/sagemaker.d.ts"" rel=""nofollow noreferrer"">https://github.com/aws/aws-sdk-js/blame/master/clients/sagemaker.d.ts</a> that it is not until <code>2.366.0</code> that the params for this method included <code>Containers</code> and did not require <code>PrimaryContainer</code>.</p>

<p>As you've noted, the <em>workaround</em> is to deploy your lambda with the <code>aws-sdk</code> version that you're using. This is sometimes noted as a best practice, as it pins the <code>aws-sdk</code> on the functionality you've built and tested against.</p>
","2531132",2
526,55270602,2,55132599,2019-03-20 21:40:34,16,"<p>SageMaker is a great tool for deployment, it simplifies a lot of processes configuring containers, you only need to write 2-3 lines to deploy the model as an endpoint and use it.  SageMaker also provides the dev platform (Jupyter Notebook) which supports Python and Scala (sparkmagic kernal) developing, and i managed installing external scala kernel in jupyter notebook. Overall, SageMaker provides end-to-end ML services. Databricks has unbeatable Notebook environment for Spark development. </p>

<p>Conclusion</p>

<ol>
<li><p>Databricks is a better platform for Big data(scala, pyspark) Developing.(unbeatable notebook environment)</p></li>
<li><p>SageMaker is better for Deployment. and if you are not working on big data, SageMaker is a perfect choice working with (Jupyter notebook + Sklearn + Mature containers + Super easy deployment). </p></li>
<li><p>SageMaker provides ""real time inference"", very easy to build and deploy, very impressive. you can check the official SageMaker Github.
<a href=""https://github.com/awslabs/amazon-sagemaker-examples/tree/master/sagemaker-python-sdk/scikit_learn_inference_pipeline"" rel=""noreferrer"">https://github.com/awslabs/amazon-sagemaker-examples/tree/master/sagemaker-python-sdk/scikit_learn_inference_pipeline</a></p></li>
</ol>
","9226721",4
527,55281703,2,55277334,2019-03-21 13:37:23,0,"<p>Model registration can be done with <a href=""https://learn.microsoft.com/en-us/python/api/azureml-core/azureml.core.model.model?view=azure-ml-py#register-workspace--model-path--model-name--tags-none--properties-none--description-none-"" rel=""nofollow noreferrer"">Model.register</a>, without the need of using a <code>run</code> object</p>

<pre><code>model = Model.register(model_name='my_model', model_path='my_model.pkl', workspace = ws)
</code></pre>

<p>for the deployment one can follow steps as outlined in the <a href=""https://learn.microsoft.com/en-us/azure/machine-learning/service/tutorial-deploy-models-with-aml#deploy-as-a-web-service"" rel=""nofollow noreferrer"">Azure ML service doc</a>.</p>
","4240413",0
528,55288200,2,55285043,2019-03-21 19:44:15,3,"<p>I think that your <code>init</code> function is failing. I would first try to isolate the image creation from the image deployment, and just test the image first:</p>

<ul>
<li>Create the image first, it's very much ok if do it through the interface</li>
<li>Pull the image locally with Docker (for this you'll need <a href=""https://www.docker.com"" rel=""nofollow noreferrer"">Docker</a> and the <a href=""https://learn.microsoft.com/en-us/cli/azure/install-azure-cli?view=azure-cli-latest"" rel=""nofollow noreferrer"">Azure CLI</a> installed):</li>
</ul>

<pre class=""lang-python prettyprint-override""><code>az acr login -n &lt;container-registry&gt;
docker run -p 8000:5001  &lt;container-registry&gt;.azurecr.io/&lt;image-name&gt;:&lt;image-version&gt;
# basically, the entire image location, see pic below
</code></pre>

<ul>
<li>test the image locally, it listens on the 8000 port:</li>
</ul>

<pre><code>POST http://localhost:8000/score
Content-Type: application/json
</code></pre>

<ul>
<li>if this works deploy it on ACI </li>
</ul>

<p><code>&lt;container-registry&gt;</code> is the name of the <code>Container Registry</code> associated with the ML Workspace, you can also extract it from the image location, taking care to remove everything after the first dot:</p>

<p><a href=""https://i.stack.imgur.com/W6YYj.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/W6YYj.png"" alt=""image location""></a></p>
","155697",3
529,55305635,2,55147861,2019-03-22 18:16:22,4,"<p>The error message indicated XGBoost was expecting the input data set as libsvm format instead of csv. SageMaker XGBoost by default assumed the input data set was in libsvm format. For using input data set in csv, please explicitly specify <code>content-type</code> as <code>text/csv</code>.</p>

<p>For more information: <a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/xgboost.html#InputOutput-XGBoost"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/sagemaker/latest/dg/xgboost.html#InputOutput-XGBoost</a></p>
","4876041",0
530,55385182,2,55353889,2019-03-27 19:34:49,1,"<p>Have you registered the model <code>'pofc_fc_model'</code> in your workspace using the <code>register()</code> function on the model object? If not, there will be no model path and that can cause failure.</p>

<p>See this section on model registration: <a href=""https://learn.microsoft.com/en-us/azure/machine-learning/service/how-to-deploy-and-where#registermodel"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/machine-learning/service/how-to-deploy-and-where#registermodel</a></p>
","6610036",1
531,55401024,2,55007190,2019-03-28 15:11:31,1,"<p>There's no way to change a node-pool from non-preemptible to preemptible.</p>

<p>What you can do is create a new preemptible <a href=""https://cloud.google.com/kubernetes-engine/docs/how-to/node-pools"" rel=""nofollow noreferrer"">node-pool</a> and <a href=""https://cloud.google.com/kubernetes-engine/docs/tutorials/migrating-node-pool"" rel=""nofollow noreferrer"">migrate</a> the workloads from the non-preemptible node-pool to the new one.</p>

<p>By the other hand, you can go ahead and recreate the GKE cluster with a preemprible node-pool.</p>

<p>Hope this helps!</p>
","10754685",0
532,55408672,2,55347910,2019-03-29 00:06:58,3,"<p>If ACI deployment fails, one solution is trying to allocate <em>less</em> resources, e.g.</p>

<pre><code>aciconfig = AciWebservice.deploy_configuration(cpu_cores=1, 
                  memory_gb=8, 
                  tags={""data"": ""text"",  ""method"" : ""NB""}, 
                  description='Predict something')
</code></pre>

<p>While the error messages thrown are not particularly informative, this is actually clearly stated in the <a href=""https://learn.microsoft.com/en-us/azure/container-instances/container-instances-region-availability"" rel=""nofollow noreferrer"">documentation</a>:</p>

<blockquote>
  <p>When a region is under heavy load, you may experience a failure when
  deploying instances. To mitigate such a deployment failure, try
  deploying instances with lower resource settings [...]</p>
</blockquote>

<p>The documentation also states which are the maximum values of the CPU/RAM resources available in the different regions (at the time of writing, requiring a deployment with <code>memory_gb=32</code> would likely fail in all regions because of insufficient resources).</p>

<p>Upon requiring less resources, deployment should succeed with </p>

<blockquote>
  <p>Creating service<br>
  Running......................................................<br>
  SucceededACI service creation operation finished, operation<br>
  ""Succeeded"" Healthy</p>
</blockquote>
","4240413",0
533,55422743,2,55158307,2019-03-29 17:29:58,2,"<p>For csv input, the label should be in the first column, as mentioned <a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/cdf-training.html"" rel=""nofollow noreferrer"">here</a>:  So you should preprocess your data to put the label (the column you want to predict) on the left.</p>

<p>Next, you need to decide whether this is a regression problem or a classification problem. </p>

<p>If you want to predict a number that's as close as possible to the true number, that's regression. For example, the truth might be 4, and the model might predict 4.15. If you need an integer prediction, you could round the model's output.</p>

<p>If you want the prediction to be one of a few categories, then you have a classification problem. For example, we might encode 'North America' = 0, 'Europe' = 1, 'Africa' = 2, and so on. In this case, a fractional prediction wouldn't make sense. </p>

<p>For regression, use <code>'predictor_type' = 'regressor'</code> and for classification with more than 2 classes, use <code>'predictor_type' = 'multiclass_classifier'</code> as documented <a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/ll_hyperparameters.html"" rel=""nofollow noreferrer"">here</a>.</p>

<p>The output of regression will contain only a <code>'score'</code> field, which is the model's prediction. The output of multiclass classification will contain a <code>'predicted_label'</code> field, which is the model's prediction, as well as a <code>'score'</code> field, which is a vector of probabilities representing the model's confidence. The index with the highest probability will be the one that's predicted as the <code>'predicted_label'</code>. The output formats are documented <a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/LL-in-formats.html"" rel=""nofollow noreferrer"">here</a>.</p>
","2564688",1
534,55461578,2,55459903,2019-04-01 18:38:23,1,"<p>Enabling multiple users to leverage the same notebook (in this case, without authentication) will involve managing your Security Groups to enable open access. You can filter, allowing access for a known IP address range, if your students are accessing it from a classroom or campus, for example.</p>

<p>Tips for this are available in <a href=""https://stackoverflow.com/questions/42617692/is-it-possible-to-grant-multiple-users-to-jupyter-notebook"">this answer</a> and this page from the documentation, diving into <a href=""https://aws.amazon.com/blogs/machine-learning/understanding-amazon-sagemaker-notebook-instance-networking-configurations-and-advanced-routing-options/"" rel=""nofollow noreferrer"">network configurations for SageMaker hosted notebook instances</a>.</p>

<p>As for enabling students to spin up their own notebooks, I'm not sure if it's possible to enable completely unauthenticated AWS-level resource provisioning -- however once you've spun up a single managed notebook instance yourself, students can create their own notebooks directly from the browser in Jupyter, once they've navigated to the publicly available IP. You may need to attach a new SageMaker IAM role that enables notebook creation (amongst other things, depending on the workload requirements). Depending on the computational needs (number, duration, and types of concurrent workloads), there will be different optimal setups of number of managed instances and instance type to prevent computational bottlenecking.</p>
","10885720",0
535,55529444,2,55525445,2019-04-05 06:29:53,4,"<p>You can place the training files outside <code>source_dir</code> so that they don't get uploaded as part of submitting the experiment, and then upload them separately to the data store (which is basically using the Azure storage associated with your workspace). All you need to do then is reference the training files from <code>train.py</code>. </p>

<p>See the <a href=""https://learn.microsoft.com/en-us/azure/machine-learning/service/tutorial-train-models-with-aml"" rel=""nofollow noreferrer"">Train model tutorial</a> for an example of how to upload data to the data store and then access it from the training file.</p>
","155697",1
536,55543162,2,55401214,2019-04-05 20:48:29,0,"<p>In order to get evaluation metrics you need to provide an extra channel called &quot;test&quot; during training. The test channel must contained labeled data. It is explained in the official documentation, <a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/randomcutforest.html"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/sagemaker/latest/dg/randomcutforest.html</a> :</p>
<blockquote>
<p>Amazon SageMaker Random Cut Forest supports the train and test data channels. The optional test channel is used to compute accuracy, precision, recall, and F1-score metrics on labeled data. Train and test data content types can be either application/x-recordio-protobuf or text/csv formats. For the test data, when using text/csv format, the content must be specified as text/csv;label_size=1 where the first column of each row represents the anomaly label: &quot;1&quot; for an anomalous data point and &quot;0&quot; for a normal data point. You can use either File mode or Pipe mode to train RCF models on data that is formatted as recordIO-wrapped-protobuf or as CSV</p>
<p>Also note ... the test channel only supports S3DataDistributionType=FullyReplicated</p>
</blockquote>
<p>Thanks,</p>
<p>Julio</p>
","4110545",0
537,55545133,2,55531608,2019-04-06 01:39:34,7,"<p>As you mentioned GCP currently authenticates using service account, credentials JSON and API tokens. Instead of storing credentials in S3 bucket you can consider using AWS Secrets Manager or AWS Systems Manager Parameter Store to store the GCP credentials and then fetch them in Jupyter notebook. This way credentials can be secured and the credentials file will be created from Secrets Manager only when needed. </p>

<p>This is sample code I used previously to connect to BigQuery from SageMaker instance.</p>

<pre class=""lang-py prettyprint-override""><code>import os
import json
import boto3
from google.cloud.bigquery import magics
from google.oauth2 import service_account

def get_gcp_credentials_from_ssm(param_name):
    # read credentials from SSM parameter store
    ssm = boto3.client('ssm')
    # Get the requested parameter
    response = ssm.get_parameters(Names=[param_name], WithDecryption=True)
    # Store the credentials in a variable
    gcp_credentials = response['Parameters'][0]['Value']
    # save credentials temporarily to a file
    credentials_file = '/tmp/.gcp/service_credentials.json'
    with open(credentials_file, 'w') as outfile:  
        json.dump(json.loads(gcp_credentials), outfile)
    # create google.auth.credentials.Credentials to use for queries 
    credentials = service_account.Credentials.from_service_account_file(credentials_file)
    # remove temporary file
    if os.path.exists(credentials_file):
        os.remove(credentials_file)
    return credentials

# this will set the context credentials to use for queries performed in jupyter 
# using bigquery cell magic
magics.context.credentials = get_gcp_credentials_from_ssm('my_gcp_credentials')
</code></pre>

<p>Please note that SageMaker execution role should have access to SSM and of course other necessary route to connect to GCP. I am not sure if this is the best way though. Hope someone has better way.</p>
","6069517",0
538,55551756,2,55551617,2019-04-06 17:38:59,1,"<p>The <code>if/else</code> can return only a single TRUE/FALSE and is not vectorized for length > 1.  It may be suitable to use <code>ifelse</code> (but that is also not required and would be less efficient compared to direct coersion of logical vector to binary (<code>as.integer</code>).   In the OP's code, the 'close' column elements are looped  (<code>sapply</code>) and subtracted from the whole 'open' column.  The intention might be to do elementwise subtraction.  In that case, <code>-</code> between the columns is much cleaner and efficient (as these operations are vectorized)</p>

<pre><code>data$result &lt;- with(data, factor(as.integer((close - open) &gt;= 0)))
</code></pre>

<p>In the above, we get the difference between the columns ('close', 'open'), check if it is greater than or equal to 0 (returns logical vector), convert it to binary (<code>as.integer</code> - TRUE -> 1, FALSE -> 0) and then change it to <code>factor</code> type (if needed)</p>
","3732271",0
539,55553190,2,55376406,2019-04-06 20:28:11,2,"<p>I'm a product manager on the Amazon SageMaker Ground Truth team, and I'm happy to help you with this question. The minimum system requirement is 1,000 objects. In practice with text classification, we typically see meaningful results (% of data auto-labeled) only once you have 2,000 to 3,000 text objects. Remember performance is variable and depends on your dataset and the complexity of your task.</p>
","10880486",0
540,55572785,2,55371858,2019-04-08 11:49:18,3,"<p>I somewhat figured out the solution but I don't know either this is the exact solution or still there is a problem. As per the comment, there is no issue with the pod or other configuration files. I somewhat thought it might be the localhost problem. So, I tried changing the port from 8085 to 8081 and re-run the <strong>start_ui.sh</strong> script. The spawning error disappeared and it redirected me to the juypter working directory. </p>

<pre><code>kubectl port-forward -n ${NAMESPACE} $(kubectl get pods -n ${NAMESPACE} --selector=service=ambassador -o jsonpath='{.items[0].metadata.name}') 8081:80
</code></pre>

<p>And If you want to avoid all this problem, then the effective way is to the run the kubeflow in <strong>endpoints</strong> instead of localhost which eliminates all this problem. To view the dashboard at endpoints you need to setup the IAM access initially while creating a cluster.</p>
","9044016",0
541,55637650,2,55621967,2019-04-11 16:54:55,4,"<p>SageMaker XGBoost currently does not provide interface to retrieve feature importance from the model. You can write some code to get the feature importance from the XGBoost model. You have to get the booster object artifacts from the model in S3 and then use the following snippet </p>

<pre class=""lang-py prettyprint-override""><code>import pickle as pkl
import xgboost
booster = pkl.load(open(model_file, 'rb'))
booster.get_score()
booster.get_fscore()
</code></pre>

<p>Refer <a href=""https://xgboost.readthedocs.io/en/latest/python/python_api.html"" rel=""nofollow noreferrer"">XGBoost doc</a> for methods to get feature importance from the Booster object such as <code>get_score()</code> or <code>get_fscore()</code>.</p>
","6069517",0
542,55639028,2,55580232,2019-04-11 18:28:36,8,"<p>Hi and thank you for using SageMaker!</p>

<p>To restart Jupyter from within a SageMaker Notebook Instance, you can issue the following command: <code>sudo initctl restart jupyter-server --no-wait</code>.</p>

<p>Best,
Kevin</p>
","977443",1
543,55647486,2,55645119,2019-04-12 08:29:21,3,"<p>On AWS, you can use AWS Glue to create a <a href=""https://docs.aws.amazon.com/glue/latest/dg/dev-endpoint.html"" rel=""nofollow noreferrer"">developer endpoint</a>, and then you create the Sagemaker notebook from there. A developer endpoint gives you access to connect to your python or Scala spark REPL via ssh, and it also allows you to tunnel the connection and access from any other tool, including PyCharm.</p>

<p>For PyCharm professional we have even <a href=""https://docs.aws.amazon.com/glue/latest/dg/dev-endpoint-tutorial-pycharm.html"" rel=""nofollow noreferrer"">tighter integration</a>, allowing you to SFTP files and debug remotely.</p>

<p>And if you need to install any dependencies on the notebook, apart from doing it directly on the notebook, you can always choose <code>new&gt;terminal</code> and you will have a connection to that machine directly from your jupyter environment where you can install <a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/nbi-add-external.html"" rel=""nofollow noreferrer"">anything you want</a>.</p>
","3035921",0
544,55657256,2,55527489,2019-04-12 17:56:30,2,"<p>I'd suggest to wait.  See the status update on the issues and acitivity on <a href=""https://github.com/kubeflow/pipelines/issues/1131"" rel=""nofollow noreferrer"">kubeflow/pipelines#1131</a> to enable the support on AWS. Similar work is in progress for supporting on-prem as well.</p>
","10415488",0
545,55676460,2,55674959,2019-04-14 14:38:45,4,"<p>I think you should  be cloning the Github repo in SageMaker instance and not importing the files from S3. I was able to reproduce the Bitcoin Trading Bot notebook from SageMaker by cloning it. You can follow the below steps</p>

<h3>Cloning Github Repo to SageMaker Notebook</h3>

<ol>
<li>Open JupyterLab from the AWS SageMaker console.</li>
<li>From the JupyterLab  Launcher, open the Terminal.</li>
<li>Change directory to SageMaker</li>
</ol>

<pre><code>cd ~/SageMaker
</code></pre>

<ol start=""4"">
<li>Clone the BitCoin Trading Bot <a href=""https://github.com/llSourcell/Bitcoin_Trading_Bot"" rel=""nofollow noreferrer"">git repo</a></li>
</ol>

<pre><code>git clone https://github.com/llSourcell/Bitcoin_Trading_Bot.git
cd Bitcoin_Trading_Bot
</code></pre>

<ol start=""5"">
<li>Now you can open the notebook <code>Bitcoin LSTM Prediction.ipynb</code> and select the Tensorflow Kernel to run the notebook.</li>
</ol>

<p><a href=""https://i.stack.imgur.com/YbKic.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/YbKic.png"" alt=""enter image description here""></a></p>

<h3>Adding files from local machine to SageMaker Notebook</h3>

<p>To add files from your local machine to SageMaker Notebook instance, you can use <a href=""https://jupyterlab.readthedocs.io/en/stable/user/files.html"" rel=""nofollow noreferrer"">file upload</a> functionality in JupyterLab</p>

<h3>Adding files from S3 to SageMaker Notebook</h3>

<p>To add files from S3 to SageMaker Notebook instance, use AWS CLI or Python SDK to upload/download files. </p>

<p>For example, to download <code>lstm.py</code> file from S3 to SageMaker using AWS CLI</p>

<pre><code>aws s3 cp s3://mybucket/bot/src/lstm.py .
</code></pre>

<p>Using <code>boto3</code> API</p>

<pre><code>import boto3
s3 = boto3.resource('s3')
s3.meta.client.download_file('mybucket', 'bot/src/lstm.py', './lstm.py')
</code></pre>
","6069517",2
546,55713804,2,55410462,2019-04-16 17:39:40,1,"<p>There are natural interpretations for these two hyper-parameters that can help you determine good starting approximations for HPO:</p>

<ul>
<li><code>num_samples_per_tree</code> -- the reciprocal of this value approximates the density of anomalies in your data set/stream. For example, if you set this to <code>200</code> then the assumption is that approximately 0.5% of the data is anomalous. Try exploring your dataset to make an educated estimate.</li>
<li><code>num_trees</code> -- the more trees in your RCF model the less noise in scores. That is, if more trees are reporting that the input inference point is an anomaly then the point is much more likely to be an anomaly than if few trees suggest so.</li>
</ul>

<p>The total number of points sampled from the input dataset is equal to <code>num_samples_per_tree * num_trees</code>. You should make sure that the input training set is at least this size.</p>

<p><em>(Disclosure - I <a href=""https://aws.amazon.com/blogs/machine-learning/use-the-built-in-amazon-sagemaker-random-cut-forest-algorithm-for-anomaly-detection/"" rel=""nofollow noreferrer"">helped create</a> SageMaker Random Cut Forest)</em></p>
","645494",6
547,55734127,2,55731954,2019-04-17 18:44:51,2,"<p>Json.load() expect a local file system path ""/..."", not an ""s3://"" URI.<br>
See answer here: <a href=""https://stackoverflow.com/a/47121263"">https://stackoverflow.com/a/47121263</a></p>
","121956",0
548,55749032,2,53660590,2019-04-18 15:14:43,0,"<p>The solution involves using a join as recommended by pault. </p>

<ol>
<li>Create a dataframe with dynamic features of length equal to Training + Prediction period</li>
<li>Create a dataframe with target values of length equal to just the Training period.</li>
<li>Use a LEFT JOIN (with the dynamic feature data on LEFT) to bring these dataframes together</li>
</ol>

<p>Now, using collect_list will create the desired result.</p>
","8691976",0
549,55766862,2,55752427,2019-04-19 19:25:34,2,"<p>If you are looking to use boto3, I use this function to find zero byte objects. You can tweak it to your needs by filtering on specific size</p>

<pre><code>import boto3

def get_empty_objects(bucket_name, prefixes):
    """"""
    get list of objects from a given s3 prefix recursively
    """"""
    results = []
    for prefix in prefixes:
        s3client = boto3.client('s3')
        paginator = s3client.get_paginator(""list_objects_v2"")
        paginator_result = paginator.paginate(
            Bucket=bucket_name, Prefix=prefix)
        try:
            for object in paginator_result.search('Contents'):
                if object['Size'] == 0:
                    results.append(""s3://"" + bucket_name + ""/"" + object['Key'])
        except Exception as err:
            print(""&gt;&gt;&gt; Error processing objects of [s3://"" + bucket_name +
                  ""/"" + prefix + ""] - "" + str(err))
        print(""&gt;&gt;&gt; Returning "" + str(len(results)) + "" objects for [s3://"" + bucket_name + ""/"" + prefix + ""]"")
    return results
</code></pre>

<p>Usage:</p>

<pre><code>get_empty_objects(""mybucket"", [""prefix1/"", ""prefix2/""])
</code></pre>
","6069517",0
550,55777420,2,55769759,2019-04-20 20:20:50,1,"<p>Looks like you are using Python <code>gensim</code> package to construct a corpus from a wiki based database dump from S3. The <a href=""https://github.com/RaRe-Technologies/gensim/blob/develop/gensim/corpora/wikicorpus.py#L678"" rel=""nofollow noreferrer"">package</a> does not support reading directly from S3. Instead you can download the file and work with it.</p>

<pre><code>import boto3
from gensim.corpora.wikicorpus import WikiCorpus

s3 = boto3.client('s3')
s3.download_file('BUCKET_NAME', 'OBJECT_NAME', 'FILE_NAME')
wiki = WikiCorpus('FILE_NAME')
</code></pre>
","6069517",0
551,55823315,2,55821963,2019-04-24 06:01:53,0,"<p>Grant specific users <strong>IAP-Secured Web App User</strong> role on <a href=""https://console.cloud.google.com/iam-admin/iam"" rel=""nofollow noreferrer"">IAM admin</a>.</p>
","723405",0
552,55840157,2,55840023,2019-04-25 00:16:35,5,"<p>Based on your error, it looks like there's a permissions issue with the SageMaker notebook trying to change IAM settings from within a notebook that does not explicitly have permission to do so.</p>

<hr>

<p>You have a few options here to remedy this:</p>

<p><strong>Option 1: Granting the SageMaker notebook permissions to define IAM role within the notebook during runtime.</strong></p>

<p>From the console, click on <code>Hosted Notebooks</code> along the left navbar, then under <code>Permissions</code>, click the attached IAM role. Here, you can add policies such as <code>IAMFullAccess</code> or <code>IAMReadOnlyAccess</code>. This should solve for the permissions error when you try to attach an IAM role from within the notebook.</p>

<p><strong>Option 2: Explicitly define the permissions you want SageMaker to have in the console.</strong></p>

<p>From the console, click on <code>Hosted Notebooks</code> along the left navbar, then under <code>Permissions</code>, click the attached IAM role. Here, you can directly add policies for resource permissions (such as Comprehend). Without attaching explicit IAM access policies to this role, you wouldn't be able to change permissions during runtime.</p>

<p><strong>Option 3: Both</strong></p>

<p>If you'd like to pre-define access for some resources, but also potentially add other resource permissions during experimentation, you can do both steps 1 and 2 (Add IAM + other resource permissions to the hosted notebook in console, with the ability to change your SageMaker IAM role inline during experimentation).</p>
","10885720",2
553,55845300,2,55837639,2019-04-25 08:49:30,2,"<p>See <a href=""https://learn.microsoft.com/en-us/azure/machine-learning/service/how-to-consume-web-service#call-the-service-c"" rel=""nofollow noreferrer"">here</a> for an example (in C#). When you enable auth, you will need to send the API key in the ""Authorization"" header in the HTTP request:</p>

<pre><code>client.DefaultRequestHeaders.Authorization = new AuthenticationHeaderValue(""Bearer"", authKey);
</code></pre>

<p>See <a href=""https://learn.microsoft.com/en-us/azure/machine-learning/service/how-to-consume-web-service#authentication-key"" rel=""nofollow noreferrer"">here</a> how to retrieve the key.</p>
","1537195",4
554,55857185,2,55844248,2019-04-25 20:31:03,3,"<p>AWS is using soft limits to prevent customers from making a mistake that might cause them more money than they expected. When you are starting to use a new service, such as Amazon SageMaker, you will hit these soft limits and you need to ask specifically to raise them using the ""Support"" link on the top right side of your AWS management console. </p>

<p>Here is a link to guide on how to do that: <a href=""https://aws.amazon.com/premiumsupport/knowledge-center/manage-service-limits/"" rel=""nofollow noreferrer"">https://aws.amazon.com/premiumsupport/knowledge-center/manage-service-limits/</a></p>

<p>You will usually get the limit increased within a couple of days. In the meanwhile, you can choose a smaller instance (such as t2) that are often available.</p>
","179529",3
555,55873467,2,55829940,2019-04-26 18:52:32,5,"<p>Thank you for using Amazon SageMaker. </p>

<p>SageMaker Notebook Instance's name cannot be edited. </p>

<p>Thanks,<br>
Neelam</p>
","4570570",2
556,55880032,2,55868121,2019-04-27 11:19:46,0,"<p>The simplest way to run the RCF model and to get the explanation for the anomaly is to use the version of RCF in Kinesis Analytics (KA). Here is a link to the documentation of how to run from the KA documentations: <a href=""https://docs.aws.amazon.com/kinesisanalytics/latest/sqlref/sqlrf-random-cut-forest-with-explanation.html"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/kinesisanalytics/latest/sqlref/sqlrf-random-cut-forest-with-explanation.html</a></p>

<p>Kinesis is taking care both for the training of the model, the inference after the initial training and for the attribution and explanation of the variables. </p>

<p><a href=""https://i.stack.imgur.com/p25FR.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/p25FR.png"" alt=""https://docs.aws.amazon.com/kinesisanalytics/latest/sqlref/images/anomaly_results.png""></a></p>
","179529",4
557,55920737,2,55479366,2019-04-30 12:13:03,0,"<p>I managed to resolve the issue, it seemed the maxpayload I was using was too high. I set  <code>MaxPayloadInMB=1</code> and it now runs like a dream</p>
","2032998",0
558,55933267,2,55854377,2019-05-01 08:07:25,0,"<p>It turns out that there was nothing wrong with the call to <code>comprehend.describe_topics_detection_job</code> -- it was just returning, in <code>describe_result</code>, something that could not be json serialized, so <code>json.dumps(describe_result))</code> was throwing an error. </p>
","364966",0
559,55940836,2,55892554,2019-05-01 18:24:23,1,"<p>The first error with data size (5000, 170) might be due to a capacity issue. SageMaker endpoint prediction has a size limit of 5mb. So if your data is larger than 5mb, you need to chop it into pieces and call predict multiple times. </p>

<p>For the second error with data size (10, 170), the error message asks you to look into logs. Did you find anything interesting in the cloudwatch log? Anything can be shared in this question?</p>
","8594468",2
560,55942927,2,55941720,2019-05-01 21:28:02,2,"<blockquote>
  <p>For security reasons, all networking from or to R code in Execute R Script modules is blocked by Azure.</p>
</blockquote>

<p><a href=""https://learn.microsoft.com/en-us/azure/machine-learning/studio-module-reference/execute-r-script#networking"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/machine-learning/studio-module-reference/execute-r-script#networking</a></p>
","1537195",4
561,55979955,2,55964972,2019-05-04 06:04:06,9,"<p>The answer is no as there is no parameter on the Estimator base class, or the fit method, that accepts arguments to pass to the entrypoint.</p>

<p>I resolved this by passing the parameter as part of the hyperparameter dictionary. This gets passed to the entrypoint as arguments.</p>
","1552416",3
562,56011913,2,55987935,2019-05-06 20:21:30,3,"<p>Could you show your ECR login command and pull command in the question?</p>

<p>For SageMaker pre-built image 520713654638.dkr.ecr.us-west-2.amazonaws.com/sagemaker-mxnet:1.3.0-cpu-py3</p>

<p>What I do is:</p>

<ol>
<li>Log in ECR</li>
</ol>

<p><code>$(aws ecr get-login --no-include-email --registry-ids 520713654638 --region us-west-2)</code></p>

<ol start=""2"">
<li>Pull the image</li>
</ol>

<p><code>docker pull 520713654638.dkr.ecr.us-west-2.amazonaws.com/sagemaker-mxnet:1.3.0-cpu-py3</code></p>

<p>These images are public readable so you can pull them from any AWS account. I guess the reason you failed is that you did not specify --registry-ids in your login. But it's better if you can provide your scripts for others to identify what's wrong.</p>
","8594468",2
563,56032375,2,55822637,2019-05-08 01:32:42,15,"<p>Generally speaking you can log arbitrary output from your code using the mlflow_log_artifact() function.  From <a href=""https://mlflow.org/docs/latest/python_api/mlflow.html#mlflow.log_artifact"" rel=""noreferrer"">the docs</a>:</p>
<blockquote>
<p><strong>mlflow.log_artifact(local_path, artifact_path=None)</strong>
Log a local file or directory as an artifact of the currently active run.</p>
</blockquote>
<blockquote>
<p><strong>Parameters:</strong><br />
<em>local_path</em> – Path to the file to write.
<em>artifact_path</em> – If provided, the directory in artifact_uri to write to.</p>
</blockquote>
<p>As an example, say you have your statistics in a pandas dataframe, <code>stat_df</code>.</p>
<pre><code>## Write csv from stats dataframe
stat_df.to_csv('dataset_statistics.csv')

## Log CSV to MLflow
mlflow.log_artifact('dataset_statistics.csv')
</code></pre>
<p>This will show up under the artifacts section of this MLflow run in the Tracking UI.  If you explore the docs further you'll see that you can also log an entire directory and the objects therein.  In general, MLflow provides you a lot of flexibility - anything you write to your file system you can track with MLflow.  Of course that doesn't mean you should. :)</p>
","4896112",2
564,56035514,2,56024354,2019-05-08 07:20:39,2,"<p>I can reproduce your issue, I sign out and log in <a href=""https://studio.azureml.net/"" rel=""nofollow noreferrer"">https://studio.azureml.net/</a> again, it solved my problem. Or you can try to clear the browsing data or change a browser. Anyway, the issue should be caused by the browser, not azure. Even if your account is not the owner of the workspace, when you click <code>Sign In</code> in <a href=""https://studio.azureml.net/"" rel=""nofollow noreferrer"">https://studio.azureml.net/</a> , it will create a free workspace(with a different workspace id) for you automatically.</p>

<p><a href=""https://i.stack.imgur.com/MXDJC.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/MXDJC.png"" alt=""enter image description here""></a></p>

<p>If you want to delete the workspace, you need to let the owner of the workspace delete it, navigate to the <code>SETTINGS</code> on the left of the studio -> <code>NAME</code> -> <code>DELETE WORKSPACE</code>. </p>

<p><a href=""https://i.stack.imgur.com/E6aUl.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/E6aUl.png"" alt=""enter image description here""></a></p>
","9455659",1
565,56055667,2,56021977,2019-05-09 08:52:09,1,"<p>You can use either the more general <a href=""https://learn.microsoft.com/en-us/azure/machine-learning/studio-module-reference/apply-sql-transformation"" rel=""nofollow noreferrer"">Apply SQL Transformation</a>, or the dedicated <a href=""https://learn.microsoft.com/en-us/azure/machine-learning/studio-module-reference/clip-values"" rel=""nofollow noreferrer"">Clip Values</a> module. If all else fails, there's also <a href=""https://learn.microsoft.com/en-us/azure/machine-learning/studio-module-reference/execute-python-script"" rel=""nofollow noreferrer"">Execute Python Script</a>.</p>

<p>Personally, for your example I'd use <code>Clip Values</code> with <code>Clip Peaks</code> and <code>Upper Threshold</code> set. For more complex rules I'd use either <code>Apply SQL Transformation</code> or <code>Execute Python Script</code>, depending on the rules but favouring SQL :).</p>
","155697",0
566,56090820,2,56046428,2019-05-11 13:32:46,4,"<p>No need to worry. These are not errors in your code. These are info messages that are calculating the error of the model on the training data (train-error) and on the validation data (validation-error), and these values should get smaller as the training progress. </p>

<p>In time, these values will be more meaningful for you. You will be able to compare different algorithms and hyper-parameters based on which is the smaller error, or you will be able to see that your model is overfitting, when the error values of the training is very different from the validation error. </p>
","179529",0
567,56098818,2,56055868,2019-05-12 11:32:51,1,"<p>From the code that you have provided, when you deploy the application in the ACI using the method <code>Webservice.deploy_from_image</code> with the parameters <code>deployment_config</code> and container image. The deployment_config makes by the <code>AciWebservice.deploy_configuration</code>.</p>

<p>When you take a look at the ML about AKS, you can also find the method <code>AksWebservice.deploy_configuration</code>. So you just need to change the method <code>AciWebservice.deploy_configuration</code> into <code>AksWebservice.deploy_configuration</code>, then the application can be deployed from ACI into AKS. And it's the minimal changes. Also, it can deploy from the docker image.</p>
","9773937",0
568,56134388,2,56134165,2019-05-14 15:57:02,1,"<p>If this doesn't work, I'm guessing it's an AWS thing</p>

<ol>
<li><code>IPython.display.HTML</code></li>
<li><code>pandas.DataFrame.to_html</code> with <code>escape=False</code></li>
<li><code>pandas.set_option('display.max_colwidth', 2000)</code> Must be a large number to accommodate length of link tag.  I'll say that I think this is broken.  It shouldn't be necessary to set <code>'display.max_colwidth'</code> in order to make sure <code>to_html</code> outputs properly.  But it is :-/</li>
</ol>

<hr>

<pre><code>from IPython import display

pd.set_option('display.max_colwidth', 2000)

display.HTML(df.assign(url=[*map(make_clickable, df.url)]).to_html(escape=False))
</code></pre>

<p><a href=""https://i.stack.imgur.com/YcVj6.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/YcVj6.png"" alt=""enter image description here""></a></p>
","2336654",2
569,56174913,2,56116926,2019-05-16 18:52:27,2,"<p>I think the general answer here is that you can log arbitrary data and artifacts from your experiment to your MLflow tracking server using <code>mlflow_log_artifact()</code> or <code>mlflow_set_tag()</code>, depending on how you want to do it.  If there's an API to get data from Glue and you can fetch it during your MLflow run, then you can log it.  Write a csv, save a .png to disk and log that, or declare a variable and access it when you are setting the tag.</p>

<p>This applies for Glue or any other API that you are getting a response from.  One of the key benefits of MLflow is that it is such a general framework, so you can track what matters to that particular experiment.</p>

<p>Hope this helps!</p>
","4896112",0
570,56247259,2,56240481,2019-05-21 22:53:14,1,"<p>As of today, <a href=""https://learn.microsoft.com/en-us/azure/machine-learning/service/how-to-load-data#load-sql-data"" rel=""nofollow noreferrer"">you can load SQL data, but only a MS SQL Server source (also on-premise) is supported</a>.</p>

<p>Using <code>azureml.dataprep</code>, code would read along the lines of</p>

<pre><code>import azureml.dataprep as dprep

secret = dprep.register_secret(value=""[SECRET-PASSWORD]"", id=""[SECRET-ID]"")

ds = dprep.MSSQLDataSource(server_name=""[SERVER-NAME]"",
                           database_name=""[DATABASE-NAME]"",
                           user_name=""[DATABASE-USERNAME]"",
                           password=secret)

dflow = dprep.read_sql(ds, ""SELECT top 100 * FROM [YourDB].[ATable]"")
# print first records
dflow.head(5)
</code></pre>

<p>As far as I understand the APIs are under heavy development and <code>azureml.dataprep</code> may be soon superseded by functionality provided by the <a href=""https://aka.ms/azureml/concepts/datasets"" rel=""nofollow noreferrer"">Dataset class</a>.</p>
","4240413",2
571,56266600,2,56260720,2019-05-23 00:27:00,3,"<p>You can add a <code>__init__.py</code> file to your <code>package</code> directory to make it a Python package. Then you will be import the modules from the package inside your Jupyter notebook</p>

<pre><code>/home/ec2-user/SageMaker
    -- Notebook.ipynb 
    -- mypackage
        -- __init__.py
        -- mymodule.py
</code></pre>

<p>Contents of Notebook.ipynb</p>

<pre><code>from mypackage.mymodule import SomeClass, SomeOtherClass
</code></pre>

<p>For more details, see <a href=""https://docs.python.org/3/tutorial/modules.html#packages"" rel=""nofollow noreferrer"">https://docs.python.org/3/tutorial/modules.html#packages</a></p>

<p>Thanks for using Amazon SageMaker!</p>
","8899112",2
572,56277411,2,56255154,2019-05-23 14:25:59,15,"<p>You can actually instantiate a Python SDK <code>model</code> object from existing artifacts, and deploy it to an endpoint. This allows you to deploy a model from trained artifacts, without having to retrain in the notebook. For example, for the semantic segmentation model:</p>

<pre><code>trainedmodel = sagemaker.model.Model(
    model_data='s3://...model path here../model.tar.gz',
    image='685385470294.dkr.ecr.eu-west-1.amazonaws.com/semantic-segmentation:latest',  # example path for the semantic segmentation in eu-west-1
    role=role)  # your role here; could be different name

trainedmodel.deploy(initial_instance_count=1, instance_type='ml.c4.xlarge')
</code></pre>

<p>And similarly, you can instantiate a predictor object on a deployed endpoint from any authenticated client supporting the SDK, with the following command:</p>

<pre><code>predictor = sagemaker.predictor.RealTimePredictor(
    endpoint='endpoint name here',
    content_type='image/jpeg',
    accept='image/png')
</code></pre>

<p>More on those abstractions:</p>

<ul>
<li><code>Model</code>: <a href=""https://sagemaker.readthedocs.io/en/stable/model.html"" rel=""noreferrer"">https://sagemaker.readthedocs.io/en/stable/model.html</a></li>
<li><code>Predictor</code>:
<a href=""https://sagemaker.readthedocs.io/en/stable/predictors.html"" rel=""noreferrer"">https://sagemaker.readthedocs.io/en/stable/predictors.html</a></li>
</ul>
","5331834",2
573,56286144,2,56285351,2019-05-24 05:04:26,2,"<p>Only when file is updated - i.e. edit <code>1.jpg</code> with your editor <strong>AND</strong> only if hadrlink or symlink cache type is enabled.</p>

<p>Please, check this <a href=""https://dvc.org/doc/user-guide/update-tracked-file"" rel=""nofollow noreferrer"">link</a>:</p>

<blockquote>
  <p>updating tracked files has to be carried out with caution to avoid data corruption when the DVC config option cache.type is set to hardlink or/and symlink</p>
</blockquote>

<p>I would strongly recommend reading this document: <a href=""https://dvc.org/doc/user-guide/cache-file-linking"" rel=""nofollow noreferrer"">Performance Optimization for Large Files</a> it explains benefits of using hardlinks/symlinks.</p>
","298182",2
574,56302668,2,56202697,2019-05-25 07:19:16,4,"<p>Have you tried using the “code_location” argument: <a href=""https://sagemaker.readthedocs.io/en/stable/estimators.html"" rel=""nofollow noreferrer"">https://sagemaker.readthedocs.io/en/stable/estimators.html</a> to specify the location for the source code?</p>

<p>Below is a snippet code example that use code_location</p>

<pre><code>from sagemaker.tensorflow import TensorFlow

code-path = ""s3://&lt;bucket&gt;/&lt;prefix&gt;""
output-path = ""s3://&lt;bucket&gt;/&lt;prefix&gt;""

abalone_estimator = TensorFlow(entry_point='abalone.py',
                           role=role,
                           framework_version='1.12.0',
                           training_steps= 100, 
                           image_name=image,
                           evaluation_steps= 100,
                           hyperparameters={'learning_rate': 0.001},
                           train_instance_count=1,
                           train_instance_type='ml.c4.xlarge',
                           code_location= code-path,
                           output_path = output-path,
                           base_job_name='my-job-name'
                           )
</code></pre>
","3458797",0
575,56314197,2,56308169,2019-05-26 14:02:37,1,"<p>I think you are on the right track. Do you get any error? Refer this <a href=""https://github.com/awslabs/amazon-sagemaker-examples/blob/e9c295c8538d29cc9fea2f73a29649126628064a/advanced_functionality/inference_pipeline_sparkml_xgboost_abalone/inference_pipeline_sparkml_xgboost_abalone.ipynb"" rel=""nofollow noreferrer"">notebook</a>  for instantiating the model from the tuner and use in inference pipeline.</p>

<p>Editing previous response based on the comment. To create model from the best training job of the hyperparameter tuning job, you can use below snippet</p>

<pre class=""lang-py prettyprint-override""><code>from sagemaker.tuner import HyperparameterTuner
from sagemaker.estimator import Estimator
from sagemaker.model import Model

# Attach to an existing hyperparameter tuning job.
xgb_tuning_job_name = 'my_xgb_hpo_tuning_job_name'
xgb_tuner = HyperparameterTuner.attach(xgb_tuning_job_name)

# Get the best XGBoost training job name from the HPO job
xgb_best_training_job = xgb_tuner.best_training_job()
print(xgb_best_training_job)

# Attach estimator to the best training job name
xgb_best_estimator = Estimator.attach(xgb_best_training_job)

# Create model to be passed to the inference pipeline
xgb_model = Model(model_data=xgb_best_estimator.model_data,
                  role=sagemaker.get_execution_role(),
                  image=xgb_best_estimator.image_name)
</code></pre>
","6069517",3
576,56322505,2,56287936,2019-05-27 08:40:51,1,"<p>I found the solution at <a href=""https://github.com/kubeflow/examples/tree/master/mnist"" rel=""nofollow noreferrer"">here</a>.</p>

<pre><code>kustomize edit add configmap mnist-map-monitoring --from-literal=GOOGLE_APPLICATION_CREDENTIALS=/var/secrets/user-gcp-sa.json     
</code></pre>

<p>In original <a href=""https://www.kubeflow.org/docs/gke/gcp-e2e/"" rel=""nofollow noreferrer"">tutorial</a>, lacking of this.</p>
","4921432",2
577,56341274,2,56341012,2019-05-28 11:32:03,2,"<p>I just found the problem and corresponding solution:</p>

<p>I deleted all images but there where still some containers based on deleted images present. Deleting the corresponding container had the desired effect that the docker image is reloaded from the server.</p>

<p>You can delete all containers with <code>docker kill $(docker ps -q)</code>.</p>
","3952274",2
578,56356107,2,56351452,2019-05-29 08:23:47,1,"<p>I had a similar problem, used python and solved it with the following steps:</p>
<ol>
<li>Install mlflow and datbricks-cli libraries.</li>
<li>Define the following env variables : DATABRICKS_HOST (databricks workspace url: <a href=""https://region.azuredatabricks.net"" rel=""nofollow noreferrer"">https://region.azuredatabricks.net</a>) and DATABRICKS_TOKEN</li>
<li>Define mlflow client:</li>
</ol>
<pre class=""lang-py prettyprint-override""><code>mlflow_client = mlflow.tracking.MlflowClient(tracking_uri='databricks')
</code></pre>
<ol start=""5"">
<li>Use mlflow_client client for logging, saving and etc..</li>
</ol>
<p>for more reference you can look at the &quot;Log to a tracking server from a notebook&quot; section <a href=""https://docs.azuredatabricks.net/applications/mlflow/tracking.html#log-to-a-tracking-server-from-a-notebook"" rel=""nofollow noreferrer"">here</a></p>
","5720414",0
579,56400590,2,56353814,2019-05-31 19:12:00,0,"<p>I assume this is a duplicate of this AWS Forum post: <a href=""https://forums.aws.amazon.com/thread.jspa?threadID=303810&amp;tstart=0"" rel=""nofollow noreferrer"">https://forums.aws.amazon.com/thread.jspa?threadID=303810&amp;tstart=0</a></p>

<p>Anyway, for completeness I'll answer here as well.</p>

<p>The issue is that you are serializing your dataset incorrectly when converting it into jsonlines:</p>

<pre><code>test_x_list = test_x.tolist()
...
with jsonlines.open(file_path, 'w') as writer:
    writer.write(test_x_list)   
</code></pre>

<p>What the above is doing is creating a very large single-line containing your full dataset which is too big for single inference call to consume.</p>

<p>I suggest you change your code to make each line a single sample so that inference can take place on individual samples instead of the whole dataset:</p>

<pre><code>test_x_list = test_x.tolist()
...
with jsonlines.open(file_path, 'w') as writer:
    for sample in test_x_list:
        writer.write(sample)
</code></pre>

<p>If one sample at a time is too slow you can also play around with the <code>max_concurrent_transforms</code>, <code>strategy</code>, and <code>max_payload</code> parameters to be able to batch the data as well as run concurrent transforms if your algorithm can run in parallel - also, of course, you can split the data into multiple files and run transformations with more than just one node. See <a href=""https://sagemaker.readthedocs.io/en/latest/transformer.html"" rel=""nofollow noreferrer"">https://sagemaker.readthedocs.io/en/latest/transformer.html</a> and <a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/API_CreateTransformJob.html"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/sagemaker/latest/dg/API_CreateTransformJob.html</a> for additional detail on what these parameters do.</p>
","1502599",0
580,56413780,2,56322632,2019-06-02 09:23:43,0,"<p>From the bug report.</p>

<blockquote>
  <p>You can work around this problem by using a shared filesystem (e.g.
  HDFS, GCS, or an NFS mount at the same mount point) on the workers and
  the parameter servers.</p>
</blockquote>

<p>Just put the data on GCS and it work fine.</p>

<p>model.py</p>

<pre><code>import tensorflow_datasets as tfds
import tensorflow as tf

# tfds works in both Eager and Graph modes
tf.enable_eager_execution()

# See available datasets
print(tfds.list_builders())

ds_train, ds_test = tfds.load(name=""mnist"", split=[""train"", ""test""], data_dir=""gs://kubeflow-tf-bucket"", batch_size=-1)
ds_train = tfds.as_numpy(ds_train)
ds_test = tfds.as_numpy(ds_test)

(x_train, y_train) = ds_train['image'], ds_train['label']
(x_test, y_test) = ds_test['image'], ds_test['label']
x_train, x_test = x_train / 255.0, x_test / 255.0

model = tf.keras.models.Sequential([
  tf.keras.layers.Flatten(input_shape=(28, 28, 1)),
  tf.keras.layers.Dense(512, activation=tf.nn.relu),
  tf.keras.layers.Dropout(0.2),
  tf.keras.layers.Dense(10, activation=tf.nn.softmax)
])
model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

model.fit(x_train, y_train, epochs=5)
print(model.evaluate(x_test, y_test))
</code></pre>
","4921432",0
581,56414728,2,56411563,2019-06-02 11:50:32,3,"<p>Yes, you can install dependencies without going the custom dockerfile route (also BYO Container)</p>

<p>Use this in your code (where <code>mypackage</code> represents a pip package of your choice)</p>

<pre><code>import subprocess as sb 
import sys 

sb.call([sys.executable, ""-m"", ""pip"", ""install"", mypackage]) 
</code></pre>
","5331834",2
582,56414804,2,56408976,2019-06-02 12:00:44,1,"<p>You can bring images to a local repo of your SageMaker instance (eg /home/ec2-user/SageMaker/Pics/ with the following command:</p>

<pre><code>aws s3 sync s3://pic_folder_in_s3 /home/ec2-user/SageMaker/Pics
</code></pre>

<p>or in python:</p>

<pre><code>import subprocess as sb

sb.call('aws s3 sync s3://pic_folder_in_s3 /home/ec2-user/SageMaker/Pics'.split())
</code></pre>

<p>Note that in order for the transfer to happen, the role carried by your SageMaker instance must have the right to read from this S3 location</p>
","5331834",0
583,56427990,2,56422325,2019-06-03 13:02:56,3,"<p>Found the problem. The CompressionType was mentioned as 'Gzip' but I had changed the actual file to be not compressed when doing the exports. As soon as I changed it to be 'None' the training went smoothly.</p>
","2235567",0
584,56435992,2,56083148,2019-06-04 00:29:38,2,"<p>SageMaker now supports associating code repository including CodeCommit and any other Git repository with Notebook Instances via CloudFormation. </p>

<p>Here's the link for more information: <a href=""https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-sagemaker-coderepository.html"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-sagemaker-coderepository.html</a></p>
","11596370",0
585,56452839,2,56024351,2019-06-05 00:19:29,1,"<p>If you followed the SageMaker tutorial you must have trained an XGBoost model. SageMaker places the model artifacts in a bucket that you own, check the output S3 location in the AWS SageMaker console. </p>

<p>For more information about XGBoost you can check the AWS SageMaker documentation <a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/xgboost.html#xgboost-sample-notebooks"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/sagemaker/latest/dg/xgboost.html#xgboost-sample-notebooks</a> and the example notebooks, e.g. <a href=""https://github.com/awslabs/amazon-sagemaker-examples/blob/master/introduction_to_amazon_algorithms/xgboost_abalone/xgboost_abalone.ipynb"" rel=""nofollow noreferrer"">https://github.com/awslabs/amazon-sagemaker-examples/blob/master/introduction_to_amazon_algorithms/xgboost_abalone/xgboost_abalone.ipynb</a></p>

<p>To consume the XGBoost artifact generated by SageMaker, check out the official documentation, which contains the following code:</p>

<pre class=""lang-py prettyprint-override""><code># SageMaker XGBoost uses the Python pickle module to serialize/deserialize 
# the model, which can be used for saving/loading the model.
# To use a model trained with SageMaker XGBoost in open source XGBoost
# Use the following Python code:

import pickle as pkl 
model = pkl.load(open(model_file_path, 'rb'))
# prediction with test data
pred = model.predict(dtest)
</code></pre>
","4110545",0
586,56465875,2,56456463,2019-06-05 18:09:37,3,"<p>The current implementation of <code>.dvcignore</code> is very limited. Read more on it <a href=""https://dvc.org/doc/user-guide/dvcignore"" rel=""nofollow noreferrer"">here</a>.</p>

<p>Please, mention that you are interested in this feature here - <a href=""https://github.com/iterative/dvc/issues/1876"" rel=""nofollow noreferrer"">https://github.com/iterative/dvc/issues/1876</a>. That would help our team to prioritize issues properly.</p>

<p>The possible workaround for now would be to use one of these approaches - <a href=""https://stackoverflow.com/questions/18015978/how-to-stop-creating-ds-store-on-mac"">How to stop creating .DS_Store on Mac?</a></p>
","298182",0
587,56469672,2,56467434,2019-06-06 00:42:46,2,"<p>Yes you are on the right track. You can send csv-serialized input to the endpoint without using the <code>predictor</code> from the SageMaker SDK, and using other SDKs such as <code>boto3</code> which is installed in lambda:</p>

<pre><code>import boto3
runtime = boto3.client('sagemaker-runtime')

payload = '0.12787057,  1.0612601,  -1.1081504'

response = runtime.invoke_endpoint(
    EndpointName=ENDPOINT_NAME,
    ContentType='text/csv',
    Body=payload.encode('utf-8'))

result = json.loads(response['Body'].read().decode()) 
</code></pre>

<p>This will pass to the endpoint a csv-formatted input, that you may need to reshape back in the <code>input_fn</code> to put in the appropriate dimension expected by the model.</p>

<p>for example:</p>

<pre><code>def input_fn(request_body, request_content_type):
    if request_content_type == 'text/csv':
        return torch.from_numpy(
            np.genfromtxt(StringIO(request_body), delimiter=',').reshape(1,3))
</code></pre>

<p><strong>Note</strong>: I wasn't able to test the specific <code>input_fn</code> above with your input content and shape but I used the approach on Sklearn RandomForest couple times, and looking at the <a href=""https://sagemaker.readthedocs.io/en/stable/using_pytorch.html#model-serving"" rel=""nofollow noreferrer"">Pytorch SageMaker serving doc</a> the above rationale should work.</p>

<p>Don't hesitate to use endpoint logs in Cloudwatch to diagnose any inference error (available from the endpoint UI in the console), those logs are usually <strong>much more verbose</strong> that the high-level logs returned by the inference SDKs</p>
","5331834",4
588,56490474,2,56450223,2019-06-07 08:15:14,2,"<p>I suppose that this is part of a series of questions that present the same scenario. And there should be definitely some constraints in the scenario. 
Moreover if you have a look on the <a href=""https://learn.microsoft.com/en-us/azure/machine-learning/studio-module-reference/multiclass-neural-network"" rel=""nofollow noreferrer"">Azure documentation</a>:</p>

<blockquote>
  <p>However, recent research has shown that deep neural networks (DNN)
  with many layers can be very effective in complex tasks such as image
  or speech recognition. The successive layers are used to model
  increasing levels of semantic depth.</p>
</blockquote>

<p>Thus, Azure recommends using Neural Networks for image classification. Remember, that the goal of the exam is to test your capacity to design data science solution <strong>using Azure</strong> so better to use their official documentation as a reference.</p>

<p>And comparing to the other solutions:</p>

<ol>
<li>You create an Azure notebook that supports the Microsoft Cognitive
Toolkit.</li>
<li>You create a Machine Learning experiment that implements
the Multiclass Decision Jungle module.</li>
<li>You create an endpoint to the
Computer vision API. </li>
<li>You create a Machine Learning experiment that
implements the Multiclass Neural Network module.</li>
<li>You create an Azure
notebook that supports the Microsoft Cognitive Toolkit.</li>
</ol>

<p>There are only 2 Azure ML Studio modules, and as the question is about constructing a <strong>workflow</strong> I guess we can only choose between them. (CNTK is actually the best solution as it allows constructing a deep neural network with ReLU whereas AML Studio doesn't, and API call is not about data science at all). </p>

<p>Finally, I do agree with the other contributors that the question is absurd. Hope this helps.</p>
","9929041",0
589,56490724,2,56418684,2019-06-07 08:31:09,0,"<p>Quote from Azure ML Exam reference:</p>

<blockquote>
  <p>By default, the architecture of neural networks is limited to a single
  hidden layer with sigmoid as the activation function and softmax in
  the last layer. You can change this in the properties of the model,
  opening the Hidden layer specification dropdown list, and selecting a
  Custom definition script. A text box will appear in which you will be
  able to insert a Net# script. This script language allows you to
  define neural networks architectures.</p>
</blockquote>

<p>For instance, if you want to create a two layer network, you may put the following code.</p>

<pre><code>input Picture [28, 28];
hidden H1 [200] from Picture all;
hidden H2 [200] from H1 all;
output Result [10] softmax from H2 all;
</code></pre>

<p>Nevertheless, with Net# you will face certain limitations as, it does not accept regularization (neither L2 nor dropout). Also, there is no ReLU activation that are
commonly used in deep learning due to their benefits in backpropagation. You cannot modify the batch size of the Stochastic Gradient Descent (SGD). Besides that, you cannot use other optimization algorithms. You can use SGD with momentum, but not others like Adam, or RMSprop. You cannot define recurrent or recursive neural networks.</p>

<p>Another great tool is CNTK (Cognitive Toolkit) that allows you defining your computational graph and create a fully customizable model.
Quote from documentation</p>

<blockquote>
  <p>It is a Microsoft open source deep learning toolkit. Like other deep
  learning tools, CNTK is based on the construction of computational
  graphs and their optimization using automatic differentiation. The
  toolkit is highly optimized and scales efficiently (from CPU, to GPU,
  to multiple machines). CNTK is also very portable and flexible; you
  can use it with programming languages like Python, C#, or C++, but you
  can also use a model description language called BrainScript.</p>
</blockquote>
","9929041",2
590,56490951,2,56364828,2019-06-07 08:46:54,0,"<p>You can use Ensembles, combining several models to improve predictions. The most direct is stacking when the outputs of all the models are trained on the entire dataset. 
The method that, I think, corresponds the best to your problem is bagging (bootstrap aggregation). You need to divide the training set into different subsets (each corresponding to a certain business), then train a different model on each subset and combine the result of each classifier. 
Another way is boosting but it is difficult to implement in Azure ML. 
You can see an example in <a href=""https://gallery.azure.ai/Experiment/b6b09fc0c26047e6b4c733ab78a86498"" rel=""nofollow noreferrer"">Azure ML Gallery</a>. </p>

<p>Quote from book:</p>

<blockquote>
  <p>Stacking and bagging can be easily implemented in Azure Machine
  Learning, but other ensemble methods are more difficult. Also, it
  turns out to be very tedious to implement in Azure Machine Learning an
  ensemble of, say, more than five models. The experiment is filled with
  modules and is quite difficult to maintain. Sometimes it is worthwhile
  to use any ensemble method available in R or Python. Adding more
  models to an ensemble written in a script can be as trivial as
  changing a number in the code, instead of copying and pasting modules
  into the experiment.</p>
</blockquote>

<p>You may also have a look at <a href=""http://scikit-learn.org/stable/modules/ensemble.html"" rel=""nofollow noreferrer"">sklearn (Python)</a> and caret (R) documentation for further details.</p>
","9929041",0
591,56493627,2,56466592,2019-06-07 11:38:55,3,"<p>When you are using SageMaker training jobs you are actually deploying a Docker image to a cluster of EC2 instances. The Docker has a python file that is running the training code in a similar way that you train it on your machine. In the training code you are referring to local folders when it expects to find the data such as the images to train on and the meta-data to use for that training. </p>

<p>The ""magic"" is how to get the data from S3 to be available locally for the training instances. This is done using the definition of the channels in your training job configuration. Each channel definition creates a local folder on the training instance and copies the data from S3 to that local folder. You need to match the names and the S3 location and file formats.</p>

<p>Here is the documentation of the definition of a channel in SageMaker: <a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/API_Channel.html"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/sagemaker/latest/dg/API_Channel.html</a></p>

<p>For the specific example of the built-in algorithm for image classification and if you use the Image format for training, specify <code>train</code>, <code>validation</code>, <code>train_lst</code>, and <code>validation_lst</code> channels as values for the <code>InputDataConfig</code> parameter of the <code>CreateTrainingJob</code> request. Specify the individual image data (.jpg or .png files) for the train and validation channels. Specify one .lst file in each of the train_lst and validation_lst channels. Set the content type for all four channels to <code>application/x-image</code>.</p>

<p>See more details here: <a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/image-classification.html#IC-inputoutput"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/sagemaker/latest/dg/image-classification.html#IC-inputoutput</a></p>
","179529",0
592,56502031,2,56497428,2019-06-07 22:58:18,2,"<p>There are 3 options to provide annotated data to the Image Classification algo: (1) packing labels in recordIO files, (2) storing labels in a JSON manifest file (""augmented manifest"" option), (3) storing labels in a list file. All options are documented here: <a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/image-classification.html"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/sagemaker/latest/dg/image-classification.html</a>.</p>

<p>Augmented Manifest and .lst files option are quick to do since they just require you to create an annotation file with a usually quick <code>for</code> loop for example. RecordIO requires you to use <code>im2rec.py</code> tool, which is a little more work.</p>

<p>Using .lst files is <strong>another option</strong> that is reasonably easy: you just need to create annotation them with a quick for loop, like this:</p>

<pre><code># assuming train_index, train_class, train_pics store the pic index, class and path

with open('train.lst', 'a') as file:
    for index, cl, pic in zip(train_index, train_class, train_pics):
        file.write(str(index) + '\t' + str(cl) + '\t' + pic + '\n')
</code></pre>
","5331834",1
593,56538488,2,56525510,2019-06-11 07:21:04,3,"<p>GKE has a great <a href=""https://cloud.google.com/kubernetes-engine/docs/how-to/cluster-access-for-kubectl"" rel=""nofollow noreferrer"">description of how you can configure access to a cluster via kubectl</a> (which uses the <code>~/.kube/config</code> file as the default location to store credentials). There are two ways to populate the file:</p>

<ol>
<li>Create your cluster through the command line using <code>gcloud container clusters create CLUSTER_NAME</code></li>
<li>Fetching credentials for an existing cluster using <code>gcloud container clusters get-credentials CLUSTER_NAME</code></li>
</ol>

<p>From the Kubeflow docs, it looks like the init step doesn't create a cluster; the apply step should create a cluster. You don't describe what issue you have later if you create an empty file. </p>

<p>Also, you pointed to the deploy cli docs, but the <a href=""https://www.kubeflow.org/docs/gke/customizing-gke/"" rel=""nofollow noreferrer"">customizing Kubeflow on GKE</a> page sounds more like what you are trying to accomplish so you might see if that page answers any of your questions. </p>
","4215791",1
594,56583967,2,56302743,2019-06-13 15:32:21,0,"<p>I believe TFRecord is planned to be released in 0.14 release. In the meantime, have you tried installing from source or building wheel from source and install?</p>

<p>Check out import_example_gen component on <a href=""https://github.com/tensorflow/tfx/blob/master/tfx/components/example_gen/import_example_gen/component.py"" rel=""nofollow noreferrer"">github</a> that has implementation for TFRecord input for TFX pipelines.</p>
","11199972",0
595,56602995,2,56469341,2019-06-14 17:47:36,1,"<p>Finally solved it. This contains some S3 specific code and S3 instance calls (the <code>!</code> commands) but you should pretty much be able to slice that out to run this.</p>

<pre class=""lang-py prettyprint-override""><code>#!python3
""""""
Assumes we've defined:

- A directory for our working files to live in, CONTAINER_DIR
- an arbitrary integer VERSION_INT
- We have established local and S3 paths for our model and their labels as variables, particularly `modelLabel` and `modelPath`
""""""

# Create a versioned path for the models to live in
# See https://stackoverflow.com/a/54014480/1877527
exportDir = os.path.join(CONTAINER_DIR, VERSION_INT)
if os.path.exists(exportDir):
    shutil.rmtree(exportDir)
os.mkdir(exportDir)
import tensorflow as tf
def load_graph(model_file, returnElements= None):
    """"""
    Code from v1.6.0 of Tensorflow's label_image.py example
    """"""
    graph = tf.Graph()
    graph_def = tf.GraphDef()
    with open(model_file, ""rb"") as f:
        graph_def.ParseFromString(f.read())
    returns = None
    with graph.as_default():
        returns = tf.import_graph_def(graph_def, return_elements= returnElements)
    if returnElements is None:
        return graph
    return graph, returns

# Add the serving metagraph tag
# We need the inputLayerName; in Inception we're feeding the resized tensor
# corresponding to resized_input_tensor_name
# May be able to get away with auto-determining this if not using Inception,
# but for Inception this is the 11th layer
inputLayerName = ""Mul:0""
# Load the graph
if inputLayerName is None:
    graph = load_graph(modelPath)
    inputTensor = None
else:
    graph, returns = load_graph(modelPath, returnElements= [inputLayerName])
    inputTensor = returns[0]
with tf.Session(graph= graph) as sess:
    # Read the layers
    try:
        from tensorflow.compat.v1.saved_model import simple_save
    except (ModuleNotFoundError, ImportError):
        from tensorflow.saved_model import simple_save
    with graph.as_default():
        layers = [n.name for n in graph.as_graph_def().node]
        outName = layers.pop() + "":0""
        if inputLayerName is None:
            inputLayerName = layers.pop(0) + "":0""
    print(""Checking outlayer"", outName)
    outLayer = tf.get_default_graph().get_tensor_by_name(outName)
    if inputTensor is None:
        print(""Checking inlayer"", inputLayerName)
        inputTensor = tf.get_default_graph().get_tensor_by_name(inputLayerName)
    inputs = {
        inputLayerName: inputTensor
    }
    outputs = {
        outName: outLayer
    }
    simple_save(sess, exportDir, inputs, outputs)
print(""Built a SavedModel"")
# Put the model label into the artifact dir
modelLabelDest = os.path.join(exportDir, ""saved_model.txt"")
!cp {modelLabel} {modelLabelDest}
# Prep for serving
import datetime as dt
modelArtifact = f""livemodel_{dt.datetime.now().timestamp()}.tar.gz""
# Copy the version directory here to package
!cp -R {exportDir} ./
# gziptar it
!tar -czvf {modelArtifact} {VERSION_INT}
# Shove it back to S3 for serving
!aws s3 cp {modelArtifact} {bucketPath}
shutil.rmtree(VERSION_INT) # Cleanup
shutil.rmtree(exportDir) # Cleanup
</code></pre>

<p>This model is then deployable as a Sagemaker endpoint (and any other Tensorflow serving environment)</p>
","1877527",0
596,56605034,2,56604989,2019-06-14 20:57:49,1,"<p>It is trying to check cache for packages. They were likely compiled in linux or some other OS and you are trying to install them in Windows.</p>

<p>This should fix your issue:</p>

<pre><code>pip install --no-cache-dir mlflow
</code></pre>
","8468264",2
597,56647327,2,56494402,2019-06-18 10:42:34,1,"<p>The problem is an upstream issue with kubeflow pipelines on microk8s, which is not working well together: <a href=""https://github.com/kubeflow/kubeflow/issues/2347"" rel=""nofollow noreferrer"">https://github.com/kubeflow/kubeflow/issues/2347</a></p>

<p>I switched to Minikube, on which Kubeflow pipelines is running fine.</p>
","8004157",0
598,56674565,2,56666667,2019-06-19 19:23:53,1,"<p>When deploying models, SageMaker looks up S3 to find your trained model artifact. It seems that there is no trained model artifact at <code>s3://tree/sklearn/output/model.tar.gz</code>. Make sure to persist your model artifact in your training script at the appropriate local location in docker which is <code>/opt/ml/model</code>.
for example, in your training script this could look like:</p>

<pre><code>joblib.dump(model, /opt/ml/model/mymodel.joblib)
</code></pre>

<p>After training, SageMaker will copy the content of <code>/opt/ml/model</code> to s3 at the <code>output_path</code> location.</p>

<p>If you deploy in the same session a <code>model.deploy()</code> will map automatically to the artifact path. If you want to deploy a model that you trained elsewhere, possibly during a different session or in a different hardware, you need to explicitly instantiate a model before deploying</p>

<pre><code>from sagemaker.sklearn.model import SKLearnModel

model = SKLearnModel(
    model_data='s3://...model.tar.gz',  # your artifact
    role=get_execution_role(),
    entry_point='script.py')  # script containing inference functions

model.deploy(
    instance_type='ml.m5.xlarge',
    initial_instance_count=1,
    endpoint_name='your_endpoint_name')
</code></pre>

<p>See more about Sklearn in SageMaker here <a href=""https://sagemaker.readthedocs.io/en/stable/using_sklearn.html"" rel=""nofollow noreferrer"">https://sagemaker.readthedocs.io/en/stable/using_sklearn.html</a></p>
","5331834",2
599,56700912,2,56700687,2019-06-21 09:46:43,3,"<p>Nope.</p>

<ol>
<li><p><code>conda</code> automatically installs dependencies of conda packages. These are resolved differently by <code>pip</code>, so you'd have to resolve the Anaconda dependency tree in your transformation script.</p></li>
<li><p>Many <code>conda</code> packages are non-Python. You couldn't install those dependencies with <code>pip</code> at all.</p></li>
<li><p>Some <code>conda</code> packages contain binaries that were compiled with the Anaconda compiler toolchain. Even if the corresponding <code>pip</code> package can compile such binaries on installation, it wouldn't be using the Anaconda toolchain. What you'd get would be fundamentally different from the corresponding <code>conda</code> package.</p></li>
<li><p>Some <code>conda</code> packages have fixes applied, which are missing from corresponding <code>pip</code> packages.</p></li>
</ol>

<p>I hope this is enough to convince you that your idea won't fly.</p>

<p>Installing Miniconda isn't really a big deal. Just do it :-)</p>
","11451509",0
600,56721869,2,56721821,2019-06-23 07:27:34,2,"<p>While doing the drudgery work of copy pasting each cell between the notebooks(my bad), I realized that we could just <strong>download the notebook as .ipynb file on Colab</strong> and <strong>upload on the Sagemaker notebook instance using the <code>Upload button</code></strong>.<a href=""https://i.stack.imgur.com/z5wHt.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/z5wHt.png"" alt=""enter image description here""></a></p>
","10547264",0
601,56738324,2,56728230,2019-06-24 14:04:57,1,"<p>RandomCutForest (RCF) is an unsupervised method primarily used for anomaly detection, while RandomForest (RF) is a supervised method that can be used for regression or classification. </p>

<p>For RCF, see documentation (<a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/randomcutforest.html"" rel=""nofollow noreferrer"">here</a>) and notebook example (<a href=""https://github.com/awslabs/amazon-sagemaker-examples/blob/master/introduction_to_amazon_algorithms/random_cut_forest/random_cut_forest.ipynb"" rel=""nofollow noreferrer"">here</a>)</p>
","11025360",0
602,56760939,2,56740609,2019-06-25 19:38:06,10,"<p><em>edit 03/30/2020: adding a link to the the <a href=""https://github.com/awslabs/amazon-sagemaker-examples/blob/master/sagemaker-python-sdk/scikit_learn_randomforest/Sklearn_on_SageMaker_end2end.ipynb"" rel=""noreferrer"">SageMaker Sklearn random forest demo</a></em></p>

<p><br/></p>

<p>in SageMaker you have 3 options to write scientific code:</p>

<ul>
<li><strong>Built-in algorithms</strong></li>
<li><strong>Open-source pre-written containers</strong> (available
for sklearn, tensorflow, pytorch, mxnet, chainer. Keras can be
written in the tensorflow and mxnet containers)</li>
<li><strong>Bring your own container</strong> (for R for example)</li>
</ul>

<p><strong>At the time of writing this post there is no random forest classifier nor regressor in the built-in library</strong>. There is an algorithm called <a href=""https://aws.amazon.com/blogs/machine-learning/use-the-built-in-amazon-sagemaker-random-cut-forest-algorithm-for-anomaly-detection/"" rel=""noreferrer"">Random Cut Forest</a> in the built-in library but it is an unsupervised algorithm for anomaly detection, a different use-case than the <a href=""https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html"" rel=""noreferrer"">scikit-learn random forest</a> used in a supervised fashion (also <a href=""https://stackoverflow.com/questions/56728230/aws-sagemaker-randomcutforest-rcf-vs-scikit-lean-randomforest-rf?noredirect=1&amp;lq=1"">answered in StackOverflow here</a>). But it is easy to use the open-source pre-written scikit-learn container to implement your own. There is a <a href=""https://github.com/awslabs/amazon-sagemaker-examples/blob/master/sagemaker-python-sdk/scikit_learn_randomforest/Sklearn_on_SageMaker_end2end.ipynb"" rel=""noreferrer"">demo showing how to use Sklearn's random forest in SageMaker</a>, with training orchestration bother from the high-level SDK and <code>boto3</code>. You can also use this other <a href=""https://github.com/awslabs/amazon-sagemaker-examples/blob/master/sagemaker-python-sdk/scikit_learn_iris/Scikit-learn%20Estimator%20Example%20With%20Batch%20Transform.ipynb"" rel=""noreferrer"">public sklearn-on-sagemaker demo</a> and change the model. A benefit of the pre-written containers over the ""Bring your own"" option is that the dockerfile is already written, and web serving stack too.</p>

<p>Regarding your surprise that Random Forest is not featured in the built-in algos, the library and its 18 algos already cover a rich set of use-cases. For example for supervised learning over structured data (the usual use-case for the random forest), if you want to stick to the built-ins, depending on your priorities (accuracy, inference latency, training scale, costs...) you can use SageMaker XGBoost (XGBoost has been winning tons of datamining competitions - every winning team in the top10 of the KDDcup 2015 used XGBoost <a href=""https://arxiv.org/pdf/1603.02754.pdf"" rel=""noreferrer"">according to the XGBoost paper</a> - and scales well) and linear learner, which is extremely fast at inference and can be trained at scale, in mini-batch fashion over GPU(s). <a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/fact-machines-howitworks.html"" rel=""noreferrer"">Factorization Machines</a> (linear + 2nd degree interaction with weights being column embedding dot-products) and <a href=""https://aws.amazon.com/blogs/machine-learning/amazon-sagemaker-supports-knn-classification-and-regression/"" rel=""noreferrer"">SageMaker kNN</a> are other options. Also, things are not frozen in stone, and the list of built-in algorithms is being improved fast.</p>
","5331834",1
603,56778165,2,56773989,2019-06-26 17:24:45,1,"<p>Inference usually refers to applying a learned transformation to input data. That learned transformation could be something else than a prediction (eg dim reduction, clustering, entity extraction etc). So calling that process a prediction would be a bit too restrictive in my opinion</p>
","5331834",0
604,56787734,2,56500704,2019-06-27 09:13:57,1,"<p>I don't think it is possible to do so using the high-level SageMaker Pyhton SDK. However, you should be able to do it by calling the CreateModel API using the low-level boto3 <a href=""https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/sagemaker.html#SageMaker.Client.create_model"" rel=""nofollow noreferrer"">https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/sagemaker.html#SageMaker.Client.create_model</a>. For your reference, below is an example snippet code on how to do it.</p>

<pre><code>%%time
import boto3
import time

sage = boto3.Session().client(service_name='sagemaker')

image_uri = '520713654638.dkr.ecr.us-east-1.amazonaws.com/sagemaker-pytorch:1.0.0-cpu-py3'
model_data ='s3://&lt;bucket&gt;/&lt;prefix&gt;/output/model.tar.gz'
source = 's3://&lt;bucket&gt;/&lt;prefix&gt;/sourcedir.tar.gz'
role = 'arn:aws:iam::xxxxxxxx:role/service-role/AmazonSageMaker-ExecutionRole-xxxxxx'

timestamp = time.strftime('-%Y-%m-%d-%H-%M-%S', time.gmtime())
model_name = 'my-pytorch-model' + timestamp

response = sage.create_model(
    ModelName=model_name,
    PrimaryContainer={
        'Image': image_uri,
        'ModelDataUrl': model_data,
        'Environment': { 'SAGEMAKER_CONTAINER_LOG_LEVEL':'20', 'SAGEMAKER_ENABLE_CLOUDWATCH_METRICS': 'False', 
                   'SAGEMAKER_PROGRAM': 'generate.py','SAGEMAKER_REGION': 'us-east-1','SAGEMAKER_SUBMIT_DIRECTORY': source}
         },
         ExecutionRoleArn=role
}
print(response)
</code></pre>

<p>If you get no error message, then the model will shows up in the SageMaker console</p>
","3458797",0
605,56791381,2,56740984,2019-06-27 12:48:06,1,"<p>When an Endpoint is created with SageMaker, the model data artifact is downloaded and uncompressed onto the associated disk/EBS volume on the instance. This volume size is proportional[1] to the instance type that you chose. </p>

<p>Please make sure that the instance type that you picked has enough disk space to accommodate the uncompressed .tar.gz file. (It does not matter if it is fully or partially loaded onto memory later on, it has to fit in the disk uncompressed).</p>

<p>[1] Volume size for instance types - <a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/host-instance-storage.html"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/sagemaker/latest/dg/host-instance-storage.html</a></p>
","6844318",1
606,56799946,2,56799763,2019-06-28 01:20:44,12,"<p>One way to solve this would be to save the CSV to the local storage on the SageMaker notebook instance, and then use the S3 API's via <code>boto3</code> to upload the file as an s3 object. 
<a href=""https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/s3.html#S3.Client.upload_file"" rel=""noreferrer"">S3 docs for <code>upload_file()</code> available here.</a></p>

<p>Note, you'll need to ensure that your SageMaker hosted notebook instance has proper <code>ReadWrite</code> permissions in its IAM role, otherwise you'll receive a permissions error.</p>

<pre><code># code you already have, saving the file locally to whatever directory you wish
file_name = ""mydata.csv"" 
df.to_csv(file_name)
</code></pre>

<pre><code># instantiate S3 client and upload to s3
import boto3

s3 = boto3.resource('s3')
s3.meta.client.upload_file(file_name, 'YOUR_S3_BUCKET_NAME', 'DESIRED_S3_OBJECT_NAME')
</code></pre>

<p>Alternatively, <code>upload_fileobj()</code> may help for parallelizing as a multi-part upload. </p>
","10885720",1
607,56808402,2,56647549,2019-06-28 14:02:54,0,"<p>Following the steps mentioned in the GitHub Issue <a href=""https://github.com/mlflow/mlflow/issues/1507"" rel=""nofollow noreferrer"">1507</a> (<a href=""https://github.com/mlflow/mlflow/issues/1507"" rel=""nofollow noreferrer"">https://github.com/mlflow/mlflow/issues/1507</a>) I was able to resolve this issue.</p>

<p>In reference to this post, the ""<strong>anaconda/bin/</strong>"" directory is never added to the list of environment variables i.e. PATH variable. In order to resolve this issue, add the ""<strong>else</strong>"" part of conda initialize code block from ~/.bashrc file to your PATH variable.</p>

<pre><code># &gt;&gt;&gt; conda initialize &gt;&gt;&gt;
# !! Contents within this block are managed by 'conda init' !!
__conda_setup=""$('/home/atulk/anaconda3/bin/conda' 'shell.bash' 'hook' 2&gt; /dev/null)""
if [ $? -eq 0 ]; then
    eval ""$__conda_setup""
else
    if [ -f ""/home/atulk/anaconda3/etc/profile.d/conda.sh"" ]; then
        . ""/home/atulk/anaconda3/etc/profile.d/conda.sh""
    else
        export PATH=""/home/atulk/anaconda3/bin:$PATH""
    fi
fi
unset __conda_setup
# &lt;&lt;&lt; conda initialize &lt;&lt;&lt;
</code></pre>

<p>In this case, I added <strong>export PATH=""/home/atulk/anaconda3/bin:$PATH""</strong> to the PATH variable. However, this is just a temporary fix until the issue is fixed in the project.</p>
","8531952",0
608,56820527,2,56818280,2019-06-29 20:02:17,1,"<p>At inference, 3 functions are used one after the other: <code>input_fn</code>, <code>predict_fn</code>, <code>output_fn</code>. They take default values, but you can override them to do desired custom actions. In your case, you can for example override the <code>predict_fn</code> to run the desired command. See more details here <a href=""https://sagemaker.readthedocs.io/en/stable/using_sklearn.html#deploying-scikit-learn-models"" rel=""nofollow noreferrer"">https://sagemaker.readthedocs.io/en/stable/using_sklearn.html#deploying-scikit-learn-models</a> </p>
","5331834",0
609,56820897,2,56818930,2019-06-29 21:10:49,2,"<p>@NShiny, there is a related ticket:</p>

<p><a href=""https://github.com/iterative/dvc/issues/1691"" rel=""nofollow noreferrer"">support push/pull/metrics/gc, etc across different commits</a>.</p>

<p>Please, give it a vote so that we know how to prioritize it.</p>

<p>As a workaround, I would recommend to run <a href=""https://dvc.org/doc/commands-reference/install"" rel=""nofollow noreferrer""><code>dvc install</code></a>. It installs a <code>pre-push</code> GIt hook and runs <code>dvc push</code> automatically:</p>

<pre><code>Git pre-push hook executes dvc push before git push to upload files and directories under DVC control to remote.
</code></pre>

<p>It means, though you need to run <code>git push</code> after every <code>git commit</code> :(</p>
","298182",2
610,56827363,2,56771758,2019-06-30 19:05:41,1,"<p>Except SageMaker XGBoost, SageMaker built-in algorithms are not designed to be used out of Amazon. That does not mean that it's impossible, for example you can find here and there snippets peeking inside model artifacts (eg for <a href=""https://aws.amazon.com/blogs/machine-learning/extending-amazon-sagemaker-factorization-machines-algorithm-to-predict-top-x-recommendations/"" rel=""nofollow noreferrer"">Factorization Machines</a> and <a href=""https://github.com/awslabs/amazon-sagemaker-examples/blob/master/introduction_to_applying_machine_learning/ntm_20newsgroups_topic_modeling/ntm_20newsgroups_topic_model.ipynb"" rel=""nofollow noreferrer"">Neural Topic Model</a>) but these things can be hacky and are usually not part of official service features. Regarding DeepAR specifically, the model was open-sourced couple weeks ago as part of <code>gluon-ts</code> python package (<a href=""https://aws.amazon.com/blogs/opensource/gluon-time-series-open-source-time-series-modeling-toolkit/"" rel=""nofollow noreferrer"">blog post</a>, <a href=""https://gluon-ts.mxnet.io/"" rel=""nofollow noreferrer"">code</a>) so if you develop a model specifically for your own hosting environment I'd recommend to use that gluon-ts code in the MXNet container, so that you'll be able to open and read the artifact out of SageMaker.</p>
","5331834",2
611,56852289,2,56848293,2019-07-02 12:17:30,2,"<blockquote>
  <p>What is a Random Seed Integer?</p>
</blockquote>

<p>Will not go into any details regarding what a random seed is in general; there is plenty of material available by a simple web search (see for example <a href=""https://stackoverflow.com/questions/22639587/random-seed-what-does-it-do"">this SO thread</a>).</p>

<p>Random seed serves just to initialize the (pseudo)random number generator, mainly in order to make ML examples reproducible.</p>

<blockquote>
  <p>How to carefully choose a Random Seed from range of integer values? What is the key or strategy to choose it?</p>
</blockquote>

<p>Arguably this is already answered implicitly above: you are simply not supposed to choose any particular random seed, and your results should be roughly the same across different random seeds.</p>

<blockquote>
  <p>Why does Random Seed significantly affect the ML Scoring, Prediction and Quality of the trained model?</p>
</blockquote>

<p>Now, to the heart of your question. The answer <em>here</em> (i.e. with the iris dataset) is the <strong>small-sample effects</strong>...</p>

<p>To start with, your reported results across different random seeds are not <em>that</em> different. Nevertheless, I agree that, at first sight, a difference in macro-average precision of 0.9 and 0.94 might <em>seem</em> large; but looking more closely it is revealed that the difference is really not an issue. Why?</p>

<p>Using the 20% of your (only) 150-samples dataset leaves you with only 30 samples in your test set (where the evaluation is performed); this is stratified, i.e. about 10 samples from each class. Now, for datasets of <em>that</em> small size, it is not difficult to imagine that a difference in the correct classification of <strong>only 1-2</strong> samples can have this apparent difference in the performance metrics reported.</p>

<p>Let's try to verify this in scikit-learn using a decision tree classifier (the essence of the issue does not depend on the specific framework or the ML algorithm used):</p>

<pre class=""lang-python prettyprint-override""><code>from sklearn.datasets import load_iris
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import confusion_matrix, classification_report
from sklearn.model_selection import train_test_split

X, y = load_iris(return_X_y=True)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=321, stratify=y)
dt = DecisionTreeClassifier()
dt.fit(X_train, y_train)
y_pred = dt.predict(X_test)
print(confusion_matrix(y_test, y_pred))
print(classification_report(y_test, y_pred))
</code></pre>

<p>Result:</p>

<pre><code>[[10  0  0]
 [ 0  9  1]
 [ 0  0 10]]
              precision    recall  f1-score   support

           0       1.00      1.00      1.00        10
           1       1.00      0.90      0.95        10
           2       0.91      1.00      0.95        10

   micro avg       0.97      0.97      0.97        30
   macro avg       0.97      0.97      0.97        30
weighted avg       0.97      0.97      0.97        30
</code></pre>

<p>Let's repeat the code above, changing only the <code>random_state</code> argument in <code>train_test_split</code>; for <code>random_state=123</code> we get:</p>

<pre><code>[[10  0  0]
 [ 0  7  3]
 [ 0  2  8]]
              precision    recall  f1-score   support

           0       1.00      1.00      1.00        10
           1       0.78      0.70      0.74        10
           2       0.73      0.80      0.76        10

   micro avg       0.83      0.83      0.83        30
   macro avg       0.84      0.83      0.83        30
weighted avg       0.84      0.83      0.83        30
</code></pre>

<p>while for <code>random_state=12345</code> we get:</p>

<pre><code>[[10  0  0]
 [ 0  8  2]
 [ 0  0 10]]
              precision    recall  f1-score   support

           0       1.00      1.00      1.00        10
           1       1.00      0.80      0.89        10
           2       0.83      1.00      0.91        10

   micro avg       0.93      0.93      0.93        30
   macro avg       0.94      0.93      0.93        30
weighted avg       0.94      0.93      0.93        30
</code></pre>

<p>Looking at the <em>absolute numbers</em> of the 3 confusion matrices (in <em>small samples</em>, percentages can be <strong>misleading</strong>), you should be able to convince yourself that the differences are not that big, and they can be arguably justified by the random element inherent in the whole procedure (here the exact split of the dataset into training and test).</p>

<p>Should your test set be significantly bigger, these discrepancies would be practically negligible... </p>

<p>A last notice; I have used the exact same seed numbers as you, but this does not actually mean anything, as in general the random number generators <em>across</em> platforms &amp; languages are not the same, hence the corresponding seeds are not actually compatible. See own answer in <a href=""https://stackoverflow.com/questions/52293899/are-random-seeds-compatible-between-systems"">Are random seeds compatible between systems?</a> for a demonstration.</p>
","4685471",0
612,56865835,2,56835306,2019-07-03 08:22:00,0,"<p>For anyone coming across this question, when creating a model, the 'Enable Network Isolation' property defaults to True.
From AWS docs:</p>
<blockquote>
<p>If you enable network isolation, the containers are not able to make any outbound network calls, even to other AWS services such as Amazon S3. Additionally, no AWS credentials are made available to the container runtime environment.</p>
</blockquote>
<p>So this property needs to be set to False in order to connect to any other AWS service.</p>
<p><img src=""https://i.stack.imgur.com/5qERm.jpg"" alt=""AWS Sagemaker UI Network Isolation set to False"" /></p>
","5981083",0
613,56873265,2,56863907,2019-07-03 15:14:42,0,"<p>If the file you are accessing is in the root directory of your s3 bucket, you can access the file like this:</p>

<pre><code>import pandas as pd

bucket='ltfs1'
data_key = 'data.csv'
data_location = 's3://{}/{}'.format(bucket, data_key)
training_data = pd.read_csv(data_location)
</code></pre>
","9801628",0
614,56887203,2,56851463,2019-07-04 11:42:23,1,"<p>For this, you completely drop the 'parameters' section as below:</p>

<pre><code>name: test

conda_env: conda.yaml

entry_points:
  main:
    command: ""python test.py""
</code></pre>

<p>(I thought I had tried it earlier but I was trying too many different ways to may be miss out on this one)</p>
","2987552",0
615,56929136,2,56558892,2019-07-08 06:01:29,2,"<p>Notebook VMs has own Jupyter environment and we don't need to use notebooks.azure.com. The former can be used in enterprise scenarios within the team to share the resources, and the latter is open, similar to google colab. When each user login to his notebook VM, there is a top level folder with his/her alias and under that all notebooks are stored. this is stored in an Azure storage and each user's notebook VM will mount same storage. Hence If I want to view other person \'s notebook, I need to navigate to his alias in the Jupyter nb in my nbvm</p>
","1835315",0
616,56934148,2,56558552,2019-07-08 11:40:40,1,"<p>Another solution is to pass <code>DataReference</code> as an input to your <code>PythonScriptStep</code>. </p>

<p>Then inside <code>transform.py</code> you're able to read this <code>DataReference</code> as a command line argument. </p>

<p>You can parse it and use it just as any regular path to save your vectorizer to.</p>

<p>E.g. you can:</p>

<pre><code>step_tfidf = PythonScriptStep(name = ""tfidf_step"",
                              script_name = ""transform.py"",
                              arguments = ['--input_data', blob_train_data, 
                                           '--output_folder', transformed_data,
                                           '--transformer_path', trained_transformer_path],
                              inputs = [blob_train_data, trained_transformer_path],
                              outputs = [transformed_data],
                              compute_target = aml_compute,
                              source_directory = project_folder,
                              runconfig = run_config,
                              allow_reuse = False)
</code></pre>

<p>Then inside your script (<code>transform.py</code> in the example above) you can e.g.:</p>

<pre><code>import argparse
import joblib as jbl
import os

from sklearn.feature_extraction.text import TfidfVectorizer

parser = argparse.ArgumentParser()
parser.add_argument('--transformer_path', dest=""transformer_path"", required=True)
args = parser.parse_args()

tfidf = ### HERE CREATE AND TRAIN YOUR VECTORIZER ###

vect_filename = os.path.join(args.transformer_path, 'my_vectorizer.jbl')

</code></pre>

<hr>

<p>EXTRA: The third way would be to just register the vectorizer as another model in your workspace. You can then use it exactly as any other registered model. (Though this option does not involve explicit writing to blob - as specified in the question above)</p>
","7757104",1
617,56935602,2,56927813,2019-07-08 13:05:49,1,"<p>if you used the built-in Tensorflow container, your model has been saved in Tensorflow Serving format, e.g.:</p>

<pre><code>$ tar tfz model.tar.gz
model/
model/1/
model/1/saved_model.pb
model/1/variables/
model/1/variables/variables.index
model/1/variables/variables.data-00000-of-00001
</code></pre>

<p>You can easily load it with Tensorflow Serving on your local machine, and send it samples to predict. More info at <a href=""https://www.tensorflow.org/tfx/guide/serving"" rel=""nofollow noreferrer"">https://www.tensorflow.org/tfx/guide/serving</a></p>
","4686192",2
618,56936097,2,56854482,2019-07-08 13:34:56,1,"<p>your Python code uses a SageMaker HTTPS endpoint for real-time prediction: although the SageMaker SDK is Python only, you can absolutely do the same thing with the (lower level) AWS SDK for Java.</p>

<p>Assuming you've already trained your model in SageMaker, you would:</p>

<ul>
<li>create an endpoint configuration, </li>
<li>create an endpoint,</li>
<li>invoke the endpoint.</li>
</ul>

<p>The corresponding APIs are detailed in:</p>

<ul>
<li><a href=""https://docs.aws.amazon.com/AWSJavaSDK/latest/javadoc/com/amazonaws/services/sagemaker/AmazonSageMaker.html"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/AWSJavaSDK/latest/javadoc/com/amazonaws/services/sagemaker/AmazonSageMaker.html</a></li>
<li><a href=""https://docs.aws.amazon.com/AWSJavaSDK/latest/javadoc/com/amazonaws/services/sagemakerruntime/AmazonSageMakerRuntime.html"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/AWSJavaSDK/latest/javadoc/com/amazonaws/services/sagemakerruntime/AmazonSageMakerRuntime.html</a></li>
</ul>
","4686192",0
619,56958837,2,56859324,2019-07-09 18:41:01,1,"<p>We are working on a similar project as you. My suggestion for your question is, you should select proper interval based on your requirement and need. For example, if you want to react based on the sentiment result of every post in a timely manner, you need to do a analysis every time you pull a new post. Also you can adjust your time interval of ""pull and analyze"" based on your business/ research need. IF you just want to train your model and predict something based on the data you get and you have no need for timely reaction, I think once a day is enough.</p>

<p>For our project, we are in the situation 1. So we will do a quick analysis when we receive any new post so that we can have a quick reaction.</p>

<p>Hope this helps.</p>
","9598801",0
620,56970043,2,56969859,2019-07-10 11:44:36,3,"<p>please see <a href=""https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/s3.html#S3.Bucket.download_file"" rel=""nofollow noreferrer"">https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/s3.html#S3.Bucket.download_file</a></p>

<p>by the doc, first argument is file key, second argument is path for local file:</p>

<pre><code>s3 = boto3.resource('s3')
bucketname = 'vemyone'

s3.Bucket(bucketname).download_file(train_fns[0], '/path/to/local/file')
</code></pre>
","1668328",8
621,56972272,2,56967364,2019-07-10 13:54:24,0,"<p>My recommendation would be to build such a tool yourself as its not too complicated,
have a wrapper script to spark submit which logs the usage in a DB and after the spark job finishes the wrapper will know to release information. could be done really easily.
In addition you can even block new spark submits if your team already asked for too much information.</p>

<p>And as you build it your self its really flexible as you can even create ""sub teams"" or anything you want.</p>
","3978141",1
622,56972899,2,56952741,2019-07-10 14:27:23,3,"<p>With the exception of XGBoost, built-in algorithms are implemented with Apache MXNet, so simply extract the model from the .tar.gz file and load it with MXNet: load_checkpoint() is the API to use.</p>

<p>XGBoost models are just pickled objects. Unpickle and load in sklearn:</p>

<pre><code>$ python3
&gt;&gt;&gt; import sklearn, pickle
&gt;&gt;&gt; model = pickle.load(open(""xgboost-model"", ""rb""))
&gt;&gt;&gt; type(model)
&lt;class 'xgboost.core.Booster'&gt;
</code></pre>

<p>Models trained with built-in library (Tensorflow, MXNet, Pytorch, etc.) are vanilla models that can be loaded as-is with the correct library.</p>

<p>Hope this helps.</p>
","4686192",0
623,56979812,2,56957206,2019-07-10 23:27:53,0,"<p>Your linked <code>notebook</code> is creating a 'blank' sparse matrix, and setting selected elements from data it reads from a <code>csv</code>.</p>

<p>A simple example of this:</p>

<pre><code>In [565]: from scipy import sparse                                                                           
In [566]: M = sparse.lil_matrix((10,5), dtype=float)                                                         
In [567]: M                                                                                                  
Out[567]: 
&lt;10x5 sparse matrix of type '&lt;class 'numpy.float64'&gt;'
    with 0 stored elements in LInked List format&gt;
</code></pre>

<p>Note that I use <code>(10,5)</code> to specify the matrix shape.  The () matter!  That's why I stressed reading the <code>docs</code>.  In the link the relevant line is:</p>

<pre><code>X = lil_matrix((lines, columns)).astype('float32')
</code></pre>

<p>Now I can set a couple elements, just as I would an dense array:</p>

<pre><code>In [568]: M[1,2] = 12.3                                                                                      
In [569]: M[3,1] = 1.1                                                                                       
In [570]: M                                                                                                  
Out[570]: 
&lt;10x5 sparse matrix of type '&lt;class 'numpy.float64'&gt;'
    with 2 stored elements in LInked List format&gt;
</code></pre>

<p>I can use <code>toarray</code> to display the matrix as a dense array (don't try this with large dimensions).</p>

<pre><code>In [571]: M.toarray()                                                                                        
Out[571]: 
array([[ 0. ,  0. ,  0. ,  0. ,  0. ],
       [ 0. ,  0. , 12.3,  0. ,  0. ],
       [ 0. ,  0. ,  0. ,  0. ,  0. ],
       [ 0. ,  1.1,  0. ,  0. ,  0. ],
       [ 0. ,  0. ,  0. ,  0. ,  0. ],
       [ 0. ,  0. ,  0. ,  0. ,  0. ],
       [ 0. ,  0. ,  0. ,  0. ,  0. ],
       [ 0. ,  0. ,  0. ,  0. ,  0. ],
       [ 0. ,  0. ,  0. ,  0. ,  0. ],
       [ 0. ,  0. ,  0. ,  0. ,  0. ]])
</code></pre>

<hr>

<p>If I omit the (), it makes a (1,1) matrix with just one element, the first number.</p>

<pre><code>In [572]: sparse.lil_matrix(10,5)                                                                            
Out[572]: 
&lt;1x1 sparse matrix of type '&lt;class 'numpy.int64'&gt;'
    with 1 stored elements in LInked List format&gt;
In [573]: _.A                                                                                                
Out[573]: array([[10]], dtype=int64)
</code></pre>

<p>Look again at your code.  You set the <code>X</code> value twice, once it is a dataframe.  The second time is this bad <code>lil</code> initialization.  The second time does not make use of the first <code>X</code>.</p>

<pre><code>X=data.drop([data.columns[0]],axis='columns')
...
X=lil_matrix(100000,15).astype('float32')
</code></pre>
","901925",3
624,57002829,2,56984854,2019-07-12 08:12:07,1,"<p>Just solved the issue: for some reason, waitress was not installed in the running environment. After installing it, everything seems working fine with the solution #1080 linked above in the question.</p>
","9961790",1
625,57005003,2,56854066,2019-07-12 10:20:02,0,"<p>Your solution number 3, </p>

<blockquote>
  <p>""Serve the model with the standard Tensorflow Modelserver and build a
  postprocessing service to restructure resp. filter the result in the
  predefined way.""</p>
</blockquote>

<p>should be the best one.</p>

<p>Links and Code Snippets: If we consider the example of MNIST using TF Serving,  the link for Saved Model is, <a href=""https://github.com/tensorflow/serving/blob/87e32bb386f156fe208df633c1a7f489b57464e1/tensorflow_serving/example/mnist_saved_model.py"" rel=""nofollow noreferrer"">https://github.com/tensorflow/serving/blob/87e32bb386f156fe208df633c1a7f489b57464e1/tensorflow_serving/example/mnist_saved_model.py</a>, </p>

<p>and the link for Client code is <a href=""https://github.com/tensorflow/serving/blob/87e32bb386f156fe208df633c1a7f489b57464e1/tensorflow_serving/example/mnist_client.py"" rel=""nofollow noreferrer"">https://github.com/tensorflow/serving/blob/87e32bb386f156fe208df633c1a7f489b57464e1/tensorflow_serving/example/mnist_client.py</a>.</p>

<p>If we want values of top-n predictions, we can tweak the code of the function, <code>_create_rpc_callback</code> in the Client file as shown below.</p>

<pre><code>def _create_rpc_callback(label, result_counter):
  """"""Creates RPC callback function.

  Args:
    label: The correct label for the predicted example.
    result_counter: Counter for the prediction result.
  Returns:
    The callback function.
  """"""
  def _callback(result_future):
    """"""Callback function.

    Calculates the statistics for the prediction result.

    Args:
      result_future: Result future of the RPC.
    """"""
    exception = result_future.exception()
    if exception:
      result_counter.inc_error()
      print(exception)
    else:
      sys.stdout.write('.')
      sys.stdout.flush()
      response = numpy.array(result_future.result().outputs['scores'].float_val)
      print('Top 4 responses = ', response[0:4]) 
</code></pre>

<p>The <code>print</code> statement in the last line will print Top-4 Predictions.</p>
","user11530462",0
626,57018959,2,57017876,2019-07-13 11:59:28,4,"<p>It really depends on the type of model that you are using. In many cases, the model inference is getting a data point (similar to the data points you trained it with) and the model will generate a prediction to that requested data point. In such cases, you need to host the model somewhere in the cloud or on the edge. </p>

<p>However, Prophet is often generating the predictions for the future as part of the training of the model. In this case, you only need to serve the predictions that were already calculated, and you can serve them as a CSV file from S3, or as lookup values from a DynamoDB or other lookup data stores. </p>
","179529",0
627,57047661,2,57047579,2019-07-15 21:55:53,4,"<p>I tried to do this using the <a href=""https://kubeflow-pipelines.readthedocs.io/en/latest/source/kfp.dsl.html"" rel=""nofollow noreferrer"">Python's DSL</a> but seems that isn't possible right now. </p>

<p>The only option that I found is to use the method that they used in <a href=""https://github.com/kubeflow/pipelines/blob/cce52f4dfbac09b73e6af617fcb82b2df35ba16c/samples/kubeflow-tf/kubeflow-training-classification.py#L44"" rel=""nofollow noreferrer"">this sample code</a>. You basically declare a string containing <code>{{workflow.uid}}</code>. It will be replaced with the actual value during execution time.</p>

<p>You can also do this in order to get the pod name, it would be <code>{{pod.name}}</code>.</p>
","4975157",3
628,57085164,2,57074382,2019-07-17 23:22:54,5,"<p>I don't think you can ssh to notebook instances. You can either use open them from the console, or grab the url with an API, re: <a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/howitworks-access-ws.html"" rel=""noreferrer"">https://docs.aws.amazon.com/sagemaker/latest/dg/howitworks-access-ws.html</a></p>

<p>If you need a terminal, then you can open one from Jupyter.</p>
","4686192",1
629,57091955,2,57032981,2019-07-18 10:01:56,1,"<p>I believe this is the problem:</p>

<pre><code>train_instance_count=len(train_features)
</code></pre>

<p>This parameter is about infrastructure (how many SageMaker instances you want to train on), not about features. You should set it to 1.</p>

<pre><code>import sagemaker
from sagemaker import LinearLearner
import numpy as np

model=LinearLearner(role=sagemaker.get_execution_role(),
                             train_instance_count=1,
                             train_instance_type='ml.t2.medium',
                             predictor_type='binary_classifier')

numpy_array = np.array(...)

record=model.record_set(numpy_array)
# This takes &lt;100 ms on my t3 notebook instance

print(record)

(&lt;class 'sagemaker.amazon.amazon_estimator.RecordSet'&gt;, {'s3_data':
's3://sagemaker-eu-west-1-123456789012/sagemaker-record-sets/LinearLearner-
2019-07-18-09-48-21-639/.amazon.manifest', 'feature_dim': 30, 'num_records': 2,
's3_data_type': 'ManifestFile', 'channel': 'train'})
</code></pre>

<p>The manifest file lists the protobuf-encoded file(s):</p>

<pre><code>[{""prefix"": ""s3://sagemaker-eu-west-1-123456789012/sagemaker-record-sets/LinearLearner-2019-07-18-09-48-21-639/""}, ""matrix_0.pbr""]
</code></pre>

<p>You can now use it for the training channel when you call fit(), re: <a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/API_S3DataSource.html"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/sagemaker/latest/dg/API_S3DataSource.html</a></p>
","4686192",2
630,57128286,2,57126765,2019-07-20 20:22:10,3,"<p>S3 is not a file system and you can't just change directory to it. Many of the libraries such as Pandas can read and write directly from S3, but it requires specific libraries to make it work. </p>

<p>The simplest option is to copy the files from S3 to the local drive (EBS or EFS) of the notebook instance:</p>

<pre><code>aws s3 cp s3://bucket_name/some_file.csv data/
</code></pre>

<p>The AWS CLI is already installed on the notebook instance, and if you gave the right IAM permission when you launched your notebook instance, then the copy command should work.</p>
","179529",0
631,57160292,2,57147396,2019-07-23 08:55:05,1,"<p>I found the error , it was simply because of the request content-type,it had to be application/json instead of application/json;utf-8</p>
","5059026",0
632,57169027,2,57164290,2019-07-23 17:01:08,2,"<p>I'm guessing that you used your own TF container, not the SageMaker one at <a href=""https://github.com/aws/sagemaker-tensorflow-container"" rel=""nofollow noreferrer"">https://github.com/aws/sagemaker-tensorflow-container</a></p>

<p>If that's the case, your container is missing the support code needed to use the TensorFlow estimator ('tf_container' package).</p>

<p>The solution is to start from the SageMaker container, customize it, push it back to ECR and pass the image name to the SageMaker estimator with the 'image_name' parameter.</p>
","4686192",0
633,57169664,2,57127689,2019-07-23 17:48:18,2,"<p>v0.6 supports multi-user profiles which allow non-admin users to manage resources in their own namespaces, but the initial installation still requires cluster admin privileges.</p>

<p>Also replied on the GitHub issue.</p>
","3893285",0
634,57185418,2,57172147,2019-07-24 14:34:18,6,"<p>I suppose you started from this example? <a href=""https://github.com/awslabs/amazon-sagemaker-examples/tree/master/sagemaker-python-sdk/tensorflow_serving_container"" rel=""noreferrer"">https://github.com/awslabs/amazon-sagemaker-examples/tree/master/sagemaker-python-sdk/tensorflow_serving_container</a></p>

<p>It looks like you're not saving the TF Serving bundle properly: the model version number is missing, because of this line:</p>

<pre><code>model_path = 'encoder_model/' + model_name
</code></pre>

<p>Replacing it with this should fix your problem:</p>

<pre><code>model_path = '{}/{}/00000001'.format('encoder_model/', model_name)
</code></pre>

<p>Your model artefact should look like this (I used the model in the notebook above):</p>

<pre><code>mobilenet/
mobilenet/mobilenet_v2_140_224/
mobilenet/mobilenet_v2_140_224/00000001/
mobilenet/mobilenet_v2_140_224/00000001/saved_model.pb
mobilenet/mobilenet_v2_140_224/00000001/variables/
mobilenet/mobilenet_v2_140_224/00000001/variables/variables.data-00000-of-00001
mobilenet/mobilenet_v2_140_224/00000001/variables/variables.index
</code></pre>

<p>Then, upload to S3 and deploy.</p>
","4686192",4
635,57188231,2,57132106,2019-07-24 17:21:12,2,"<p>From <a href=""https://dvc.org/doc/commands-reference/run"" rel=""nofollow noreferrer"">docs</a>:</p>

<blockquote>
  <p>Use single quotes ' instead of "" to wrap the command if there are environment variables in it, that you want to be evaluated dynamically. E.g. dvc run -d script.sh './myscript.sh $MYENVVAR'</p>
</blockquote>
","2628602",2
636,57225350,2,57213293,2019-07-26 18:27:55,2,"<p>I create a training job that you have specified and had got the same error. To resolve the error <strong>ClientError: lst should at least has three parts, but only has 1 parts for</strong>, make sure that the file <strong>.lst</strong> is well-formatted with tab-separated like this:</p>

<pre><code>5      1   iphone/iphone7_1.jpg
1000   0   iphone/iphone6_1.jpg
22     1   iphone/iphone7_2.jpg
</code></pre>

<p>I used  <code>nano</code> on <strong>MAC OS X</strong> to validate the tab-separated format.</p>
","6393053",9
637,57240769,2,57212696,2019-07-28 12:10:13,0,"<p>this is as simple as creating a Jupyter notebook using the 'conda_mxnet_p36' kernel, and adding a cell containing:</p>

<pre><code>!pip install gluonnlp
</code></pre>
","4686192",5
638,57243397,2,57242692,2019-07-28 17:33:06,0,"<p>The definition of RMSE is:</p>

<p><a href=""https://i.stack.imgur.com/YrYZx.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/YrYZx.jpg"" alt=""enter image description here""></a></p>

<p>The scale of this value directly depends on the scale on predictions and actuals, so it's quite normal that you get a higher RMSE value when you don't normalize the dataset.</p>

<p>This is why normalization is important, as it lets us compare error metrics across models and datasets.</p>
","4686192",0
639,57244037,2,57243583,2019-07-28 18:55:23,2,"<p>the manifest looks fine, but based on the error message, it looks like you haven't set the right data format for you S3 data. It's expecting protobuf, which is the default format :)</p>

<p>You have to set the CSV data format explicitly. See <a href=""https://sagemaker.readthedocs.io/en/stable/session.html#sagemaker.session.s3_input"" rel=""nofollow noreferrer"">https://sagemaker.readthedocs.io/en/stable/session.html#sagemaker.session.s3_input</a>. </p>

<p>It should look something like this:</p>

<pre><code>s3_input_train = sagemaker.s3_input(
  s3_data='s3://{}/{}/train/manifest_file'.format(bucket, prefix),    
  s3_data_type='ManifestFile',
  content_type='csv')

...

kmeans_estimator = sagemaker.estimator.Estimator(kmeans_image, ...)
kmeans_estimator.set_hyperparameters(...)

s3_data = {'train': s3_input_train}
kmeans_estimator.fit(s3_data)
</code></pre>

<p>Please note the KMeans estimator in the SDK only supports protobuf, see <a href=""https://sagemaker.readthedocs.io/en/stable/kmeans.html"" rel=""nofollow noreferrer"">https://sagemaker.readthedocs.io/en/stable/kmeans.html</a></p>
","4686192",0
640,57247935,2,57199472,2019-07-29 06:02:35,9,"<p>It is possible to edit run names from the MLflow UI. First, click into the run whose name you'd like to edit.</p>

<p>Then, edit the run name by clicking the dropdown next the run name (i.e. the downward-pointing caret in this image):</p>

<p><a href=""https://i.stack.imgur.com/sl6Qs.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/sl6Qs.png"" alt=""Rename run dropdown""></a></p>

<p>There's currently no stable public API for setting run names - however, you can programmatically set/edit run names by setting the tag with key <code>mlflow.runName</code>, which is what the UI (currently) does under the hood.</p>
","3151283",1
641,57273227,2,57010184,2019-07-30 13:44:54,0,"<p>The problem occurred was that the file sent for predictions was csv but the XGBoost settings were set to receive libsvm.</p>
","4951603",0
642,57324649,2,57273357,2019-08-02 10:28:09,0,"<p>I found the problem, I needed to add the ARN of the role used by my Lamda function as a Trusted Entity on the Role used for the Sagemaker Labeling Job.</p>

<p>I just went to <code>Roles &gt; MySagemakerExecutionRole &gt; Trust Relationships</code> and added:</p>

<pre><code>{
  ""Version"": ""2012-10-17"",
  ""Statement"": [
    {
      ""Effect"": ""Allow"",
      ""Principal"": {
        ""AWS"": [
          ""arn:aws:iam::xxxxxxxxx:role/My-Lambda-Role"",
           ...
        ],
        ""Service"": [
          ""lambda.amazonaws.com"",
          ""sagemaker.amazonaws.com"",
           ...
        ]
      },
      ""Action"": ""sts:AssumeRole""
    }
  ]
}
</code></pre>

<p>This made it work for me.</p>
","9742287",0
643,57328572,2,57064819,2019-08-02 14:32:43,1,"<p>GitLab actually does exactly what Kubeflow Pipelines out of the box, it is similar Yaml to CircleCI or TravisCI. I ended up using that for an alternative to Kubeflow Pipelines.</p>

<p>Regarding Kubeflow...
After experimenting with Kubeflow at version 0.5 and 0.6 our feeling was that is quite unstable yet. Installation never went smooth neither into MiniKube ( local K8S ) not into the AWS EKS. For MiniKube the install scripts from the documentation are broken and you will be able to see many people having issues and editing the install scripts by hand ( which is what I had to do to get it install properly ). On EKS we were not able to install 0.5 and had to install a much older version. Kubeflow wants to manage worker nodes in a particular manner and our security policies to not allow that, only in an order version you can overwrite that option.</p>

<p>Kubeflow is also switching to Kuztomize and it is not stable yet, so if you use it now you will be using Ksonnet which is not supported anymore and you will learn a tool that you will through out the window sooner or later.</p>

<p>All in all, should wait for version 1.0 but Gitlab does an awesome job as an alternative to kubeflow Pipelines.</p>

<p>Hope this help other who have the same thoughts</p>
","553984",0
644,57353082,2,57347278,2019-08-05 06:16:50,3,"<p>You are charged for the S3 GET calls that you do similarly to what you would be charged if you used the FILE option of the training. However, these charges are usually marginal compared to the alternatives. </p>

<p>When you are using the FILE mode, you need to pay for the local EBS on the instances, and for the extra time that your instances are up and only copying the data from S3. If you are running multiple epochs, you will not benefit much from the PIPE mode, however, when you have so much data (6.3 TB), you don't really need to run multiple epochs. </p>

<p>The best usage of PIPE mode is when you can use a <strong>single pass</strong> over the data. In the era of big data, this is a better model of operation, as you can't retrain your models often. In SageMaker, you can point to your ""old"" model in the ""model"" channel, and your ""new"" data in the ""train"" channel and benefit from the PIPE mode to the maximum. </p>
","179529",0
645,57389110,2,57386269,2019-08-07 07:31:15,1,"<p>On the bottom of the page that you have linked above, there is a link:</p>

<p><a href=""https://learn.microsoft.com/en-us/azure/machine-learning/service/how-to-consume-web-service"" rel=""nofollow noreferrer"">Learn how to consume a web service.</a></p>

<p>This is exactly on that topic on how to use the deployed web service for scoring (sending an input and getting an output).</p>
","1537195",0
646,57390387,2,57379173,2019-08-07 08:46:16,1,"<p>SageMaker is automating the deployment of the Docker image with your code using the convention of channel->local-folder. Everything that you define with a channel in your <a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/your-algorithms-training-algo-running-container.html#your-algorithms-training-algo-running-container-inputdataconfig"" rel=""nofollow noreferrer"">input data configuration</a>, will be copied to the local Docker file system under <em>/opt/ml/</em> folder, using the name of the channel as the name of the sub-folder.</p>

<pre><code>{
""train"" : {""ContentType"":  ""trainingContentType"", 
           ""TrainingInputMode"": ""File"", 
           ""S3DistributionType"": ""FullyReplicated"", 
           ""RecordWrapperType"": ""None""},
""evaluation"" : {""ContentType"":  ""evalContentType"", 
                ""TrainingInputMode"": ""File"", 
                ""S3DistributionType"": ""FullyReplicated"", 
                ""RecordWrapperType"": ""None""},
""validation"" : {""TrainingInputMode"": ""File"", 
                ""S3DistributionType"": ""FullyReplicated"", 
                ""RecordWrapperType"": ""None""}
} 
</code></pre>

<p>to:</p>

<pre><code>/opt/ml/input/data/training
/opt/ml/input/data/validation
/opt/ml/input/data/testing
</code></pre>
","179529",0
647,57402921,2,57397150,2019-08-07 22:16:53,0,"<p>Unfortunately this is not supported today, but ARM support is in our roadmap</p>
","11898763",0
648,57418847,2,56455761,2019-08-08 18:37:38,2,"<p>You can pass in the DataReference object you created as the input to your training product (scriptrun/estimator/hyperdrive/pipeline). Then in your training script, you can access the mounted path via argument.
full tutorial: <a href=""https://learn.microsoft.com/en-us/azure/machine-learning/service/tutorial-train-models-with-aml"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/machine-learning/service/tutorial-train-models-with-aml</a></p>
","11903935",1
649,57418981,2,57405022,2019-08-08 18:48:36,3,"<p>All AWS Services that uses Cloudwatch you will be charged according to Cloudwatch log pricing.</p>
","6393053",2
650,57421284,2,57403281,2019-08-08 22:14:34,0,"<p>I can now answer my own question thanks to <a href=""https://twitter.com/bryan_hepworth/status/1159432174225055749"" rel=""nofollow noreferrer"">a hint on Twitter</a> from @bryan_hepworth.</p>

<p>The R packages were installed correctly, but not in the standard library location. So when a function from <code>sentimentr</code> runs, R tries to load the dependency package <code>textshape</code>:</p>

<pre><code>library(textshape)
</code></pre>

<p>Which of course does not exist <em>in the standard location</em> as Azure ML does not support it.</p>

<p>The solution is to load <code>textshape</code> explicitly from its installed location:</p>

<pre><code>library(textshape, lib.loc = ""."")
</code></pre>

<p>So the solution is: explicitly load packages that you installed at the start of your R code, rather than letting R try to load them as dependencies, which will fail.</p>
","89482",0
651,57447588,2,57436140,2019-08-11 05:38:27,3,"<p>AzureML should actually cache your docker image once it was created. The service will hash the base docker info and the contents of the conda.yaml file and will use that as the hash key -- unless you change any of that information, the docker should come from the ACR. </p>

<p>As for the custom docker usage, did you set the parameter <code>user_managed=True</code>? Otherwise, AzureML will consider your docker to be a base image on top of which it will create the conda environment per your yaml file.<br>
There is an example of how to use a custom docker image in this notebook:
<a href=""https://github.com/Azure/MachineLearningNotebooks/blob/4170a394edd36413edebdbab347afb0d833c94ee/how-to-use-azureml/training-with-deep-learning/how-to-use-estimator/how-to-use-estimator.ipynb"" rel=""nofollow noreferrer"">https://github.com/Azure/MachineLearningNotebooks/blob/4170a394edd36413edebdbab347afb0d833c94ee/how-to-use-azureml/training-with-deep-learning/how-to-use-estimator/how-to-use-estimator.ipynb</a></p>
","8821969",2
652,57479572,2,57471129,2019-08-13 14:10:12,1,"<p>Instead of base Estimator, you can use the Tensorflow Estimator with Keras and other libraries layered on top. That way you don't have to worry about setting up and configuring the GPU libraries, as the Tensorflow Estimator uses a Docker image with GPU libraries pre-configured. </p>

<p>See here for documentation:</p>

<p><a href=""https://learn.microsoft.com/en-us/python/api/azureml-train-core/azureml.train.dnn.tensorflow?view=azure-ml-py"" rel=""nofollow noreferrer"">API Reference</a> You can use <code>conda_packages</code> argument to specify additional libraries. Also set argument <code>use_gpu = True</code>.</p>

<p><a href=""https://github.com/Azure/MachineLearningNotebooks/blob/master/how-to-use-azureml/training-with-deep-learning/train-hyperparameter-tune-deploy-with-keras/train-hyperparameter-tune-deploy-with-keras.ipynb"" rel=""nofollow noreferrer"">Example Notebook</a></p>
","5784983",2
653,57484823,2,57483440,2019-08-13 20:21:47,0,"<p>It looks like this can be accomplished using <code>Flask-APScheduler</code> as follows:</p>

<p><code>pip install flask_apscheduler</code></p>

<p>server.py</p>

<pre><code>from flask import Flask
from apscheduler.schedulers.background import BackgroundScheduler
import atexit

app = Flask(__name__)

message = True

def update():
  global message
  message = not message

scheduler = BackgroundScheduler()
scheduler.add_job(func=update,trigger=""interval"",seconds=10)
scheduler.start()
# shut down the scheduler when exiting the app
atexit.register(scheduler.shutdown)

@app.route(""/"")
def hello():
  global message
  return message

if __name__ == ""__main__"":
  app.run()
</code></pre>

<p>Then launching as usual with 
<code>gunicorn -k gevent -b unix:/tmp/gunicorn.sock -w 4 server:app</code></p>
","4138583",0
654,57484874,2,57431340,2019-08-13 20:26:43,1,"<p>AzureML HyperDrive is a black box optimizer, meaning that it will just run your code with different parameter combinations based on the configuration you chose. At the same time, it supports Random and Bayesian sampling and has different policies for early stopping (see here for relevant <a href=""https://learn.microsoft.com/en-us/azure/machine-learning/service/how-to-tune-hyperparameters"" rel=""nofollow noreferrer"">docs</a> and here for an <a href=""https://github.com/Azure/MachineLearningNotebooks/blob/master/how-to-use-azureml/training-with-deep-learning/train-hyperparameter-tune-deploy-with-tensorflow/train-hyperparameter-tune-deploy-with-tensorflow.ipynb"" rel=""nofollow noreferrer"">example</a> -- HyperDrive is towards the end of the notebook).</p>

<p>The only thing that your model/script/training needs to adhere to is to be launched from a script that takes <code>--param</code> style parameters. As long as that holds you could optimize the parameters for each of your models individually and then tune the meta-model, or you could tune them all in one run. It will mainly depend on the size of the parameter space and the amount of compute you want to use (or pay for).</p>
","8821969",2
655,57503831,2,57500954,2019-08-15 01:23:49,5,"<p>For automatically delete all these files in blob storage, you can use the <a href=""https://learn.microsoft.com/en-us/azure/storage/blobs/storage-lifecycle-management-concepts#azure-portal-list-view"" rel=""nofollow noreferrer"">Lifecycle Management</a> of blob storage.</p>
<p>It's easy to set up a rule and filter, after the rule is set up, all the files will be deleted as per the rule you defined.</p>
<p>Simple steps:</p>
<p>1.Nav to azure portal -&gt; your storage account -&gt; Blob services -&gt; Lifecycle Management, then click &quot;Add rule&quot;.</p>
<p><a href=""https://i.stack.imgur.com/n2Wne.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/n2Wne.jpg"" alt=""enter image description here"" /></a></p>
<p>2.In the &quot;Action set&quot; tab, select Delete blob and fill in the textbox; Then in &quot;Filter set&quot; tab, select a path.</p>
<p><a href=""https://i.stack.imgur.com/a2cdQ.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/a2cdQ.jpg"" alt=""enter image description here"" /></a></p>
<p>For more details/instructions, please follow this <a href=""https://learn.microsoft.com/en-us/azure/storage/blobs/storage-lifecycle-management-concepts#azure-portal-list-view"" rel=""nofollow noreferrer"">article</a>.</p>
<p>Also note that the rule runs once per day, and for the first time, it may take 24 hours to take effect.</p>
","10185816",5
656,57511236,2,57488496,2019-08-15 14:31:22,1,"<p>In a SeldonDeployment resource definition you have a graph section. The top-level element is the first step and it contains children for a next step. Here is a snippet <a href=""https://github.com/SeldonIO/seldon-core/issues/772"" rel=""nofollow noreferrer"">from an example</a> showing a transformer being used to transform a request before passing it on to a model:</p>

<pre><code>    graph:
      children:
        - children: []
          implementation: XGBOOST_SERVER
          name: model
          modelUri: s3://model-image
          type: MODEL
      endpoint:
        type: REST
      name: feature-transformer
      type: TRANSFORMER
</code></pre>

<p>There's an <a href=""https://docs.seldon.io/projects/seldon-core/en/latest/examples/kubeflow_seldon_e2e_pipeline.html#Breaking-down-the-code"" rel=""nofollow noreferrer"">example in the seldon documentation</a> where steps such as clean and tokenize are used to perform inference for natural language processing. That example has multiple steps as type MODEL. There's also <a href=""https://seldondev.slack.com/archives/C8Y9A8G0Y/p1560761929110900?thread_ts=1560423848.095200&amp;cid=C8Y9A8G0Y"" rel=""nofollow noreferrer"">discussion on this in the seldon slack channel</a>.</p>
","9705485",1
657,57525918,2,57479389,2019-08-16 13:56:13,3,"<p>Documentation of <code>pyathena</code> is not super extensive, but after looking into source code we can see that <code>connect</code> simply creates instance of <code>Connection</code> class.</p>

<pre class=""lang-py prettyprint-override""><code>def connect(*args, **kwargs):
    from pyathena.connection import Connection
    return Connection(*args, **kwargs)
</code></pre>

<p>Now, after looking into signature of <code>Connection.__init__</code> on <a href=""https://github.com/laughingman7743/PyAthena/blob/master/pyathena/connection.py"" rel=""nofollow noreferrer"">GitHub</a> we can see parameter <code>work_group=None</code> which name in the same way as one of the parameters for <code>start_query_execution</code> from the <a href=""https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/athena.html#Athena.Client.start_query_execution"" rel=""nofollow noreferrer"">official</a> AWS Python API <code>boto3</code>. Here is what their documentation say about it:</p>

<blockquote>
  <p>WorkGroup (string) -- The name of the workgroup in which the query is being started.</p>
</blockquote>

<p>After following through usages and imports in <code>Connection</code> we endup with <a href=""https://github.com/laughingman7743/PyAthena/blob/master/pyathena/common.py"" rel=""nofollow noreferrer"">BaseCursor</a> class that under the hood makes a call to <code>start_query_execution</code> while unpacking a dictionary with parameters assembled by <code>BaseCursor._build_start_query_execution_request</code> method. That is excatly where we can see familar syntax for submitting queries to AWS Athena, in particular the following part:</p>

<pre class=""lang-py prettyprint-override""><code>if self._work_group or work_group:
    request.update({
        'WorkGroup': work_group if work_group else self._work_group
    })
</code></pre>

<p>So this should do a trick for your case:</p>

<pre class=""lang-py prettyprint-override""><code>import pandas as pd
from pyathena import connect


conn = connect(
    s3_staging_dir='&lt;ATHENA QUERY RESULTS LOCATION&gt;',
    region_name='&lt;YOUR REGION, for example, us-west-2&gt;',
    work_group='&lt;USER SPECIFIC WORKGROUP&gt;'
)

df = pd.read_sql(""SELECT * FROM &lt;DATABASE-NAME&gt;.&lt;YOUR TABLE NAME&gt; limit 8;"", conn)
</code></pre>
","6147064",0
658,57531262,2,57526707,2019-08-16 21:18:45,2,"<p>There are a number of ways you could tackle this with AzureML. The simplest would be to just launch a number of jobs using the AzureML Python SDK (the underlying example is taken from <a href=""https://github.com/Azure/MachineLearningNotebooks/blob/4170a394edd36413edebdbab347afb0d833c94ee/how-to-use-azureml/training/train-hyperparameter-tune-deploy-with-sklearn/train-hyperparameter-tune-deploy-with-sklearn.ipynb"" rel=""nofollow noreferrer"">here</a>)</p>

<pre><code>from azureml.train.sklearn import SKLearn

runs = []

for kernel in ['linear', 'rbf', 'poly', 'sigmoid']:
    for penalty in [0.5, 1, 1.5]:
        print ('submitting run for kernel', kernel, 'penalty', penalty)
        script_params = {
            '--kernel': kernel,
            '--penalty': penalty,
        }

        estimator = SKLearn(source_directory=project_folder, 
                            script_params=script_params,
                            compute_target=compute_target,
                            entry_script='train_iris.py',
                            pip_packages=['joblib==0.13.2'])

        runs.append(experiment.submit(estimator))
</code></pre>

<p>The above requires you to factor your training out into a script (or a set of scripts in a folder) along with the python packages required. The above estimator is a convenience wrapper for using Scikit Learn. There are also estimators for Tensorflow, Pytorch, Chainer and a generic one (<code>azureml.train.estimator.Estimator</code>) -- they all differ in the Python packages and base docker they use.</p>

<p>A second option, if you are actually tuning parameters, is to use the HyperDrive service like so (using the same <code>SKLearn</code> Estimator as above):</p>

<pre><code>from azureml.train.sklearn import SKLearn
from azureml.train.hyperdrive.runconfig import HyperDriveConfig
from azureml.train.hyperdrive.sampling import RandomParameterSampling
from azureml.train.hyperdrive.run import PrimaryMetricGoal
from azureml.train.hyperdrive.parameter_expressions import choice

estimator = SKLearn(source_directory=project_folder, 
                    script_params=script_params,
                    compute_target=compute_target,
                    entry_script='train_iris.py',
                    pip_packages=['joblib==0.13.2'])

param_sampling = RandomParameterSampling( {
    ""--kernel"": choice('linear', 'rbf', 'poly', 'sigmoid'),
    ""--penalty"": choice(0.5, 1, 1.5)
    }
)

hyperdrive_run_config = HyperDriveConfig(estimator=estimator,
                                         hyperparameter_sampling=param_sampling, 
                                         primary_metric_name='Accuracy',
                                         primary_metric_goal=PrimaryMetricGoal.MAXIMIZE,
                                         max_total_runs=12,
                                         max_concurrent_runs=4)

hyperdrive_run = experiment.submit(hyperdrive_run_config)
</code></pre>

<p>Or you could use DASK to schedule the work as you were mentioning. Here is a sample of how to set up DASK on and AzureML Compute Cluster so you can do interactive work on it: <a href=""https://github.com/danielsc/azureml-and-dask"" rel=""nofollow noreferrer"">https://github.com/danielsc/azureml-and-dask</a></p>
","8821969",0
659,57552797,2,57488706,2019-08-19 08:14:31,0,"<ol>
<li>Take the model.pkl file, zip it, and upload it into Azure Machine Learning Studio. Click the “New” icon in the bottom left:
<a href=""https://i.stack.imgur.com/Iwvhi.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Iwvhi.jpg"" alt=""""></a></li>
<li>In the pane that comes up, click on dataset, and then “From Local File”:
<a href=""https://i.stack.imgur.com/DvyjO.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/DvyjO.jpg"" alt=""""></a></li>
<li>Select the zip file where you stored your serialized model and click the tick. You expirement should look like this:
<a href=""https://i.stack.imgur.com/0efka.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/0efka.jpg"" alt=""""></a></li>
<li>Put the following code to run your classification experiment:</li>
</ol>

<pre class=""lang-py prettyprint-override""><code>import pandas as pd
import sys
import pickle

def azureml_main(dataframe1 = None, dataframe2 = None):
    sys.path.insert(0,"".\Script Bundle"")
    model = pickle.load(open("".\Script Bundle\model.pkl"", 'rb'))
    pred = model.predict(dataframe1)
    return pd.DataFrame([pred[0]])
</code></pre>

<p><strong>Update</strong> 
If you want to declare this experiment as an API you need to add web input and output to the Python script module.
<a href=""https://i.stack.imgur.com/eqV8W.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/eqV8W.png"" alt=""enter image description here""></a></p>
","9929041",7
660,57561392,2,56861525,2019-08-19 17:25:46,1,"<p>Thank you for your question. I’m the product manager for Amazon SageMaker Ground Truth and am happy to answer your question here.</p>

<p>We have a feature called annotation consolidation that takes the responses from multiple workers for a single image and then consolidates those responses into a single set of bounding boxes for the image. The bounding boxes referenced in the manifest file are the consolidated responses whereas what you see in the annotations folders are the raw annotations (which is why you have the respective worker IDs). </p>

<p>You can find out more about the annotation consolidation feature here: <a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/sms-annotation-consolidation.html"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/sagemaker/latest/dg/sms-annotation-consolidation.html</a></p>

<p>Please let us know if you have any further questions.</p>
","10880486",1
661,57579012,2,57578332,2019-08-20 17:44:18,0,"<p>Thanks for reporting this issue! This appears to be a bug that our team is investigating.</p>
","5784983",2
662,57580682,2,55509207,2019-08-20 19:51:57,1,"<p>Thanks for your question. You can import data from Azure Blob, Azure File, ADLS Gen1, ADLS Gen2, Azure SQL, Azure PostgreSQL. 
For more information: <a href=""https://learn.microsoft.com/en-us/azure/machine-learning/service/how-to-access-data"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/machine-learning/service/how-to-access-data</a></p>

<p>You can create an Azure ML Dataset for your training scenarios. Dataset can be created either from the data store mentioned above or from public urls.
For more information: <a href=""https://learn.microsoft.com/en-us/azure/machine-learning/service/how-to-create-register-datasets"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/machine-learning/service/how-to-create-register-datasets</a></p>
","11903935",0
663,57581935,2,57500105,2019-08-20 21:47:19,2,"<p>You can download objects to files using <a href=""https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/s3.html#S3.Client.download_file"" rel=""nofollow noreferrer""><code>s3.download_file()</code></a>. This will make your code look like:</p>

<pre><code>s3 = boto3.client('s3')
bucket = 'my-bukkit'
prefix = 'aleks-weekly/models/'

# List objects matching your criteria
response = s3.list_objects(
    Bucket = bucket,
    Prefix = prefix,
    Delimiter = '.csv'
)

# Iterate over each file found and download it
for i in response['Contents']:
    key = i['Key']
    dest = os.path.join('/tmp',key)
    print(""Downloading file"",key,""from bucket"",bucket)
    s3.download_file(
        Bucket = bucket,
        Key = key,
        Filename = dest
    )
</code></pre>
","463213",2
664,57586000,2,57508382,2019-08-21 06:53:51,15,"<p>I found a solution to which uses <code>ExitHandler</code>. I post my code below, hope it can help someone else.</p>
<pre class=""lang-py prettyprint-override""><code>
def slack_notification(slack_channel: str, status: str, name: str, is_exit_handler: bool = False):
    &quot;&quot;&quot;
    performs slack notifications
    &quot;&quot;&quot;    
    send_slack_op = dsl.ContainerOp(
        name=name,
        image='wenmin.wu/slack-cli:latest',
        is_exit_handler=is_exit_handler,
        command=['sh', '-c'],
        arguments=[&quot;/send-message.sh -d {} '{}'&quot;.format(slack_channel, status)]
    )
    send_slack_op.add_env_variable(V1EnvVar(name = 'SLACK_CLI_TOKEN', value_from=V1EnvVarSource(config_map_key_ref=V1ConfigMapKeySelector(name='workspace-config', key='SLACK_CLI_TOKEN'))))
    return send_slack_op

@dsl.pipeline(
    name='forecasting-supply',
    description='forecasting supply ...'
)
def ml_pipeline(
    param1,
    param2,
    param3,
):
    exit_task = slack_notification(
        slack_channel = slack_channel,
        name = &quot;supply-forecasting&quot;,
        status = &quot;Kubeflow pipeline: {{workflow.name}} has {{workflow.status}}!&quot;,
        is_exit_handler = True
    )

    with dsl.ExitHandler(exit_task):
        # put other tasks here

</code></pre>
","7011287",6
665,57598796,2,57596224,2019-08-21 20:27:54,1,"<p>Unfortunately az ml deploy local doesn't support binding any ports other then the port hosting the scoring server. </p>
","11885543",0
666,57665110,2,57660058,2019-08-26 21:23:08,1,"<p>You could pass pointer to folder as an input parameter for the pipeline, and then your step can mount the folder to iterate over the json files.</p>
","10952233",2
667,57762933,2,57724414,2019-09-02 21:41:06,1,"<p><strong>Possible reason:</strong></p>
<p>The error message:</p>
<p><code>ValueError: Input 0 of layer default/fc1 is incompatible with the layer: : expected min_ndim=2, found ndim=1. Full shape received: [None]</code></p>
<p>Your original environment obs space is <code>self.observation_space = Box(np.array(0.0),np.array(1000))</code>.</p>
<p>Displaying the shape of your environment obs space gives:</p>
<p><code>print(Box(np.array(0.0), np.array(1000), dtype=np.float32).shape)</code> = <code>()</code></p>
<p>This could be indicated by <code>Full shape received: [None]</code> in the error message.</p>
<p>If you pass the shape <code>(1,1)</code> into <code>np.zeros</code>, you get the expected  <code>min_ndim=2</code>:</p>
<p><code>x = np.zeros((1, 1)) print(x) [[0.]] print(x.ndim) 2</code></p>
<p><strong>Suggested solution:</strong></p>
<p>I assume that you want your environment obs space to range from 0.0 to 1000.0 as indicated by the <code>self.price = np.random.rand()</code> in your <code>reset</code> function.</p>
<p>Try using the following for your environment obs space:</p>
<p><code>self.observation_space = Box(0.0, 1000.0, shape=(1,1), dtype=np.float32)</code></p>
<p>I hope that by setting the <code>Box</code> with an explicit <code>shape</code> helps.</p>
<p><strong>EDIT (20190910):</strong></p>
<p>To show that it works, truncated output from Sagemaker (Jupyter notebook instance):</p>
<pre><code>.
.
.
algo-1-y2ayw_1  | price b = 0.439261780930142
algo-1-y2ayw_1  | price a = 0.439261780930142
algo-1-y2ayw_1  | (self.price).shape = (1,)
algo-1-y2ayw_1  | [0.43926178] 10.103020961393266 False {}
algo-1-y2ayw_1  | price b = 0.439261780930142
algo-1-y2ayw_1  | price a = 0.439261780930142
algo-1-y2ayw_1  | (self.price).shape = (1,)
algo-1-y2ayw_1  | [0.43926178] 9.663759180463124 False {}
algo-1-y2ayw_1  | price b = 0.439261780930142
algo-1-y2ayw_1  | price a = 0.189261780930142
algo-1-y2ayw_1  | (self.price).shape = (1,)
algo-1-y2ayw_1  | [0.18926178] 5.67785342790426 False {}
algo-1-y2ayw_1  | price b = 0.189261780930142
algo-1-y2ayw_1  | price a = -0.06073821906985799
algo-1-y2ayw_1  | (self.price).shape = (1,)
algo-1-y2ayw_1  | [-0.06073822] 0 True {}
algo-1-y2ayw_1  | Result for PPO_ArrivalSim-v0_0:
algo-1-y2ayw_1  |   date: 2019-09-10_11-51-13
algo-1-y2ayw_1  |   done: true
algo-1-y2ayw_1  |   episode_len_mean: 126.72727272727273
algo-1-y2ayw_1  |   episode_reward_max: 15772.677709596366
algo-1-y2ayw_1  |   episode_reward_mean: 2964.4609668691965
algo-1-y2ayw_1  |   episode_reward_min: 0.0
algo-1-y2ayw_1  |   episodes: 5
algo-1-y2ayw_1  |   experiment_id: 5d3b9f2988854a0db164a2e5e9a7550f
algo-1-y2ayw_1  |   hostname: 2dae585dcc65
algo-1-y2ayw_1  |   info:
algo-1-y2ayw_1  |     cur_lr: 4.999999873689376e-05
algo-1-y2ayw_1  |     entropy: 1.0670874118804932
algo-1-y2ayw_1  |     grad_time_ms: 1195.066
algo-1-y2ayw_1  |     kl: 3.391784191131592
algo-1-y2ayw_1  |     load_time_ms: 44.725
algo-1-y2ayw_1  |     num_steps_sampled: 463
algo-1-y2ayw_1  |     num_steps_trained: 463
algo-1-y2ayw_1  |     policy_loss: -0.05383850634098053
algo-1-y2ayw_1  |     sample_time_ms: 621.282
algo-1-y2ayw_1  |     total_loss: 2194493.5
algo-1-y2ayw_1  |     update_time_ms: 145.352
algo-1-y2ayw_1  |     vf_explained_var: -5.519390106201172e-05
algo-1-y2ayw_1  |     vf_loss: 2194492.5
algo-1-y2ayw_1  |   iterations_since_restore: 2
algo-1-y2ayw_1  |   node_ip: 172.18.0.2
algo-1-y2ayw_1  |   pid: 77
algo-1-y2ayw_1  |   policy_reward_mean: {}
algo-1-y2ayw_1  |   time_since_restore: 4.55129861831665
algo-1-y2ayw_1  |   time_this_iter_s: 1.3484764099121094
algo-1-y2ayw_1  |   time_total_s: 4.55129861831665
algo-1-y2ayw_1  |   timestamp: 1568116273
algo-1-y2ayw_1  |   timesteps_since_restore: 463
algo-1-y2ayw_1  |   timesteps_this_iter: 234
algo-1-y2ayw_1  |   timesteps_total: 463
algo-1-y2ayw_1  |   training_iteration: 2
algo-1-y2ayw_1  |
algo-1-y2ayw_1  | A worker died or was killed while executing task 00000000781a7b5b94a203683f8f789e593abbb1.
algo-1-y2ayw_1  | A worker died or was killed while executing task 00000000d3507bc6b41ee1c9fc36292eeae69557.
algo-1-y2ayw_1  | == Status ==
algo-1-y2ayw_1  | Using FIFO scheduling algorithm.
algo-1-y2ayw_1  | Resources requested: 0/3 CPUs, 0/0 GPUs
algo-1-y2ayw_1  | Result logdir: /opt/ml/output/intermediate/training
algo-1-y2ayw_1  | TERMINATED trials:
algo-1-y2ayw_1  |  - PPO_ArrivalSim-v0_0:   TERMINATED [pid=77], 4 s, 2 iter, 463 ts, 2.96e+03 rew
algo-1-y2ayw_1  |
algo-1-y2ayw_1  | Saved model configuration.
algo-1-y2ayw_1  | Saved the checkpoint file /opt/ml/output/intermediate/training/PPO_ArrivalSim-v0_0_2019-09-10_11-50-53vd32vlux/checkpoint-2.extra_data as /opt/ml/model/checkpoint.extra_data
algo-1-y2ayw_1  | Saved the checkpoint file /opt/ml/output/intermediate/training/PPO_ArrivalSim-v0_0_2019-09-10_11-50-53vd32vlux/checkpoint-2.tune_metadata as /opt/ml/model/checkpoint.tune_metadata
algo-1-y2ayw_1  | Created LogSyncer for /root/ray_results/PPO_ArrivalSim-v0_2019-09-10_11-51-13xdn_5i34 -&gt; None
algo-1-y2ayw_1  | 2019-09-10 11:51:13.941718: I tensorflow/core/common_runtime/process_util.cc:71] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.
algo-1-y2ayw_1  | reset -&gt; (self.price).shape =  (1,)
algo-1-y2ayw_1  | LocalMultiGPUOptimizer devices ['/cpu:0']
algo-1-y2ayw_1  | reset -&gt; (self.price).shape =  (1,)
algo-1-y2ayw_1  | INFO:tensorflow:No assets to save.
algo-1-y2ayw_1  | No assets to save.
algo-1-y2ayw_1  | INFO:tensorflow:No assets to write.
algo-1-y2ayw_1  | No assets to write.
algo-1-y2ayw_1  | INFO:tensorflow:SavedModel written to: /opt/ml/model/1/saved_model.pb
algo-1-y2ayw_1  | SavedModel written to: /opt/ml/model/1/saved_model.pb
algo-1-y2ayw_1  | Saved TensorFlow serving model!
algo-1-y2ayw_1  | A worker died or was killed while executing task 00000000f352d985b807ca399460941fe2264899.

algo-1-y2ayw_1  | 2019-09-10 11:51:20,075 sagemaker-containers INFO
    
 Reporting training SUCCESS

tmpwwb4b358_algo-1-y2ayw_1 exited with code 0

Aborting on container exit...
Failed to delete: /tmp/tmpwwb4b358/algo-1-y2ayw Please remove it manually.

===== Job Complete =====
</code></pre>
<p>This time I make edits in all 3 files. Your environment, training script &amp; the Jupyter notebook but it turns out that there isn't a need to define custom models for your custom environment. However, that remains viable. And you're right, the main cause of the issue is still in the obs space.</p>
<p>I set <code>self.price</code> to be a 1D numpy array to make it talk better with Ray RLlib. The creation of the custom environment in the training script was done in a simpler way as shown below. As for the notebook, I used version 0.5.3 instead of 0.6.5 for toolkit_version &amp; the training is done in local mode (in the docker container on the Sagemaker Jupyter notebook instance, still on AWS) with CPU only. However, it will also work with any ML instance (e.g ml.m4.xlarge) with GPU.</p>
<p>The entire package along with all dependencies is in <a href=""https://github.com/ChuaCheowHuan/sagemaker_Ray_RLlib_custom_env"" rel=""nofollow noreferrer"">here</a>.</p>
<p>The edited env:</p>
<pre><code># new
from __future__ import absolute_import
from __future__ import division
from __future__ import print_function
# end new


from enum import Enum
import math

import gym
from gym import error, spaces, utils, wrappers
from gym.utils import seeding
from gym.envs.registration import register
from gym.spaces import Discrete, Box

import numpy as np


def sigmoid_price_fun(x, maxcust, gamma):
    return maxcust / (1 + math.exp(gamma * max(0, x)))


class Actions(Enum):
    DECREASE_PRICE = 0
    INCREASE_PRICE = 1
    HOLD = 2


PRICE_ADJUSTMENT = {
    Actions.DECREASE_PRICE: -0.25,
    Actions.INCREASE_PRICE: 0.25,
    Actions.HOLD: 0
}


class ArrivalSim(gym.Env):
    &quot;&quot;&quot; Simple environment for price optimising RL learner. &quot;&quot;&quot;

    def __init__(self, price):
        &quot;&quot;&quot;
        Parameters
        ----------
        price : float
            The initial price to use.
        &quot;&quot;&quot;
        super().__init__()

        self.price = price
        self.revenue = 0
        self.action_space = Discrete(3)  # [0, 1, 2]  #increase or decrease
        # original obs space:
        #self.observation_space = Box(0.0, 1000.0, shape=(1,1), dtype=np.float32)
        # obs space initially suggested:
        #self.observation_space = Box(0.0, 1000.0, shape=(1,1), dtype=np.float32)
        # obs space suggested in this edit:
        self.observation_space = spaces.Box(np.array([0.0]), np.array([1000.0]), dtype=np.float32)

    def step(self, action):
        &quot;&quot;&quot; Enacts the specified action in the environment.

        Returns the new price, reward, whether we're finished and an empty dict for compatibility with Gym's
        interface. &quot;&quot;&quot;

        self._take_action(Actions(action))

        next_state = self.price
        print('(self.price).shape =', (self.price).shape)
        #next_state = self.observation_space.sample()

        reward = self._get_reward()
        done = False

        if next_state &lt; 0 or reward == 0:
            done = True
        
        print(next_state, reward, done, {})

        return np.array(next_state), reward, done, {}

    def reset(self):
        &quot;&quot;&quot; Resets the environment, selecting a random initial price. Returns the price. &quot;&quot;&quot;
        #self.observation_space.value = np.random.rand()
        #return self.observation_space.sample()
        
        self.price = np.random.rand(1)
        
        print('reset -&gt; (self.price).shape = ', (self.price).shape)

        return self.price

    def _take_action(self, action):
#         self.observation_space.value += PRICE_ADJUSTMENT[action]
        #print('price b =', self.price)
        print('price b =', self.price[0])
        #print('price b =', self.price[[0]])
        #self.price += PRICE_ADJUSTMENT[action]
        self.price[0] += PRICE_ADJUSTMENT[action]
        #self.price[[0]] += PRICE_ADJUSTMENT[action]
        #print('price a =', self.price)
        print('price a =', self.price[0])
        #print('price a =', self.price[[0]])

    #def _get_reward(self, price):
    def _get_reward(self):
#         price = self.observation_space.value
#         return max(np.random.poisson(sigmoid_price_fun(price, 50, 0.5)) * price, 0)
        #self.revenue = max(np.random.poisson(sigmoid_price_fun(self.price, 50, 0.5)) * self.price, 0)
        #return max(np.random.poisson(sigmoid_price_fun(self.price, 50, 0.5)) * self.price, 0)
        self.revenue = max(np.random.poisson(sigmoid_price_fun(self.price[0], 50, 0.5)) * self.price[0], 0)
        return max(np.random.poisson(sigmoid_price_fun(self.price[0], 50, 0.5)) * self.price[0], 0)

#     def render(self, mode='human'):
#         super().render(mode)

def testEnv():
    &quot;&quot;&quot;
    register(
        id='ArrivalSim-v0',
        entry_point='env:ArrivalSim',
        kwargs= {'price' : 40.0}
    )
    env = gym.make('ArrivalSim-v0')
    &quot;&quot;&quot;
    env = ArrivalSim(30.0)

    val = env.reset()
    print('val.shape = ', val.shape)

    for _ in range(5):
        print('env.observation_space =', env.observation_space)
        act = env.action_space.sample()
        print('\nact =', act)
        next_state, reward, done, _ = env.step(act)  # take a random action
        print('next_state = ', next_state)
    env.close()



if __name__ =='__main__':

    testEnv()
</code></pre>
<p>The edited training script:</p>
<pre><code>import json
import os

import gym
import ray
from ray.tune import run_experiments
import ray.rllib.agents.a3c as a3c
import ray.rllib.agents.ppo as ppo
from ray.tune.registry import register_env
from mod_op_env import ArrivalSim

from sagemaker_rl.ray_launcher import SageMakerRayLauncher
        
&quot;&quot;&quot;
def create_environment(env_config):
    import gym
#     from gym.spaces import Space
    from gym.envs.registration import register

    # This import must happen inside the method so that worker processes import this code
    register(
        id='ArrivalSim-v0',
        entry_point='env:ArrivalSim',
        kwargs= {'price' : 40}
    )
    return gym.make('ArrivalSim-v0')
&quot;&quot;&quot;
def create_environment(env_config):
    price = 30.0
    # This import must happen inside the method so that worker processes import this code
    from mod_op_env import ArrivalSim
    return ArrivalSim(price)


class MyLauncher(SageMakerRayLauncher):
    def __init__(self):        
        super(MyLauncher, self).__init__()
        self.num_gpus = int(os.environ.get(&quot;SM_NUM_GPUS&quot;, 0))
        self.hosts_info = json.loads(os.environ.get(&quot;SM_RESOURCE_CONFIG&quot;))[&quot;hosts&quot;]
        self.num_total_gpus = self.num_gpus * len(self.hosts_info)
        
    def register_env_creator(self):
        register_env(&quot;ArrivalSim-v0&quot;, create_environment)

    def get_experiment_config(self):
        return {
          &quot;training&quot;: {
            &quot;env&quot;: &quot;ArrivalSim-v0&quot;,
            &quot;run&quot;: &quot;PPO&quot;,
            &quot;stop&quot;: {
              &quot;training_iteration&quot;: 3,
            },
              
            &quot;local_dir&quot;: &quot;/opt/ml/model/&quot;,
            &quot;checkpoint_freq&quot; : 3,
              
            &quot;config&quot;: {                                
              #&quot;num_workers&quot;: max(self.num_total_gpus-1, 1),
              &quot;num_workers&quot;: max(self.num_cpus-1, 1),
              #&quot;use_gpu_for_workers&quot;: False,
              &quot;train_batch_size&quot;: 128, #5,
              &quot;sample_batch_size&quot;: 32, #1,
              &quot;gpu_fraction&quot;: 0.3,
              &quot;optimizer&quot;: {
                &quot;grads_per_step&quot;: 10
              },
            },
            #&quot;trial_resources&quot;: {&quot;cpu&quot;: 1, &quot;gpu&quot;: 0, &quot;extra_gpu&quot;: max(self.num_total_gpus-1, 1), &quot;extra_cpu&quot;: 0},
            #&quot;trial_resources&quot;: {&quot;cpu&quot;: 1, &quot;gpu&quot;: 0, &quot;extra_gpu&quot;: max(self.num_total_gpus-1, 0),
            #                    &quot;extra_cpu&quot;: max(self.num_cpus-1, 1)},
            &quot;trial_resources&quot;: {&quot;cpu&quot;: 1,
                                &quot;extra_cpu&quot;: max(self.num_cpus-1, 1)},              
          }
        }

if __name__ == &quot;__main__&quot;:
    os.environ[&quot;LC_ALL&quot;] = &quot;C.UTF-8&quot;
    os.environ[&quot;LANG&quot;] = &quot;C.UTF-8&quot;
    os.environ[&quot;RAY_USE_XRAY&quot;] = &quot;1&quot;
    print(ppo.DEFAULT_CONFIG)
    MyLauncher().train_main()

</code></pre>
<p>The notebook code:</p>
<pre><code>!/bin/bash ./setup.sh

from time import gmtime, strftime
import sagemaker 
role = sagemaker.get_execution_role()

sage_session = sagemaker.session.Session()
s3_bucket = sage_session.default_bucket()  
s3_output_path = 's3://{}/'.format(s3_bucket)
print(&quot;S3 bucket path: {}&quot;.format(s3_output_path))

job_name_prefix = 'ArrivalSim'

from sagemaker.rl import RLEstimator, RLToolkit, RLFramework

estimator = RLEstimator(entry_point=&quot;mod_op_train.py&quot;, # Our launcher code
                        source_dir='src', # Directory where the supporting files are at. All of this will be
                                          # copied into the container.
                        dependencies=[&quot;common/sagemaker_rl&quot;], # some other utils files.
                        toolkit=RLToolkit.RAY, # We want to run using the Ray toolkit against the ray container image.
                        framework=RLFramework.TENSORFLOW, # The code is in tensorflow backend.
                        toolkit_version='0.5.3', # Toolkit version. This will also choose an apporpriate tf version.                                               
                        #toolkit_version='0.6.5', # Toolkit version. This will also choose an apporpriate tf version.                        
                        role=role, # The IAM role that we created at the begining.
                        #train_instance_type=&quot;ml.m4.xlarge&quot;, # Since we want to run fast, lets run on GPUs.
                        train_instance_type=&quot;local&quot;, # Since we want to run fast, lets run on GPUs.
                        train_instance_count=1, # Single instance will also work, but running distributed makes things 
                                                # fast, particularly in the case of multiple rollout training.
                        output_path=s3_output_path, # The path where we can expect our trained model.
                        base_job_name=job_name_prefix, # This is the name we setup above to be to track our job.
                        hyperparameters = {      # Some hyperparameters for Ray toolkit to operate.
                          &quot;s3_bucket&quot;: s3_bucket,
                          &quot;rl.training.stop.training_iteration&quot;: 2, # Number of iterations.
                          &quot;rl.training.checkpoint_freq&quot;: 2,
                        },
                        #metric_definitions=metric_definitions, # This will bring all the logs out into the notebook.
                    )

estimator.fit()
</code></pre>
","11427968",19
668,57809314,2,57808963,2019-09-05 16:07:12,1,"<p>The issue is not specific to SageMaker Notebook instances. Rather, it is a bug in the Git extension of JupyterLab. You can find details around this here: <a href=""https://github.com/jupyterlab/jupyterlab-git/issues/346"" rel=""nofollow noreferrer"">https://github.com/jupyterlab/jupyterlab-git/issues/346</a></p>
","4592183",0
669,57814156,2,57811873,2019-09-05 23:49:07,1,"<p><a href=""http://docs.h2o.ai/h2o/latest-stable/h2o-docs/automl.html"" rel=""nofollow noreferrer"">H2O AutoML</a> contains a handful of algorithms and one of them is <a href=""http://docs.h2o.ai/h2o/latest-stable/h2o-docs/data-science/xgboost.html"" rel=""nofollow noreferrer"">XGBoost</a>, which has been part of H2O AutoML since H2O version 3.22.0.1.  XGBoost is the only GPU-capable algorithm inside of H2O AutoML, however, a lot of the models that are trained in AutoML are XGBoost models, so it still can be useful to utilize a GPU. Keep in mind that you must use H2O 3.22 or above to use this feature.</p>

<p>My suggestion is to test it on a GPU-enabled instance and compare the results to a non-GPU instance and see if it's worth the extra cost.  </p>
","5451344",5
670,57829754,2,57819173,2019-09-07 00:20:55,1,"<p>You would have to use inference pipeline for your use case. What that means is that you will need to use a pre-processing step to featurize your text into tfidf and then feed into Sagemaker classification. Here's a <a href=""https://stackoverflow.com/questions/57767899/how-to-create-a-pipeline-in-sagemaker-with-pytorch"">SO answer</a> with more details around this.</p>
","5597065",0
671,57830593,2,57661142,2019-09-07 04:07:48,2,"<p>Thanks for using Amazon SageMaker!</p>

<p>I sort of guessed from your description, but are you trying to use the Keras load_img function to load images directly from your S3 bucket?</p>

<p>Unfortunately, <a href=""https://github.com/keras-team/keras/issues/11684"" rel=""nofollow noreferrer"">the load_img function is designed to only load files from disk</a>, so passing an s3:// URL to that function will always return a <code>FileNotFoundError</code>.</p>

<p>It's common to first download images from S3 before using them, so you can use boto3 or the AWS CLI to download the file before calling load_img.</p>

<p><strong>Alternatively</strong>, since the load_img function simply creates a <a href=""https://en.wikipedia.org/wiki/Python_Imaging_Library"" rel=""nofollow noreferrer"">PIL Image</a> object, you can create the PIL object directly from the data in S3 using boto3, and not use the load_img function at all.</p>

<p>In other words, you could do something like this:</p>

<pre class=""lang-py prettyprint-override""><code>from PIL import Image

s3 = boto3.client('s3')
test = Image.open(BytesIO(
    s3.get_object(Bucket=bucket, Key=data_key)['Body'].read()
    ))
</code></pre>

<p>Hope this helps you out in your project!</p>
","977443",1
672,57859531,2,57854136,2019-09-09 18:37:20,2,"<p>The Azure Machine Learning service SDK has a bug with how it interacts with Azure Storage, which causes it to upload corrupted files if it has to retry uploading. </p>

<p>A couple workarounds:</p>

<ol>
<li>The bug was introduced in 1.0.60 release. If you downgrade to AzureML-SDK 1.0.55, the code should fail when there are issue uploading instead of silently corrupting data.</li>
<li>It's possible that the retry is being triggered by the low timeout values that the AzureML-SDK defaults to. You could investigate changing the timeout in <code>site-packages/azureml/_restclient/artifacts_client.py</code></li>
</ol>

<p>This bug should be fixed in the next release of the AzureML-SDK.</p>
","11885543",4
673,57911797,2,57896195,2019-09-12 17:26:46,1,"<p>Seems like you've run into Azure file share constraints. You can use the following sample code to change your runs to use blob storage which can scale to large number of jobs running in parallel:</p>

<p><a href=""https://learn.microsoft.com/en-us/azure/machine-learning/service/how-to-access-data#accessing-source-code-during-training"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/machine-learning/service/how-to-access-data#accessing-source-code-during-training</a></p>
","8807958",0
674,57920151,2,57892580,2019-09-13 08:42:37,1,"<p>Jupyter Notebook are predominantly designed for exploration and
   development. If you want to launch long-running or scheduled jobs on
   ephemeral hardware, it will be a much better experience to use the
   training API, such as the <a href=""https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/sagemaker.html#SageMaker.Client.create_training_job"" rel=""nofollow noreferrer""><code>create_training_job</code></a> in boto3 or the
   <code>estimator.fit()</code> of the <a href=""https://sagemaker.readthedocs.io/en/stable/estimators.html"" rel=""nofollow noreferrer"">Python SDK</a>. The code passed to training jobs
   can be completely arbitrary - not necessarily ML code - so whatever
   you write in jupyter could likely be scheduled and ran in those
   training jobs. See the <a href=""https://github.com/awslabs/amazon-sagemaker-examples/blob/master/sagemaker-python-sdk/scikit_learn_randomforest/Sklearn_on_SageMaker_end2end.ipynb"" rel=""nofollow noreferrer"">random forest sklearn demo here</a> for an
   example. That being said, if you still want to programmatically shut
   down a SageMaker notebook instance, you can use that boto3 call:</p>

<pre><code>import boto3

sm = boto3.client('sagemaker')
sm.stop_notebook_instance(NotebookInstanceName='string')
</code></pre>
","5331834",1
675,57952974,2,57923187,2019-09-16 08:24:09,0,"<p>I found the solution.</p>

<p>For those who have the same problem, it is pretty simple in fact. You need to add two <strong>Select Columns in Dataset</strong> box in your <strong>Predictive experiment</strong> schema.</p>

<p><a href=""https://i.stack.imgur.com/JA3q2.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/JA3q2.png"" alt=""enter image description here""></a></p>

<p><strong>Update 2020:</strong> Following some updates done on the service, the solution proposed is partially broken. Indeed, if you decide to not include the outcome in the first Select columns box, you well not be able to retrieve it in the second <strong><em>Select Column box</em></strong> leading to an error. To solve that, you have to remove the first Select Column box and take all features. For the second <strong><em>Select Column box</em></strong> nothing change, you select the features you want for your predictive response.</p>
","7873912",0
676,57955177,2,57946451,2019-09-16 10:43:56,2,"<p>Unfortunately, the SageMaker built-in DeepAR model doesn't support learning rate scheduling nor incremental learning.  If you want to implement learning rate plateau schedule on a DeepAR architecture I recommend to consider:</p>

<ul>
<li>using the open-source DeepAR implementation (<a href=""https://gluon-ts.mxnet.io/api/gluonts/gluonts.model.deepar.html"" rel=""nofollow noreferrer"">code</a>, <a href=""https://aws.amazon.com/blogs/machine-learning/creating-neural-time-series-models-with-gluon-time-series/"" rel=""nofollow noreferrer"">demo</a>)</li>
<li>or using the <a href=""https://docs.aws.amazon.com/forecast/latest/dg/aws-forecast-recipe-deeparplus.html"" rel=""nofollow noreferrer"">DeepAR+ algo of the Amazon Forecast service</a>, that features learning rate scheduling ability.</li>
</ul>
","5331834",0
677,57966362,2,57915603,2019-09-17 01:52:47,3,"<p>Please upgrade your SDK to the latest version. Seems like this issue was fixed sometime after 1.0.43.</p>
","11876373",0
678,57967291,2,57966851,2019-09-17 04:18:09,7,"<p>As per mroutis on the DVC Discord server:</p>

<ol>
<li><code>dvc unprotect</code> the file; this won't be necessary if you don't use <code>symlink</code> or <code>hardlink</code> caching, but it can't hurt.</li>
<li>Remove the .dvc file</li>
<li>If you need to delete the cache entry itself, run <code>dvc gc</code>, or look up the MD5 in <code>data.dvc</code> and manually remove it from <code>.dvc/cache</code>.</li>
</ol>

<p><em>Edit</em> -- there is now an issue on their Github page to add this to the manual: <a href=""https://github.com/iterative/dvc.org/issues/625"" rel=""nofollow noreferrer"">https://github.com/iterative/dvc.org/issues/625</a></p>
","2954547",6
679,57970562,2,57968208,2019-09-17 08:46:41,3,"<p>you need to have a default storage class in your cluster, so if a pvc does not specify any storage class then default one would be selected. </p>

<p>List the StorageClasses in your cluster:</p>

<p><code>kubectl get storageclass</code></p>

<p>Mark a StorageClass as default:
 set the annotation storageclass.kubernetes.io/is-default-class=true.</p>

<p><code>kubectl patch storageclass &lt;your-class-name&gt; -p '{""metadata"": {""annotations"":{""storageclass.kubernetes.io/is-default-class"":""true""}}}'</code></p>

<p>Here are the detail steps <a href=""https://kubernetes.io/docs/tasks/administer-cluster/change-default-storage-class/"" rel=""nofollow noreferrer"">change-default-storage-class</a></p>
","8803619",3
680,57975364,2,57966245,2019-09-17 13:30:43,0,"<p>After I asked I came across an explanation from <a href=""https://github.com/awslabs/amazon-sagemaker-examples/blob/master/aws_marketplace/using_algorithms/automl/AutoML_-_Train_multiple_models_in_parallel.ipynb"" rel=""nofollow noreferrer"">SageMaker examples.</a></p>

<p>{classification': 'true', 'categorical_columns': 'SIT','HOL','CTH','YTT','target': 'y'}.</p>

<p>Problem solved!</p>
","8836963",0
681,57977973,2,57971689,2019-09-17 15:56:27,6,"<p>A few notes on the drives that an AzureML remote run has available:</p>

<p>Here is what I see when I run <code>df</code> on a remote run (in this one, I am using a blob <code>Datastore</code> via <code>as_mount()</code>):</p>

<pre><code>Filesystem                             1K-blocks     Used  Available Use% Mounted on
overlay                                103080160 11530364   86290588  12% /
tmpfs                                      65536        0      65536   0% /dev
tmpfs                                    3568556        0    3568556   0% /sys/fs/cgroup
/dev/sdb1                              103080160 11530364   86290588  12% /etc/hosts
shm                                      2097152        0    2097152   0% /dev/shm
//danielscstorageezoh...-620830f140ab 5368709120  3702848 5365006272   1% /mnt/batch/tasks/.../workspacefilestore
blobfuse                               103080160 11530364   86290588  12% /mnt/batch/tasks/.../workspaceblobstore
</code></pre>

<p>The interesting items are <code>overlay</code>, <code>/dev/sdb1</code>, <code>//danielscstorageezoh...-620830f140ab</code> and <code>blobfuse</code>:</p>

<ol>
<li><code>overlay</code> and <code>/dev/sdb1</code> are both the mount of the <strong>local SSD</strong> on the machine (I am using a STANDARD_D2_V2 which has a 100GB SSD).</li>
<li><code>//danielscstorageezoh...-620830f140ab</code> is the mount of the <strong>Azure File Share</strong> that contains the project files (your script, etc.). It is also the <em>current working directory</em> for your run.</li>
<li><strong><code>blobfuse</code></strong> is the blob store that I had requested to mount in the <code>Estimator</code> as I executed the run.</li>
</ol>

<p>I was curious about the performance differences between these 3 types of drives. My mini benchmark was to download and extract this file: <a href=""http://download.tensorflow.org/example_images/flower_photos.tgz"" rel=""noreferrer"">http://download.tensorflow.org/example_images/flower_photos.tgz</a> (it is a 220 MB tar file that contains about 3600 jpeg images of flowers).</p>

<p>Here the results:</p>

<pre><code>Filesystem/Drive         Download_and_save       Extract
Local_SSD                               2s            2s  
Azure File Share                        9s          386s
Premium File Share                     10s          120s
Blobfuse                               10s          133s
Blobfuse w/ Premium Blob                8s          121s
</code></pre>

<p>In summary, writing small files is much, much slower on the network drives, so it is highly recommended to use /tmp or Python <code>tempfile</code> if you are writing smaller files. </p>

<p>For reference, here the script I ran to measure: <a href=""https://gist.github.com/danielsc/9f062da5e66421d48ac5ed84aabf8535"" rel=""noreferrer"">https://gist.github.com/danielsc/9f062da5e66421d48ac5ed84aabf8535</a></p>

<p>And this is how I ran it: <a href=""https://gist.github.com/danielsc/6273a43c9b1790d82216bdaea6e10e5c"" rel=""noreferrer"">https://gist.github.com/danielsc/6273a43c9b1790d82216bdaea6e10e5c</a></p>
","8821969",1
682,58044925,2,58040933,2019-09-21 23:35:21,1,"<p>I'd consider using <a href=""https://learn.microsoft.com/en-us/python/api/azureml-dataprep/azureml.dataprep?view=azure-dataprep-py"" rel=""nofollow noreferrer""><code>azureml.dataprep</code></a> over pyodbc for this task (the API may change, but this worked last time I tried):</p>

<pre><code>import azureml.dataprep as dprep

ds = dprep.MSSQLDataSource(server_name=&lt;server-name,port&gt;,
                           database_name=&lt;database-name&gt;,
                           user_name=&lt;username&gt;,
                           password=&lt;password&gt;)
</code></pre>

<p>You should then be able to collect the result of an SQL query in pandas e.g. via</p>

<pre><code>dataflow = dprep.read_sql(ds, ""SELECT top 100 * FROM [dbo].[MYTABLE]"")
dataflow.to_pandas_dataframe()
</code></pre>
","4240413",3
683,58063739,2,58060865,2019-09-23 13:36:39,1,"<p>You can do following:</p>

<ol>
<li>Create an [Environment][1] with the coordinates of your custom Docker image specified in Docker section.</li>
<li>Create [InferenceConfig][2] with that Environment as argument, and use it when deploying the model.</li>
</ol>

<p>For example, assuming you have a model already and eliding other arguments:</p>

<pre><code>from azureml.core.environment import Environment
from azureml.core.model import InferenceConfig

env = Environment(name=""myenv"")
env.docker.base_image = ""mybaseimage""
env.docker.base_image_registry.address = ""ip-address""
env.docker.base_image_registry.username = ""my-username""
env.docker.base_image_registry.password = ""my-password""

ic = InferenceConfig(…,environment = env)
model.deploy(…,inference_config = ic)
</code></pre>

<pre><code>
  [1]: https://learn.microsoft.com/en-us/python/api/azureml-core/azureml.core.environment.environment?view=azure-ml-py
  [2]: https://learn.microsoft.com/en-us/python/api/azureml-core/azureml.core.model.inferenceconfig?view=azure-ml-py
</code></pre>
","5784983",5
684,58076895,2,58076263,2019-09-24 09:14:17,0,"<p>The root cause is </p>

<pre><code>MLflow UI and client code expects a default experiment with ID 0.
This method uses SQL insert statement to create the default experiment as a hack, since
experiment table uses 'experiment_id' column is a PK and is also set to auto increment.
MySQL and other implementation do not allow value '0' for such cases.
</code></pre>

<p>ref: <a href=""https://github.com/mlflow/mlflow/blob/v1.2.0/mlflow/store/sqlalchemy_store.py#L171"" rel=""nofollow noreferrer"">https://github.com/mlflow/mlflow/blob/v1.2.0/mlflow/store/sqlalchemy_store.py#L171</a> </p>

<p>No error is raised during migration, so no error shows up, and alembic version is the newest when fail silently. 
ref:<a href=""https://github.com/mlflow/mlflow/blob/v1.2.0/mlflow/store/db_migrations/env.py#L71"" rel=""nofollow noreferrer"">https://github.com/mlflow/mlflow/blob/v1.2.0/mlflow/store/db_migrations/env.py#L71</a></p>

<p>If using the same idea as the MySQL test(<a href=""https://github.com/mlflow/mlflow/blob/v1.2.0/mlflow/store/sqlalchemy_store.py#L171"" rel=""nofollow noreferrer"">https://github.com/mlflow/mlflow/blob/v1.2.0/mlflow/store/sqlalchemy_store.py#L171</a>), the exception is raised - <code>Cannot insert explicit value for identity column in table 'experiment' when IDENTITY_INSERT is set to OFF.</code></p>

<p>Test snippet:</p>

<pre class=""lang-py prettyprint-override""><code>class TestSqlAlchemyStoreMssqlDb(unittest.TestCase):
    """"""
    Run tests against a MSSQL database
    """"""
    def setUp(self):
        db_username = ""test""
        db_password = ""test""
        host = ""test""
        db_name = ""TEST_DB""

        db_server_url = ""mssql+pymssql://%s:%s@%s"" % (db_username, db_password, host)
        self._engine = sqlalchemy.create_engine(db_server_url)

        self._db_url = ""%s/%s"" % (db_server_url, db_name)
        print(""Connect to %s"" % self._db_url)

    def test_store(self):
        self.store = SqlAlchemyStore(db_uri=self._db_url, default_artifact_root=ARTIFACT_URI)
</code></pre>

<p>Using postgres server completes the migration as the log shows.</p>

<pre><code>mlflow_1    | 2019/09/24 09:03:55 INFO mlflow.store.sqlalchemy_store: Creating initial MLflow database tables...
mlflow_1    | 2019/09/24 09:03:55 INFO mlflow.store.db.utils: Updating database tables at postgresql://postgres:postgres@postgres:5432/postgres
mlflow_1    | INFO  [alembic.runtime.migration] Context impl PostgresqlImpl.
mlflow_1    | INFO  [alembic.runtime.migration] Will assume transactional DDL.
mlflow_1    | INFO  [alembic.runtime.migration] Running upgrade  -&gt; 451aebb31d03, add metric step
mlflow_1    | INFO  [alembic.runtime.migration] Running upgrade 451aebb31d03 -&gt; 90e64c465722, migrate user column to tags
mlflow_1    | INFO  [alembic.runtime.migration] Running upgrade 90e64c465722 -&gt; 181f10493468, allow nulls for metric values
mlflow_1    | INFO  [alembic.runtime.migration] Running upgrade 181f10493468 -&gt; df50e92ffc5e, Add Experiment Tags Table
mlflow_1    | INFO  [alembic.runtime.migration] Running upgrade df50e92ffc5e -&gt; 7ac759974ad8, Update run tags with larger limit
mlflow_1    | INFO  [alembic.runtime.migration] Context impl PostgresqlImpl.
mlflow_1    | INFO  [alembic.runtime.migration] Will assume transactional DDL.
mlflow_1    | [2019-09-24 09:03:55 +0000] [15] [INFO] Starting gunicorn 19.9.0
mlflow_1    | [2019-09-24 09:03:55 +0000] [15] [INFO] Listening at: http://0.0.0.0:5000 (15)
mlflow_1    | [2019-09-24 09:03:55 +0000] [15] [INFO] Using worker: sync
mlflow_1    | [2019-09-24 09:03:55 +0000] [18] [INFO] Booting worker with pid: 18
mlflow_1    | [2019-09-24 09:03:56 +0000] [22] [INFO] Booting worker with pid: 22
mlflow_1    | [2019-09-24 09:03:56 +0000] [26] [INFO] Booting worker with pid: 26
mlflow_1    | [2019-09-24 09:03:56 +0000] [27] [INFO] Booting worker with pid: 27
</code></pre>
","3952994",0
685,58081888,2,58031370,2019-09-24 13:52:55,0,"<p>What kind of run is this? Canceling is not currently enabled for pipeline runs in the UI, but is supported for other run types.</p>
","3753604",1
686,58102579,2,58035744,2019-09-25 16:11:01,2,"<p>Add run.flush() at the end of the script.</p>
","11876373",0
687,58146087,2,58110595,2019-09-28 11:58:54,1,"<p>To summarize the conversation in the comments:</p>

<p>Once you have your model trained, tuned, and deployed (which is not a simple process), you can call the endpoint of the model using the <a href=""https://github.com/aws/aws-sdk-js"" rel=""nofollow noreferrer"">AWS SDK for JavaScript</a>, that you install by:</p>

<pre><code>npm install aws-sdk
var AWS = require('aws-sdk/dist/aws-sdk-react-native');
</code></pre>

<p>you include in the HTML as:</p>

<pre><code>&lt;script src=""https://sdk.amazonaws.com/js/aws-sdk-2.538.0.min.js""&gt;&lt;/script&gt;
</code></pre>

<p>And when you want to call the endpoint you invoke it like that:</p>

<pre><code>var params = {
  Body: Buffer.from('...') || 'STRING_VALUE' /* Strings will be Base-64 encoded on your behalf */, /* required */
  EndpointName: 'STRING_VALUE', /* required */
  Accept: 'STRING_VALUE',
  ContentType: 'STRING_VALUE',
  CustomAttributes: 'STRING_VALUE'
};
sagemakerruntime.invokeEndpoint(params, function(err, data) {
  if (err) console.log(err, err.stack); // an error occurred
  else     console.log(data);           // successful response
});
</code></pre>

<p>You can check out the <a href=""https://aws-amplify.github.io"" rel=""nofollow noreferrer"">Amplify Library</a> that can take some of the heavy liftings such as getting IAM permissions to call the API, a user log in and many others. </p>
","179529",0
688,58179143,2,58150368,2019-10-01 06:27:41,7,"<p>Files created in one Kubeflow pipeline component are local to the container. To reference it in the subsequent steps, you would need to pass it as:</p>

<pre><code>data_preprocessor = dsl.ContainerOp(
        name='data preprocessor',
        image='eu.gcr.io/kubeflow-demo-254012/data-preprocessor',
        arguments=[""--fetched_dataset"", data_collector.outputs['output'],
                   ""--project_id"", project_id,
                  ]
</code></pre>

<p><strong>Note:</strong> <code>data_collector.outputs['output']</code> will contain the actual string contents of the file <code>/output.txt</code> (not a path to the file). If you want for it to contain the path of the file, you'll need to write the dataset to shared storage (like s3, or a mounted PVC volume) and write the path/link to the shared storage to  <code>/output.txt</code>. <code>data_preprocessor</code> can then read the dataset based on the path.</p>
","4438213",2
689,58214626,2,58213125,2019-10-03 07:51:11,0,"<p>From the way you have specified your environment, it's hard to see if it's a proper RunConfiguration object. If it is, it should be a matter of adding it to you PythonScriptStep.</p>

<pre><code>trainStep = PythonScriptStep(
    script_name=""dogs_vs_cats.py"",
    arguments=[""--input"", blob_input_data, ""--output"", output_data1],
    inputs=[blob_input_data],
    outputs=[output_data1],
    compute_target=compute_target,
    source_directory=""../dogs-vs-cats"",
    runconfig=myenv
)
</code></pre>

<p>Right now you're defining the environment, but no using it anywhere it seems. If your trouble persists maybe try defining your RunConfiguration like they do under the ""Specify the environment to run the script"" step in this notebook:</p>

<p><a href=""https://github.com/Azure/MachineLearningNotebooks/blob/master/how-to-use-azureml/machine-learning-pipelines/pipeline-batch-scoring/pipeline-batch-scoring.ipynb"" rel=""nofollow noreferrer"">https://github.com/Azure/MachineLearningNotebooks/blob/master/how-to-use-azureml/machine-learning-pipelines/pipeline-batch-scoring/pipeline-batch-scoring.ipynb</a></p>
","2706290",0
690,58308116,2,58278844,2019-10-09 16:04:02,1,"<p>The az cli is likely failing to parse the provided data input. If I attempt to run the same command I see the following error:</p>

<p><code>az: error: unrecognized arguments: [{""width"": 50, ""shoe_size"": 28}]}</code></p>

<p>You need to wrap the input in quotes for it to appropriately be taken as a single input parameter:</p>

<p><code>az ml service run -n ""rj-aci-5"" -d ""{\""input_df\"": [{\""width\"": 50, \""shoe_size\"": 28}]}""</code></p>
","5245304",1
691,58310995,2,58292566,2019-10-09 19:27:22,0,"<p>You can use an input type pipe parameter like so: </p>

<pre><code>hyperparameters = {'save_checkpoints_secs':None,
                   'save_checkpoints_steps':1000}

tf_estimator = TensorFlow(entry_point='./my-training-file', role=role,
                          training_steps=5100, evaluation_steps=100,
                          train_instance_count=1, train_instance_type='ml.p3.2xlarge',
                          input_mode = 'Pipe',
                          train_volume_size=300, output_path = 's3://sagemaker-pocs/test-carlsoa/kepler/model',
                          framework_version = '1.12.0', hyperparameters=hyperparameters, checkpoint_path = None)
</code></pre>

<p>And create the manifest file pipe as an input:</p>

<pre><code>train_data = sagemaker.session.s3_input('s3://sagemaker-pocs/test-carlsoa/manifest.json',
                                        distribution='FullyReplicated',
                                        content_type='image/jpeg',
                                        s3_data_type='ManifestFile',
                                        attribute_names=['source-ref']) 
                                        #attribute_names=['source-ref', 'annotations']) 
data_channels = {'train': train_data}
</code></pre>

<p>Note that you can use ManifestFile or AugmentedManifestFile depending on whether you have extra data or labels to provide. Now you can use data_channels as the input to the tf estimator:</p>

<p><code>tf_estimator.fit(inputs=data_channels, logs=True)</code></p>
","4259581",1
692,58314078,2,58307950,2019-10-10 01:05:28,2,"<p>You probably want to try something like this command : </p>

<pre class=""lang-sh prettyprint-override""><code>az vm list-usage --location eastus --out table
</code></pre>

<p>It would get you the core usage for the region, which is what is important for deployment of resources.</p>

<p>Other choices (az + Powershell) are available <a href=""https://learn.microsoft.com/en-us/azure/networking/check-usage-against-limits"" rel=""nofollow noreferrer"">here</a>.</p>

<p>Hope this helps!</p>
","10435556",3
693,58391776,2,53609409,2019-10-15 09:46:13,27,"<p>You can use <a href=""https://aws.amazon.com/blogs/machine-learning/customize-your-amazon-sagemaker-notebook-instances-with-lifecycle-configurations-and-the-option-to-disable-internet-access/"" rel=""noreferrer"">Lifecycle configurations</a> to set up an automatic job that will stop your instance after inactivity.</p>

<p>There's <a href=""https://github.com/aws-samples/amazon-sagemaker-notebook-instance-lifecycle-config-samples"" rel=""noreferrer"">a GitHub repository</a> which has samples that you can use. In the repository, there's a <a href=""https://github.com/aws-samples/amazon-sagemaker-notebook-instance-lifecycle-config-samples/blob/master/scripts/auto-stop-idle/on-start.sh"" rel=""noreferrer"">auto-stop-idle</a> script which will shutdown your instance once it's idle for more than 1 hour.</p>

<p>What you need to do is</p>

<ol>
<li>to create a Lifecycle configuration using the script and</li>
<li>associate the configuration with the instance. You can do this when you edit or create a Notebook instance.</li>
</ol>

<p>If you think 1 hour is too long you can tweak the script. <a href=""https://github.com/aws-samples/amazon-sagemaker-notebook-instance-lifecycle-config-samples/blob/master/scripts/auto-stop-idle/on-start.sh#L17"" rel=""noreferrer"">This line</a> has the value.</p>
","524588",2
694,58404585,2,57857726,2019-10-16 01:35:51,1,"<p>Here you go.</p>

<p>Airflow is looking in the local (user) Python installation for the library but <code>urllib3</code> is installed for all users. It's weird but try doing <code>pip3 install --user urllib3==1.22</code>.</p>
","6092420",0
695,58455151,2,58377444,2019-10-18 16:52:16,1,"<p>Try using nan as the value for y_query. and make sure the date is the next time unit after the one that was used in the training set.</p>
","12239714",0
696,58615164,2,58601697,2019-10-29 20:44:54,3,"<p>If you're looking to use the same deployed container and switch models between requests; it's not the preferred design choice for Azure machine learning service, we need to specify the model name to load during build/deploy.</p>

<p>Ideally, each deployed web-service endpoint should allow inference of one model only; with the model name defined before the container the image starts building/deploying. </p>

<p>It is mandatory that the entry script has both <code>init()</code> and <code>run(raw_data)</code> with those <strong>exact</strong> signatures. </p>

<p>At the moment, we can't change the signature of <code>init()</code> method to take a parameter like in <code>init(model_name)</code>.  </p>

<p>The only dynamic user input you'd ever get to pass into this web-service is via <code>run(raw_data)</code> method. As you have tried, given the size of your model passing it via run is not feasible. </p>

<p><code>init()</code> is run first and only <strong>once</strong> after your web-service deploy. Even if <code>init()</code> took the <code>model_name</code> parameter, there isn't a straight forward way to call this method directly and pass your desired model name.</p>

<hr>

<p>But, one possible solution is: </p>

<p>You can create params file like below and store the file in azure blob storage.</p>

<p>Example runtime parameters generation script:</p>

<pre class=""lang-py prettyprint-override""><code>import pickle

params = {'model_name': 'YOUR_MODEL_NAME_TO_USE'}

with open('runtime_params.pkl', 'wb') as file:
    pickle.dump(params, file)

</code></pre>

<p>You'll need to use <a href=""https://github.com/Azure/azure-storage-python"" rel=""nofollow noreferrer"">Azure Storage Python SDK</a> to write code that can read from your blob storage account. This also mentioned in the official docs <a href=""https://learn.microsoft.com/en-us/azure/machine-learning/service/how-to-deploy-and-where#prepare-to-deploy"" rel=""nofollow noreferrer"">here</a>.</p>

<p>Then you can access this from <code>init()</code> function in your score script. </p>

<p>Example <code>score.py</code> script:</p>

<pre class=""lang-py prettyprint-override""><code>from azure.storage.blob import BlockBlobService
import pickle

def init():

  global model

  block_blob_service = BlockBlobService(connection_string='your_connection_string')

  blob_item = block_blob_service.get_blob_to_bytes('your-container-name','runtime_params.pkl')

  params = pickle.load(blob_item.content)

  model = loadModel(params['model_name'])
</code></pre>

<p>You can store connection strings in Azure KeyVault for secure access. Azure ML Workspaces comes with built-in KeyVault integration. More info <a href=""https://learn.microsoft.com/en-us/python/api/azureml-core/azureml.core.keyvault.keyvault?view=azure-ml-py#get-secret-name-"" rel=""nofollow noreferrer"">here</a>.</p>

<p>With this approach, you're abstracting runtime params config to another cloud location rather than the container itself. So you wouldn't need to re-build the image or deploy the web-service again. Simply restarting the container will work.</p>

<hr>

<p>If you're looking to simply re-use <code>score.py</code> (not changing code) for <strong>multiple model deployments in multiple containers</strong> then here's another possible solution.</p>

<p>You can define your model name to use in web-service in a text file and read it in score.py. You'll need to pass this text file as a dependency when setting up the image config.</p>

<p>This would, however, need multiple params files for each container deployment.</p>

<p>Passing 'runtime_params.pkl' in <code>dependencies</code> to your image config (More detail example <a href=""https://github.com/rithinch/heartfulness-similar-content-service/blob/master/experiments/notebooks/Deploy%20Model%20-%20Azure.ipynb"" rel=""nofollow noreferrer"">here</a>):</p>

<pre class=""lang-py prettyprint-override""><code>image_config = ContainerImage.image_configuration(execution_script=""score.py"", 
                                                  runtime=""python"", 
                                                  conda_file=""myenv.yml"",
                                                  dependencies=[""runtime_params.pkl""],
                                                  docker_file=""Dockerfile"")
</code></pre>

<p>Reading this in your score.py <code>init()</code> function:</p>

<pre class=""lang-py prettyprint-override""><code>def init():

  global model

  with open('runtime_params.pkl', 'rb') as file:
    params = pickle.load(file)

  model = loadModel(params['model_name'])

</code></pre>

<p>Since your creating a new image config with this approach, you'll need to build the image and re-deploy the service.</p>
","11914067",1
697,58633479,2,58600732,2019-10-30 20:52:32,1,"<p>@Jason good question, those environment variables are read at build-time for the MLflow UI's Javascript assets. Since the PyPI MLflow wheel comes with pre-built Javascript assets, it's difficult to achieve your use case using a PyPI installation of <code>mlflow</code>.</p>

<p>However, you can build a custom MLflow wheel from source with the UI header hidden by following the instructions <a href=""https://github.com/mlflow/mlflow/blob/branch-1.3/CONTRIBUTING.rst#building-a-distributable-artifact"" rel=""nofollow noreferrer"">here</a>, replacing the <code>npm run build</code> step with <code>HIDE_HEADER=true npm run build</code> (basically, the idea is to set the desired environment variables prior to building Javascript assets via <code>npm run build</code>). You can then pip-install that wheel on the node hosting your MLflow server &amp; launch the server via <code>mlflow server</code>.</p>
","3151283",0
698,58674728,2,58646871,2019-11-02 19:46:52,2,"<p>The image must be passed in JSON as list of lists of floats (<em>pixel</em> is list of 3 RGB <em>values</em>, <em>row</em> is list of <em>pixels</em> and <em>image</em> is list of <em>rows</em>).</p>

<p>We need to decode and resize the JPEG image. Install <a href=""https://www.npmjs.com/package/sharp"" rel=""nofollow noreferrer"">sharp</a> package with <code>npm install sharp</code>.</p>

<p>Image preparation is as follows:</p>

<pre class=""lang-js prettyprint-override""><code>const fs = require('fs');
const sharp = require('sharp');

function imgToJson(buffer) {
  var decoded = [];

  var h;
  var w;
  var line;
  var pixel;

  var b = 0;
  for (h = 0; h &lt; 224; h++) {
      var line = [];
      for (w = 0; w &lt; 224; w++) {
          var pixel = [];

          pixel.push(buffer[b++] / 255.0); /* r */
          pixel.push(buffer[b++] / 255.0); /* g */
          pixel.push(buffer[b++] / 255.0); /* b */

          line.push(pixel);
      }
      decoded.push(line);
  }

  return decoded;
}

async function prepare_image(imagePath) {
  var jpegData = fs.readFileSync(imagePath); /* for example sake synchronous */
  const buffer = await sharp(jpegData).resize(224, 224).raw().toBuffer();
  return imgToJson(buffer);
}
</code></pre>

<p>The result of <code>prepare_image</code> is future returning the list of lists of floats representing image. The last step is to perform request:</p>

<pre class=""lang-js prettyprint-override""><code>var request = require('request');

async function perform_request(imagePath) {
  var decoded = await prepare_image(imagePath);
  var options = {
      method: 'POST',
      url: 'http://ip:port/v1/models/color:predict',
      json: { 
            instances: [{'vgg16_input': decoded}]
      }
  };

  request(options, function (err, resp, body) {
      if (err)
        cb(err);

        console.log(body);
    });
}

perform_request(""image.jpeg"");
</code></pre>
","10075090",7
699,58691478,2,58592206,2019-11-04 10:26:48,1,"<p>Though 'category_id' in the COCO JSON file starts from 1, 'class_id' in the Amazon SageMaker JSON file starts from 0.</p>

<p>Your conversion code should be like this.</p>

<pre><code>def fixCategoryId(category_id):
    return category_id - 1;

with open(coco_json_path) as f:
    js = json.load(f)
    images = js['images']
    categories = js['categories']
    annotations = js['annotations']
    for i in images:
        jsonFile = i['file_name']
        jsonFile = jsonFile.split('.')[0] + '.json'

        line = {}
        line['file'] = i['file_name']
        line['image_size'] = [{
            'width': int(i['width']),
            'height': int(i['height']),
            'depth': 3
        }]
        line['annotations'] = []
        line['categories'] = []
        for j in annotations:
            if j['image_id'] == i['id'] and len(j['bbox']) &gt; 0:
                line['annotations'].append({
                    'class_id': fixCategoryId(int(j['category_id'])),
                    'top': int(j['bbox'][1]),
                    'left': int(j['bbox'][0]),
                    'width': int(j['bbox'][2]),
                    'height': int(j['bbox'][3])
                })
                class_name = ''
                for k in categories:
                    if int(j['category_id']) == k['id']:
                        class_name = str(k['name'])
                assert class_name is not ''
                line['categories'].append({
                    'class_id': fixCategoryId(int(j['category_id'])),
                    'name': class_name
                })
        if line['annotations']:
            with open(os.path.join(sagemaker_json_path, jsonFile), 'w') as p:
                json.dump(line, p)
</code></pre>

<p><a href=""https://github.com/awslabs/amazon-sagemaker-examples/blob/master/introduction_to_amazon_algorithms/object_detection_pascalvoc_coco/object_detection_image_json_format.ipynb"" rel=""nofollow noreferrer"">In the Amazon SageMaker doc</a>, they are doing this using get_coco_mapper().</p>

<pre><code>import json
import logging

def get_coco_mapper():
    original_list = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 13, 14, 15, 16, 17, 18, 19, 20,
                    21, 22, 23, 24, 25, 27, 28, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40,
                    41, 42, 43, 44, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60,
                    61, 62, 63, 64, 65, 67, 70, 72, 73, 74, 75, 76, 77, 78, 79, 80,
                    81, 82, 84, 85, 86, 87, 88, 89, 90]
    iter_counter = 0
    COCO = {}
    for orig in original_list:
        COCO[orig] = iter_counter
        iter_counter += 1
    return COCO
</code></pre>

<p>After you trained the model, you have to check whether each loss has decreased or not.</p>

<pre><code>od_model.fit(inputs=data_channels, logs=True)

[11/04/2019 09:26:46 INFO 140651482974016] #quality_metric: host=algo-1, epoch=499, batch=11 train cross_entropy &lt;loss&gt;=(0.20304460724736212)
[11/04/2019 09:26:46 INFO 140651482974016] #quality_metric: host=algo-1, epoch=499, batch=11 train smooth_l1 &lt;loss&gt;=(0.06970448779799958)
</code></pre>

<p>If you have some questions, let us know.</p>
","7872863",1
700,58696115,2,58691120,2019-11-04 15:07:15,3,"<p>You could break the run history list writes into smaller blocks like this:</p>

<pre><code>run.log_list(""loss"", history.history[""loss""][:N])
run.log_list(""loss"", history.history[""loss""][N:])
</code></pre>

<p>Internally, the run history service concatenates the blocks with same metric name into a contiguous list.</p>
","5784983",1
701,58749320,2,58325923,2019-11-07 12:59:51,4,"<p>DeepAR returns probabilistic forecasts in terms of quantiles: by default, the 0.1, 0.2, 0.3, ..., 0.9 quantiles are returned. This means that, according to the model, in each future time step you have 10% chance of observing something lower than the 0.1 quantile, 20% chance of observing something lower than the 0.2 quantile, and so on. Quantiles are in fact in order, and they must be by definition of quantile. Hope this clarifies is a bit!</p>
","387656",0
702,58753297,2,58301879,2019-11-07 16:37:35,2,"<p>I have switched to Azure Jupyter Notebook where I installed package using pip</p>

<pre><code>!pip install statsmodels==0.9.0rc1
</code></pre>
","10962477",2
703,58756308,2,58755708,2019-11-07 20:16:02,7,"<p><strong>For choosing a SageMaker hosted notebook type:</strong></p>

<p>Do you plan to do all of your preprocessing of your data in-memory on the notebook, or do you plan to orchestrate ETL with external services? </p>

<p>If you're planning to load the dataset into memory on the notebook instance for exploration/preprocessing, the primary bottleneck here would be ensuring the instance has enough memory for your dataset. This would require at least the 16gb types (<em>.xlarge</em>) (full list of ML instance types <a href=""https://aws.amazon.com/sagemaker/pricing/instance-types/"" rel=""noreferrer"">available here</a>). Further, depending on how compute intensive your pre-processing is, and your desired pre-processing completion time, you can opt for a compute optimized instance (<em>c4, c5</em>) to speed this up.</p>

<hr>

<p><strong>For the training job, specifically:</strong></p>

<p>Using the Amazon SageMaker SDK, your training data will be loaded and distributed to the training cluster, allowing your training job to be completely separate from the instance your hosted notebook is running on.</p>

<p>Figuring out the ideal instance type for training will depend on whether your algorithm of choice/training job is memory, CPU, or IO bound. Since your dataset will likely be loaded onto your training cluster from S3, the instance you choose for your hosted notebook will have no bearing on the speed of your training job.</p>

<hr>

<p><strong>Broadly:</strong>
When it comes to SageMaker notebooks, the best practice is to use your notebook as a ""puppeteer"" or orchestrator, that calls out to external services (AWS Glue or Amazon EMR for preprocessing, SageMaker for training, S3 for storage, etc). It is best to treat them as ephemeral forms of compute/storage for building and kicking off your experiment pipeline.</p>

<p>This will allow you to more closely pair compute, storage, and hosting resources/services with the demands for your workload, ultimately resulting in the best bang for your buck by not having you pay for latent or unused resources.</p>

<hr>
","10885720",1
704,58822132,2,58652233,2019-11-12 15:49:19,0,"<p>As described in <a href=""https://www.kubeflow.org/docs/components/training/tftraining/"" rel=""nofollow noreferrer"">TFJob documentation</a> since kubeflow v0.7 <code>apiVersion</code> is  <code>kubeflow.org/v1</code></p>

<p>To fix given error run <code>kubectl edit tfjob</code>, hit <code>i</code> or <code>Insert</code> key and change <code>apiVersion:</code> to  <code>kubeflow.org/v1</code>. Then hit <code>ESC</code> <code>:wq</code> to quit editing and TFJob object will be changed.</p>
","12237732",2
705,58832363,2,58815367,2019-11-13 07:40:07,0,"<p>Using Sagemaker script mode can be much simpler than dealing with container and nginx low-level stuff like you're trying to do, have you considered that?<br>
You only need to provide the keras script:   </p>

<blockquote>
  <p>With Script Mode, you can use training scripts similar to those you would use outside SageMaker with SageMaker's prebuilt containers for various deep learning frameworks such TensorFlow, PyTorch, and Apache MXNet.</p>
</blockquote>

<p><a href=""https://github.com/aws-samples/amazon-sagemaker-script-mode/blob/master/tf-sentiment-script-mode/sentiment-analysis.ipynb"" rel=""nofollow noreferrer"">https://github.com/aws-samples/amazon-sagemaker-script-mode/blob/master/tf-sentiment-script-mode/sentiment-analysis.ipynb</a></p>
","121956",2
706,58838806,2,58802366,2019-11-13 14:00:26,6,"<p>To share your model as an endpoint, you should use lambda and API Gateway to create your API.</p>

<ol>
<li>Create an API gateway that triggers a Lambda with the HTTP POST method;</li>
<li>your lambda should instantiate the SageMaker endpoint, get the requested parameter in the event, call the SageMaker endpoint and return the predicted value. you can also create a DynamoDB to store commonly requested parameters with their answers;</li>
<li>Send the API Gateway Endpoint to your friend.</li>
</ol>

<p><a href=""https://i.stack.imgur.com/qLss4.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/qLss4.png"" alt=""enter image description here""></a></p>
","6393053",2
707,58842224,2,52437599,2019-11-13 17:08:24,11,"<p>In general terms, they serve different purposes.</p>

<p><a href=""https://aws.amazon.com/emr/"" rel=""noreferrer""><strong>EMR</strong></a> is when you need to process massive amounts of data and heavily rely on Spark, Hadoop, and MapReduce (EMR = Elastic MapReduce). Essentially, if your data is in large enough volume to make use of the efficiencies of Spark, Hadoop, Hive, HDFS, HBase and Pig stack then go with EMR.</p>

<p><strong>EMR Pros:</strong></p>

<ul>
<li>Generally, low cost compared to EC2 instances</li>
<li>As the name suggests Elastic meaning you can provision what you need when you need it</li>
<li>Hive, Pig, and HBase out of the box</li>
</ul>

<p><strong>EMR Cons:</strong></p>

<ul>
<li>You need a very specific use case to truly benefit from all the offerings in EMR. Most don't take advantage of its entire offering</li>
</ul>

<p><a href=""https://aws.amazon.com/sagemaker/"" rel=""noreferrer""><strong>SageMaker</strong></a> is an attempt to make Machine Learning easier and distributed. The marketplace provides out of the box algos and models for quick use. It's a great service if you conform to the workflows it enforces. Meaning creating training jobs, deploying inference endpoints </p>

<p><strong>SageMaker Pros:</strong></p>

<ul>
<li>Easy to get up and running with Notebooks</li>
<li>Rich marketplace to quickly try existing models</li>
<li>Many different example notebooks for popular algorithms</li>
<li>Predefined kernels that minimize configuration</li>
<li>Easy to deploy models</li>
<li>Allows you to distribute inference compute by deploying endpoints</li>
</ul>

<p><strong>SageMaker Cons:</strong></p>

<ul>
<li>Expensive!</li>
<li>Enforces a certain workflow making it hard to be fully custom</li>
<li>Expensive!</li>
</ul>
","1308771",1
708,58894446,2,58345935,2019-11-16 19:31:39,2,"<p>This is fixed in version <a href=""https://learn.microsoft.com/en-us/azure/machine-learning/service/azure-machine-learning-release-notes#azure-machine-learning-sdk-for-python-v1074"" rel=""nofollow noreferrer"">1.0.74 of azureml-sdk</a></p>
","9365426",0
709,58906510,2,58906453,2019-11-18 00:20:23,1,"<p>H2O Documentation says that:</p>

<p><code>h2o.h2o.download_csv(data, filename)</code></p>

<p><code>data : H2OFrame</code> An H2OFrame object to be downloaded.</p>

<p><code>filename : str</code> A string indicating the name that the CSV file should be should be saved to.</p>

<p>Additionally, as you have written in your question <code>/mnt/azmnt/code/Users/SA/data_pred.csv'</code> should be the path.</p>
","10489887",1
710,58920437,2,58892606,2019-11-18 18:00:08,3,"<p>I was missing the fact that SageMaker looks for your data channels in S3 and copies those to your container at <code>/opt/ml/input/data</code></p>

<p>By default it seems to use <code>training</code> and <code>validation</code> as the channel names.  Therefore, in my example above, it would have never copied data from my <code>external</code> folder on S3 to the right <code>external</code> folder in my container.  In fact, I discovered it was copying it instead to <code>/opt/ml/input/data/training/external/train.csv</code>.</p>

<p>To resolve this, I would have either had to change my folder names, or use <code>InputDataConfig</code> to define other channels.  I chose the later and was able to get it working.</p>

<p>More info on <code>InputDataConfig</code> here: <a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/API_CreateTrainingJob.html"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/sagemaker/latest/dg/API_CreateTrainingJob.html</a></p>
","2047476",0
711,58920521,2,58885873,2019-11-18 18:06:52,0,"<p>You should be able to specify nccl or gloo as distributed data parallel backend, in addition to MPI with Horovod. See the <em>distributed_training</em> parameter of <a href=""https://learn.microsoft.com/en-us/python/api/azureml-train-core/azureml.train.dnn.pytorch?view=azure-ml-py"" rel=""nofollow noreferrer"">PyTorch Estimator</a>.</p>
","5784983",0
712,58924697,2,57291797,2019-11-18 23:59:28,1,"<p>I was facing a similar issue when trying to read from s3 files, ultimately solved by updating dask to most recent version (I think the one sagemaker instances start with by default is deprecated)</p>

<h2>Install/Upgrade packages and dependencies (from notebook)</h2>

<pre><code>! python -m pip install --upgrade dask
! python -m pip install fsspec
! python -m pip install --upgrade s3fs
</code></pre>

<p>Hope this helps!</p>
","6011629",1
713,58995609,2,58992447,2019-11-22 13:47:30,1,"<p>Figured out what the issue was, you need to end an endpoint to tour VPC for DyanmoDB.</p>

<p>To do this navigate to:</p>

<ol>
<li>AWS VPC</li>
<li>Endpoints</li>
<li>Create Endpoint</li>
<li>Select the dynamodb service (will be type Gateway)</li>
<li>Select the VPC your Notebook is using</li>
</ol>
","3005674",0
714,59002763,2,58985124,2019-11-22 22:49:29,1,"<p>Good question :) using the exact same code for batch inference and online inference reduces development overhead - the exact same stack can be used for both use-cases - and also reduces risks of having different results between something done in Batch and something done online. That being said, SageMaker is very flexible and what you describe can easily be done using the Training API. There is nothing in the Training API forcing you to use it for ML training, it is actually a very versatile docker orchestrator with advanced logging, metadata persistance, and built for fast and distributed data ingestion.</p>
","5331834",0
715,59007539,2,59007538,2019-11-23 12:20:12,0,"<p>On closer inspection of my request, I noticed that I had the content type set to <code>text/plain</code>:</p>

<pre><code>===========================request begin================================================
URI         : {}http://xxxx:10026/diabetes%20model/1/predict
Method      : {}POST
Headers     : {}{Accept=[application/json], X-Auth-Token=[6EMbYcQsBD], Content-Type=[text/plain;charset=ISO-8859-1], Content-Length=[174], Accept-Charset=[big5, ...]}
Request body: {}{""use_scoring"": true, ""scoring_args"": {""NumPreg"":1.0,""Glucose"": 85.0,""BloodPressure"": 66.0,""SkinThick"": 29.0,""Insulin"": 0.0,""BMI"": 26.6,""DiabetesPedFunc"": 0.351,""Age"": 35.0}}
==========================request end===================================================
</code></pre>

<p>Changing that to <code>application/json</code> resolved the issue for me.</p>
","1033422",0
716,59048434,2,58671450,2019-11-26 10:09:49,1,"<p>At the moment it is not possible to simply stop the deployment it needs to be deleted. You can delete the deployment but retain the data in storage.
The commands needed are.</p>

<h1>If you want to delete all the resources, including storage:</h1>

<pre><code>kfctl delete -f ${CONFIG_FILE} --delete_storage
</code></pre>

<h1>If you want to preserve storage, which contains metadata and information</h1>

<h1>from Kubeflow Pipelines:</h1>

<pre><code>kfctl delete -f ${CONFIG_FILE}
</code></pre>

<p>You can also delete via the GCP console, by navigating to the Deployment Manager and deleting the resources from there, see the docs here<a href=""https://www.kubeflow.org/docs/gke/deploy/delete-ui/"" rel=""nofollow noreferrer"">1</a></p>
","6345993",0
717,59059466,2,59046257,2019-11-26 21:18:16,7,"<p>You can definitely use Airflow to orchestrate Machine Learning tasks, but you probably want to execute ML tasks remotely with operators.</p>

<p>For example, Dailymotion uses the KubernetesPodOperator to scale Airflow for ML tasks.</p>

<p>If you don't have the resources to setup a Kubernetes cluster yourself, you can use a ML platforms like <a href=""https://valohai.com"" rel=""noreferrer"">Valohai</a> that have an Airflow operator.</p>

<p>When doing ML on production, ideally you want to also version control your models to keep track of the data, code, parameters and metrics of each execution.</p>

<p>You can find more details on this article on  <a href=""https://towardsdatascience.com/scaling-apache-airflow-for-machine-learning-workflows-f2446257e495?source=friends_link&amp;sk=28aa76c4eaaaf39b041b7240ce0a2c58"" rel=""noreferrer"">Scaling Apache Airflow for Machine Learning Workflows</a></p>
","1876157",2
718,59076977,2,56870285,2019-11-27 19:05:45,1,"<p>TFX allows you to define your transform logic inside the training pipeline, and save the logic as part of the resulting model graph, so that your saved model will include both the transformations and the usual model, and tf serving will be able to accept request in pre-transform data format, and do both the proper transformations and model inference without any additional work. Therefore by design TFX is not involved in inference.</p>
","2912207",3
719,59133816,2,59069043,2019-12-02 06:42:04,0,"<p>AzureML for RStudio is now not supported as the package is removed from CRAN repository from <a href=""https://cran.r-project.org/web/packages/AzureML/index.html"" rel=""nofollow noreferrer"">2019-07-29</a>.
Azure ML studio with this package will not work as the package(AzureML) is removed.</p>

<p>Azure Machine Learning SDK for R can be Downloaded from CRAN at <a href=""https://cloud.r-project.org/web/packages/azuremlsdk/index.html"" rel=""nofollow noreferrer"">https://cloud.r-project.org/web/packages/azuremlsdk/index.html</a></p>
","11297406",0
720,59145584,2,59145582,2019-12-02 19:37:00,0,"<p>It appears the project repo is created with root as owner and no write permissions on group level. </p>

<p>To fix, you need to:</p>

<ul>
<li>create a notebook cluster</li>
<li>open a Jupyter Terminal </li>
<li><code>sudo chmod -R 777 /bd-fs-mnt/nfsrepo</code> (this only works if you create that cluster as tenant admin, as user you don't have sudo permission)</li>
</ul>
","1033422",0
721,59213104,2,59176241,2019-12-06 12:24:12,4,"<p>In fact, the solution was to import my customized class <strong>MultiColumnLabelEncoder</strong> as a pip package (You can find it through pip install multilllabelencoder==1.0.5).
Then I passed the pip package to the .yml file or in the InferenceConfig of the azure ml environment.
In the score.py file, I imported the class as follows :</p>

<pre><code>from multilabelencoder import multilabelencoder
def init():
    global model

    # Call the custom encoder to be used dfor unpickling the model
    encoder = multilabelencoder.MultiColumnLabelEncoder() 
    # Get the path where the deployed model can be found.
    model_path = os.path.join(os.getenv('AZUREML_MODEL_DIR'), 'k_means_model_45.pkl')
    model = joblib.load(model_path)
</code></pre>

<p>Then the deployment was successful. 
One more important thing is I had to use the same pip package (multilabelencoder) in the training pipeline as here :</p>

<pre><code>from multilabelencoder import multilabelencoder 
pipe = Pipeline([
    (""encoder"", multilabelencoder.MultiColumnLabelEncoder(columns)),
    ('k-means', kmeans),
])
#Training the pipeline
trainedModel = pipe.fit(df)
</code></pre>
","5159740",0
722,59213483,2,59201775,2019-12-06 12:48:22,1,"<p><p></p>

<p>I have never used Amazon RCF, but in general tree based models do not perform particularly well when using One Hot Encoding (or dummy encoding). In that sense, I would rather use a Numeric Encoding (giving numbers from 1 to len(category)) or a Binary Encoder (same thing, but with binary variables). This should allow the trees to have more meaningful splits on those variables. <p></p>

<p>In terms of Hyperparameters is hard to say, num_samples_per_trees depends on the ratio of outliers you expect to have, while num_trees will impact the amount of data in each partition, and therefore the size of the single trees, so it depends on the size of your dataset. <p></p>

<p>Try changing these things, and if you see no improvement you can try different stuff. but I suggest DBSCAN over Kmeans honestly, but to my knowledge they all need the definition of some distance or measure between your points, which is not trivial since you are using a mix of categorical an numeric variables <br></p>

<p>EDIT: <br>
1 - No, I dont think there's a way to weight features in RCF, like usually there's no way to do it in any tree based algorithm as long as I know. However, if you use distance based methods (hierarchical clustering, Kmeans, etc.) you define your own distance metric that weights differently on your features <br>
2 - Well, that's what the algorithm is for. It is supposed to find outliers based on the distribution of all features, not just one. </p>

<p><p>
You can also try Isolation Forest if you want. It does not require any metric and it is easier to understand than RCF in my opinion.</p>
","8830950",4
723,59240475,2,59238265,2019-12-08 22:27:38,1,"<p>As of April 2019: yes. An official AWS blog post was created to show how to open the SageMaker Factorization Machines artifact and extract its parameters: <a href=""https://aws.amazon.com/blogs/machine-learning/extending-amazon-sagemaker-factorization-machines-algorithm-to-predict-top-x-recommendations/"" rel=""nofollow noreferrer"">https://aws.amazon.com/blogs/machine-learning/extending-amazon-sagemaker-factorization-machines-algorithm-to-predict-top-x-recommendations/</a></p>

<p>That being said, be aware that Amazon SageMaker built-in algorithm are primarily built for deployment on AWS, and only <a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/xgboost.html"" rel=""nofollow noreferrer"">SageMaker XGBoost</a> and <a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/blazingtext.html"" rel=""nofollow noreferrer"">SageMaker BlazingText</a> are designed to produce artifacts interoperable with their open-source equivalent.</p>
","5331834",2
724,59326556,2,58956459,2019-12-13 16:37:32,9,"<p>the problem here is that both <code>mlflow</code> and <code>nginx</code> are trying to run on the <strong>same port</strong>... </p>

<ol>
<li><p>first lets deal with nginx:</p>

<p>1.1 in /etc/nginx/sites-enable make a new file <code>sudo nano mlflow</code> and delete the exist default.</p>

<p>1.2 in mlflow file:</p></li>
</ol>

<pre><code>server {
    listen YOUR_PORT;
    server_name YOUR_IP_OR_DOMAIN;
    auth_basic           “Administrator’s Area”;
    auth_basic_user_file /etc/apache2/.htpasswd; #read the link below how to set username and pwd in nginx

    location / {
        proxy_pass http://localhost:8000;
        include /etc/nginx/proxy_params;
        proxy_redirect off;
    }
}
</code></pre>

<p>1.3.  restart nginx <code>sudo systemctl restart nginx</code></p>

<ol start=""2"">
<li>on your server run mlflow  <code>mlflow server --host localhost --port 8000</code></li>
</ol>

<p>Now if you try access the YOUR_IP_OR_DOMAIN:YOUR_PORT within your browser an auth popup should appear, enter your host and pass and now you in mlflow</p>

<ol start=""3"">
<li><p>now there are 2 options to tell the mlflow server about it:</p>

<p>3.1 set username and pwd as environment variable 
<code>export MLFLOW_TRACKING_USERNAME=user export MLFLOW_TRACKING_PASSWORD=pwd</code></p>

<p>3.2 edit in your <code>/venv/lib/python3.6/site-packages/mlflowpackages/mlflow/tracking/_tracking_service/utils.py</code> the function </p></li>
</ol>

<pre><code>def _get_rest_store(store_uri, **_):
    def get_default_host_creds():
        return rest_utils.MlflowHostCreds(
            host=store_uri,
            username=replace with nginx user
            password=replace with nginx pwd
            token=os.environ.get(_TRACKING_TOKEN_ENV_VAR),
            ignore_tls_verification=os.environ.get(_TRACKING_INSECURE_TLS_ENV_VAR) == 'true',
        )
</code></pre>

<p>in your .py file where you work with mlflow:</p>

<pre><code>import mlflow
remote_server_uri = ""YOUR_IP_OR_DOMAIN:YOUR_PORT"" # set to your server URI
mlflow.set_tracking_uri(remote_server_uri)
mlflow.set_experiment(""/my-experiment"")
with mlflow.start_run():
    mlflow.log_param(""a"", 1)
    mlflow.log_metric(""b"", 2)
</code></pre>

<p>A link to nginx authentication doc <a href=""https://docs.nginx.com/nginx/admin-guide/security-controls/configuring-http-basic-authentication/"" rel=""noreferrer"">https://docs.nginx.com/nginx/admin-guide/security-controls/configuring-http-basic-authentication/</a></p>
","11306005",3
725,59349953,2,59349805,2019-12-16 02:46:24,1,"<p>I expect your rational is correct. There isn't an entry for your region in the code. I don't know if there's a list of these containers per region. That being said, you find them in ECR (Elastic Container Registry). </p>

<p>Keep in mind, that you can probably fix this quickly by switching to one of the supported regions. Otherwise:</p>

<p>If AWS doesn't have a publicly listed container in your region, you can register the container yourself in AWS with ECR. You'll need to login to ECR using the AWS CLI and docker login.</p>

<p>You can use the command <code>aws ecr get-login --region ap-northeast-2</code> in order to get the token you'll need for docker login.</p>

<p>Then, clone this repo: <a href=""https://github.com/aws/sagemaker-xgboost-container"" rel=""nofollow noreferrer"">https://github.com/aws/sagemaker-xgboost-container</a></p>

<p>You can build this image locally and push it up to ECR. After that, login to the AWS console (or use the AWS CLI) and find the ARN of the image. It should match the format of the others in your code. </p>

<p>After that, just add another key/value entry into the code for your <code>containers</code> variable and use <code>'ap-northeast-2': '&lt;ARN of the docker image&gt;'</code></p>
","5310188",0
726,59356107,2,59328925,2019-12-16 11:53:02,1,"<p>Please follow the below notebook, If you want to test deploying a model rapidly you should check out 
<a href=""https://github.com/Azure/MachineLearningNotebooks/tree/master/how-to-use-azureml/deployment/deploy-to-local"" rel=""nofollow noreferrer"">https://github.com/Azure/MachineLearningNotebooks/tree/master/how-to-use-azureml/deployment/deploy-to-local</a></p>

<p>the SDK enables building and running the docker locally and updating in place as you iterate on your script to save time.</p>
","11297406",1
727,59383733,2,59150100,2019-12-17 23:52:25,2,"<p>Looks like you are using one of the older Tensorflow versions.
We would recommend switching to a newer more straight-forward way of running Tensorflow in SageMaker (script mode) by switching to a more recent Tensorflow version.</p>

<p>You can read more about it in our documentation:
<a href=""https://sagemaker.readthedocs.io/en/stable/using_tf.html"" rel=""nofollow noreferrer"">https://sagemaker.readthedocs.io/en/stable/using_tf.html</a></p>

<p>Here is an example that might help:
<a href=""https://github.com/awslabs/amazon-sagemaker-examples/blob/master/sagemaker-python-sdk/tensorflow_script_mode_training_and_serving/tensorflow_script_mode_training_and_serving.ipynb"" rel=""nofollow noreferrer"">https://github.com/awslabs/amazon-sagemaker-examples/blob/master/sagemaker-python-sdk/tensorflow_script_mode_training_and_serving/tensorflow_script_mode_training_and_serving.ipynb</a> </p>
","10298529",0
728,59419604,2,59392060,2019-12-20 03:50:12,1,"<p>our system does our best guess over file settings when you try to create a dataset, but cannot guarantee perfect guesses in all cases. </p>

<p>In such scenarios, you should be able to adjust the settings. We had a bug with the ability to change those settings, but rolled out a fix. Can you try to change those now?  </p>
","12568566",1
729,59454463,2,59446807,2019-12-23 11:21:55,1,"<p>distributed training is model and framework specific. Not all models are easy to distribute, and from ML framework to ML framework things are not equally easy. <strong>It is rarely automatic, even less so with TensorFlow and Keras.</strong></p>

<p>Neural nets are conceptually easy to distribute under the data-parallel paradigm, whereby the gradient computation of a given mini-batch is split among workers, which could be multiple devices in the same host (multi-device) or multiple hosts with each multiple devices (multi-device multi-host). The D2L.ai course provides an in-depth view of how neural nets are distributed <a href=""http://d2l.ai/chapter_computational-performance/multiple-gpus.html"" rel=""nofollow noreferrer"">here</a> and <a href=""http://d2l.ai/chapter_computational-performance/parameterserver.html"" rel=""nofollow noreferrer"">here</a>.</p>

<p>Keras used to be trivial to distribute in multi-device, single host fashion with the <a href=""https://www.tensorflow.org/api_docs/python/tf/keras/utils/multi_gpu_model"" rel=""nofollow noreferrer""><code>multi_gpu_model</code>, which will sadly get deprecated in 4 months</a>. In your case, you seem to refer to multi-host model (more than one machine), and that requires writing ad-hoc synchronization code such as the one seen in <a href=""https://www.tensorflow.org/tutorials/distribute/multi_worker_with_keras"" rel=""nofollow noreferrer"">this official tutorial</a>. </p>

<p>Now let's look at how does this relate to SageMaker.</p>

<p>SageMaker comes with 3 options for algorithm development. Using distributed training may require a varying amount of custom work depending on the option you choose:</p>

<ol>
<li><p>The <strong>built-in algorithms</strong> is a library of 18 pre-written algorithms. Many of them are written to be distributed in single-host multi-GPU or multi-GPU multi-host. With that first option, you don't have anything to do apart from setting <code>train_instance_count</code> > 1 to distribute over multiple instances</p></li>
<li><p>The <strong>Framework containers</strong> (the option you are using) are containers developed for popular frameworks (TensorFlow, PyTorch, Sklearn, MXNet) and provide pre-written docker environment in which you can write arbitrary code. In this options, some container will support one-click creation of ephemeral training clusters to do distributed training, <strong>however using <code>train_instance_count</code> greater than one is not enough to distribute the training of your model. It will just run your script on multiple machines. In order to distribute your training, you must write appropriate distribution and synchronization code in your <code>mnist_keras_tf.py</code> script.</strong> For some frameworks such code modification will be very simple, for example for TensorFlow and Keras, SageMaker comes with Horovod pre-installed. Horovod is a peer-to-peer ring-style communication mechanism that requires very little code modification and is highly scalable (<a href=""https://eng.uber.com/horovod/"" rel=""nofollow noreferrer"">initial annoucement from Uber</a>, <a href=""https://sagemaker.readthedocs.io/en/stable/using_tf.html#training-with-horovod"" rel=""nofollow noreferrer"">SageMaker doc</a>, <a href=""https://github.com/awslabs/amazon-sagemaker-examples/blob/master/sagemaker-python-sdk/tensorflow_script_mode_horovod/tensorflow_script_mode_horovod.ipynb"" rel=""nofollow noreferrer"">SageMaker example</a>, <a href=""https://aws.amazon.com/fr/blogs/machine-learning/launching-tensorflow-distributed-training-easily-with-horovod-or-parameter-servers-in-amazon-sagemaker/"" rel=""nofollow noreferrer"">SageMaker blog post</a>). My recommendation would be to try using Horovod to distribute your code. Similarly, in Apache MXNet you can easily create Parameter Stores to host model parameters in a distributed fashion and sync with them from multiple nodes. MXNet scalability and ease of distribution is one of the reason Amazon loves it.</p></li>
<li><p>The <strong>Bring-Your-Own Container</strong> requires you to write both docker container and algorithm code. In this situation, you can of course distribute your training over multiple machines but you also have to write machine-to-machine communication code</p></li>
</ol>

<p>For your specific situation my recommendation would be to scale horizontally first in a single node with multiple GPUs over bigger and bigger machine types, because latency and complexity increase drastically as you switch from single-host to multi-host context. If truly necessary, use multi-node context and things maybe easier if that's done with Horovod.
In any case, things are still much easier to do with SageMaker since it manages creation of ephemeral, billed-per-second clusters with built-in, logging and metadata and artifacts persistence and also handles fast training data loading from s3, sharded over training nodes. </p>

<p><strong>Note on the relevancy of distributed training</strong>: Keep in mind that when you distribute over N devices a model that was running fine on one device, you usually grow the batch size by N so that the per-device batch size stays constant and each device keeps being busy. This will disturb your model convergence, because bigger batches means a less noisy SGD. A common heuristic is to grow the learning rate by N (more info in <a href=""https://arxiv.org/abs/1706.02677"" rel=""nofollow noreferrer"">this great paper from Priya Goyal et al</a>), but this on the other hand induces instability at the first couple epochs, so it is sometimes associated with a learning rate warmup. Scaling SGD to work well with very large batches is still an active research problem, with new ideas coming up frequently. Reaching good model performance with very large batches sometimes require ad-hoc research and a fair amount of parameter tuning, occasionally to the extent where the extra money spent on finding how to distribute well overcome the benefits of the faster training you eventually manage to run. A situation where distributed training makes sense is when an individual record represent too much compute to form a big enough physical batch on a device, a situation seen on big input sizes (eg vision over HD pictures) or big parameter counts (eg BERT). That being said, for those models requiring very big logical batch you don't necessarily have to distribute things physically: you can run sequentially N batches through your single GPU and wait N per-device batches before doing the gradient averaging and parameter update to simulate having an N times bigger GPU. (a clever hack sometimes called <em>gradient accumulation</em>)</p>
","5331834",1
730,59465813,2,59464404,2019-12-24 08:15:50,1,"<p>Try 
<code>docker build -t test .</code></p>

<p>instead of
<code>docker build - &lt; Dockerfile</code></p>
","7647323",3
731,59466911,2,59430560,2019-12-24 09:51:31,3,"<p>Some Python packages (such as Pandas) support reading data directly from S3, as it is the most popular location for data. See <a href=""https://stackoverflow.com/questions/37703634/how-to-import-a-text-file-on-aws-s3-into-pandas-without-writing-to-disk"">this question</a> for example on the way to do that with Pandas.</p>

<p>If the package (npTDMS) doesn't support reading directly from S3, you should copy the data to the local disk of the notebook instance.</p>

<p>The simplest way to copy is to run the AWS CLI in a cell in your notebook</p>

<pre><code>!aws s3 cp s3://bucket_name/path_to_your_data/ data/
</code></pre>

<p>This command will copy all the files under the ""folder"" in S3 to the local folder <code>data</code></p>

<p>You can use more fine-grained copy using the filtering of the files and other specific requirements using the boto3 rich capabilities. For example:</p>

<pre><code>s3 = boto3.resource('s3')
bucket = s3.Bucket('my-bucket')
objs = bucket.objects.filter(Prefix='myprefix')
for obj in objs:
   obj.download_file(obj.key)
</code></pre>
","179529",1
732,59474266,2,59194004,2019-12-25 00:04:03,1,"<p>That style of referencing model artefacts is fixed from mlflow v1.5 (<a href=""https://github.com/mlflow/mlflow/pull/2067"" rel=""nofollow noreferrer"">Bug Fix</a>).</p>

<p>You'll need to run <code>mlflow db upgrade &lt;db uri&gt;</code> to refresh your schemas before restarting your mlflow server.</p>

<p>You may find listing registered models helpful:</p>

<p><code>&lt;server&gt;:&lt;port&gt;/api/2.0/preview/mlflow/registered-models/list</code></p>
","3159586",0
733,59486545,2,59386244,2019-12-26 09:42:08,1,"<p>Sorry for this. Take a look at the Jupyter Notebook version of the same tutorial:
<a href=""https://github.com/Azure/MachineLearningNotebooks/blob/master/how-to-use-azureml/ml-frameworks/tensorflow/deployment/train-hyperparameter-tune-deploy-with-tensorflow/train-hyperparameter-tune-deploy-with-tensorflow.ipynb"" rel=""nofollow noreferrer"">https://github.com/Azure/MachineLearningNotebooks/blob/master/how-to-use-azureml/ml-frameworks/tensorflow/deployment/train-hyperparameter-tune-deploy-with-tensorflow/train-hyperparameter-tune-deploy-with-tensorflow.ipynb</a></p>

<p>When configuring estimator, you need to specify the pip packages u wanna install on the remote compute. In this case, azureml-dataprep[fuse, blob]. Installing the package to your local computer is not useful since the training script is executed on the remote compute target, which doesn't have the required package installed yet. </p>

<pre><code>est = TensorFlow(source_directory=script_folder,
             script_params=script_params,
             compute_target=compute_target,
             entry_script='tf_mnist.py',
             use_gpu=True,
             pip_packages=['azureml-dataprep[pandas,fuse]'])
</code></pre>

<p>Can you pls try the fix and let us know whether it solves your issue :) In the mean time, I will update the public documentation to include pip_packages in estimator config.</p>
","12600227",0
734,59569185,2,59517355,2020-01-02 19:50:37,0,"<p>I resolved this answer by setting the path and the data_script variables in the AutoMLConfig task object, like this (relevant code indicated by -->):</p>

<pre><code>automl_config = AutoMLConfig(task = 'forecasting',
                             debug_log = 'automl_errors.log',
                             compute_target=compute_target,
                             run_configuration=conda_run_config,
                             --&gt;path = ""c:\\users\\me"",
                             data_script =""script.py"",&lt;--
                             **automl_settings
                            )
</code></pre>

<p>Setting the data_script variable to include the full path, as shown below, did not work.</p>

<pre><code>automl_config = AutoMLConfig(task = 'forecasting',
                             debug_log = 'automl_errors.log',
                             path = ""."",
                             --&gt;data_script = ""c:\\users\\me\\script.py""&lt;--
                             compute_target=compute_target,
                             run_configuration=conda_run_config, 
                             **automl_settings
                            )
</code></pre>
","10631172",0
735,59694422,2,58933565,2020-01-11 12:33:03,15,"<p>I was able to fix the same issue (<a href=""https://learn.microsoft.com/en-us/python/api/azureml-core/azureml.exceptions.modelpathnotfoundexception?view=azure-ml-py"" rel=""noreferrer""><code>ModelPathNotFoundException</code></a>) by explicitly uploading the model into the run history record before trying to register the model:</p>

<pre><code>run.upload_file(""outputs/my_model.pickle"", ""outputs/my_model.pickle"")
</code></pre>

<p>Which I found surprising because this wasn't mentioned in many of the official examples and according to the <code>upload_file()</code> <a href=""https://learn.microsoft.com/en-us/python/api/azureml-core/azureml.core.run.run?view=azure-ml-py#upload-file-name--path-or-stream-"" rel=""noreferrer"">documentation</a>:</p>

<blockquote>
  <p>Runs automatically capture file in the specified output directory, which defaults to ""./outputs"" for most run types. Use upload_file only when additional files need to be uploaded or an output directory is not specified.</p>
</blockquote>
","5524090",2
736,59789014,2,59648275,2020-01-17 14:08:07,0,"<p>If you really would like to use a complete separate by yourself build docker image, you should create an Amazon Sagemaker algorithm (which is one of the options in the Sagemaker menu). Here you have to specify a link to your docker image on amazon ECR as well as the input parameters and data channels etc. When choosing this options, you should <strong>not</strong> use the PyTorch estimater but the <a href=""https://sagemaker.readthedocs.io/en/stable/algorithm.html"" rel=""nofollow noreferrer"">Algoritm estimater</a>. This way you indeed don't have to specify an entrypoint because it simple runs the docker when training and the default entrypoint can be defined in your docker file.</p>

<p>The Pytorch estimator can be used when having you own model code, but you would like to run this code in an off-the-shelf Sagemaker PyTorch docker image. This is why you have to for example specify the PyTorch framework version. In this case the entrypoint file by default should be placed next to where your jupyter notebook is stored (just upload the file by clicking on the upload button). The PyTorch estimator inherits all options from the <a href=""https://sagemaker.readthedocs.io/en/stable/estimators.html#sagemaker.estimator.Framework"" rel=""nofollow noreferrer"">framework estimator</a> where options can be found where to place the entrypoint and model, for example <em>source_dir</em>.</p>
","5546174",0
737,59789909,2,59762829,2020-01-17 15:03:32,2,"<p>The 'x86_64-linux-gnu-gcc' binary can't be found in environment where you're building the container. Make sure that gcc is installed, and that you use the right name (gcc?).</p>
","4686192",0
738,59869802,2,59773167,2020-01-22 23:47:09,1,"<p>I am assuming you use sql store?</p>

<p>Currently there is no way to tell mlflow to hard-delete experiments. We are working with open source contributors to add a cli command that would perform garbage-collection of deleted experiments. This should be added soon in one of the upcoming mlflow releases. In the meantime, you can connect to your sql store and delete the experiments manually.</p>
","12763888",0
739,59876371,2,59864823,2020-01-23 10:28:59,1,"<p>You can't pass a dataframe directly to the built-in KNN algo. It supports two input training formats: CSV, or RecordIO protobuf: <a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/kNN-in-formats.html"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/sagemaker/latest/dg/kNN-in-formats.html</a>.</p>

<p>The latter is more efficient, so it's the one we recommend.</p>

<p>In your case, you would simply need to convert your dataframe to a numpy array with to_numpy(), and then you can reuse the code in the notebook.</p>

<pre class=""lang-python prettyprint-override""><code>import pandas as pd
index = [1, 2, 3, 4]
a = ['a', 'b', 'c', 'd']
b = [1, 2, 3, 4]
df = pd.DataFrame({'A': a, 'B': b}, index=index)
n = df.to_numpy()
print(n)
type(n)
</code></pre>



<p>The notebook you're using is actually showing how to use KNN for classification. This clustering example may be easier to understand: <a href=""https://data.solita.fi/machine-learning-building-blocks-in-aws-sagemaker/"" rel=""nofollow noreferrer"">https://data.solita.fi/machine-learning-building-blocks-in-aws-sagemaker/</a></p>
","4686192",2
740,59883967,2,59882941,2020-01-23 17:17:27,4,"<p>I actually tried to modify my training script to the following : </p>

<pre><code>### Code to add in a tensorflow_estimator.py file

import argparse
import os
import pathlib
import tensorflow as tf


if __name__ == '__main__':


    parser = argparse.ArgumentParser()

    # hyperparameters sent by the client are passed as command-line arguments to the script.
    parser.add_argument('--epochs', type=int, default=10)
    parser.add_argument('--batch_size', type=int, default=100)
    parser.add_argument('--learning_rate', type=float, default=0.1)

    # Data, model, and output directories
    parser.add_argument('--output-data-dir', type=str, default=os.environ.get('SM_OUTPUT_DATA_DIR'))
    parser.add_argument('--model-dir', type=str, default=os.environ.get('SM_MODEL_DIR'))
    parser.add_argument('--train', type=str, default=os.environ.get('SM_CHANNEL_TRAIN'))
    parser.add_argument('--test', type=str, default=os.environ.get('SM_CHANNEL_TEST'))

    args, _ = parser.parse_known_args()

    print(""##### ARGS ##### \n{}"".format(args))

    # Get files 
    path = pathlib.Path(args.train)

    # Print out folder content 
    for item in path.iterdir():
        print(""##### DIRECTORIES ##### \n{}"".format(item))

    # Get all images 
    all_images = list(path.glob(""*/*""))
    all_image_paths = [str(path) for path in list(path.glob(""*/*""))]

    # Transform images into tensors
    def preprocess_and_load_images(path):
        image = tf.io.read_file(path)
        image = tf.image.decode_jpeg(image, channels=3)
        image = tf.image.resize(image, [192, 192])
        return image

    # Apply preprocessing function
    ds_paths = tf.data.Dataset.from_tensor_slices(all_image_paths)
    ds_images = ds_paths.map(preprocess_and_load_images)

    # Map Labels
    labels = []
    for data in path.iterdir():  
        if data.is_dir():               
            labels += [data.name]    

    labels_index = {}
    for i,label in enumerate(labels):
        labels_index[label]=i

    print(""##### Label Index ##### \n{}"".format(labels_index))

    all_image_labels = [labels_index[path.parent.name] for path in list(path.glob(""*/*""))]

    # Create a tf Dataset
    labels_ds = tf.data.Dataset.from_tensor_slices(all_image_labels)

    # Zip train and labeled dataset
    full_ds = tf.data.Dataset.zip((ds_images, labels_ds))

    # Shuffle Dataset and batch it 
    full_ds = full_ds.shuffle(len(all_images)).batch(args.batch_size)

    # Create a pre-trained model 
    base_model = tf.keras.applications.InceptionV3(input_shape=(192,192,3), 
                                               include_top=False,
                                               weights = ""imagenet""
                                               )

    base_model.trainable = False
    model = tf.keras.Sequential([
                base_model,
                tf.keras.layers.GlobalAveragePooling2D(),
                tf.keras.layers.Dense(len(labels), activation=""softmax"")
            ])

    initial_learning_rate = args.learning_rate

    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(
        initial_learning_rate,
        decay_steps=1000,
        decay_rate=0.96,
        staircase=True) 

    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate = lr_schedule),
              loss = tf.keras.losses.SparseCategoricalCrossentropy(),
              metrics = [tf.keras.metrics.SparseCategoricalAccuracy()])

    # Train the model
    model.fit(full_ds, epochs=args.epochs)

    # Save the model 
    model.save(os.path.join(args.model_dir, ""tensorflow_model/1""), save_format=""tf"")

</code></pre>

<p>It seems that it's important to have a numerical name for your folder:</p>

<pre class=""lang-py prettyprint-override""><code># Save the model
model.save(os.path.join(args.model_dir, ""tensorflow_model/1""), save_format=""tf"")
</code></pre>
","11728128",2
741,59889346,2,59881727,2020-01-24 01:38:28,2,"<p>Can you please click on the ""Execute R module"" and download the 70_driver.log? I tried message(""STARTING R script run."") in an R sample and can found the output there.</p>

<p><a href=""https://i.stack.imgur.com/Z7s7h.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Z7s7h.png"" alt=""view logs for a execute R script module""></a></p>
","11999153",5
742,60099034,2,60072981,2020-02-06 15:59:31,7,"<p>Not every python library that is designed to work with a file system (tarfile.open, in this example) knows how to read an object from S3 as a file. </p>

<p>The simple way to solve it is to first copy the object into the local file system as a file.</p>

<pre><code>import boto3

s3 = boto3.client('s3')
s3.download_file('BUCKET_NAME', 'OBJECT_NAME', 'FILE_NAME')
</code></pre>
","179529",1
743,60103666,2,60103606,2020-02-06 21:18:55,1,"<p>It looks like you're trying to save a file <code>data.pkl</code> to a directory <code>/mnt/batch/tasks/shared/LS_root/jobs/azureml/mounts/workspaceblobstore/azureml/output_data</code> that already has a directory (not file) named <code>data.pkl</code>. Try removing the directory <code>data.pkl</code> first before re-running. If the error appears again, you must be accidentally creating the directory <code>data.pkl</code> somewhere in your code.</p>
","3081283",1
744,60156371,2,60156370,2020-02-10 18:31:36,2,"<p>The problem is still IAM permission. </p>

<p>I created a new notebook instance and a new IAM role. You would be asked how to access s3 bucket. I chose <code>all s3 bucket</code>. Then the problem solved. 
<a href=""https://i.stack.imgur.com/B0qOO.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/B0qOO.png"" alt=""enter image description here""></a></p>

<p><br>
<br>
<strong>[Solution]</strong>
In Resource tab, check whether bucket name is general.
 <a href=""https://i.stack.imgur.com/LL6Fw.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/LL6Fw.png"" alt=""enter image description here""></a></p>

<p>If you changed old IAM and it is not working, you can create a new IAM role. And attach this role to the notebook.</p>
","9355534",0
745,60234772,2,60197897,2020-02-14 23:39:55,1,"<p>you can bring to the endpoint instance (either in the <code>model.tar.gz</code> or via later download) a file storing the mapping between indexes and record names; this way you can translate from neighbor IDs to record names on the fly in the <code>predict_fn</code> or in the <code>output_fn</code>. For giant indexes this mapping (along with other metadata) can be in an external database too (eg dynamoDB, redis)</p>

<p>the link you attach (SageMaker Batch Transform) is quite a different concept; it's for instantiating ephemeral fleet of machine(s) to run a one-time prediction task with input data in S3 and results written to s3. You question seem to refer to the alternative, permanent, real-time endpoint deployment mode.</p>
","5331834",0
746,60234841,2,60190365,2020-02-14 23:54:16,2,"<p>you're correct, <strong>there has been a major, beneficial change last year in the SageMaker TensorFlow experience named the <em>Script Mode</em> formalism</strong>. As you can see in the <a href=""https://sagemaker.readthedocs.io/en/stable/"" rel=""nofollow noreferrer"">SDK Documentation</a>:</p>

<p><em>""Warning.
We have added a new format of your TensorFlow training script with TensorFlow version 1.11. This new way gives the user script more flexibility. This new format is called Script Mode, as opposed to Legacy Mode, which is what we support with TensorFlow 1.11 and older versions. In addition we are adding Python 3 support with Script Mode. The last supported version of Legacy Mode will be TensorFlow 1.12. Script Mode is available with TensorFlow version 1.11 and newer. Make sure you refer to the correct version of this README when you prepare your script. You can find the Legacy Mode README here.""</em></p>

<p>with TensorFlow 2, you need to follow that <em>Script Mode</em> formalism and save your model in the <code>opt/ml/model</code> path, otherwise nothing will be sent to S3. <em>Script Mode</em> is quite straightforward to implement and gives better flexibility and portability, and this spec is shared with SageMaker Sklearn container, SageMaker Pytorch container and SageMaker MXNet container so definitely worth adopting</p>
","5331834",1
747,60266473,2,60265992,2020-02-17 16:18:12,2,"<p>I'm assuming that you're:</p>

<ul>
<li>using Azure ML Studio (i.e. <a href=""https://ml.azure.com/"" rel=""nofollow noreferrer""><code>ml.azure.com/</code></a>)'s Pipeline Designer, and </li>
<li>creating a new compute target before running?</li>
</ul>

<p>If so, then 10 minutes is normal for the first run, given that a cluster of VMs has to be created and provisioned with a Docker container and Conda environment. After the run first completes the compute target is configured to stay on and available for two hours so future runs should execute without the 10 minute delay (provided you don't change the Conda dependencies or choose a new compute target).</p>

<p><a href=""https://learn.microsoft.com/en-us/azure/machine-learning/how-to-set-up-training-targets#amlcompute"" rel=""nofollow noreferrer"">more information about AMLCompute</a></p>
","3842610",0
748,60303889,2,60303714,2020-02-19 15:34:56,2,"<p>Okay, I found an answer myself. Hope that is okay.</p>

<p>In my input sheet, the title was something like this ""Interest Rate %"". Looks like azure was trying to say that it does not like special characters it the column names.</p>

<p>I edited my original csv file in excel, and removed the % in all the titles. </p>

<p>Then, created a new data store. problem solved. </p>
","5338888",1
749,60346880,2,60334889,2020-02-21 22:12:37,1,"<p>We can't repro the issue, can you help gives us more details? One possibility is that the kernel has bugs and hangs (could be due to extensions, widgets installed) or the resources on the machine are exhausted and kernel dies. What VM type are you using? If it's a small VM you may ran out of resources.</p>
","988956",2
750,60353322,2,60341782,2020-02-22 14:47:24,2,"<p>The reason was that we granted permission to the bucket not the objects.  That would be granting <code>""Resource"": ""arn:aws:s3:::bucket-name/""</code> but not <code>""Resource"": ""arn:aws:s3:::bucket-name/*""</code></p>
","3763782",0
751,60366262,2,60365473,2020-02-23 19:57:47,13,"<p>Let me try to summarize how does DVC store data and I hope you'll be able to figure our from this how much space will be saved/consumed in your specific scenario.</p>

<p><strong>DVC is storing and deduplicating data on the individual <em>file level</em>.</strong> So, what does it usually mean from a practical perspective.</p>

<p>I will use <code>dvc add</code> as an example, but the same logic applies to all commands that save data files or directories into DVC cache - <code>dvc add</code>, <code>dvc run</code>, etc.</p>

<h2>Scenario 1: Modifying file</h2>

<p>Let's imagine I have a single 1GB XML file. I start tracking it with DVC:</p>

<pre class=""lang-sh prettyprint-override""><code>$ dvc add data.xml
</code></pre>

<p>On the modern file system (or if <code>hardlinks</code>, <code>symlinks</code> are enabled, see <a href=""https://dvc.org/doc/user-guide/large-dataset-optimization"" rel=""noreferrer"">this</a> for more details) after this command we still consume 1GB (even though file is moved into DVC cache and is still present in the workspace).</p>

<p>Now, let's change it a bit and save it again:</p>

<pre class=""lang-sh prettyprint-override""><code>$ echo ""&lt;test/&gt;"" &gt;&gt; data.xml
$ dvc add data.xml
</code></pre>

<p>In this case we will have 2GB consumed. <strong>DVC does not do diff between two versions of the same file</strong>, neither it splits files into chunks or blocks to understand that only small portion of data has changed.</p>

<blockquote>
  <p>To be precise, it calculates <code>md5</code> of each file and save it in the content addressable key-value storage. <code>md5</code> of the files serves as a key (path of the file in cache) and value is the file itself:</p>
  
  <pre class=""lang-sh prettyprint-override""><code>(.env) [ivan@ivan ~/Projects/test]$ md5 data.xml
0c12dce03223117e423606e92650192c

(.env) [ivan@ivan ~/Projects/test]$ tree .dvc/cache
.dvc/cache
└── 0c
   └── 12dce03223117e423606e92650192c

1 directory, 1 file

(.env) [ivan@ivan ~/Projects/test]$ ls -lh data.xml
data.xml ----&gt; .dvc/cache/0c/12dce03223117e423606e92650192c (some type of link)
</code></pre>
</blockquote>

<h2>Scenario 2: Modifying directory</h2>

<p>Let's now imagine we have a single large 1GB directory <code>images</code> with a lot of files:</p>

<pre class=""lang-sh prettyprint-override""><code>$ du -hs images
1GB

$ ls -l images | wc -l
1001

$ dvc add images
</code></pre>

<p>At this point we still consume 1GB. Nothing has changed. But if we modify the directory by adding more files (or removing some of them):</p>

<pre><code>$ cp /tmp/new-image.png images

$ ls -l images | wc -l
1002

$ dvc add images
</code></pre>

<p>In this case, after saving the new version we <strong>still close to 1GB</strong> consumption. <strong>DVC calculates diff on the directory level.</strong> It won't be saving all the files that were existing before in the directory.</p>

<p>The same logic applies to all commands that save data files or directories into DVC cache - <code>dvc add</code>, <code>dvc run</code>, etc.</p>

<p>Please, let me know if it's clear or we need to add more details, clarifications.</p>
","298182",4
752,60378633,2,60342645,2020-02-24 15:03:17,2,"<p>I appear to have had User Access Administrator role only (in addition to Classic Service Administrator). As soon as I added myself to the Owner role in the Access Control (IAM) section of the Azure Portal, the deployment succeeded.</p>
","12939723",0
753,60381962,2,60380154,2020-02-24 18:30:06,7,"<p>You can follow the steps below: <br>
1. write dataframe to a local file (e.g. csv, parquet)</p>

<pre><code>local_path = 'data/prepared.csv'
df.to_csv(local_path)
</code></pre>

<ol start=""2"">
<li>upload the local file to a datastore on the cloud</li>
</ol>

<pre><code># azureml-core of version 1.0.72 or higher is required
# azureml-dataprep[pandas] of version 1.1.34 or higher is required
from azureml.core import Workspace, Dataset

subscription_id = 'xxxxxxxxxxxxxxxxxxxxx'
resource_group = 'xxxxxx'
workspace_name = 'xxxxxxxxxxxxxxxx'

workspace = Workspace(subscription_id, resource_group, workspace_name)
# get the datastore to upload prepared data
datastore = workspace.get_default_datastore()
# upload the local file from src_dir to the target_path in datastore
datastore.upload(src_dir='data', target_path='data')
</code></pre>

<ol start=""3"">
<li>create a dataset referencing the cloud location</li>
</ol>

<pre><code>ds = Dataset.Tabular.from_delimited_files(datastore.path('data/prepared.csv'))
</code></pre>
","12600227",2
754,60385894,2,59091944,2020-02-25 00:24:13,0,"<p>SageMaker's Linear Learner algorithm container does not currently implement the requirements for <a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/build-multi-model-build-container.html"" rel=""nofollow noreferrer"">multi-model endpoints</a>. You could request support in the <a href=""https://forums.aws.amazon.com/forum.jspa?forumID=285&amp;start=0"" rel=""nofollow noreferrer"">AWS Forums</a>.</p>

<p>You could also build your own version of the Linear Learner algorithm. To deploy the models to a multi-model endpoint you would need to build your own container that meets the requirements for multi-model endpoints and implement your own version of the Linear Learner algorithm. This sample notebook gives an example of how you would create your multi-model compatible container that serves MxNet models, but you could adapt it to implement a Linear Learner algorithm:</p>

<p><a href=""https://github.com/awslabs/amazon-sagemaker-examples/blob/master/advanced_functionality/multi_model_bring_your_own/multi_model_endpoint_bring_your_own.ipynb"" rel=""nofollow noreferrer"">https://github.com/awslabs/amazon-sagemaker-examples/blob/master/advanced_functionality/multi_model_bring_your_own/multi_model_endpoint_bring_your_own.ipynb</a></p>
","11036005",0
755,60387955,2,60000573,2020-02-25 05:20:36,3,"<p>This was due to incompatible CUDA and Tensorflow versions.
below versions work well with each other</p>
<blockquote>
<p>tensorflow-gpu==2.0.0</p>
<p>tensorflow-addons==0.6.0</p>
<p>nvidia/cuda:10.0-cudnn7-runtime</p>
</blockquote>
","207817",0
756,60406190,2,60397252,2020-02-26 03:11:50,2,"<p>I can reproduce your issue, the interactive mode should support the <code>azure-cli-ml</code> extension, because when I run <code>az ml workspace list</code>, it works, once I pass the <code>-g</code> parameter, it gives the same error, maybe it is a bug, but I am not sure, the <code>interactive</code> is in preview currently.</p>

<p><a href=""https://i.stack.imgur.com/g4FvM.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/g4FvM.png"" alt=""enter image description here""></a></p>

<p>If you want to run <code>az ml folder attach -w yhd-mlws -g yhd-mlws-rg</code> in the interactive mode, my workaround is to pass the <code>#</code>, i.e. <code># az ml folder attach -w yhd-mlws -g yhd-mlws-rg</code>.</p>

<p><a href=""https://i.stack.imgur.com/pWMyH.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/pWMyH.png"" alt=""enter image description here""></a></p>
","9455659",2
757,60441403,2,60429339,2020-02-27 20:38:12,1,"<p>You should have no problem updating your Endpoint instance type without taking the availability hit. The basic method looks like this when you have an active autoscaling policy:</p>

<ol>
<li>Create a new EndpointConfig that uses the new instance type, <code>ml.t2.2xlarge</code>

<ol>
<li>Do this by calling <a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/API_CreateEndpointConfig.html"" rel=""nofollow noreferrer""><code>CreateEndpointConfig</code></a>.</li>
<li>Pass in the same values you used for your previous Endpoint config. You can point to the same <code>ModelName</code> that you did as well. By reusing the same model, you don't have to retrain it or anything</li>
</ol></li>
<li><a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/endpoint-auto-scaling-delete.html"" rel=""nofollow noreferrer"">Delete the existing autoscaling policy</a>

<ol>
<li>Depending on your autoscaling, you might want to increase the desired count of your Endpoint in the event it needs to scale while you are doing this.</li>
<li>If you are experience a spike in traffic while you are making these API calls, you risk an outage of your model if it can't keep up with traffic. Just keep this in mind and possibly scale in advance for this possibility.</li>
</ol></li>
<li>Call <code>UpdateEndpoint</code> like you did previously and specify this new <code>EndpointConfigName</code></li>
<li>Wait for your Endpoint status to be <code>InService</code>. This should take 10-20 mins.</li>
<li><a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/endpoint-auto-scaling-add-policy.html"" rel=""nofollow noreferrer"">Create a new autoscaling policy</a> for this new Endpoint and production variant</li>
</ol>

<p>You should be good to go without sacrificing availability.</p>
","2337627",1
758,60508821,2,60490195,2020-03-03 13:55:58,1,"<p>The issue was with out firewall blocking the required requests between AMLS and the storage container (I presume to get the environment definitions/ private wheels).</p>

<p>We resolved this by updating the firewall with appropriate ALLOW rules for the AMLS service to contact and read from the attached storage container.</p>
","9821873",1
759,60512298,2,60506398,2020-03-03 17:07:50,9,"<p>You're almost there, but you need to use <code>RunConfiguration</code> instead of <code>ScriptRunConfig</code>. More info <a href=""https://github.com/Azure/MachineLearningNotebooks/blob/master/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-getting-started.ipynb"" rel=""noreferrer"">here</a></p>

<pre class=""lang-py prettyprint-override""><code>from azureml.core.runconfig import RunConfiguration

env = Environment.get(workspace=ws, name='my-environment', version='1')
# create a new runconfig object
runconfig = RunConfiguration()
runconfig.environment = env

pipeline_step = PythonScriptStep(
    source_directory='script', script_name='my-script.py',
    arguments=['-a', param1, '-b', param2],
    compute_target=compute_target,
    runconfig=runconfig
)

pipeline = Pipeline(workspace=ws, steps=[pipeline_step])

pipeline_run = Experiment(ws, 'my_pipeline_run').submit(pipeline)
</code></pre>
","3842610",0
760,60532320,2,60525454,2020-03-04 18:14:24,-1,"<p>The Azure ML base images use <a href=""https://docs.conda.io/en/latest/miniconda.html"" rel=""nofollow noreferrer"">Miniconda</a> Python distribution, which uses MKL.</p>

<p>You can find the details of the base images here:<a href=""https://github.com/Azure/AzureML-Containers"" rel=""nofollow noreferrer"">https://github.com/Azure/AzureML-Containers</a></p>

<p>Also, if you install using Anaconda numpy in following way</p>

<pre><code>conda_dep.add_conda_package(""numpy"")
runconfig.run_config.environment.python.conda_dependencies = conda_dep
</code></pre>

<p>you should see this kind of output from <code>numpy.show_config()</code>. </p>

<blockquote>
  <p>blas_mkl_info:</p>
  
  <p>libraries = ['blas', 'cblas', 'lapack', 'pthread', 'blas', 'cblas',
  'lapack']</p>
  
  <p>library_dirs =
  ['/azureml-envs/azureml_a8ad8e485613e21e6e8adc1bfda86b40/lib']</p>
  
  <p>define_macros = [('SCIPY_MKL_H', None), ('HAVE_CBLAS', None)]</p>
  
  <p>include_dirs =
  ['/azureml-envs/azureml_a8ad8e485613e21e6e8adc1bfda86b40/include']</p>
</blockquote>
","5784983",5
761,60540183,2,60539094,2020-03-05 07:30:58,9,"<p>According to MS, all users in the workspace contributor and owner role can create, delete, start, stop, and restart compute instances across the workspace. However, only the creator of a specific compute instance is allowed to access Jupyter, JupyterLab, and RStudio on that compute instance. The creator of the compute instance has the compute instance dedicated to them, have root access, and can terminal in through Jupyter. Compute instance will have single-user login of creator user and all actions will use that user’s identity for RBAC and attribution of experiment runs. SSH access is controlled through public/private key mechanism.</p>
","9204563",2
762,60564614,2,60529048,2020-03-06 13:00:35,0,"<p>I actually solved using PyTorchModel with the following settings:</p>

<pre><code>estimator = PyTorchModel(model_data='#path to model, 
                             role=role,
                             source_dir='pytorch_source',
                             entry_point='deploy.py',
                            predictor_cls = ImgPredictor,
                           framework_version = '1.1.0')
</code></pre>

<p>where ImgPredictor is</p>

<pre><code>from sagemaker.predictor import RealTimePredictor, json_deserializer

class ImgPredictor(RealTimePredictor):
    def __init__(self, endpoint_name, sagemaker_session):
        super(ImgPredictor, self).__init__(endpoint_name, sagemaker_session, content_type='application/x-image', 
                                           deserializer = json_deserializer ,accept='application/json')
</code></pre>

<p>and deploy.py contains the required functions input_fn, output_fn, model_fn and predict_fn.
Also, a requirements.txt file was missing in the source directory.</p>
","9961790",0
763,60564784,2,60554471,2020-03-06 13:11:44,2,"<p><em>edited 03/08/2020 to add pointers for visual search</em></p>

<p>Since you seem interested both in the tasks of <strong>object detection</strong> (input an image, and return bounding boxes with object classes) and <strong>visual search</strong> (input an image and return relevant images) let me give you pointers for both :)</p>

<p>For <strong>object detection</strong> you have 3 options:</p>

<ul>
<li><strong>Using the managed service <a href=""https://aws.amazon.com/rekognition/custom-labels-features/?nc1=h_ls"" rel=""nofollow noreferrer"">Amazon Rekognition Custom Labels</a></strong>. The key benefits of this service is that (1) it doesn't require writing ML code, as the service runs autoML internally to find the best model, (2) it is very flexible in terms of interaction (SDKs, console), data loading and annotation and (3) it can work even with small datasets (typically a few hundred images or less).</li>
<li><strong>Using SageMaker Object Detection model</strong> (<a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/object-detection.html"" rel=""nofollow noreferrer"">documentation</a>, <a href=""https://github.com/awslabs/amazon-sagemaker-examples/blob/master/introduction_to_amazon_algorithms/object_detection_birds/object_detection_birds.ipynb"" rel=""nofollow noreferrer"">demo</a>). In this option, the model is also already written (SSD architecture with Resnet or VGG backbone) and you just need to choose  or tune <a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/object-detection-api-config.html"" rel=""nofollow noreferrer"">hyperparameters</a></li>
<li><strong>Using your own model on Amazon SageMaker</strong>. This could be your own code in docker, or code from an ML framework in a SageMaker ML Framework container. There are such containers for <a href=""https://sagemaker.readthedocs.io/en/stable/using_pytorch.html"" rel=""nofollow noreferrer"">Pytorch</a>, <a href=""https://sagemaker.readthedocs.io/en/stable/using_tf.html"" rel=""nofollow noreferrer"">Tensorflow</a>, <a href=""https://sagemaker.readthedocs.io/en/stable/using_mxnet.html"" rel=""nofollow noreferrer"">MXNet</a>, <a href=""https://sagemaker.readthedocs.io/en/stable/using_chainer.html"" rel=""nofollow noreferrer"">Chainer</a> and <a href=""https://sagemaker.readthedocs.io/en/stable/using_sklearn.html"" rel=""nofollow noreferrer"">Sklearn</a>. In terms of model code, I recommend considering <a href=""https://gluon-cv.mxnet.io/"" rel=""nofollow noreferrer""><strong><code>gluoncv</code></strong></a>, a compact python computer vision toolkit (based on mxnet backend) that comes with <a href=""https://gluon-cv.mxnet.io/model_zoo/detection.html"" rel=""nofollow noreferrer"">many state-of-the-art models</a> and <a href=""https://gluon-cv.mxnet.io/build/examples_detection/index.html"" rel=""nofollow noreferrer"">tutorials</a> for object detection</li>
</ul>

<p>The task of <strong>visual search</strong> requires more customization, since you need to provide the info of (1) what you define as search relevancy (eg is it visual similarity? or object complementarity? etc) and (2) the collection among which to search. If all you need is visual similarity, a popular option is to transform images into vectors with a pre-trained neural network and run kNN search between the query image and the collection of transformed images. There are 2 tutos showing how to build such systems on AWS here:</p>

<ul>
<li><a href=""https://aws.amazon.com/fr/blogs/machine-learning/visual-search-on-aws-part-1-engine-implementation-with-amazon-sagemaker/"" rel=""nofollow noreferrer"">Blog Post Visual Search on AWS</a> (MXNet resnet embeddings +
SageMaker kNN)</li>
<li><a href=""https://thomasdelteil.github.io/VisualSearch_MXNet/"" rel=""nofollow noreferrer"">Visual Search on MMS demo</a> (MXNet resnet
embeddings + HNSW kNN on AWS Fargate)</li>
</ul>
","5331834",5
764,60603667,2,60597319,2020-03-09 15:26:38,5,"<p>I've decided to check on my test VM and run mlflow server on GCE VM. Have a look at my steps below:</p>

<ol>
<li>create VM instance based on Ubuntu Linux 18.04 LTS</li>
<li><p><a href=""https://www.mlflow.org/docs/latest/quickstart.html"" rel=""noreferrer"">install MLflow</a>:</p>

<pre><code>$ sudo apt update
$ sudo apt upgrade
$ cd ~
$ git clone https://github.com/mlflow/mlflow
$ cd mlflow
$ sudo apt install python3-pip
$ pip3 install mlflow
$ python3 setup.py build
$ sudo python3 setup.py install
$ mlflow --version
mlflow, version 1.7.1.dev0
</code></pre></li>
<li><p>run mlflow server on internal IP of VM instance (default 127.0.0.1):</p>

<pre><code>$ ifconfig 
ens4: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt;  mtu 1460
inet 10.XXX.15.XXX  netmask 255.255.255.255  broadcast 0.0.0.0
...

$ mlflow server --host 10.XXX.15.XXX
[2020-03-09 15:05:50 +0000] [8631] [INFO] Starting gunicorn 20.0.4
[2020-03-09 15:05:50 +0000] [8631] [INFO] Listening at: http://10.128.15.211:5000 (8631)
[2020-03-09 15:05:50 +0000] [8631] [INFO] Using worker: sync
[2020-03-09 15:05:50 +0000] [8634] [INFO] Booting worker with pid: 8634
[2020-03-09 15:05:51 +0000] [8635] [INFO] Booting worker with pid: 8635
[2020-03-09 15:05:51 +0000] [8636] [INFO] Booting worker with pid: 8636
[2020-03-09 15:05:51 +0000] [8638] [INFO] Booting worker with pid: 8638
</code></pre></li>
<li><p>check from VM instance (from second connection):</p>

<pre><code>$ curl -I http://10.XXX.15.XXX:5000
HTTP/1.1 200 OK
Server: gunicorn/20.0.4
Date: Mon, 09 Mar 2020 15:06:08 GMT
Connection: close
Content-Length: 853
Content-Type: text/html; charset=utf-8
Last-Modified: Mon, 09 Mar 2020 14:57:11 GMT
Cache-Control: public, max-age=43200
Expires: Tue, 10 Mar 2020 03:06:08 GMT
ETag: ""1583765831.3202355-853-3764264575""
</code></pre></li>
<li><p><a href=""https://cloud.google.com/vpc/docs/add-remove-network-tags"" rel=""noreferrer"">set network tag</a> <code>mlflow-server</code> </p></li>
<li><p><a href=""https://cloud.google.com/vpc/docs/using-firewalls#creating_firewall_rules"" rel=""noreferrer"">create firewall rule</a> to allow access on port 5000</p>

<pre><code>$ gcloud compute --project=test-prj firewall-rules create mlflow-server --direction=INGRESS --priority=999 --network=default --action=ALLOW --rules=tcp:5000 --source-ranges=0.0.0.0/0 --target-tags=mlflow-server
</code></pre></li>
<li><p>check from on-premises Linux machine <code>nmap -Pn 35.225.XXX.XXX</code></p>

<pre><code>Starting Nmap 7.80 ( https://nmap.org ) at 2020-03-09 16:20 CET
Nmap scan report for 74.123.225.35.bc.googleusercontent.com (35.225.XXX.XXX)
Host is up (0.20s latency).
Not shown: 993 filtered ports
PORT     STATE  SERVICE
...
5000/tcp open   upnp
...
</code></pre></li>
<li><p>go to web browser <a href=""http://35.225.XXX.XXX:5000/"" rel=""noreferrer"">http://35.225.XXX.XXX:5000/</a></p></li>
</ol>

<p><a href=""https://i.stack.imgur.com/u2aFt.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/u2aFt.png"" alt=""mlflow""></a></p>
","12428794",0
765,60609215,2,60573260,2020-03-09 22:33:10,1,"<p>You are correctly setting <code>dask_kwargs</code> in DFS. I think the slow down happens as a result of additional overhead and less cores in each worker. The more workers there are, the more overhead exists from transmitting data. Additionally, 8 cores from 1 worker can be leveraged to make computations run faster than 1 core from 8 workers.</p>
","5386567",4
766,60622713,2,60037376,2020-03-10 17:15:47,2,"<p>Well for starters, you can always make your own docker image using the Tensorflow image as a base. I work in Tensorflow 2.0 so this will be slightly different for you but here is an example of my image pattern:</p>

<pre><code># Downloads the TensorFlow library used to run the Python script
FROM tensorflow/tensorflow:2.0.0a0 # you would use the equivalent for your TF version

# Contains the common functionality necessary to create a container compatible with Amazon SageMaker
RUN pip install sagemaker-containers -q 

# Wandb allows us to customize and centralize logging while maintaining open-source agility
RUN pip install wandb -q # here you would install pandas

# Copies the training code inside the container to the design pattern created by the Tensorflow estimator
# here you could copy over a callbacks csv
COPY mnist-2.py /opt/ml/code/mnist-2.py 
COPY callbacks.py /opt/ml/code/callbacks.py 
COPY wandb_setup.sh /opt/ml/code/wandb_setup.sh

# Set the login script as the entry point
ENV SAGEMAKER_PROGRAM wandb_setup.sh # here you would instead launch lstm_model.py
</code></pre>

<p>I believe you are looking for a pattern similar to this, but I prefer to log all of my model data using <a href=""https://www.wandb.com/"" rel=""nofollow noreferrer"">Weights and Biases</a>. They're a little out of data on their SageMaker integration but I'm actually in the midst of writing an updated tutorial for them. It should certainly be finished this month and include logging and comparing runs from hyperparameter tuning jobs</p>
","10521726",2
767,60623517,2,60619460,2020-03-10 18:06:36,0,"<p>This solution is working for me.</p>

<pre class=""lang-py prettyprint-override""><code>import boto3
import pandas as pd
import io
import pyarrow
import fastparquet

def dynamically_read_filename_key(bucket, prefix='', suffix=''):
    s3 = boto3\
    .client(""s3"",\
            region_name=os.environ['AWS_DEFAULT_REGION'],\
            aws_access_key_id=os.environ['AWS_ACCESS_KEY_ID'],\
            aws_secret_access_key=os.environ['AWS_SECRET_ACCESS_KEY'])
    kwargs = {'Bucket': bucket}
    if isinstance(prefix, str):
        kwargs['Prefix'] = prefix
    resp = s3\
    .list_objects_v2(**kwargs)
    for obj in resp['Contents']:
        key = obj['Key']
    if key.startswith(prefix) and key.endswith(suffix):
        return key

filename = """".join(i for i in dynamically_read_filename_key\
                   (bucket=""my-bucket"",\
                    prefix=""datasets/"",\
                    suffix="".parquet""))

bucket = ""my-bucket""

def parquet_read_filename_key(bucket, filename):
    client = boto3\
    .resource(""s3"",\
            region_name=os.environ['AWS_DEFAULT_REGION'],\
            aws_access_key_id=os.environ['AWS_ACCESS_KEY_ID'],\
            aws_secret_access_key=os.environ['AWS_SECRET_ACCESS_KEY'])
    buffer = io.BytesIO()
    obj = client.Object(bucket, filename)
    obj.download_fileobj(buffer)
    df = pd.read_parquet(buffer)
    return df

df = parquet_read_filename_key(bucket, filename)
</code></pre>
","11286588",0
768,60628585,2,60562966,2020-03-11 02:52:07,3,"<p><em>Edited to be more answer-like:</em></p>

<p>It'd be helpful to include: what versions of azureml-core and azureml-dataprep SDK you are using, what type of VM you are running as the compute instance, and what types of files (e.g. jpg? txt?) your dataset is using. Also, what are you trying to achieve by downloading the complete dataset to your compute?</p>

<p>Currently, compute instance image comes with azureml-core 1.0.83 and azureml-dataprep 1.1.35 pre-installed, which are 1-2 months old. You might be using even older versions. You can try upgrading by running in your notebook:</p>

<pre><code>%pip install -U azureml-sdk
</code></pre>

<p>If you don't see any improvements to your scenario, you can file an issue on the official docs page to get someone to help debug your issue, such as the ref page for <a href=""https://learn.microsoft.com/en-us/python/api/azureml-core/azureml.data.file_dataset.filedataset?view=azure-ml-py"" rel=""nofollow noreferrer"">FileDataset</a>.</p>

<p><em>(edited on June 9, 2020 to remove mention of experimental release because that is not happening anymore)</em></p>
","2341220",4
769,60645259,2,60644771,2020-03-11 22:30:47,0,"<p>Following <a href=""https://stackoverflow.com/users/1126841/chepner"">@chepner</a>'s comment an example solution would be something like this:</p>

<p>config.ini file:</p>

<pre><code>[Docker]
home_dir = /opt
SM_MODEL_DIR = %(home_dir)s/ml/model
SM_CHANNELS = [""training""]
SM_NUM_GPUS = 1
SM_NUM_CPUS =
SM_LOG_LEVEL = 20
SM_USER_ARGS = [""--test_size"",""0.2"",""--random_seed"",""42"", ""--not_optimize""]
SM_INPUT_DIR = %(home_dir)s/ml/input
SM_INPUT_CONFIG_DIR = %(home_dir)s/ml/input/config
SM_OUTPUT_DIR = %(home_dir)s/ml/output
SM_OUTPUT_INTERMEDIATE_DIR = %(home_dir)s/ml/output/intermediate
</code></pre>

<p>A <code>TrainArgParser</code> class like this:</p>

<pre><code>class ArgParser(ABC):

    @abstractmethod
    def get_arguments(self) -&gt; Dict[str, Any]:
        pass


class TrainArgParser(ArgParser):

    def __init__(self):
        configuration_file_path = 'config.ini'

        self.environment = ""Sagemaker"" \
            if os.environ.get(""SM_MODEL_DIR"", False) \
            else os.environ.get(""ENVIRON"", ""Default"")

        config = configparser.ConfigParser()
        config.read(configuration_file_path)
        if self.environment == ""Local"":
            config[self.environment][""home_dir""] = str(pathlib.Path(__file__).parent.absolute())
        if self.environment != 'Sagemaker':
            config[self.environment][""SM_NUM_CPUS""] = str(multiprocessing.cpu_count())

        for key, value in config[self.environment].items():
            os.environ[key.upper()] = value

        self.parser = argparse.ArgumentParser()
        # AWS Sagemaker default environmental arguments
        self.parser.add_argument(
            '--model_dir',
            type=str,
            default=os.environ['SM_MODEL_DIR'],
        )
        self.parser.add_argument(
            '--channel_names',
            default=json.loads(os.environ['SM_CHANNELS']),
        )
        self.parser.add_argument(
            '--num_gpus',
            type=int,
            default=os.environ['SM_NUM_GPUS'],
        )
        self.parser.add_argument(
            '--num_cpus',
            type=int,
            default=os.environ['SM_NUM_CPUS'],
        )
        self.parser.add_argument(
            '--user_args',
            default=json.loads(os.environ['SM_USER_ARGS']),
        )
        self.parser.add_argument(
            '--input_dir',
            type=str,
            default=os.environ['SM_INPUT_DIR'],
        )
        self.parser.add_argument(
            '--input_config_dir',
            type=Path,
            default=os.environ['SM_INPUT_CONFIG_DIR'],
        )
        self.parser.add_argument(
            '--output_dir',
            type=Path,
            default=os.environ['SM_OUTPUT_DIR'],
        )

        # Extra arguments
        self.run_tag = datetime.datetime \
            .fromtimestamp(time.time()) \
            .strftime('%Y-%m-%d-%H-%M-%S')
        self.parser.add_argument(
            '--run_tag',
            default=self.run_tag,
            type=str,
            help=f""Run tag (default: 'datetime.fromtimestamp')"",
        )
        self.parser.add_argument(
            '--environment',
            type=str,
            default=self.environment,
        )

        self.args = self.parser.parse_args()

    def get_arguments(self) -&gt; Dict[str, Any]:
        # Not in AWS Sagemaker arguments
        self.parser.add_argument(
            '--test_size',
            default=0.2,
            type=float,
            help=""Test dataset size (default: '0.2')"",
        )
        self.parser.add_argument(
            '--random_seed',
            default=42,
            type=int,
            help=""Random number for initialization (default: '42')"",
        )
        self.parser.add_argument(
            '--secrets',
            type=YAMLFile.parse_string,
            default='',
            help=""An yaml formated string (default: '')""
        )
        self.parser.add_argument(
            '--bucket_name',
            type=str,
            default='',
            help=""Bucket name of a remote storage (default: '')""
        )
        self.args = self.parser.parse_args(self.args.user_args)

        return self.args
</code></pre>

<p>and a entry_script for <code>train</code> would start like this:</p>

<pre><code>#!/usr/bin/env python

from utils.arg_parser import TrainArgParser

if __name__ == '__main__':
    logger.info(f""Begin train.py"")

    arg_parser = TrainArgParser()
    args = arg_parser.get_arguments()
    print(args)
</code></pre>

<p>This should output something like this:</p>

<pre><code>Namespace(bucket_name='', channel_names=['training'], environment='Docker', input_config_dir=PosixPath('/opt/ml/input/config'), input_dir='/opt/ml/input', model_dir='/opt/ml/model', num_cpus=8, num_gpus=1, output_dir=PosixPath('/opt/ml/output'), random_seed=42, run_tag='2020-03-11-22-18-21', secrets={}, test_size=0.2, user_args=['--test_size', '0.2', '--random_seed', '42'])
</code></pre>

<p>But that is useless if AWS Sagemaker treats <code>SM_USER_ARGS</code> and <code>SM_HPS</code> as the same thing. :(</p>
","5924820",0
770,60651192,2,60637170,2020-03-12 09:36:25,-1,"<p>How to deploy using environments can be found here <a href=""https://nam06.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2FAzure%2FMachineLearningNotebooks%2Fblob%2Fmaster%2Fhow-to-use-azureml%2Fdeployment%2Fdeploy-to-cloud%2Fmodel-register-and-deploy.ipynb&amp;data=02%7C01%7CRamprasad.Mula%40microsoft.com%7Ce06d310b0447416ab46b08d7bc836a81%7C72f988bf86f141af91ab2d7cd011db47%7C1%7C0%7C637185146436156499&amp;sdata=uQo332dpjuiNqWFCguvs3Kgg7UUMN8MBEzLxTPyH4MM%3D&amp;reserved=0"" rel=""nofollow noreferrer"">model-register-and-deploy.ipynb</a> .  InferenceConfig class accepts  source_directory and entry_script <a href=""https://learn.microsoft.com/en-us/python/api/azureml-core/azureml.core.model.inferenceconfig?view=azure-ml-py#parameters"" rel=""nofollow noreferrer"">parameters</a>, where source_directory  is a path to the folder that contains all files(score.py and any other additional files) to create the image. </p>

<p>This <a href=""https://github.com/Azure/MachineLearningNotebooks/blob/master/how-to-use-azureml/deployment/deploy-multi-model/multi-model-register-and-deploy.ipynb"" rel=""nofollow noreferrer"">multi-model-register-and-deploy.ipynb</a> has code snippets on how to create InferenceConfig with source_directory and entry_script.</p>

<pre><code>from azureml.core.webservice import Webservice
from azureml.core.model import InferenceConfig
from azureml.core.environment import Environment

myenv = Environment.from_conda_specification(name=""myenv"", file_path=""myenv.yml"")
inference_config = InferenceConfig(entry_script=""score.py"", environment=myenv)

service = Model.deploy(workspace=ws,
                       name='sklearn-mnist-svc',
                       models=[model], 
                       inference_config=inference_config,
                       deployment_config=aciconfig)

service.wait_for_deployment(show_output=True)

print(service.scoring_uri)
</code></pre>
","11297406",4
771,60653959,2,60636558,2020-03-12 12:14:16,2,"<p>Unfortunately there seems to an issue with this version of Conda (4.5.11). To complete this task in the tutorial, you can just update the dependency for Tensorflow and Keras to be from <code>pip</code> and not <code>conda</code>. There are reasons why this is less than ideal for a production environment. The Azure ML <a href=""https://learn.microsoft.com/en-us/python/api/azureml-core/azureml.core.conda_dependencies.condadependencies?view=azure-ml-py"" rel=""nofollow noreferrer"">documentation states</a>:</p>

<blockquote>
  <p>""If your dependency is available through both Conda and pip (from
  PyPi), use the Conda version, as Conda packages typically come with
  pre-built binaries that make installation more reliable.""</p>
</blockquote>

<p>In this case though, if you update the following code block:</p>

<pre><code>from azureml.core.conda_dependencies import CondaDependencies 

myenv = CondaDependencies()
myenv.add_conda_package(""tensorflow"")
myenv.add_conda_package(""keras"")

with open(""myenv.yml"",""w"") as f:
    f.write(myenv.serialize_to_string())

# Review environment file
with open(""myenv.yml"",""r"") as f:
    print(f.read())
</code></pre>

<p>To be the following:</p>

<pre><code>from azureml.core.conda_dependencies import CondaDependencies 

myenv = CondaDependencies()
myenv.add_pip_package(""tensorflow==2.0.0"")
myenv.add_pip_package(""azureml-defaults"")
myenv.add_pip_package(""keras"")

with open(""myenv.yml"", ""w"") as f:
    f.write(myenv.serialize_to_string())

with open(""myenv.yml"", ""r"") as f:
    print(f.read())
</code></pre>

<p>The tutorial should be able to be completed.  Let me know if any of this does not work for you once this update has been made.</p>

<p>I have also reported this issue to Microsoft (in regards to the Conda version).</p>
","765031",0
772,60659819,2,43892213,2020-03-12 18:17:33,0,"<p>We support GPU now. Documentation <a href=""https://cloud.google.com/ai-platform/prediction/docs/machine-types-online-prediction"" rel=""nofollow noreferrer"">here</a>!</p>

<p>Example:</p>

<pre><code>gcloud beta ai-platform versions create version_name \
  --model model_name \
  --origin gs://model-directory-uri \
  --runtime-version 2.1 \
  --python-version 3.7 \
  --framework tensorflow \
  --machine-type n1-standard-4 \
  --accelerator count=1,type=nvidia-tesla-t4 \
  --config config.yaml

</code></pre>

<p>If you use one of the Compute Engine (N1) machine types for your model version, you can optionally add GPUs to accelerate each prediction node.</p>

<ul>
<li>NVIDIA Tesla K80 </li>
<li>NVIDIA Tesla P4   </li>
<li>NVIDIA Tesla P100     </li>
<li>NVIDIA Tesla T4   </li>
<li>NVIDIA Tesla V100</li>
</ul>
","260826",0
773,60677213,2,60638587,2020-03-13 21:02:44,2,"<p>Usually the timeout is caused by an error in init() function in scoring script. You can get the detailed logs using <code>print(service.get_logs())</code> to find the Python error.</p>

<p>For more comprehensive troubleshooting guide, see:</p>

<p><a href=""https://learn.microsoft.com/en-us/azure/machine-learning/how-to-troubleshoot-deployment"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/machine-learning/how-to-troubleshoot-deployment</a></p>
","5784983",0
774,60705022,2,60643094,2020-03-16 11:20:57,2,"<p>Which version of XGBoost are you using in the notebook? The model format has changed in XGBoost 1.0. See <a href=""https://xgboost.readthedocs.io/en/latest/tutorials/saving_model.html"" rel=""nofollow noreferrer"">https://xgboost.readthedocs.io/en/latest/tutorials/saving_model.html</a>. Short version: if you're using 1.0 in the notebook, you can't load a pickled model. </p>

<p>Here's a working example using XGBoost in script mode (which is much more flexible than the built in algo):</p>

<ul>
<li><a href=""https://gitlab.com/juliensimon/dlnotebooks/-/blob/master/sagemaker/09-XGBoost-script-mode.ipynb"" rel=""nofollow noreferrer"">https://gitlab.com/juliensimon/dlnotebooks/-/blob/master/sagemaker/09-XGBoost-script-mode.ipynb</a></li>
<li><a href=""https://gitlab.com/juliensimon/dlnotebooks/-/blob/master/sagemaker/xgb.py"" rel=""nofollow noreferrer"">https://gitlab.com/juliensimon/dlnotebooks/-/blob/master/sagemaker/xgb.py</a></li>
</ul>
","4686192",1
775,60705311,2,60668635,2020-03-16 11:40:13,6,"<p>The following works for me  on a fresh notebook instance. I'm using the python3 environment.</p>

<pre><code>sh-4.2$ source activate python3
(python3) sh-4.2$ conda install -c conda-forge fbprophet --yes
Solving environment: done


==&gt; WARNING: A newer version of conda exists. &lt;==
  current version: 4.5.12
  latest version: 4.8.2

Please update conda by running

    $ conda update -n base -c defaults conda



## Package Plan ##

  environment location: /home/ec2-user/anaconda3/envs/python3

  added / updated specs:
    - fbprophet


The following packages will be downloaded:

    package                    |            build
    ---------------------------|-----------------
    pykerberos-1.2.1           |   py36h505690d_0          26 KB  conda-forge
    python-3.6.7               |    h381d211_1004        34.5 MB  conda-forge
    cryptography-2.8           |   py36h45558ae_2         628 KB  conda-forge
    python_abi-3.6             |          1_cp36m           4 KB  conda-forge
    binutils_impl_linux-64-2.33.1|       h53a641e_8         9.1 MB  conda-forge
    certifi-2019.11.28         |   py36h9f0ad1d_1         149 KB  conda-forge
    gcc_linux-64-7.3.0         |      h553295d_17          21 KB  conda-forge
    gxx_linux-64-7.3.0         |      h553295d_17          21 KB  conda-forge
    fbprophet-0.6              |   py36he1b5a44_0         642 KB  conda-forge
    holidays-0.10.1            |             py_0          56 KB  conda-forge
    tk-8.6.10                  |       hed695b0_0         3.2 MB  conda-forge
    lunarcalendar-0.0.9        |             py_0          20 KB  conda-forge
    convertdate-2.1.3          |          py_1000          30 KB  conda-forge
    curl-7.68.0                |       hf8cf82a_0         137 KB  conda-forge
    krb5-1.16.4                |       h2fd8d38_0         1.4 MB  conda-forge
    libcurl-7.68.0             |       hda55be3_0         564 KB  conda-forge
    expat-2.2.9                |       he1b5a44_2         191 KB  conda-forge
    binutils_linux-64-2.33.1   |      h9595d00_17          21 KB  conda-forge
    python-dateutil-2.8.0      |             py_0         219 KB  conda-forge
    matplotlib-base-3.1.0      |   py36h5f35d83_0         6.7 MB  conda-forge
    pycurl-7.43.0.5            |   py36h16ce93b_0          69 KB  conda-forge
    ld_impl_linux-64-2.33.1    |       h53a641e_8         589 KB  conda-forge
    pystan-2.17.1.0            |py36hf2d7682_1004        14.0 MB  conda-forge
    ephem-3.7.7.1              |   py36h516909a_0         722 KB  conda-forge
    ------------------------------------------------------------
                                           Total:        72.9 MB

The following NEW packages will be INSTALLED:

    binutils_impl_linux-64: 2.33.1-h53a641e_8          conda-forge
    binutils_linux-64:      2.33.1-h9595d00_17         conda-forge
    convertdate:            2.1.3-py_1000              conda-forge
    ephem:                  3.7.7.1-py36h516909a_0     conda-forge
    fbprophet:              0.6-py36he1b5a44_0         conda-forge
    gcc_impl_linux-64:      7.3.0-habb00fd_1
    gcc_linux-64:           7.3.0-h553295d_17          conda-forge
    gettext:                0.19.8.1-hc5be6a0_1002     conda-forge
    gxx_impl_linux-64:      7.3.0-hdf63c60_1
    gxx_linux-64:           7.3.0-h553295d_17          conda-forge
    holidays:               0.10.1-py_0                conda-forge
    ld_impl_linux-64:       2.33.1-h53a641e_8          conda-forge
    lunarcalendar:          0.0.9-py_0                 conda-forge
    matplotlib-base:        3.1.0-py36h5f35d83_0       conda-forge
    pystan:                 2.17.1.0-py36hf2d7682_1004 conda-forge
    python_abi:             3.6-1_cp36m                conda-forge

The following packages will be UPDATED:

    ca-certificates:        2019.10.16-0                           --&gt; 2019.11.28-hecc5488_0     conda-forge
    certifi:                2019.9.11-py36_0                       --&gt; 2019.11.28-py36h9f0ad1d_1 conda-forge
    cryptography:           2.2.2-py36h14c3975_0                   --&gt; 2.8-py36h45558ae_2        conda-forge
    curl:                   7.60.0-h84994c4_0                      --&gt; 7.68.0-hf8cf82a_0         conda-forge
    expat:                  2.2.5-he0dffb1_0                       --&gt; 2.2.9-he1b5a44_2          conda-forge
    glib:                   2.56.1-h000015b_0                      --&gt; 2.58.3-h6f030ca_1002      conda-forge
    krb5:                   1.14.2-hcdc1b81_6                      --&gt; 1.16.4-h2fd8d38_0         conda-forge
    libcurl:                7.60.0-h1ad7b7a_0                      --&gt; 7.68.0-hda55be3_0         conda-forge
    libpng:                 1.6.34-hb9fc6fc_0                      --&gt; 1.6.37-hed695b0_0         conda-forge
    libssh2:                1.8.0-h9cfc8f7_4                       --&gt; 1.8.2-h22169c7_2          conda-forge
    openssl:                1.0.2t-h7b6447c_1                      --&gt; 1.1.1d-h516909a_0         conda-forge
    pycurl:                 7.43.0.1-py36hb7f436b_0                --&gt; 7.43.0.5-py36h16ce93b_0   conda-forge
    pykerberos:             1.2.1-py36h14c3975_0                   --&gt; 1.2.1-py36h505690d_0      conda-forge
    python:                 3.6.5-hc3d631a_2                       --&gt; 3.6.7-h381d211_1004       conda-forge
    python-dateutil:        2.7.3-py36_0                           --&gt; 2.8.0-py_0                conda-forge
    qt:                     5.9.6-h52aff34_0                       --&gt; 5.9.7-h5867ecd_1
    sqlite:                 3.23.1-he433501_0                      --&gt; 3.28.0-h8b20d00_0         conda-forge
    tk:                     8.6.7-hc745277_3                       --&gt; 8.6.10-hed695b0_0         conda-forge


Downloading and Extracting Packages
pykerberos-1.2.1     | 26 KB     | ################################################################################ | 100%
python-3.6.7         | 34.5 MB   | ################################################################################ | 100%
cryptography-2.8     | 628 KB    | ################################################################################ | 100%
python_abi-3.6       | 4 KB      | ################################################################################ | 100%
binutils_impl_linux- | 9.1 MB    | ################################################################################ | 100%
certifi-2019.11.28   | 149 KB    | ################################################################################ | 100%
gcc_linux-64-7.3.0   | 21 KB     | ################################################################################ | 100%
gxx_linux-64-7.3.0   | 21 KB     | ################################################################################ | 100%
fbprophet-0.6        | 642 KB    | ################################################################################ | 100%
holidays-0.10.1      | 56 KB     | ################################################################################ | 100%
tk-8.6.10            | 3.2 MB    | ################################################################################ | 100%
lunarcalendar-0.0.9  | 20 KB     | ################################################################################ | 100%
convertdate-2.1.3    | 30 KB     | ################################################################################ | 100%
curl-7.68.0          | 137 KB    | ################################################################################ | 100%
krb5-1.16.4          | 1.4 MB    | ################################################################################ | 100%
libcurl-7.68.0       | 564 KB    | ################################################################################ | 100%
expat-2.2.9          | 191 KB    | ################################################################################ | 100%
binutils_linux-64-2. | 21 KB     | ################################################################################ | 100%
python-dateutil-2.8. | 219 KB    | ################################################################################ | 100%
matplotlib-base-3.1. | 6.7 MB    | ################################################################################ | 100%
pycurl-7.43.0.5      | 69 KB     | ################################################################################ | 100%
ld_impl_linux-64-2.3 | 589 KB    | ################################################################################ | 100%
pystan-2.17.1.0      | 14.0 MB   | ################################################################################ | 100%
ephem-3.7.7.1        | 722 KB    | ################################################################################ | 100%
Preparing transaction: done
Verifying transaction: done
Executing transaction: done
(python3) sh-4.2$
(python3) sh-4.2$
(python3) sh-4.2$
(python3) sh-4.2$ python
Python 3.6.7 | packaged by conda-forge | (default, Feb 28 2019, 09:07:38)
[GCC 7.3.0] on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
&gt;&gt;&gt; import fbprophet
&gt;&gt;&gt; 
</code></pre>
","4686192",0
776,60721885,2,60707641,2020-03-17 11:53:25,1,"<p>Ok, so for anyone with similar problems. I forgot to add the following snippet in the code <code>step.apply(gcp.use_gcp_secret('user-gcp-sa'))</code></p>

<p>This works even with <code>kfp.components.load_component_from_file()</code></p>

<p>Figured this out thanks to the amazing folks at the kubeflow slack channel!</p>
","7105492",0
777,60726481,2,60712225,2020-03-17 16:31:44,0,"<blockquote>
  <p>Only three built-in algorithms currently support incremental training: Object Detection Algorithm, Image Classification Algorithm, and Semantic Segmentation Algorithm.</p>
</blockquote>

<p><a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/incremental-training.html"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/sagemaker/latest/dg/incremental-training.html</a></p>
","9618462",1
778,60729810,2,60718452,2020-03-17 20:32:51,5,"<ul>
<li><a href=""https://www.kubeflow.org/docs/started/kubeflow-overview/"" rel=""noreferrer""><strong>Kubeflow</strong></a> <em>is a platform for developing and deploying a machine learning (ML) systems</em>. <a href=""https://www.kubeflow.org/docs/components/"" rel=""noreferrer"">Its components</a> are focused on creating workflows aimed to build ML systems.</li>
<li><strong>Cloud Composer</strong> <em>provides the infraestructure to run <a href=""https://airflow.apache.org/docs/stable/"" rel=""noreferrer"">Apache Airflow worflows</a></em>. Its components are known as <a href=""https://airflow.apache.org/docs/stable/_api/airflow/operators/index.html"" rel=""noreferrer"">Airflow Operators</a> and the workflows are connections between these operators that are known as DAGs.</li>
</ul>

<p>Both services run on Kubernetes, but they are based on different programming frameworks; therefore, you are correct, Kuberflow deploys and monitors Machine Learning models. See below the answer for your questions:</p>

<ol>
<li>In that case, since Machine Learning models are also objects, can't we orchestrate them using Cloud Composer? </li>
</ol>

<p>You would need to find an operator that meet your needs, or create a custom operator with the structure required to create a model, see <a href=""https://medium.com/zenofai/machine-learning-operations-mlops-pipeline-using-google-cloud-composer-a8ebcbd6d766#0319"" rel=""noreferrer"">this example</a>. Even when it can be performed, this could be more difficult that using Kubeflow.</p>

<ol start=""2"">
<li>How does Kubeflow help in any way, better than Cloud Composer when it comes to managing Machine Learning models??</li>
</ol>

<p>Kubeflow hides complexity as it is focused on Machine Learninig models. The frameworks specialized on machine learning makes those things easier than using Cloud Composer which in this context can be considered as a general purpose tool (focused on linking existing services supported by the Airflow Operators).</p>
","9457843",0
779,60742184,2,60740522,2020-03-18 15:01:33,2,"<p>So after some trial and error I finally came to a solution. And I'm happy to say that my intuition was right. It did have something to do with the <code>file_outputs</code> I didn't specify.
To be able to export your metrics you will have to set <code>file_outputs</code> as follows.</p>

<pre class=""lang-py prettyprint-override""><code>def metric_op(accuracy, f1_scores):
    return dsl.ContainerOp(
        name='visualize_metrics',
        image='gcr.io/mgcp-1190085-asml-lpd-dev/kfp/jonas/container_tests/image_metric_comp',
        arguments=[
            '--accuracy', accuracy,
            '--f1_scores', f1_scores,
        ],
        file_outputs={
            'mlpipeline-metrics': '/mlpipeline-metrics.json'
        }
    )
</code></pre>
","12062724",0
780,60742917,2,60716202,2020-03-18 15:42:56,2,"<p>SageMaker Studio is using the SageMaker API to pull all of the data its displaying.  Essentially there's no secret API here getting invoked.</p>

<p>Quite a bit of what's being displayed with respect to experiments is from the <a href=""https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_Search.html"" rel=""nofollow noreferrer"">search results</a>, the rest coming from either List* or Describe* calls.  Studio is taking the results from the search request and displaying it in the table format that you're seeing.  Search results when searching over resource ExperimentTrialComponent that have a source (such as a training job) will be enhanced with the original sources data ([result]::SourceDetail::TrainingJob) if supported (work is ongoing to add additional source detail resource types).</p>

<p>All of the metadata that is related to resources in SageMaker is available via the APIs; there is no other location (in the cloud) like s3 for that data.</p>

<p>As of this time there is no effort to determine if it's possible to add support into <a href=""https://github.com/VertaAI/modeldb"" rel=""nofollow noreferrer"">modeldb</a> for SageMaker that I'm aware of.  Given that modeldb appears to make some assumptions about it's talking to a relational database it would appear unlikely to be something that would be doable. (I only read the overview very quickly so this might be inaccurate.)</p>
","52136",0
781,60744596,2,60667610,2020-03-18 17:25:58,1,"<p>You can log a custom python model: 
<a href=""https://www.mlflow.org/docs/latest/models.html#custom-python-models"" rel=""nofollow noreferrer"">https://www.mlflow.org/docs/latest/models.html#custom-python-models</a></p>
","12763888",0
782,60745148,2,60641337,2020-03-18 18:03:05,4,"<p>The install_mlflow command only works with conda right now, sorry about the confusing message. You can either:</p>
<ul>
<li>install conda - this is the recommended way of installing and using mlflow</li>
</ul>
<p>or</p>
<ul>
<li>install mlflow python package yourself via pip</li>
</ul>
<p>To install mlflow yourself, pip install correct (matching the the R package) python version of mlflow and set the MLFLOW_PYTHON_BIN environment variable as well as MLFLOW_BIN evn variable: e.g.</p>
<pre><code>library(mlflow)
system(paste(&quot;pip install -U mlflow==&quot;, mlflow:::mlflow_version(), sep=&quot;&quot;))
Sys.setenv(MLFLOW_BIN=system(&quot;which mlflow&quot;))
Sys.setenv(MLFLOW_PYTHON_BIN=system(&quot;which python&quot;))
</code></pre>
","12763888",2
783,60781187,2,60778546,2020-03-20 20:23:55,1,"<p>You can use the eager strategy to force an upgrade of requirements:</p>

<pre><code>pip install -U --upgrade-strategy eager azureml-sdk
</code></pre>
","5784983",0
784,60803476,2,60801292,2020-03-22 18:40:12,4,"<p>It depends on how large your graph is as to how well this will perform but you can get a sense of the type of nodes and edges you have using something like the example below. From the tags you used I assume you are using Gremlin:</p>

<pre><code>g.V().groupCount().by(label)
g.E().groupCount().by(label)
</code></pre>

<p>If you have a very large graph try putting something like <code>limit(100000)</code> before the <code>groupCount</code> step.</p>

<p>If you are using a programming language like Python (with gremlin python installed) then you will need to add a <code>next()</code> terminal step to the queries as in:</p>

<pre><code>g.V().groupCount().by(label).next()
g.E().groupCount().by(label).next()
</code></pre>

<p>Having found the labels and distribution of the labels you could use one of them to explore some properties. Let's imagine there is a label called ""person"".</p>

<pre><code>g.V().hasLabel('person').limit(10).valueMap().toList()
</code></pre>

<p>Remember with Gremlin property graphs vertices with the same label may not necessarily have all the same properties so it's good to look at more than one vertex to get a sense for that as well.</p>
","5442034",0
785,60827088,2,60826366,2020-03-24 08:10:43,1,"<p>List all ComputeTarget objects within the workspace:
Please follow the below link.
<a href=""https://learn.microsoft.com/en-us/python/api/azureml-core/azureml.core.compute.computetarget?view=azure-ml-py#list-workspace-"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/python/api/azureml-core/azureml.core.compute.computetarget?view=azure-ml-py#list-workspace-</a></p>

<p>you can do like as shown below.</p>

<pre><code>from azureml.core.compute import AksCompute, ComputeTarget
aks_name = 'YOUR_EXISTING_CLUSTER_NAME’
aks_target =AksCompute(ws, aks_name)
</code></pre>
","11297406",0
786,60827946,2,60816944,2020-03-24 09:11:16,3,"<p>The simplest way is to create a small notebook instance for each student. This way you can have the needed isolation and also the responsibility of each student for their notebook to stop them when they are not in use.</p>

<p>The smallest instance type <a href=""https://aws.amazon.com/sagemaker/pricing/"" rel=""nofollow noreferrer"">costs</a> $0.0464 per hour. If you have it running 24/7 it costs about $30 per month. But if the students are responsible and stop their instances when they are not using them, it can be about $1 for 20 hours of work.</p>

<p>If you want to enable isolation to the notebooks, you can use the ability to presign the URL that is used to open the Jupyter interface. See here on the way to use the CLI to create the URL: <a href=""https://docs.aws.amazon.com/cli/latest/reference/sagemaker/create-presigned-notebook-instance-url.html"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/cli/latest/reference/sagemaker/create-presigned-notebook-instance-url.html</a>. It is also supported in other SDK.</p>

<pre><code>create-presigned-notebook-instance-url
--notebook-instance-name &lt;student-instance-name&gt;
--session-expiration-duration-in-seconds 3600
</code></pre>

<p>You can integrate it into the internal portal that you have in your institute. </p>
","179529",2
787,60869104,2,60088889,2020-03-26 14:05:13,27,"<p>Unfortunately it seems there is no way to do this via the UI or CLI at the moment :-/</p>

<p>The way to do it depends on the type of backend file store that you are using.</p>

<p><strong>Filestore</strong>:</p>

<p>If you are using the filesystem as a storage mechanism (the default) then it is easy. The 'deleted' experiments are moved to a <code>.trash</code> folder. You just need to clear that out:</p>

<pre class=""lang-sh prettyprint-override""><code>rm -rf mlruns/.trash/*
</code></pre>

<p>As of the current version of the <a href=""https://www.mlflow.org/docs/latest/cli.html#mlflow-experiments-delete"" rel=""noreferrer"">documentation</a> (1.7.2), they remark:</p>

<blockquote>
  <p>It is recommended to use a cron job or an alternate workflow mechanism to clear <code>.trash</code> folder.</p>
</blockquote>

<p><strong>SQL Database:</strong></p>

<p>This is more tricky, as there are dependencies that need to be deleted. I am using MySQL, and these commands work for me:</p>

<pre class=""lang-sql prettyprint-override""><code>USE mlflow_db;  # the name of your database
DELETE FROM experiment_tags WHERE experiment_id=ANY(
    SELECT experiment_id FROM experiments where lifecycle_stage=""deleted""
);
DELETE FROM latest_metrics WHERE run_uuid=ANY(
    SELECT run_uuid FROM runs WHERE experiment_id=ANY(
        SELECT experiment_id FROM experiments where lifecycle_stage=""deleted""
    )
);
DELETE FROM metrics WHERE run_uuid=ANY(
    SELECT run_uuid FROM runs WHERE experiment_id=ANY(
        SELECT experiment_id FROM experiments where lifecycle_stage=""deleted""
    )
);
DELETE FROM tags WHERE run_uuid=ANY(
    SELECT run_uuid FROM runs WHERE experiment_id=ANY(
        SELECT experiment_id FROM experiments where lifecycle_stage=""deleted""
    )
);
DELETE FROM runs WHERE experiment_id=ANY(
    SELECT experiment_id FROM experiments where lifecycle_stage=""deleted""
);
DELETE FROM experiments where lifecycle_stage=""deleted"";
</code></pre>
","341459",1
788,60877213,2,60868257,2020-03-26 22:32:27,28,"<p>You train models on GPU in the SageMaker ecosystem via 2 different components:</p>

<ol>
<li><p>You can instantiate a GPU-powered <strong><a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/nbi.html"" rel=""noreferrer"">SageMaker Notebook Instance</a></strong>, for example <code>p2.xlarge</code> (NVIDIA K80) or <code>p3.2xlarge</code> (NVIDIA V100). This is convenient for interactive development - you have the GPU right under your notebook and can run code on the GPU interactively and monitor the GPU via <code>nvidia-smi</code> in a terminal tab - a great development experience. However when you develop directly from a GPU-powered machine, there are times when you may not use the GPU. For example when you write code or browse some documentation. All that time you pay for a GPU that sits idle. In that regard, it may not be the most cost-effective option for your use-case. </p></li>
<li><p>Another option is to use a <a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/how-it-works-training.html"" rel=""noreferrer""><strong>SageMaker Training Job</strong></a> running on a GPU instance. This is a preferred option for training, because training metadata (data and model path, hyperparameters, cluster specification, etc) is persisted in the SageMaker metadata store, logs and metrics stored in Cloudwatch and the instance automatically shuts down itself at the end of training. Developing on a small CPU instance and launching training tasks using SageMaker Training API will help you make the most of your budget, while helping you retain metadata and artifacts of all your experiments. You can see <a href=""https://aws.amazon.com/fr/blogs/machine-learning/using-tensorflow-eager-execution-with-amazon-sagemaker-script-mode/"" rel=""noreferrer"">here a well documented TensorFlow example</a></p></li>
</ol>
","5331834",0
789,60955242,2,60953289,2020-03-31 16:22:40,1,"<p>I found a hint in <a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/your-algorithms-training-algo-dockerfile.html"" rel=""nofollow noreferrer"">the AWS docs</a> and came up with this solution:</p>

<pre><code>ENTRYPOINT [""python3.7"", ""/opt/ml/code/train.py""]
</code></pre>

<p>With this the container <a href=""https://docs.docker.com/engine/reference/builder/#entrypoint"" rel=""nofollow noreferrer"">will run as an executable</a>.</p>
","5613588",0
790,60976337,2,60892850,2020-04-01 17:00:37,1,"<p>AWS support recommended below solution:</p>

<p>This seems to be a known issue when executing the code locally, as mentioned in the following Github issue [3]. A work-around to fix the issue is also defined in that issue [3] and can be referred to using the following link: aws/sagemaker-python-sdk#300 (comment)</p>

<p>The steps in the work-around given in the above link are:</p>

<ol>
<li><p>Login to the AWS console -> IAM -> Roles -> Create Role</p></li>
<li><p>Create an IAM role and select the ""SageMaker"" service</p></li>
<li><p>Give the role ""AmazonSageMakerFullAccess"" permission</p></li>
<li><p>Review and create the role</p></li>
<li><p>Next, also attach the ""AWSRoboMakerFullAccess"" permission policy to the above created role (as required in the Github notebook [1]).</p></li>
<li><p>The original code would then need to be modified to fetch the IAM role directly when the code is executed on a local machine. The code snippet to be used is given below:</p></li>
</ol>

<pre><code>try:
   sagemaker_role = sagemaker.get_execution_role()
 except ValueError:
   iam = boto3.client('iam')
   sagemaker_role = iam.get_role(RoleName='&lt;sagemaker-IAM-role-name&gt;')['Role']['Arn']
</code></pre>

<p>In the above snippet, replace the """" text with the IAM role name created in Step 4.</p>

<p>References:</p>

<p>[1] <a href=""https://github.com/aws-samples/aws-deepracer-workshops/blob/master/log-analysis/DeepRacer%20Log%20Analysis.ipynb"" rel=""nofollow noreferrer"">https://github.com/aws-samples/aws-deepracer-workshops/blob/master/log-analysis/DeepRacer%20Log%20Analysis.ipynb</a></p>

<p>[2] <a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/automatic-model-tuning-ex-role.html"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/sagemaker/latest/dg/automatic-model-tuning-ex-role.html</a></p>

<p>[3] aws/sagemaker-python-sdk#300</p>
","3794466",0
791,60976540,2,60975078,2020-04-01 17:12:16,1,"<p>You're right there are the <code>.time_*()</code> filtering methods available with a <code>TabularDataset</code>.</p>

<p>I'm not aware of anyway to do filtering as you suggest (but I agree it would be a useful feature). To get per-device partitioning, my recommendation would be to structure your container like so:</p>

<pre><code>- device1
    - 2020
        - 2020-03-31.csv
        - 2020-04-01.csv
- device2
   - 2020
        - 2020-03-31.csv
        - 2020-04-01.csv
</code></pre>

<p>In this way you can define an all-up Dataset, but also per-device Datasets by passing folder of the device to the DataPath</p>

<pre class=""lang-py prettyprint-override""><code># all up dataset
ds_all = Dataset.Tabular.from_delimited_files(
    path=DataPath(datastore, '*')
)
# device 1 dataset
ds_d1 = Dataset.Tabular.from_delimited_files(
    path=DataPath(datastore, 'device1/*')
)
</code></pre>

<p><strong>CAVEAT</strong></p>

<p>dataprep SDK is optimized for blobs around 200MB in size. So you can work with many small files, but sometimes it can be slower than expected, especially considering the overhead of enumerating all blobs in a container.</p>
","3842610",0
792,60530990,2,60523435,2020-03-04 16:49:44,2,"<p>Currently, we have a python script, <code>pipeline.py</code> that uses the <code>azureml-sdk</code>to create, register and run all of our ML artifacts (envs, pipelines, models). We call this script in our Azure DevOps CI pipeline with a Python Script task after building the right pip env from the requirements file in our repo.</p>

<p>However, it is worth noting there is YAML support for ML artifact definition. Though I don't know if the existing support will cover all of your bases (though that is the plan).</p>

<p>Here's some great docs from MSFT to get you started:</p>

<ul>
<li><a href=""https://github.com/microsoft/MLOpsPython"" rel=""nofollow noreferrer"">GitHub Template repo of an end-to-end example of ML pipeline + deployment</a></li>
<li><a href=""https://learn.microsoft.com/en-us/azure/machine-learning/how-to-use-environments"" rel=""nofollow noreferrer"">How to define/create an environment (using Pip or Conda) and use it in a remote compute context</a></li>
<li><a href=""https://learn.microsoft.com/en-us/azure/devops/pipelines/targets/azure-machine-learning?context=azure%2Fmachine-learning%2Fservice%2Fcontext%2Fml-context&amp;view=azure-devops&amp;tabs=yaml"" rel=""nofollow noreferrer"">Azure Pipelines guidance on CI/CD for ML Service</a></li>
<li><a href=""https://learn.microsoft.com/en-us/azure/machine-learning/reference-pipeline-yaml"" rel=""nofollow noreferrer"">Defining ML pipelines in YAML</a></li>
</ul>
","3842610",0
793,60482892,2,60405600,2020-03-02 05:15:37,0,"<p>The following code works well for me.</p>

<pre class=""lang-py prettyprint-override""><code>import mxnet as mx
from mxnet import image
from gluoncv.data.transforms.presets.segmentation import test_transform
import gluoncv

# use cpu
ctx = mx.cpu(0)

# load test image
img = image.imread('./img/IMG_4015.jpg')
img = test_transform(img, ctx)
img = img.astype('float32')

# reconstruct the PSP network model
model = gluoncv.model_zoo.PSPNet(2)

# load the trained model
model.load_parameters('./model/model_algo-1')

# make inference
output = model.predict(img)
predict = mx.nd.squeeze(mx.nd.argmax(output, 1)).asnumpy()
</code></pre>
","6792401",0
794,60331869,2,60331084,2020-02-21 03:38:14,3,"<p>Data is cached by default in a storage account that is created along with the the ML service workspace. It has the same name as the workspace plus some numbers. Inside the account there is a blobstore called <code>azureml-blobstore-{GUID}</code> Inside of that container your data is cached,  organized by runs.</p>

<p>This data is made available to ML service as a <code>Datastore</code> that you can navigate to in the UI by clicking ""Datastores"" in the blade on the left-hand of the Studio.</p>

<p><a href=""https://i.stack.imgur.com/YVwPl.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/YVwPl.png"" alt=""enter image description here""></a></p>
","3842610",0
795,60330160,2,58487710,2020-02-20 23:40:07,0,"<p>While you can use any registry when working with Docker on a SageMaker notebook, as of this writing other SageMaker components presently only support images from Amazon ECR repositories.</p>
","10392672",0
796,60307627,2,60306240,2020-02-19 19:12:54,6,"<p>This is not currently supported, but I am 80% sure it is in the roadmap.
An alternative would be to use the SDK to create the same pipeline using <code>ModuleStep</code> where  I <em>believe</em> you can reference a Designer Module by its name to use it like a <code>PythonScriptStep</code></p>
","3842610",1
797,60199351,2,59956042,2020-02-13 01:48:23,3,"<p>Here's an example of how to get the instance name</p>

<pre class=""lang-py prettyprint-override""><code>def get_notebook_name():
    log_path = '/opt/ml/metadata/resource-metadata.json'
    with open(log_path, 'r') as logs:
        _logs = json.load(logs)
    return _logs['ResourceName']
</code></pre>

<p>From:</p>

<p><a href=""https://github.com/aws-samples/amazon-sagemaker-notebook-instance-lifecycle-config-samples/blob/62c44aa5e69f4266955476f24647b99d9b597aaf/scripts/auto-stop-idle/autostop.py#L79"" rel=""nofollow noreferrer"">https://github.com/aws-samples/amazon-sagemaker-notebook-instance-lifecycle-config-samples/blob/62c44aa5e69f4266955476f24647b99d9b597aaf/scripts/auto-stop-idle/autostop.py#L79</a></p>
","3363678",1
798,60165089,2,60163614,2020-02-11 09:00:05,2,"<p><a href=""https://github.com/awslabs/fraud-detection-using-machine-learning/blob/master/source/notebooks/sagemaker_fraud_detection.ipynb"" rel=""nofollow noreferrer"">This end-to-end demo</a> shows usage of Linear Learner from input data pre-processed in <code>pandas</code> dataframes and then converted to protobuf using the SDK. But note that:</p>

<ul>
<li>There is no need to use protobuf, you can also pass csv data with the target variable on the first column of the files, as <a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/linear-learner.html#ll-input_output"" rel=""nofollow noreferrer"">indicated here</a>.</li>
<li>There is no need to know MXNet in order to use the SageMaker Linear Learner, just use the SDK of your choice, bring data to S3, and orchestrate training and inference :)</li>
</ul>
","5331834",2
799,60162285,2,60159875,2020-02-11 05:00:45,2,"<p>You are using Python in SageMaker, so you could use:</p>

<pre class=""lang-py prettyprint-override""><code>import boto3

s3_client = boto3.client('s3')
s3_client.download_file('deepfake2020', 'dfdc_train_part_1/foo.mp4', foo.mp4')
</code></pre>

<p>This will download the file from Amazon S3 to the local disk, in a file called <code>foo.mp4</code>.</p>

<p>See: <a href=""https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/s3.html#S3.Client.download_file"" rel=""nofollow noreferrer""><code>download_file()</code></a> in boto3</p>

<p>This requires that the SageMaker instance has been granted permissions to access the Amazon S3 bucket.</p>
","174777",0
800,60154539,2,60036916,2020-02-10 16:30:36,1,"<p>if you want to install the  packages only in for the python3 environment, use the following script in your <strong>Create Sagemaker Lifecycle</strong> configurations. </p>

<pre><code>#!/bin/bash
sudo -u ec2-user -i &lt;&lt;'EOF'

# This will affect only the Jupyter kernel called ""conda_python3"".
source activate python3

# Replace myPackage with the name of the package you want to install.
pip install pandas==0.25.3
# You can also perform ""conda install"" here as well.
source deactivate
EOF
</code></pre>

<p>Reference : ""<a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/notebook-lifecycle-config.html"" rel=""nofollow noreferrer"">Lifecycle Configuration Best Practices</a>"" </p>
","9064794",1
801,60154285,2,60151965,2020-02-10 16:15:54,5,"<p>dataset.mount() actually returns a MountContext which has a mount_point attribute. </p>

<p>So:</p>

<p>img = cv2.imread(mounted_images.mount_point +’/my_file_name.jpg')</p>

<p>Should hopefully work.</p>
","8821969",2
802,60133607,2,60078143,2020-02-09 05:00:38,5,"<p>That's a great question. To enable TF 2.0 on the SageMaker notebooks, you can go to the Conda tab (on Jupyter and not JupyterLab)  and select the tensorflow_p36 environment. On the bottom left, you can search ""tensorflow"" in the available Conda packages. There will be a TF2.0 option. Check that box, and the click on the right arrow, which will install TF2.0 into your tensorflow_p36 environment. </p>

<p>Restart any existing notebooks that are open.</p>

<p>Note: This works only with Jupyter and not JupyterLab.</p>

<p>Alex Chung<br>
Senior Product Manager - AWS</p>
","2044299",3
803,60132420,2,56942046,2020-02-09 00:11:29,0,"<p>I was utilizing the sample notebook Julien Simon mentioned earlier, but at some point the data was ending up as strings! The funny thing about RCF algorithms is they have to run on integer data. 
What I did is I made sure to cast the array as an int array as a double check and vallah! It worked. I am at loss over how the data ended up in a string format but alas, that was the issue. Simple solution.</p>
","10648536",0
804,60130411,2,60127972,2020-02-08 19:30:14,0,"<p>This is is because your user is not able to assume role to access sagemaker, ultimately you are getting permission denied error. </p>

<p>FYI: SageMaker is not included in amazon educate account. You can ask your teacher/account admin to allow you to access amazon sage maker.</p>
","3447977",3
805,60130351,2,60122070,2020-02-08 19:23:03,4,"<p><em>(edit 2/9/2020 with extra code snippets)</em></p>

<p>Your serving code tries to use the <code>sagemaker</code> module internally. The <code>sagemaker</code> module (also called <a href=""http://sagemaker.readthedocs.io"" rel=""nofollow noreferrer"">SageMaker Python SDK</a>, one of the numerous orchestration SDKs for SageMaker) is not designed to be used in model containers, but instead out of models, to orchestrate their activity (train, deploy, bayesian tuning, etc). In your specific example, you shouldn't include the deployment and model call code to server code, as those are actually actions that will be conducted from outside the server to orchestrate its lifecyle and interact with it. For model deployment with the Sagemaker Pytorch container, your entry point script just needs to contain the required <code>model_fn</code> function for model deserialization, and optionally an <code>input_fn</code>, <code>predict_fn</code> and <code>output_fn</code>, respectively for pre-processing, inference and post-processing (<a href=""https://sagemaker.readthedocs.io/en/stable/using_pytorch.html#the-sagemaker-pytorch-model-server"" rel=""nofollow noreferrer"">detailed in the documentation here</a>). This logic is beautiful :) : you don't need anything else to deploy a production-ready deep learning server! (MMS in the case of Pytorch and MXNet, Flask+Gunicorn in the case of sklearn).</p>

<p>In summary, this is how your code should be split:</p>

<p>An entry_point script <code>serve.py</code> that contains model serving code and looks like this:</p>

<pre><code>import os

import numpy as np
import torch
from torch import cuda
from torchvision.models import resnet50

def model_fn(model_dir):
    # TODO instantiate a model from its artifact stored in model_dir
    return model

def predict_fn(input_data, model):
    # TODO apply model to the input_data, return result of interest
    return result
</code></pre>

<p>and some orchestration code to instantiate a SageMaker Model object, deploy it to a server and query it. This is run from the orchestration runtime of your choice, which could be a SageMaker Notebook, your laptop, an AWS Lambda function, an Apache Airflow operator, etc - and with the SDK for your choice; don't need to use python for this.</p>

<pre><code>import numpy as np
from sagemaker.pytorch.model import PyTorchModel

pytorch_model = PyTorchModel(
    model_data='s3://&lt;bucket name&gt;/resnet50/model.tar.gz',
    entry_point='serve.py',
    role='jiashenC-sagemaker',
    py_version='py3',
    framework_version='1.3.1')

predictor = pytorch_model.deploy(instance_type='ml.t2.medium', initial_instance_count=1)

print(predictor.predict(np.random.random_sample([1, 3, 224, 224]).astype(np.float32)))
</code></pre>
","5331834",2
806,60067490,2,60067075,2020-02-05 00:36:40,1,"<p>Sagemaker allows you to use a custom Docker image (<a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/your-algorithms.html"" rel=""nofollow noreferrer"">AWS document</a>)</p>

<blockquote>
  <p>Build your own custom container image: If there is no pre-built Amazon
  SageMaker container image that you can use or modify for an advanced
  scenario, you can package your own script or algorithm to use with
  Amazon SageMaker.You can use any programming language or framework to
  develop your container</p>
</blockquote>

<ul>
<li>Create a docker image with your code (FFmpeg, TensorFlow)</li>
<li>Testing the docker container locally</li>
<li>Deploying the image on Amazon ECR (Elastic Container Repository)</li>
<li>Create a SageMaker model and point to the image</li>
</ul>

<p>For details, you can learn more from <a href=""https://towardsdatascience.com/brewing-up-custom-ml-models-on-aws-sagemaker-e09b64627722"" rel=""nofollow noreferrer"">this tutorial</a></p>
","9618462",0
807,60049246,2,59983062,2020-02-04 01:08:51,1,"<p>I am a developer at AWS SageMaker. Autopilot currently only supports CSV data. While we are working on extending the support to more file formats: JSON, TSV, etc, this might be something that you can try to convert your .tsv file to .csv:</p>

<pre><code>import csv

# read tab-delimited file
with open('yourfile.tsv','rb') as fin:
    cr = csv.reader(fin, delimiter='\t')
    filecontents = [line for line in cr]

# write comma-delimited file (comma is the default delimiter)
with open('yourfile.csv','wb') as fou:
    cw = csv.writer(fou, quotechar='', quoting=csv.QUOTE_NONE)
    cw.writerows(filecontents)
</code></pre>

<p>Hope this helps.</p>

<p>Ref: <a href=""https://stackoverflow.com/questions/5590631/how-to-convert-a-tab-separated-file-to-csv-format"">How to convert a tab separated file to CSV format?</a></p>
","7815954",1
808,60047166,2,60045326,2020-02-03 21:07:16,2,"<p>You have created a role for SageMaker to access S3 bucket, but it seems your IAM user doesn't have access to SageMaker service. Please make sure your IAM user has permission to SageMaker.</p>
","3447977",2
809,60009024,2,60006106,2020-01-31 17:58:06,4,"<p>Can you Please try</p>

<pre><code>df.to_csv(""s3://bucket/key/file.csv"", index=False, mode='wb')
</code></pre>

<p>It should fix your error. The default mode is <strong>w</strong> which writes in the file system as text and not bytes. Where as s3 expects the data to be bytes. hence you have to specify mode as <strong>wb</strong>(write bytes) while writing the dataframe as csv to the filesystem.</p>
","6146828",3
810,59939687,2,59890328,2020-01-27 22:17:10,0,"<p>Thanks to a colleague at work we worked this out. The parameter to the <code>graph_util.convert_variables_to_constants</code> method isn't the layer name, but instead is the operation name (<code>op.name</code>).</p>

<p>The correct code is:</p>

<pre><code>sess = K.get_session()

outputs = [out.op.name for out in model.outputs] # Note this new line

constant_graph = graph_util.convert_variables_to_constants(sess, 
                                             sess.graph.as_graph_def(), 
                                             outputs)

graph_io.write_graph(constant_graph, 'export', 'output.pb', as_text=False)

</code></pre>
","280795",0
811,59928976,2,59921196,2020-01-27 10:11:23,2,"<p>Built-in algorithms are implemented with Apache MXNet, so that's how you'd load the model locally. load_checkpoint() is the appropriate API: <a href=""https://mxnet.apache.org/api/python/docs/api/mxnet/model/index.html#mxnet.model.load_checkpoint"" rel=""nofollow noreferrer"">https://mxnet.apache.org/api/python/docs/api/mxnet/model/index.html#mxnet.model.load_checkpoint</a></p>
","4686192",1
812,59927444,2,57189292,2020-01-27 08:31:38,0,"<p>I was running into this exact issue earlier this week while trying to modify this example <a href=""https://github.com/awslabs/amazon-sagemaker-examples/blob/master/sagemaker-python-sdk/tensorflow_serving_container/tensorflow_serving_container.ipynb"" rel=""nofollow noreferrer"">Sagemaker notebook</a>. Particularly the part where serving the model. That is, running <code>predictor.predict()</code> on the Sagemaker Tensorflow Estimator.</p>

<p>The solution outlined in the issue worked perfectly for me- <a href=""https://github.com/awslabs/amazon-sagemaker-examples/issues/773#issuecomment-509433290"" rel=""nofollow noreferrer"">https://github.com/awslabs/amazon-sagemaker-examples/issues/773#issuecomment-509433290</a></p>

<p>I think it's just because <code>tf.tables_initializer()</code> only runs for training but it needs to be specified through the <code>legacy_init_op</code> if you want to run it during prediction.</p>
","4073736",0
813,59924551,2,59873804,2020-01-27 02:18:45,0,"<p>The issue has to do with your file path. Ensure that you have included the correct path.</p>
","11297406",0
814,59908752,2,59863842,2020-01-25 11:25:18,4,"<p>Amazon SageMaker is offering model hosting service (<a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/how-it-works-hosting.html"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/sagemaker/latest/dg/how-it-works-hosting.html</a>), which gives you a lot of flexibility based on your inference requirements. </p>

<p>As you noted, first you can choose the instance type to use for your model hosting. The large set of options is important to tune to your models. You can host the model on a GPU based machines (P2/P3/P4) or CPU ones. You can have instances with faster CPU (C4, for example), or more RAM (R4, for example). You can also choose instances with more cores (16xl, for example) or less (medium, for example). Here is a list of the full range of instances that you can choose: <a href=""https://aws.amazon.com/sagemaker/pricing/instance-types/"" rel=""nofollow noreferrer"">https://aws.amazon.com/sagemaker/pricing/instance-types/</a> . It is important to balance your performance and costs. The selection of the instance type and the type and size of your model will determine the invocations-per-second that you can expect from your model in this single-node configuration. It is important to measure this number to avoid hitting the throttle errors that you saw. </p>

<p>The second important feature of the SageMaker hosting that you use is the ability to auto-scale your model to multiple instances. You can configure the endpoint of your model hosting to automatically add and remove instances based on the load on the endpoint. AWS is adding a load balancer in front of the multiple instances that are hosting your models and distributing the requests among them. Using the autoscaling functionality allows you to keep a smaller instance for low traffic hours, and to be able to scale up during peak traffic hours, and still keep your costs low and your throttle errors to the minimum. See here for documentation on the SageMaker autoscaling options: <a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/endpoint-auto-scaling.html"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/sagemaker/latest/dg/endpoint-auto-scaling.html</a></p>
","179529",2
815,59866535,2,59856641,2020-01-22 18:56:59,1,"<p>Unfortunately not at the moment. mlflow run starts a new process and there is no protocol for exception passing right now. In general the other project does not even have to be in the same language. </p>

<p>One workaround I can think of is to pass the exception via mlflow by setting run tag. E.g.:</p>

<pre><code>try:
    ...
except Exception as ex:
    mlflow.set_tag(""exception"", str(ex))
</code></pre>
","12763888",0
816,59834564,2,59829017,2020-01-21 05:17:29,1,"<p>you need to provide the right encoding when calling get_blob_to_text, please refer to the <a href=""https://github.com/Azure/azure-storage-python/blob/master/samples/blob/block_blob_usage.py#L390"" rel=""nofollow noreferrer"">sample</a>.</p>

<p>The code below is what  normally use for reading data file in blob storages. Basically, you can use blob’s url along with sas token and use a request method. However, You might want to edit the ‘for loop’ depending what types of data you have (e.g. csv, jpg, and etc).</p>

<p>-- Python code below --</p>

<pre><code>import requests
from azure.storage.blob import BlockBlobService, BlobPermissions
from azure.storage.blob.baseblobservice import BaseBlobService
from datetime import datetime, timedelta

account_name = '&lt;account_name&gt;'
account_key = '&lt;account_key&gt;'
container_name = '&lt;container_name&gt;'

blob_service=BlockBlobService(account_name,account_key)
generator = blob_service.list_blobs(container_name)

for blob in generator:
    url = f""https://{account_name}.blob.core.windows.net/{container_name}""
    service = BaseBlobService(account_name=account_name, account_key=account_key)
    token = service.generate_blob_shared_access_signature(container_name, img_name, permission=BlobPermissions.READ, expiry=datetime.utcnow() + timedelta(hours=1),)
    url_with_sas = f""{url}?{token}""
    response = requests.get(url_with_sas)
</code></pre>

<p>Please follow the below link to read data on Azure Blob Storage.
<a href=""https://learn.microsoft.com/en-us/azure/machine-learning/how-to-access-data"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/machine-learning/how-to-access-data</a></p>
","11297406",0
817,59833818,2,59833019,2020-01-21 03:29:39,2,"<p>I solved this issue by changing the instance type from <strong>ml.t2.medium</strong> to <strong>ml.t2.2xlarge</strong> and it worked perfectly.</p>

<p>The original issue was with the RAM of the instance type and not with S3.</p>
","8163412",0
818,59820375,2,59801874,2020-01-20 09:33:25,5,"<p>It depends on the scientific paradigm you're using in SageMaker :)</p>

<ul>
<li>SageMaker Built-in algorithms all have their input specification,
described in their respective documentation. For example, for
<a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/linear-learner.html#ll-input_output"" rel=""noreferrer"">SageMaker Linear Learner</a> and <a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/xgboost.html#InputOutput-XGBoost"" rel=""noreferrer"">SageMaker XGBoost</a> the target is assumed
to be the first column.</li>
<li>With custom code, such as Bring-Your-Own-Docker or SageMaker Framework containers (for Sklearn, TF, PyTorch, MXNet) since you are the one writing the code you can write any sort of logic, and the target can be any column of your dataset.</li>
</ul>
","5331834",0
819,59777366,2,59773503,2020-01-16 20:30:04,1,"<p>The udf function will be executed by multiple spark tasks in parallel. Those tasks run in completely isolated python processes and they are scheduled to physically different machines. Hence each data, those functions reference, must be on the same node. This is the case for everything created within the udf.</p>

<p>Whenever you reference any object outside of the udf from the function, this data structure needs to be serialised (pickled) to each executor. Some object state, like open connections to a socket, cannot be pickled.</p>

<p>You need to make sure, that connections are lazily opened each executor. It must happen only on the first function call on that executor. The <a href=""https://spark.apache.org/docs/latest/streaming-programming-guide.html#design-patterns-for-using-foreachrdd"" rel=""nofollow noreferrer"">connection pooling topic</a> is covered in the docs, however only in the spark streaming guide (though it also applies for normal batch jobs).</p>

<p>Normally one can use the Singleton Pattern for this. But in python people use the Borgh pattern.</p>

<pre><code>class Env:
    _shared_state = {
        ""sagemaker_client"": None
        ""sagemaker_runtime_client"": None
        ""boto_session"": None
        ""sagemaker_session"": None
        ""predictor"": None
    }
    def __init__(self):
        self.__dict__ = self._shared_state
        if not self.predictor:
            self.sagemaker_client = boto3.client('sagemaker', aws_access_key_id=ACCESS_KEY, aws_secret_access_key=SECRET_KEY, region_name='us-east-1')
            self.sagemaker_runtime_client = boto3.client('sagemaker-runtime', aws_access_key_id=ACCESS_KEY, aws_secret_access_key=SECRET_KEY, region_name='us-east-1')

            self.boto_session = boto3.Session(region_name='us-east-1')
            self.sagemaker_session = sagemaker.Session(self.boto_session, sagemaker_client=self.sagemaker_client, sagemaker_runtime_client=self.sagemaker_runtime_client)

            self.predictor = TensorFlowPredictor('endpoint-poc', self.sagemaker_session)


#....
def call_predict():
   env = Env()
   batch_size = 1
   data = [[0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.2]]
   tensor_proto = tf.make_tensor_proto(values=np.asarray(data), shape=[batch_size, len(data[0])], dtype=tf.float32)      
   prediction = env.predictor.predict(tensor_proto)

   print(""Process time: {}"".format((time.clock() - start)))
        return prediction

new_data = rangeRDD.map(lambda x : call_predict())
</code></pre>

<p>The Env class is defined on the master node. Its <code>_shared_state</code> has empty entries. When then Env object is instantiated first time, it shares the state with all further instances of Env on any subsequent call to the udf. On each separate parallel running process this will happen exactly one time. This way the sessions are shared and do not need to pickled. </p>
","1035375",2
820,59773189,2,59717227,2020-01-16 15:42:48,3,"<p>There are few issues with you code snippet.</p>
<p>Here are the links to <a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/sms-ui-template-reference.html"" rel=""nofollow noreferrer"">SageMaker's HTML Reference</a> and <a href=""https://awsfeed.com/whats-new/machine-learning/build-a-custom-data-labeling-workflow-with-amazon-sagemaker-ground-truth/"" rel=""nofollow noreferrer"">Example for building custom Labeling template</a></p>
<p>First remove all those submit buttons (<code>&lt;crowd-button&gt;</code> elements) and the <code>onClick</code> event handler. From here you have two options use default SageMaker submit button or create your own in the template.</p>
<h2>Use SageMaker's Submit Button</h2>
<p>Leave out submit buttons (<code>crowd-button</code>) and SageMaker will automatically append one inside <code>crowd-form</code>. According to documentation <a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/sms-ui-template-crowd-form.html"" rel=""nofollow noreferrer"">here</a></p>
<h2>Use custom Submit Button</h2>
<p>In this case you need to:</p>
<ol>
<li>Prevent SageMaker adding button by including <code>crowd-button</code> <strong>inside</strong> the <code>crowd-form</code> element and setting <code>style=&quot;display: none;</code></li>
<li>Add your own Submit button elsewhere on the template and add <code>onclick</code> even handler that will execute <code>form.submit()</code></li>
</ol>
<p>Here is the working example of the template (taken from the Example mentioned above).</p>
<pre><code>&lt;script src=&quot;https://assets.crowd.aws/crowd-html-elements.js&quot;&gt;&lt;/script&gt;

&lt;link rel=&quot;stylesheet&quot; href=&quot;https://s3.amazonaws.com/smgtannotation/web/static/css/1.3fc3007b.chunk.css&quot;&gt;
&lt;link rel=&quot;stylesheet&quot; href=&quot;https://s3.amazonaws.com/smgtannotation/web/static/css/main.9504782e.chunk.css&quot;&gt;

&lt;div id='document-text' style=&quot;display: none;&quot;&gt;
  {{ task.input.text }}
&lt;/div&gt;
&lt;div id='document-image' style=&quot;display: none;&quot;&gt;
        {{ task.input.taskObject | grant_read_access }}
&lt;/div&gt;
&lt;div id=&quot;metadata&quot; style=&quot;display: none;&quot;&gt;
  {{ task.input.metadata }}
&lt;/div&gt;

&lt;crowd-form&gt;
    &lt;input name=&quot;annotations&quot; id=&quot;annotations&quot; type=&quot;hidden&quot;&gt;

     &lt;!-- Prevent crowd-form from creating its own button --&gt;
    &lt;crowd-button form-action=&quot;submit&quot; style=&quot;display: none;&quot;&gt;&lt;/crowd-button&gt;
&lt;/crowd-form&gt;

&lt;!-- Custom annotation user interface is rendered here --&gt;
&lt;div id=&quot;root&quot;&gt;&lt;/div&gt;

&lt;crowd-button id=&quot;submitButton&quot;&gt;Submit&lt;/crowd-button&gt;

&lt;script&gt;
    document.querySelector('crowd-form').onsubmit = function() {
        document.getElementById('annotations').value = JSON.stringify(JSON.parse(document.querySelector('pre').innerText));
    };

    document.getElementById('submitButton').onclick = function() {
        document.querySelector('crowd-form').submit();
    };
&lt;/script&gt;

&lt;script src=&quot;https://s3.amazonaws.com/smgtannotation/web/static/js/1.3e5a6849.chunk.js&quot;&gt;&lt;/script&gt;
&lt;script src=&quot;https://s3.amazonaws.com/smgtannotation/web/static/js/main.96e12312.chunk.js&quot;&gt;&lt;/script&gt;
&lt;script src=&quot;https://s3.amazonaws.com/smgtannotation/web/static/js/runtime~main.229c360f.js&quot;&gt;&lt;/script&gt;
</code></pre>
<p>Code source</p>
","1998518",1
821,59722803,2,59711677,2020-01-13 19:15:05,3,"<p>Wondering if you're configuration for Livy endpoint is valid? Livy runs on port 8998. You should check if the port is open in the security group. </p>

<p>This might be useful: <a href=""https://aws.amazon.com/blogs/machine-learning/build-amazon-sagemaker-notebooks-backed-by-spark-in-amazon-emr/"" rel=""nofollow noreferrer"">https://aws.amazon.com/blogs/machine-learning/build-amazon-sagemaker-notebooks-backed-by-spark-in-amazon-emr/</a></p>

<p>Also, if that does not help, you should try stopping and restarting the notebook once. That has helped in the past. </p>
","11376324",3
822,59695756,2,59599721,2020-01-11 15:22:27,2,"<p>No. SageMaker AutoPilot doesn't support deep learning at the moment, only classification and regression problems on tabular data. Technically, I guess you could pass embeddings in CSV format, and pray that XGBoost figures them out, but I seriously doubt that this would deliver meaningful results :)</p>

<p>Amazon Comprehend does support fully managed custom classification models <a href=""https://docs.aws.amazon.com/comprehend/latest/dg/how-document-classification.html"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/comprehend/latest/dg/how-document-classification.html</a>. It may be worth taking a look at it.</p>
","4686192",1
823,59695729,2,59615549,2020-01-11 15:18:43,0,"<p>Depending on the algo used, they might be visible in the training log of the top candidate.</p>

<p>What you could also do is keep a test set on the side, and use it to compute precision, recall and other metrics on the trained model.</p>
","4686192",1
824,59695641,2,59679192,2020-01-11 15:08:53,1,"<p>Yes, this is called a multi-model endpoint. You can use a large number of models on the same endpoint. They get loaded and unloaded dynamically as needed, and you simply have to pass the model name in your prediction request.</p>

<p>Here are some resources:</p>

<ul>
<li><p>Blog post + example : <a href=""https://aws.amazon.com/blogs/machine-learning/save-on-inference-costs-by-using-amazon-sagemaker-multi-model-endpoints/"" rel=""nofollow noreferrer"">https://aws.amazon.com/blogs/machine-learning/save-on-inference-costs-by-using-amazon-sagemaker-multi-model-endpoints/</a></p></li>
<li><p>Video explaining model deployment scenarios on SageMaker: <a href=""https://youtu.be/dT8jmdF-ZWw"" rel=""nofollow noreferrer"">https://youtu.be/dT8jmdF-ZWw</a></p></li>
</ul>
","4686192",0
825,59672893,2,59653641,2020-01-09 22:14:16,2,"<p>For Azure Machine Learning (Classic) Studio notebooks, you need to install Tensorflow. Furthermore, the notebook server session times out after a period of inactivity, hence, you need to re-install Tensorflow once the server shuts down or after starting a new session. Thanks.</p>

<p>Here are some references:</p>

<p><a href=""https://notebooks.azure.com/help/jupyter-notebooks/timeouts"" rel=""nofollow noreferrer"">https://notebooks.azure.com/help/jupyter-notebooks/timeouts</a></p>

<p><a href=""https://learn.microsoft.com/en-us/azure/notebooks/install-packages-jupyter-notebook"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/notebooks/install-packages-jupyter-notebook</a></p>
","11968855",0
826,59658693,2,59577521,2020-01-09 06:59:22,11,"<p>SageMaker has a higher price mark but it is taking a lot of the heavy lifting of deploying a machine learning model, such as wiring the pieces (load balancer, gunicorn, CloudWatch, Auto-Scaling...) and it is easier to automate the processes such as A/B testing.</p>

<p>If you have a strong team of DevOps that have nothing more important to do, you can build a flow that will be cheaper than the SageMaker option. ECS and EKS are doing at the same time a lot of work to make it very easy for you to automate the machine learning model deployments. However, they will always be more general purpose and SageMaker with its focus on machine learning will be easier for these use cases. </p>

<p>The usual pattern of using the cloud is to use the managed services early on as you want to move fast and you don't really know where are your future problems. Once the system is growing and you start feeling some pains here and there, you can decide to spend the time and improve that part of the system. Therefore, if you don't know the pros/cons, start with using the simpler options. </p>
","179529",0
827,59640602,2,59637596,2020-01-08 06:39:03,0,"<p>Here is my screenshots for tabs <code>EXPERIMENTS</code> and <code>NOTEBOOKS</code> in Azure Machine Learning Studio (classic), as the figures below.</p>

<p>Fig 1. I created a <code>Excute Python Script</code> module with the code to print the <code>sys.path</code> and the real path of <code>pandas</code> installed.</p>

<p><a href=""https://i.stack.imgur.com/KdkJa.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/KdkJa.png"" alt=""enter image description here""></a></p>

<p>Fig 2. The <code>View output log</code> page of the code in Fig 1 shows <code>EXPERIMENTS</code> is a runtime of Anaconda on Windows. </p>

<p><a href=""https://i.stack.imgur.com/zo9te.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/zo9te.png"" alt=""enter image description here""></a></p>

<p>Fig 3. I created a notebook named <code>demo</code> and run the same code as Fig 1, the result shows <code>NOTEBOOKS</code> is a runtime of Anaconda on Linux, even the notenooks url is started with <code>notebooks.azure.com</code>.</p>

<p><a href=""https://i.stack.imgur.com/uAGRk.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/uAGRk.png"" alt=""enter image description here""></a></p>

<p>Fig 4. I used different commands like <code>lsb_release -a</code>, <code>fdisk -l</code>, <code>lsdev</code>, <code>ls /dev</code>, <code>df -a</code> to try to see the Linux version and its disk or partition information, the result shows it's a Ubuntu Linux container.</p>

<p><a href=""https://i.stack.imgur.com/R4wC6.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/R4wC6.png"" alt=""enter image description here""></a></p>

<p>Other infomation what you want to know, you can try to check by yourself.</p>
","4989676",3
828,59500105,2,59367321,2019-12-27 11:27:07,2,"<p>This is issues can occur in some scenarios like:</p>

<ul>
<li><p>Your kubeflow setup (Kubernetes cluster) and GCR are in different project</p></li>
<li><p>No GCR secret for the ml-pipeline service account which is responsible to run the pipeline. (you can see this kubectl --namespace=kubeflow get serviceaccount)</p></li>
</ul>

<p>In your case, I think it is the second scenario. Though the following path will work on both scenarios.</p>

<ol>
<li>Create service_account.json with sufficient permission (GCR needs storage permission so give 'Storage admin') using the GCP console</li>
</ol>

<pre><code>Select “API &amp; Services” &gt; “Credentials”Select “Create credentials” &gt; “Services Account Key” &gt; “Create New Services Account”

</code></pre>

<ol start=""2"">
<li>Add a Kubernetes Secret in Kubernetes Cluster to access GCR</li>
</ol>

<pre class=""lang-sh prettyprint-override""><code>kubectl create secret docker-registry $SECRETNAME \       
--docker-server=https://gcr.io \                          
--docker-username=_json_key \                             
--docker-email=user@example.com \                          
--docker-password=""$(cat ./service_account.json.json)""
#username should be _json_key
</code></pre>

<ul>
<li>Above method is for default service account. But patch this in Kufelow namespace</li>
</ul>

<pre class=""lang-sh prettyprint-override""><code>kubectl --namespace=kubeflow create secret docker-registry $SECRETNAME \  
--docker-server=https://gcr.io \                          
--docker-username=_json_key \                             
--docker-email=user@example.com \                          
--docker-password=""$(cat ./service_account.json.json)""
#username should be _json_key
</code></pre>

<ol start=""3"">
<li>Patching GCR secret with respective service account</li>
</ol>

<pre class=""lang-sh prettyprint-override""><code># For Kubeflow specific problem path pipeline-runner serviceaccount
kubectl --namespace=kubeflow patch serviceaccount pipeline-runner -p '{""imagePullSecrets"": [{""name"": ""$SECRETNAME""}]}'
</code></pre>
","10097045",1
829,59415019,2,59412213,2019-12-19 18:19:55,1,"<p>I am not familar with Terraform or that robust on ML Services; however, the error you provided lends itself to needing to have MSI authentication configured which is configured in the link you provided.</p>

<p>Try updating your ARM to include the identity section like this:</p>

<pre><code>   ...  },
""identity"": {
        ""type"": ""systemAssigned""
      },
                ""properties"": {
                    ""storageAccount"": ""[resourceId('Microsoft.Storage/storageAccounts',parameters('storageAccountName'))]"",
                    ""applicationInsights"": ""[resourceId('Microsoft.Insights/components',parameters('appInsightsName'))]"",
                    ""keyVault"": ""[resourceId('Microsoft.KeyVault/vaults',parameters('keyVaultName'))]""
                }
</code></pre>

<p>This will create the <a href=""https://learn.microsoft.com/en-us/azure/active-directory/managed-identities-azure-resources/overview"" rel=""nofollow noreferrer"">Managed Service Identity</a>.</p>
","8446413",2
830,59292863,2,59249245,2019-12-11 19:36:04,5,"<p>The change to suport <code>withItem</code>(static looping) and <code>withParams</code> (dynamic looping) was done in multiple parts, but they're all available now. Refer <a href=""https://github.com/kubeflow/pipelines/pull/2044"" rel=""noreferrer"">PR</a>.</p>

<p>Ensure that your KPF version is <code>0.1.31</code> or above.</p>

<p>It's possible to loop over the output of a previous <code>container_op</code> as below</p>

<pre><code>echo_op = dsl.ContainerOp(
        name='echo',
        image='library/bash:4.4.23',
        command=['sh', '-c'],
        arguments=['echo ""[1,2,3]""&gt; /tmp/output.txt'],
        file_outputs={'output': '/tmp/output.txt'})

with dsl.ParallelFor(echo_op.output) as item:
        iterate_op = dsl.ContainerOp(
        name='iterate',
        image='library/bash:4.4.23',
        command=['sh', '-c'],
        arguments=[f""echo {item} &gt; /tmp/output.txt""],
        file_outputs={'output': '/tmp/output.txt'})
</code></pre>

<p>Ensure that your output YAML looks something like this:</p>

<pre><code>        name: for-loop-for-loop-3c29048d-1
        template: for-loop-for-loop-3c29048d-1
        withParam: '{{tasks.echo.outputs.parameters.echo-output}}'
</code></pre>
","4438213",1
831,59263317,2,58995329,2019-12-10 08:50:01,2,"<p>I don't know if I will get an answer to my problem but I did <em>solved</em> it this way.</p>

<p>On the server I created the directory <code>/var/mlruns</code>. I pass this directory to mlflow via <code>--backend-store-uri file:///var/mlruns</code></p>

<p>Then I mount this directory via e.g. <code>sshfs</code> on my local machine under the same path.</p>

<p>I don't like this solution but it solved the problem good enough for now.</p>
","3446982",1
832,59197744,2,59150465,2019-12-05 14:53:16,0,"<p>Posting this as Community Wiki for better visibility as Original Poster was able to pass this variable.</p>

<p>It's the best <code>Kubernetes</code> way to pass value.</p>

<blockquote>
  <p><a href=""https://matthewpalmer.net/kubernetes-app-developer/articles/ultimate-configmap-guide-kubernetes.html"" rel=""nofollow noreferrer"">ConfigMap</a> is a dictionary of configuration settings. This
  dictionary consists of key-value pairs of strings. Kubernetes provides
  these values to your containers. ConfigMap stores configuration
  settings for your code. Store connection strings, public credentials,
  hostnames, and URLs in your ConfigMap.</p>
</blockquote>

<p>You can create <code>ConfigMap</code> in many ways (from file, manually, etc). More information can be found <a href=""https://kubernetes.io/docs/tasks/configure-pod-container/configure-pod-configmap/#create-a-configmap"" rel=""nofollow noreferrer"">here</a>.</p>

<p><strong>Solution</strong></p>

<p>According to Original Poster comment:</p>

<p><strong>1.</strong> Pass environmental variable using pipeline python file and <code>container</code> function <code>add_env_variable</code>:</p>

<p><code>web_ui.container.add_env_variable(V1EnvVar(name='modelurl', value=Model_Path))</code></p>

<p><strong>2.</strong> Prepare command which will create config map with proper value:</p>

<p><code>kubectl create configmap modelurl --from-literal=modelurl=Model_Path</code></p>

<p><strong>3.</strong> Put previous command to script which will be used in <code>Kubeflow</code>.</p>
","11148139",0
833,59197599,2,59196919,2019-12-05 14:44:53,1,"<p>In short:
If you are building a .NET application and want to integrate ML, use ML.NET.
If you don't do .NET, use Azure ML.
Docs are helpful here: <a href=""https://learn.microsoft.com/en-us/azure/architecture/data-guide/technology-choices/data-science-and-machine-learning"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/architecture/data-guide/technology-choices/data-science-and-machine-learning</a></p>
","2785283",2
834,59186484,2,58989610,2019-12-05 00:24:45,1,"<p>Apologies for the late response.</p>

<p>Below is some documentation on inference pipelines:
<a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/inference-pipelines.html"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/sagemaker/latest/dg/inference-pipelines.html</a>
<a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/inference-pipeline-real-time.html"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/sagemaker/latest/dg/inference-pipeline-real-time.html</a></p>

<blockquote>
  <p>Should I also have to create a container which is not needed in my use case as I am not using any sci-kit-learn code?</p>
</blockquote>

<p>Your container is an encapsulation of the environment needed for your custom code needed to run properly. Based on the requirements listed above, <code>numpy, pandas, statsmodels etc &amp; later a machine learning algorithm</code>, I would create a container if you wish to isolate your dependencies or modify an existing predefined SageMaker container, such as the scikit-learn one, and add your dependencies into that.</p>

<blockquote>
  <p>Can someone give me a custom example of using these pipelines? Any help is appreciated!</p>
</blockquote>

<p>Unfortunately, the two example notebooks referenced above are the only examples utilizing inference pipelines. The biggest hurdle most likely is creating containers that fulfill the preprocessing and prediction task you are seeking and then combining those two together into the inference pipeline.</p>
","9090582",2
835,59096925,2,59088199,2019-11-28 22:49:48,0,"<p>Incremental training is a native feature for the built-in <a href=""https://aws.amazon.com/blogs/machine-learning/now-easily-perform-incremental-learning-on-amazon-sagemaker/"" rel=""nofollow noreferrer"">Image Classifier and Object Detector</a>. For custom code, it is the developer responsibility to write the incremental training logic and to verify its validity. Here is a possible path:</p>

<ol>
<li>use one of the data channels passed in the <code>fit</code> to load a model state (artifact to fine-tune)</li>
<li>in your code, check if the model state channel is filled
with artifacts. If it is, instantiate a model from that state
and continue training. This is framework specific and you may to take
necessary precautions to avoid forgetting previous learnings.</li>
</ol>

<p>Some frameworks provide better support for incremental learning that others. For example some sklearn models provide an <a href=""https://scikit-learn.org/0.15/modules/scaling_strategies.html#incremental-learning"" rel=""nofollow noreferrer"">incremental_fit</a> method. For DL frameworks it is technically very easy to continue training from a checkpoint, but if new data is very different from previously-seen data this may lead your model to forget previous learnings.</p>
","5331834",2
836,58959140,2,58952962,2019-11-20 16:31:15,13,"<p>You can first add the different <a href=""https://dvc.org/doc/command-reference/remote"" rel=""nofollow noreferrer"">DVC remotes</a> you want to establish (let's say you call them <code>data</code> and <code>models</code>, each one pointing to a different <a href=""https://cloud.google.com/storage/docs/json_api/v1/buckets"" rel=""nofollow noreferrer"">GC bucket</a>). <strong>But don't set any remote as the project's default</strong>; This way, <a href=""https://dvc.org/doc/command-reference/push"" rel=""nofollow noreferrer""><code>dvc push</code></a> won't work without the <code>-r</code> (or <code>--remote</code>) option.</p>
<p>You would then need to push each directory or file individually to the appropriate remote, like <code>dvc push data/ -r data</code> and <code>dvc push model.dat -r models</code>.</p>
<p>Note that a feature request to configure this exists on the DVC repo too. See <a href=""https://github.com/iterative/dvc/issues/2095"" rel=""nofollow noreferrer"">Specify file types that can be pushed to remote</a>.</p>
","761963",4
837,58943366,2,58943117,2019-11-19 21:51:31,3,"<p><strong>If there are no endpoints active under the ""Endpoints"" tab in the SageMaker service console, then you will not be incurring any charges for inference or endpoint infrastructure.</strong></p>

<p>If this is the case, your Endpoints tab should look like the following:
<a href=""https://i.stack.imgur.com/5QvEn.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/5QvEn.png"" alt=""enter image description here""></a></p>

<p><strong>Endpoint Configurations</strong>, on the other hand, involve the metadata necessary for an endpoint deployment. This is just the metadata, and are stored (without cost) in your account, visible in the console under the ""Endpoint Configurations"" tab. You do not need to remove these configurations when tearing down an endpoint.</p>

<p><strong>Important note:</strong> Double check that you are checking in the console for the <em>region you would have deployed to</em>. For example, if you ran the notebook and deployed an endpoint in <code>us-east-1</code>, but check the SageMaker console for <code>us-west-2</code>, it would not be displaying endpoints from the other region.</p>
","10885720",1
838,58932992,2,58879596,2019-11-19 11:25:55,1,"<p><code>data=json.loads(json.dumps(event))</code> is a redundant operation. <code>data=event</code> will return <code>True</code>. The event we provided for the test case is of type dict. It has a key value pair. key can be anything and the value should be a single string of all the predictor variables separated by comas. For predicting the output, we need value of the test case. So declare, for example, <code>payload=data['key']</code> then change <code>Body=payload</code> inside <code>response</code>. Then it will work.</p>
","12060322",0
839,58932857,2,58801976,2019-11-19 11:18:40,1,"<p>""unable to evaluate payload provided"" occurs only when the input data format is not compatible with the ML model you created. In this case, to get results on training set, we need to remove the last column(label column) before passing it to endpoint</p>
","12060322",0
840,58882478,2,58816515,2019-11-15 18:06:07,1,"<p>I am not expert in DataBricks or Spark, but pickling functions from the local notebook context is always problematic when you are touching complex objects like the <code>service</code> object. In this particular case, I would recommend removing the dependency on the azureML <code>service</code> object and just use <code>requests</code> to call the service. </p>

<p>Pull the key from the service:</p>

<pre><code># retrieve the API keys. two keys were generated.
key1, key2 = service.get_keys()
scoring_uri = service.scoring_uri
</code></pre>

<p>You should be able to use these strings in the UDF directly without pickling issues -- <a href=""https://github.com/Azure/MachineLearningNotebooks/blob/9233ce089afb81d466076e36e7e61c3ce4cfafec/how-to-use-azureml/ml-frameworks/chainer/deployment/train-hyperparameter-tune-deploy-with-chainer/train-hyperparameter-tune-deploy-with-chainer.ipynb"" rel=""nofollow noreferrer"">here is an example</a> of  how you would call the service with just requests. Below applied to your UDF:</p>

<pre><code>import requests, json
def predictModelValue2(summary, modelName, modelLabel):  
  input_data = json.dumps({""summary"": summary, ""modelName"":, ....})

  headers = {'Content-Type':'application/json', 'Authorization': 'Bearer ' + key1}

  # call the service for scoring
  resp = requests.post(scoring_uri, input_data, headers=headers)

  return resp.text[1]

</code></pre>

<p>On a side node, though: your UDF will be called for each row in your data frame and each time it will make a network call -- that will be very slow. I would recommend looking for ways to batch the execution. As you can see from your constructed json <code>service.run</code> will accept an array of items, so you should call it in batches of 100s or so.</p>
","8821969",2
841,58806988,2,58806807,2019-11-11 18:44:41,0,"<p>The (python) <code>Predictors</code> <a href=""https://sagemaker.readthedocs.io/en/stable/predictors.html"" rel=""nofollow noreferrer"">documentation</a> shows that you can pass a <code>Session</code> object. In turn, the <code>Session</code> can be <a href=""https://sagemaker.readthedocs.io/en/stable/session.html#sagemaker.session.Session"" rel=""nofollow noreferrer"">initialized</a> with a <em>client</em> and a <em>runtime client</em> - the former does everything except endpoint invocations, the latter does... endpoint invocations.</p>

<p>Those clients are tied to specific regions. It seems like you should be able to set the runtime client region to match your endpoint, by manually instantiating it, while leaving the regular client alone (disclaimer here: I haven't tried this - if you do, let me/us know how it goes :)).</p>
","7859515",0
842,58792003,2,58666136,2019-11-10 19:05:41,6,"<p>One of the parameters to <code>mlflow.start_run()</code> is <code>run_name</code>.  This would give you programmatic access to set the run name with each iteration. See the docs <a href=""https://mlflow.org/docs/latest/python_api/mlflow.html#mlflow.start_run"" rel=""noreferrer"">here</a>. </p>

<p>Here's an example:</p>

<pre class=""lang-py prettyprint-override""><code>from datetime import datetime

## Define the name of our run
name = ""this run is gonna be bananas"" + datetime.now()

## Start a new mlflow run and set the run name
with mlflow.start_run(run_name = name):

    ## ...train model, log metrics/params/model...

    ## End the run
    mlflow.end_run()
</code></pre>

<p>If you want to include set the name as part of an MLflow Project, you'll have to specify it as a parameter in the entry points to the project.  This is located in in the <a href=""https://mlflow.org/docs/latest/projects.html#mlproject-file"" rel=""noreferrer"">MLproject file</a>.  Then you can pass those values into the <code>mlflow.start_run()</code> function from the command line.</p>
","4896112",1
843,58716132,2,58659160,2019-11-05 17:01:58,1,"<p>It appears, there is a <a href=""https://learn.microsoft.com/en-us/python/api/azureml-core/azureml.core.conda_dependencies.condadependencies?view=azure-ml-py#set-pip-option-pip-option-"" rel=""nofollow noreferrer""><code>set_pip_option</code> method</a> in the SDK which sorts out the problem with one single extra-index-url, e.g.</p>

<pre class=""lang-py prettyprint-override""><code>from azureml.core.environment import CondaDependencies
dep = CondaDependencies.create(pip_packages=[""pyyaml"", ""param""])
dep.set_pip_option(""--extra-index-url https://user:password@extra.index/url"")
</code></pre>

<p>Unfortunately, second call to this function replaces the first value with the new one. For the <code>--extra-index-url</code> option this logic should be changed in order to support search in more than 2 indices (one public, one private).</p>
","9365426",3
844,58663275,2,56701139,2019-11-01 17:18:24,3,"<p>You can add extra dependencies when you save your model, for example if you have a keras step in your pipeline you can add keras &amp; tensorflow:</p>

<pre><code>  conda_env = mlflow.sklearn.get_default_conda_env()
  conda_env[""dependencies""] = ['keras==2.2.4', 'tensorflow==1.14.0'] + conda_env[""dependencies""]
  mlflow.sklearn.log_model(pipeline, ""modelstorage/model43"", conda_env = conda_env)
</code></pre>
","823112",0
845,58643527,2,58541794,2019-10-31 12:38:13,1,"<p>Save the weights to disk and then log them as an artifact.  As long as the checkpoints/weights are saved to disk, you can log them with <code>mlflow_log_artifact()</code> or <code>mlflow_log_artifacts()</code>.  From the <a href=""https://www.mlflow.org/docs/latest/tracking.html#logging-functions"" rel=""nofollow noreferrer"">docs</a>,</p>

<blockquote>
  <p><strong>mlflow.log_artifact()</strong> logs a local file or directory as an artifact,
  optionally taking an artifact_path to place it in within the run’s
  artifact URI. Run artifacts can be organized into directories, so you
  can place the artifact in a directory this way.</p>
  
  <p><strong>mlflow.log_artifacts()</strong> logs all the files in a given directory as
  artifacts, again taking an optional artifact_path.</p>
</blockquote>
","4896112",0
846,58546809,2,58194899,2019-10-24 17:57:09,1,"<p>Unfortunately, this is a gap in functionality. There is some related work in <a href=""https://github.com/aws/sagemaker-python-sdk/pull/941"" rel=""nofollow noreferrer"">https://github.com/aws/sagemaker-python-sdk/pull/941</a> which should also solve this issue, but for now, you do need to put <code>my_script.py</code> in <code>source_dir</code>.</p>
","9074534",0
847,58542946,2,58541260,2019-10-24 13:54:37,12,"<p>DVC is a better replacement for <code>git-lfs</code>. </p>

<p>Unlike git-lfs, DVC doesn't require installing a dedicated server; It can be used on-premises (NAS, SSH, for example) or with any major cloud provider (S3, Google Cloud, Azure).</p>

<p>For more information: <a href=""https://dvc.org/doc/use-cases/data-and-model-files-versioning"" rel=""noreferrer"">https://dvc.org/doc/use-cases/data-and-model-files-versioning</a></p>
","10614328",5
848,58523923,2,58500807,2019-10-23 13:25:18,1,"<p>When your training script is running in azure, it's not able to find all your local imports i.e. <code>amlrun.py</code> script. </p>

<p>The submitted training job to azure builds a docker image with your files first and runs the experiment; but in this case the extension hasn't included <code>amlrun.py</code>. </p>

<p>This is probably because when you have submit the training job with the extension, the visual studio code window opened is not pointing to be in <code>scripts</code> folder.</p>

<p>Taken from one of the replies to a <a href=""https://github.com/MicrosoftDocs/azure-docs/issues/24032"" rel=""nofollow noreferrer"">previously raised github issue</a>:</p>

<blockquote>
  <p>The extension currently requires the script you are working on to be
  in the folder that is open in VS Code and not in a sub-directory.</p>
</blockquote>

<hr>

<p>To fix this you can do <strong>either</strong> of the following:</p>

<ol>
<li><p>You would need to re-open Visual Studio Code in <code>scripts</code> folder instead of parent directory.</p></li>
<li><p>Move all files in <code>script</code> directory to be in it's parent directory.</p></li>
</ol>

<hr>

<p>If you're looking for more flexible way to submit training jobs and managing aml - you can use the <a href=""https://learn.microsoft.com/en-us/python/api/overview/azure/ml/intro?view=azure-ml-py"" rel=""nofollow noreferrer"">azure machine learning sdk</a> for python.</p>

<p>Some examples of using the SDK to manage expirements can be found in the links below:</p>

<ol>
<li><p><a href=""https://github.com/MicrosoftDocs/azure-docs/blob/master/articles/machine-learning/service/tutorial-train-models-with-aml.md"" rel=""nofollow noreferrer"">Scikit Learn Model Training Docs</a> </p></li>
<li><p><a href=""https://github.com/rithinch/heartfulness-similar-content-service"" rel=""nofollow noreferrer"">Basic Pytorch Model Training and Deployment Example Repo</a></p></li>
</ol>
","11914067",2
849,58383799,2,55992973,2019-10-14 20:39:49,2,"<p><strong>Update</strong>: This has changed recently and can be done simply by using <code>ParallerlFor</code> over the output. Refer: <a href=""https://stackoverflow.com/a/59292863/4438213"">https://stackoverflow.com/a/59292863/4438213</a></p>

<p>----- Below for KF 0.6 and before ----</p>

<p>This is a recognized issue with Kubeflow DSL: <em>to use the output of one component (A) and iterate over it running a new component (B) for each entry in the previous output</em>. It's hard since the DSL, Kubeflow uses, is at compile time and it's not possible to know how many elements there would be in the output at that time. </p>

<p>Ref: </p>

<ul>
<li><a href=""https://github.com/kubeflow/pipelines/issues/1967"" rel=""nofollow noreferrer"">https://github.com/kubeflow/pipelines/issues/1967</a></li>
<li><a href=""https://github.com/kubeflow/pipelines/issues/1481"" rel=""nofollow noreferrer"">https://github.com/kubeflow/pipelines/issues/1481</a></li>
</ul>

<p>The only form of dynamic (run-time) iteration supported as of KF v0.6 is: <a href=""https://www.kubeflow.org/docs/pipelines/sdk/dsl-recursion/"" rel=""nofollow noreferrer"">dsl-recursion</a>. I've made it work in 2 ways lacking the pending work on the issues above:</p>

<p>If the size of the result of A is going to be a constant in each run and is pre-known, this is straight forward. </p>

<p><strong>CASE A: The size of the output from the previous step is known</strong></p>

<ol>
<li>Create a lightweight comp to get the image path at a given index</li>
</ol>

<pre><code># Write a python code to extract the path from
# the string of refs the previous step returns 
def get_path(str_of_paths: str, idx: int) -&gt; str:
    return str_of_paths.split("" "")[idx] # or some other delimiter
</code></pre>

<ol start=""2"">
<li>Wrap the python code in a <a href=""https://www.kubeflow.org/docs/pipelines/sdk/lightweight-python-components/"" rel=""nofollow noreferrer"">Kubeflow lightweight components</a></li>
</ol>

<pre><code>get_img_path_comp = comp.func_to_container_op(get_path,base_image='tensorflow/tensorflow') # or any appropriate base image
</code></pre>

<p>And then a regular for loop in your pipeline dsl code would work</p>

<pre><code>image_path_res = ContainerOP_A() # run your container Op
for idx in range(4):
    path = get_path(image_path_res.output, i)
    ContainerOp_B(path.output)
</code></pre>

<p><strong>CASE B: When the output of the previous step is not of fixed size</strong></p>

<p>This is a little tricky and intricate. The only form of dynamic looping Kubeflow allows as of KF v0.6 is <a href=""https://www.kubeflow.org/docs/pipelines/sdk/dsl-recursion/"" rel=""nofollow noreferrer"">dsl-recursion</a></p>

<p><strong><em>Option 1</em></strong></p>

<ol>
<li>Create 2 lightweight components, one for calculating the size of the result <code>sizer_op</code> and then reuse the same <code>get_img_path_comp</code> from above.</li>
</ol>

<pre><code>@dsl.component
def sizer_op(str_of_refs) -&gt; int:
    return len(str_of_refs.split(""|""))
sizer_op_comp = comp.func_to_container_op(sizer_op,base_image='tensorflow/tensorflow')
</code></pre>

<p>Then you can run the recusive function</p>

<pre><code>@dsl.component
def subtracter_op(cur_idx) -&gt; int:
    return cur_idx - 1
sub_op_comp = comp.func_to_container_op(subtracter_op,base_image='tensorflow/tensorflow')

@dsl.graph_component
def recursive_run(list_of_images, cur_ref):
    with dsl.Condition(cur_ref &gt;= 0):
        path = get_path(image_path_res.output, i)
        ContainerOp_B(path.output)

        # call recursively
        next_ref = sub_op_comp(cur_ref)
        recursive_run(list_of_images, next_ref)


image_path_res = ContainerOP_A() # run your container Op
sizer = sizer_op_comp(image_path_res)
recursive_run(image_path_res.output, sizer.output)
</code></pre>

<p><strong><em>Option 2</em></strong></p>

<p>After running ContainerOp_A, create a Kubeflow Component that reads the results from ContainerOp_A, parses the results in python code itself and then spawns new runs that run just Containerop_B using kfclient. You can connect to KF pipeline client using:</p>

<pre><code>kf_client = Client(host=localhost:9990)
</code></pre>

<p>Refer: <a href=""https://github.com/kubeflow/pipelines/blob/master/sdk/python/kfp/_client.py"" rel=""nofollow noreferrer"">kf_client</a></p>
","4438213",2
850,58332541,2,58234777,2019-10-11 00:00:25,1,"<p>Thanks for the catch, you're right that using <code>os.path.join</code> when working with DBFS paths is incorrect, resulting in a malformed path that breaks project execution. I've filed to <a href=""https://github.com/mlflow/mlflow/issues/1926"" rel=""nofollow noreferrer"">https://github.com/mlflow/mlflow/issues/1926</a> track this, if you're interested in making a bugfix PR (<a href=""https://github.com/mlflow/mlflow/blob/master/CONTRIBUTING.rst"" rel=""nofollow noreferrer"">see the MLflow contributor guide for info on how to do this</a>) to replace <code>os.path.join</code> here with <code>os.posixpath.join</code> I'd be happy to review :)</p>
","3151283",1
851,58327595,2,58323029,2019-10-10 16:46:03,1,"<p>There was a bug with the retry logic when files were being uploaded. That bug has since been fixed, so updating your SDK should fix the issue.</p>

<p>Similar post: <a href=""https://stackoverflow.com/questions/57854136/registering-and-downloading-a-fasttext-bin-model-fails-with-azure-machine-learn"">Registering and downloading a fastText .bin model fails with Azure Machine Learning Service</a></p>
","5245304",0
852,58290950,2,58188104,2019-10-08 17:04:23,1,"<p>Don't worry about the input schema for the <code>Col21</code> field. The <code>Col21</code> field in the input data just adapt for the <code>Edit Metadata</code> module which requires the <code>Col21</code> data in the training stage.</p>

<p>You just fill an invalid value like <code>0</code> (<code>0</code> is an invalid classified value for risk) into <code>Col21</code> field, and then the web service will return a prediction classified value to replace the <code>Col21</code> value of your input data.</p>

<p>At here, I use the first data record of the sample data with the <code>Col21</code> value <code>0</code> for testing via the link of <code>Test</code> feature on portal, it works fine and return <code>1</code> for <code>Credit risk</code></p>

<p>Fig 1. To click <code>Test</code> link to test for <code>Col21</code> with <code>0</code></p>

<p><a href=""https://i.stack.imgur.com/9jqyo.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/9jqyo.png"" alt=""enter image description here""></a></p>

<p>Fig 2. Use the first record of sample to test</p>

<p><a href=""https://i.stack.imgur.com/lIUiP.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/lIUiP.png"" alt=""enter image description here""></a></p>

<p>Fig 3. The <code>Col21</code> value of <code>input1</code> is <code>0</code>, and the <code>Credit risk</code> value of <code>output1</code> is <code>1</code></p>

<p><a href=""https://i.stack.imgur.com/h5OEH.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/h5OEH.png"" alt=""enter image description here""></a></p>
","4989676",0
853,58278470,2,58177548,2019-10-07 23:51:14,1,"<p>I ran my script again today and it worked fine, no <code>botocore.exceptions.ClientError</code> exceptions.  Given that this issue affected both the Python SDK for Sagemaker and the console, I suspect it might have been an issue with the backend API and not my client code.</p>
<p>Either way, it's working now.</p>
","38458",0
854,58238824,2,58237749,2019-10-04 14:56:53,2,"<p>You'll need to download the library files from <a href=""https://s3.amazonaws.com/aws-glue-jes-prod-us-east-1-assets/etl/python/PyGlue.zip"" rel=""nofollow noreferrer"">here</a> for Glue 0.9 or <a href=""https://s3.amazonaws.com/aws-glue-jes-prod-us-east-1-assets/etl-1.0/python/PyGlue.zip"" rel=""nofollow noreferrer"">here</a> for Glue 1.0 (Check your Glue jobs for the version). </p>

<p>Put the zip in S3 and reference it in the ""Python library path"" on your Dev Endpoint.</p>
","8325820",2
855,58164544,2,58163305,2019-09-30 08:57:26,4,"<p>one of DVC maintainers here.</p>

<p>Short answer: 2. is correct.</p>

<p>A bit of additional information:
Please be careful when using <code>dvc gc</code>. It will clear your cache from all dependencies that are not mentioned in the current HEAD of your git repository. 
We are working on making <code>dvc gc</code> preserving whole history by default. </p>

<p>So if you don't want to delete files from your history commits, it would be better to wait for completion of <a href=""https://github.com/iterative/dvc/issues/2325"" rel=""nofollow noreferrer"">this</a> task.</p>

<p>[EDIT]
Please see comment below.</p>
","3406563",2
856,58159408,2,57987999,2019-09-29 21:39:34,0,"<p>You can't do it via the web UI but you can from a python terminal</p>

<pre class=""lang-py prettyprint-override""><code>import mlflow

mlflow.delete_experiment(69)
</code></pre>

<p>Where 69 is the experiment ID</p>
","196732",3
857,58121517,2,58117200,2019-09-26 16:56:32,1,"<p>For getting status of run, you can use REST APIs described here <a href=""https://github.com/Azure/azure-rest-api-specs/tree/master/specification/machinelearningservices/data-plane"" rel=""nofollow noreferrer"">https://github.com/Azure/azure-rest-api-specs/tree/master/specification/machinelearningservices/data-plane</a> </p>

<p>Specifically you need <a href=""https://github.com/Azure/azure-rest-api-specs/blob/master/specification/machinelearningservices/data-plane/Microsoft.MachineLearningServices/preview/2019-08-01/runHistory.json"" rel=""nofollow noreferrer"">https://github.com/Azure/azure-rest-api-specs/blob/master/specification/machinelearningservices/data-plane/Microsoft.MachineLearningServices/preview/2019-08-01/runHistory.json</a></p>

<p>use this call to get run information including status:</p>

<blockquote>
  <p>/history/v1.0/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.MachineLearningServices/workspaces/{workspaceName}/experiments/{experimentName}/runs/{runId}/details</p>
</blockquote>
","11892119",1
858,58020935,2,58019308,2019-09-20 02:14:46,4,"<p>Being able to use <code>DataReference</code> in <code>ScriptRunConfig</code> is a bit more involved than doing just <code>ds.as_mount()</code>. You will need to convert it into a string in <code>arguments</code> and then update the <code>RunConfiguration</code>'s <code>data_references</code> section with the <code>DataReferenceConfiguration</code> created from <code>ds</code>. Please <a href=""https://github.com/Azure/MachineLearningNotebooks/blob/master/how-to-use-azureml/training/train-on-remote-vm/train-on-remote-vm.ipynb"" rel=""nofollow noreferrer"">see here</a> for an example notebook on how to do that.</p>
<p>If you are just reading from the input location and not doing any writes to it, please check out <a href=""https://learn.microsoft.com/en-us/azure/machine-learning/service/how-to-create-register-datasets"" rel=""nofollow noreferrer""><code>Dataset</code></a>. It allows you to do exactly what you are doing without doing anything extra. <a href=""https://github.com/Azure/MachineLearningNotebooks/blob/master/how-to-use-azureml/work-with-data/datasets-tutorial/train-with-datasets.ipynb"" rel=""nofollow noreferrer"">Here is an example notebook</a> that shows this in action.</p>
<p>Below is a short version of the notebook</p>
<pre class=""lang-py prettyprint-override""><code>from azureml.core import Dataset

# more imports and code

ds = Datastore(workspace, 'mydatastore')
dataset = Dataset.File.from_files(path=(ds, 'path/to/input-data/within-datastore'))

src = ScriptRunConfig(source_directory=project_folder, 
                      script='train.py', 
                      arguments=['--input-data-dir', dataset.as_named_input('input').as_mount(),
                                 '--reg', '0.99'],
                      run_config=run_config) 
run = experiment.submit(config=src)
</code></pre>
","8877982",1
859,58019147,2,58018893,2019-09-19 21:28:42,1,"<h2>Downloading a file from S3:</h2>

<p>This code block in the Q2 section defines the function that downloads a file from S3. The user instantiates an S3 client, and then passes the S3 URL along to the <code>s3.Bucket.download_file()</code> method.</p>

<pre><code>def download_from_s3(url):
    """"""ex: url = s3://sagemakerbucketname/data/validation.tfrecords""""""
    url_parts = url.split(""/"")  # =&gt; ['s3:', '', 'sagemakerbucketname', 'data', ...
    bucket_name = url_parts[2]
    key = os.path.join(*url_parts[3:])
    filename = url_parts[-1]
    if not os.path.exists(filename):
        try:
            # Create an S3 client
            s3 = boto3.resource('s3')
            print('Downloading {} to {}'.format(url, filename))
            s3.Bucket(bucket_name).download_file(key, filename)
        except botocore.exceptions.ClientError as e:
            if e.response['Error']['Code'] == ""404"":
                print('The object {} does not exist in bucket {}'.format(
                    key, bucket_name))
            else:
                raise
</code></pre>

<h2>Estimator.fit() explanation:</h2>

<p>The <code>estimator.fit(train_data_location)</code> line is what initiates the training process with SageMaker. When run, SageMaker will provision the necessary infrastructure, fetch the data from the location the user designated (here, <code>train_data_location</code> which is a path to Amazon S3) and distribute it amongst the training cluster, carry out the training process, return the resulting model, and tear down the training infrastructure. </p>

<p>You can find the result of this training job in the SageMaker console.</p>
","10885720",1
860,58014202,2,57978333,2019-09-19 15:16:28,1,"<p>thanks @Marcel Mendes Reis for following up on your solution in the comments. I will repost here for others to easily find:</p>

<p><em>I realized the issue was due to the max_runtime. When I trained the model with more time I didn't have the problem.</em> </p>
","6312126",0
861,57991106,2,57908395,2019-09-18 10:54:26,1,"<p>I have solved it using s3_input objects:</p>

<pre><code>s3_input_train = sagemaker.s3_input(s3_data='s3://antifraud/production/data/{domain}-{product}-{today}/train_data.csv',
content_type='text/csv')
s3_input_validation = sagemaker.s3_input(s3_data='s3://antifraud/production/data/{domain}-{product}-{today}/validation_data.csv',
content_type='text/csv')

train_config = training_config(estimator=estimator,
inputs = {'train':s3_input_train,
          'validation':s3_input_validation})
</code></pre>
","9621172",1
862,57931369,2,57692681,2019-09-13 23:37:02,1,"<p>There are multiple options for algorithms in SageMaker:</p>

<ol>
<li><a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/algos.html"" rel=""nofollow noreferrer"">Built-in algorithms</a>, like the SageMaker XGBoost you mention</li>
<li>Custom, user-created algorithm code, which can be:

<ul>
<li>Written for a pre-built docker image, available for Sklearn, TensorFlow, Pytorch, MXNet</li>
<li>Written in your own container</li>
</ul></li>
</ol>

<p>When you use built-ins (option 1), your choice of data format options is limited to what the built-ins support, <a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/xgboost.html#InputOutput-XGBoost"" rel=""nofollow noreferrer"">which is only csv and libsvm in the case of the built-in XGBoost</a>. If you want to use custom data formats and pre-processing logic before XGBoost, it is absolutely possible if you use your own script leveraging the open-source XGBoost. You can get inspiration from the <a href=""https://github.com/awslabs/amazon-sagemaker-examples/blob/master/sagemaker-python-sdk/scikit_learn_randomforest/Sklearn_on_SageMaker_end2end.ipynb"" rel=""nofollow noreferrer"">Random Forest demo</a> to see how to create custom models in pre-built containers</p>
","5331834",0
863,57810735,2,57806505,2019-09-05 17:51:56,2,"<p>Here is a workaround I discovered, serializing arguments as a json string.  Not sure this is really the best way...</p>

<p>The bare function becomes:</p>

<pre><code>def my_func(json_arg_str: str) -&gt; bool:
    import json
    args = json.loads(json_arg_str)
    my_list = args['my_list']
    print(f'my_list is {my_list}')
    print(f'my_list is of type {type(my_list)}')
    print(f'elem 0 is {my_list[0]}')
    print(f'elem 1 is {my_list[1]}')
    return True
</code></pre>

<p>Which still works as long as you pass the args as a json string instead of a list:</p>

<p>test_data = '{""my_list"":[""abc"", ""def""]}'
my_func(test_data)</p>

<p>Which produces expected results:</p>

<pre><code>my_list is ['abc', 'def']
my_list is of type &lt;class 'list'&gt;
elem 0 is abc
elem 1 is def
</code></pre>

<p>And now the pipeline is changed to accept a <code>str</code> instead of a <code>PipelineParam</code> of type <code>kfp.dsl.types.List</code>:</p>

<pre><code>import kfp 

my_op = kfp.components.func_to_container_op(my_func)

@kfp.dsl.pipeline()
def my_pipeline(json_arg_str: str):
    my_op(json_arg_str)

kfp.compiler.Compiler().compile(my_pipeline, 'my_pipeline.zip')
</code></pre>

<p>Which, when executed like this:</p>

<pre><code>client = kfp.Client()
experiment = client.create_experiment('Default')
client.run_pipeline(experiment.id, 'my job', 'my_pipeline.zip', params={'json_arg_str': test_data})
</code></pre>

<p>Produces the same result:</p>

<pre><code>my_list is ['abc', 'def']
my_list is of type &lt;class 'list'&gt;
elem 0 is abc
elem 1 is def
</code></pre>

<p>Although it works, I nevertheless find this workaround annoying.  What then is the point of <a href=""https://kubeflow-pipelines.readthedocs.io/en/latest/source/kfp.dsl.types.html#kfp.dsl.types.List"" rel=""nofollow noreferrer"">kfp.dsl.types.List</a>, if not for allowing a PipelineParam that is a List?  </p>
","19269",0
864,57796296,2,57396212,2019-09-04 22:22:31,10,"<p>EDIT: <strong>Amazon SageMaker does now support TF 2.0 and higher.</strong></p>
<ul>
<li>SageMaker + TensorFlow docs: <a href=""https://sagemaker.readthedocs.io/en/stable/frameworks/tensorflow/using_tf.html"" rel=""nofollow noreferrer"">https://sagemaker.readthedocs.io/en/stable/frameworks/tensorflow/using_tf.html</a></li>
<li>Supported Tensorflow versions (and Docker URIs): <a href=""https://aws.amazon.com/releasenotes/available-deep-learning-containers-images"" rel=""nofollow noreferrer"">https://aws.amazon.com/releasenotes/available-deep-learning-containers-images</a></li>
</ul>
<hr />
<p><em>Original answer</em></p>
<p>Here is an example Dockerfile that uses <a href=""https://github.com/aws/sagemaker-containers"" rel=""nofollow noreferrer"">the underlying SageMaker Containers library</a> (this is what is used in the official pre-built Docker images):</p>
<pre><code>FROM tensorflow/tensorflow:2.0.0b1

RUN pip install sagemaker-containers

# Copies the training code inside the container
COPY train.py /opt/ml/code/train.py

# Defines train.py as script entrypoint
ENV SAGEMAKER_PROGRAM train.py
</code></pre>
<p>For more information on this approach, see <a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/build-container-to-train-script-get-started.html"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/sagemaker/latest/dg/build-container-to-train-script-get-started.html</a></p>
","9074534",4
865,57788209,2,57759556,2019-09-04 12:15:01,4,"<p>In order to be able to log in to the nodes of an AML Compute cluster, you have to provide a username and password and ssh key (ssh key is optional), <strong>when you create the cluster</strong>. You can only do that at creation time.</p>

<p><a href=""https://i.stack.imgur.com/Ckh6j.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Ckh6j.png"" alt=""enter image description here""></a></p>
","8821969",0
866,57771934,2,50611864,2019-09-03 12:37:54,1,"<p>So, your input config is not correctly formatted. 
Checkout the sample json here:
<a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/API_CreateTrainingJob.html"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/sagemaker/latest/dg/API_CreateTrainingJob.html</a></p>

<pre><code># look at the format of input-data-config, it is a dictionary
  ""InputDataConfig"": [ 
      { 
         ""ChannelName"": ""string"",
         ""CompressionType"": ""string"",
         ""ContentType"": ""string"",
         ""DataSource"": { 
            ""FileSystemDataSource"": { 
               ""DirectoryPath"": ""string"",
               ""FileSystemAccessMode"": ""string"",
               ""FileSystemId"": ""string"",
               ""FileSystemType"": ""string""
            },
            ""S3DataSource"": { 
               ""AttributeNames"": [ ""string"" ],
               ""S3DataDistributionType"": ""string"",
               ""S3DataType"": ""string"",
               ""S3Uri"": ""string""
            }
         },
         ""InputMode"": ""string"",
         ""RecordWrapperType"": ""string"",
         ""ShuffleConfig"": { 
            ""Seed"": number
         }
      }
   ]
</code></pre>
","6025065",0
867,57765228,2,57765164,2019-09-03 04:53:36,1,"<p>That should be what a <a href=""https://www.kubeflow.org/docs/pipelines/overview/concepts/run/"" rel=""nofollow noreferrer"">recurring run</a> is for.</p>

<p>That would be using a <a href=""https://www.kubeflow.org/docs/pipelines/overview/concepts/run-trigger/"" rel=""nofollow noreferrer"">run trigger</a>, which does have a cron field, for specifying cron semantics for scheduling runs.</p>
","6309",0
868,57732305,2,57522553,2019-08-30 19:28:52,2,"<p>When running training jobs in SageMaker the S3 URL containing your training data provided ends up being copied into the docker container (aka training job) from the specified url. Thus the environment variable SM_CHANNEL_TRAIN is pointing to the local path of the training data that was copied from the S3 URL provided.</p>

<p><a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/API_CreateTrainingJob.html#SageMaker-CreateTrainingJob-request-InputDataConfig"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/sagemaker/latest/dg/API_CreateTrainingJob.html#SageMaker-CreateTrainingJob-request-InputDataConfig</a></p>
","9090582",0
869,57731696,2,57716459,2019-08-30 18:33:30,1,"<p>You could use model tags to set up your own identifiers that are shared across workspace, and query models with specific tags:</p>

<pre><code>az ml model update --add-tag
az ml model list --tag
</code></pre>
","5784983",2
870,57685377,2,57655516,2019-08-28 05:05:57,5,"<p>When you create a SageMaker notebook for the Glue dev endpoint, it launches a SageMaker notebook instance with a specific lifecycle configuration. This LC provides the configurations to create a connection between the SageMaker notebook and the development endpoint. Upon running cells from the PySpark kernel, the code is sent to the Livy server running in the development endpoint via REST APIs. </p>

<p>Thus, the PySpark version that you see and on which the SageMaker notebook runs depends on the development endpoint and is not configurable from the SageMaker point of view.</p>

<p>Since Glue is a managed service, root access is restricted for the development endpoint. Thus, you cannot update the spark version to a more later version. The feature of using Spark version 2.4 has been newly introduced in Glue and it seems that it has not yet been released for dev endpoint.</p>
","4592183",0
871,57643650,2,57622122,2019-08-25 06:07:36,1,"<p>The way to trigger Step Functions on schedule is by using CloudWatch Events (sort of cron). Check out this tutorial: <a href=""https://docs.aws.amazon.com/step-functions/latest/dg/tutorial-cloudwatch-events-target.html"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/step-functions/latest/dg/tutorial-cloudwatch-events-target.html</a></p>

<p>Don't write to the RDS from your Python code! It is better to write the output to S3 and then ""copy"" the files from S3 into the RDS. Decoupling these batches will make a more reliable and scalable process. You can trigger the bulk copy into the RDS when the files are written to S3 or to a later time when your DB is not too busy. </p>
","179529",5
872,57626080,2,57601733,2019-08-23 12:10:40,1,"<p>I think you just need to specify a repo. For example, setting the RStudio CRAN repo, I can install perfectly fine.</p>

<pre class=""lang-r prettyprint-override""><code>install.packages(""disk.frame"", repo=""https://cran.rstudio.com/"")
</code></pre>
","239923",4
873,57618361,2,56857709,2019-08-23 00:06:47,1,"<p>I don't believe this is possible today, as the design choice is to associate the runs tab to the notebook experiment.  From the <a href=""https://docs.databricks.com/applications/mlflow/tracking.html#notebook-experiments"" rel=""nofollow noreferrer"">docs</a>:</p>
<blockquote>
<p>Every Python and R notebook in a Databricks workspace has its own experiment. When you use MLflow in a notebook, it records runs in the notebook experiment.</p>
<p>A notebook experiment shares the same name and ID as its corresponding notebook. The notebook ID is the numerical identifier at the end of a Notebook URL.</p>
</blockquote>
<p>You can create experiments independent of the notebook experiment and log runs to it from different sources.  You'll still have to open up the tracking UI to explore the results though.</p>
<p>In other words, you can send multiple runs from different notebooks to the same experiment, but today you cannot log multiple runs to the 'Runs' tab in a specific notebook.</p>
","4896112",1
874,57610706,2,57600154,2019-08-22 13:40:55,2,"<p>There's a separate address property for a custom image registry. Try specifying it this way:</p>

<pre><code>run_config.environment.docker.base_image = ""mydockerimage:0.0.1""
run_config.environment.docker.base_image_registry.address = ""myprivateacr.azurecr.io""
run_config.environment.docker.base_image_registry.username = ""MyPrivateACR""
run_config.environment.docker.base_image_registry.password = ""&lt;the password for the registry&gt;""
</code></pre>
","5784983",1
875,60181177,2,60180314,2020-02-12 04:30:02,2,"<p>Prerequisites:
The tutorial and accompanying utils.py file is also available on <a href=""https://github.com/Azure/MachineLearningNotebooks/tree/master/tutorials"" rel=""nofollow noreferrer"">GitHub</a> if you wish to use it on your own <a href=""https://learn.microsoft.com/en-us/azure/machine-learning/how-to-configure-environment#local"" rel=""nofollow noreferrer"">local environment</a>. Run pip install azureml-sdk[notebooks] azureml-opendatasets matplotlib to install dependencies for this tutorial.</p>

<p>If you are using older version then upgrade to the latest Azure ML SDK Version 1.0.85.</p>

<p>!pip install --upgrade azureml-sdk</p>

<pre><code># check core SDK version number
print(""Azure ML SDK Version: "", azureml.core.VERSION)
</code></pre>

<p>Also </p>

<p>!pip install --upgrade azureml-opendataset </p>
","11297406",0
876,60172932,2,60160773,2020-02-11 16:01:32,0,"<p>Tried again this AM and it worked. let's file this under ""transient error""</p>
","3842610",0
877,60439473,2,60434642,2020-02-27 18:20:41,1,"<p>AFAIK, Git isn't currently supported by Azure Machine Learning Notebooks. If you're looking for a more fully-featured development environment, I suggest setting one up locally. There's more work up front, but it will give you the ability to version control. Check out this development environment set-up guide. <a href=""https://learn.microsoft.com/en-us/azure/machine-learning/how-to-configure-environment"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/machine-learning/how-to-configure-environment</a></p>

<pre><code>| Environment                                                   | Pros                                                                                                                                                                                                                                    | Cons                                                                                                                                                                                 |
|---------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| Cloud-based Azure Machine Learning compute instance (preview) | Easiest way to get started. The entire SDK is already installed in your workspace VM, and notebook tutorials are pre-cloned and ready to run.                                                                                           | Lack of control over your development environment and dependencies. Additional cost incurred for Linux VM (VM can be stopped when not in use to avoid charges). See pricing details. |
| Local environment                                             | Full control of your development environment and dependencies. Run with any build tool, environment, or IDE of your choice.                                                                                                             | Takes longer to get started. Necessary SDK packages must be installed, and an environment must also be installed if you don't already have one.                                      |
| Azure Databricks                                              | Ideal for running large-scale intensive machine learning workflows on the scalable Apache Spark platform.                                                                                                                               | Overkill for experimental machine learning, or smaller-scale experiments and workflows. Additional cost incurred for Azure Databricks. See pricing details.                          |
| The Data Science Virtual Machine (DSVM)                       | Similar to the cloud-based compute instance (Python and the SDK are pre-installed), but with additional popular data science and machine learning tools pre-installed. Easy to scale and combine with other custom tools and workflows. | A slower getting started experience compared to the cloud-based compute instance.                                                                                                    |
</code></pre>
","3842610",1
878,60989443,2,60978808,2020-04-02 10:28:00,10,"<p>@Nethim, from my pov these are the main difference:<br></p>

<ol>
<li><p>Data Distribution:</p>

<ul>
<li>Azure ML Notebooks are good when you are training with a limited data on single machine. Though Azure ML provides training clusters, the data distribution among the nodes is to be handled in the code.</li>
<li>Azure Databricks with its RDDs are designed to handle data distributed on multiple nodes.This is advantageous when your data size is huge.When your data size is small and can fit in a scaled up single machine/ you are using a pandas dataframe, then use of Azure databricks is a overkill</li>
</ul></li>
<li><p>Data Cleaning:
Databricks can support a lot of file formats natively and querying and cleaning huge datasets are easy where as this has to be handled custom in AzureML notebooks. This can be done with a aml notebooks but cleaning and writing to stores has to be handled.</p></li>
<li>Training
Both has the capabilities if distributing the training, Databricks provides inbuilt ML algorithms that can act on chunk of data on that node and coordinate with other nodes. Though this can be done on both AzureMachineLearning and Databricks with tf,horovod etc.,</li>
</ol>

<p>In general(just my opinion), if the dataset is small, aml notebooks is good.If the data size is huge, then Azure databricks is easy for datacleanup and format conversions.Then the training can happen on AML or databricks.Though databricks has a learning curve whereas Azure ML can be easy with the python and pandas.</p>

<p>Thanks.</p>
","12451380",3
879,61002796,2,60903256,2020-04-02 23:26:08,0,"<p>So I was able to figure it out with the help of an aws engineer (i got lucky I suppose). I'm including the complete lambda function. Nothing changed on the client.</p>

<pre><code>from chalice import Chalice
from chalice import BadRequestError
import base64
import os
import boto3
import ast
import json
import sys


from chalice import Chalice
if sys.version_info[0] == 3:
    # Python 3 imports.
    from urllib.parse import urlparse, parse_qs
else:
    # Python 2 imports.
    from urlparse import urlparse, parse_qs

app = Chalice(app_name='app_name')
app.debug = True


@app.route('/', methods=['POST'])
def index():
    parsed = parse_qs(app.current_request.raw_body.decode())

    body = parsed['data'][0]
    print(type(body))

    try:
        body = base64.b64decode(body)
        body = bytearray(body)
    except e:
        return {'error': str(e)}


    endpoint = ""object-detection-endpoint_name""
    runtime = boto3.Session().client(service_name='sagemaker-runtime', region_name='us-east-2')

    response = runtime.invoke_endpoint(EndpointName=endpoint, ContentType='image/jpeg', Body=body)

    print(response)
    results = response['Body'].read().decode(""utf-8"")
    results = results['predictions']

    results = json.loads(results)
    results = results['predictions']

    return {'result': results}
</code></pre>
","11909464",0
880,61020847,2,61010532,2020-04-03 21:41:47,3,"<p>Update - See ark-kun's comment, the approach in my original answer is deprecated and should not be used. It is better to let Kubeflow Pipelines specify where you should store your pipeline's artifacts.</p>
<hr />
<p>For lightweight components (such as the one in your example), Kubeflow Pipelines builds the container image for your component and specifies the paths for inputs and outputs (based upon the types you use to decorate your component function). I would recommend using those paths directly, instead of writing to one location and then renaming the file. The <a href=""https://github.com/kubeflow/pipelines/blob/master/samples/tutorials/Data%20passing%20in%20python%20components/Data%20passing%20in%20python%20components%20-%20Files.py"" rel=""nofollow noreferrer"">Kubeflow Pipelines samples</a> follow this pattern.</p>
<p>For <a href=""https://www.kubeflow.org/docs/pipelines/sdk/component-development/"" rel=""nofollow noreferrer"">reusable components</a>, you define the pipeline inputs and outputs as part of the <a href=""https://www.kubeflow.org/docs/pipelines/reference/component-spec/"" rel=""nofollow noreferrer"">YAML specification for the component</a>. In that case you can specify your preferred location for the output files. That being said, reusable components take a bit more effort to create, since you need to build a Docker container image and component specification in YAML.</p>
","74510",2
881,61055036,2,60929678,2020-04-06 07:40:24,3,"<p>I had to pass in bitmap and change <code>ContentType</code> to <code>""image/jpeg""</code></p>

<pre><code>const AWS = require(""aws-sdk"");
const fs = require(""fs"");

const sageMakerRuntime = new AWS.SageMakerRuntime({
  region: ""us-east-1"",
  accessKeyId: ""XXXXXXXXXXXX"",
  secretAccessKey: ""XXXXXXXXXXXXXXXXXXXXXXXXXXX""
});

const bitmap = fs.readFileSync(""sample.jpeg"");

var params = {
  Body: bitmap,
  EndpointName: ""wireframe-to-code"",
  ContentType: ""image/jpeg""
};

sageMakerRuntime.invokeEndpoint(params, function(err, data) {
  if (err) {
    console.log(err, err.stack);
  } else {
    responseData = JSON.parse(Buffer.from(data.Body).toString());
    console.log(responseData);
  }
});
</code></pre>
","5008598",0
882,61063412,2,61008435,2020-04-06 15:35:18,4,"<p>KFServing is an abstraction on top of inferencing rather than a replacement. It seeks to simplify deployment and make inferencing clients agnostic to what inference server is doing the actual work behind the scenes (be it TF Serving, Triton (formerly TRT-IS), Seldon, etc). It does this by seeking agreement among inference server vendors on an inferencing dataplane specification which allows extra components (such as transformations and explainers) to be more pluggable. </p>
","1050291",1
883,61092974,2,61091659,2020-04-08 03:43:18,3,"<p>I would say it all depends on how heavy your model is / how much data you're running through it. You're right to identify that Lambda will likely be less work. It's quite easy to get a lambda up and running to do the things that you need, and <a href=""https://aws.amazon.com/lambda/pricing/"" rel=""nofollow noreferrer"">Lambda has a very generous free tier</a>. The problem is:</p>

<ol>
<li><p>Lambda functions are fundamentally limited in their processing capacity (they timeout after <em>max</em> 15 minutes).</p></li>
<li><p>Your model might be expensive to load.</p></li>
</ol>

<p>If you have a lot of data to run through your model, you will need multiple lambdas. Multiple lambdas means you have to load your model multiple times, and that's wasted work. If you're working with ""big data"" this will get expensive once you get through the free tier.</p>

<p>If you don't have much data, Lambda will work just fine. I would eyeball it as follows: assuming your data processing step is dominated by your model step, and if all your model interactions (loading the model + evaluating all your data) take less than 15min, you're definitely fine. If they take more, you'll need to do a back-of-the-envelope calculation to figure out whether you'd leave the Lambda free tier.</p>

<p>Regarding Lambda: You can literally copy-paste code in to setup a prototype. If your execution takes more than 15min for all your data, you'll need a method of splitting your data up between multiple Lambdas. Consider <a href=""https://aws.amazon.com/step-functions/"" rel=""nofollow noreferrer"">Step Functions</a> for this.</p>
","7859515",4
884,61095301,2,61087568,2020-04-08 07:22:19,1,"<p>Once you have trained a model in AutoML Tables, you will find it in Tables > Models. There, you will have the option to <a href=""https://cloud.google.com/automl-tables/docs/model-export"" rel=""nofollow noreferrer"">export it into several formats</a> for you to deploy on different ways. From what you say, you should save it as a TF Saved Model in order to later <a href=""https://cloud.google.com/ai-platform/prediction/docs/deploying-models#upload-model"" rel=""nofollow noreferrer"">deploy it in AI Platform.</a> 
If you would also like to use Kubeflow, you can find <a href=""https://www.kubeflow.org/docs/gke/"" rel=""nofollow noreferrer"">here</a> the corresponding Kubeflow documentation. <a href=""https://cloud.google.com/blog/products/ai-machine-learning/getting-started-kubeflow-pipelines"" rel=""nofollow noreferrer"">Here</a> there is also a Google article that talks about the several tools that it has and how to use them.</p>
","12232493",0
885,61117139,2,61052173,2020-04-09 08:39:09,4,"<p>Exceeding the payload size limit does result in a connection reset from the SageMaker Runtime service.</p>

<p>From the SageMaker <a href=""https://docs.aws.amazon.com/general/latest/gr/sagemaker.html"" rel=""nofollow noreferrer"">documentation</a>:</p>

<blockquote>
  <p>Maximum payload size for endpoint invocation |    5 MB</p>
</blockquote>

<p>There are likely more space-efficient data formats than JSON that you could use to transmit the payload, but the available options will depend on the type of data and what model image you are using (i.e. whether Amazon-provided or a custom implementation).</p>
","10286863",2
886,61120785,2,61089001,2020-04-09 12:06:32,2,"<p>The issue here is related to Persistent Volume Claim that is not provisioned by Your minikube cluster.</p>

<p>You will need to make a decision to switch to platform managed kubernetes service or to stick with minikube and manually satisfy the Persistent Volume Claim or 
with alternative solutions.</p>

<p>The simplest option would be to use <a href=""https://helm.sh/"" rel=""nofollow noreferrer"">helm</a> charts for mflow installation like <a href=""https://hub.helm.sh/charts/cetic/mlflow"" rel=""nofollow noreferrer"">this</a> or <a href=""https://hub.helm.sh/charts/larribas/mlflow"" rel=""nofollow noreferrer"">this</a>.</p>

<p>The first helm <a href=""https://hub.helm.sh/charts/cetic/mlflow"" rel=""nofollow noreferrer"">chart</a> has listed requirements:</p>

<blockquote>
  <h2>Prerequisites</h2>
  
  <ul>
  <li>Kubernetes cluster 1.10+</li>
  <li>Helm 2.8.0+</li>
  <li>PV provisioner support in the underlying infrastructure.</li>
  </ul>
</blockquote>

<p>Just like in the guide You followed this one requires PV provisioner support.</p>

<p>So by switching to EKS You most likely will have easier time deploying mflow with artifact storing with s3.</p>

<p>If You wish to stay on minikube, You will need to modify the helm chart values or the yaml files from the guide You linked to be compatible with You manual configuration of PV. It might also need permissions configuration for s3.</p>

<p>The second helm <a href=""https://hub.helm.sh/charts/larribas/mlflow"" rel=""nofollow noreferrer"">chart</a> has the following limitation/feature:</p>

<blockquote>
  <h2>Known limitations of this Chart</h2>
  
  <p>I've created this Chart to use it in a production-ready environment in my company. We are using MLFlow with a Postgres backend store.</p>
  
  <p>Therefore, the following capabilities have been left out of the Chart:</p>
  
  <ul>
  <li>Using persistent volumes as a backend store.</li>
  <li>Using other database engines like MySQL or SQLServer.</li>
  </ul>
</blockquote>

<p>You can try to install it on minikube. This setup would result in artifacts being stored on remote a database. It would still need tweaking in order to connect to s3.</p>

<p>Anyway minikube still is a lightweight distribution of kubernetes targeted mainly for learning, so You will eventually reach another limitation if You stick to it for too long.</p>

<p>Hope it helps.</p>
","12014434",1
887,61121652,2,59642900,2020-04-09 12:53:57,0,"<p>The problem came from  docker for windows, I was unable to make working docker compose on it but there are no problem to build it when I run it on virtual machine with ubuntu.</p>
","6150081",0
888,61150293,2,61090530,2020-04-10 23:44:37,1,"<p>I'd recommend trying the <code>aws.s3</code> package instead:</p>

<p><a href=""https://github.com/cloudyr/aws.s3"" rel=""nofollow noreferrer"">https://github.com/cloudyr/aws.s3</a></p>

<p>Pretty simple - set your env variables:</p>

<pre><code>Sys.setenv(""AWS_ACCESS_KEY_ID"" = ""mykey"",
           ""AWS_SECRET_ACCESS_KEY"" = ""mysecretkey"",
           ""AWS_DEFAULT_REGION"" = ""us-east-1"",
           ""AWS_SESSION_TOKEN"" = ""mytoken"")
</code></pre>

<p>and then once that is out of the way:</p>

<p><code>aws.s3::s3read_using(read.csv, object = ""s3://bucket/folder/data.csv"")</code></p>

<p>Update: I see you're also already familiar with boto and trying to use reticulate so leaving this easy wrapper for that here:
<a href=""https://github.com/cloudyr/roto.s3"" rel=""nofollow noreferrer"">https://github.com/cloudyr/roto.s3</a></p>

<p>Looks like it has a great api for example the variable layout you're aiming to use:</p>

<pre><code>download_file(
  bucket = ""is.rud.test"", 
  key = ""mtcars.csv"", 
  filename = ""/tmp/mtcars-again.csv"", 
  profile_name = ""personal""
)

read_csv(""/tmp/mtcars-again.csv"")
</code></pre>
","3073340",1
889,61167204,2,61094767,2020-04-12 05:21:09,0,"<p>Please follow the APIs available at the <a href=""https://learn.microsoft.com/en-us/rest/api/azureml/"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/rest/api/azureml/</a> 
 for Azure ML studio through REST API calls, but other than dataset-related API.</p>

<p><a href=""https://i.stack.imgur.com/rTTNz.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/rTTNz.png"" alt=""enter image description here""></a></p>
","11297406",0
890,61169003,2,61168984,2020-04-12 08:55:59,1,"<p>Please check the announcement from MS Team regarding this:</p>

<p><a href=""https://azure.microsoft.com/blog/our-commitment-to-customers-and-microsoft-cloud-services-continuity/"" rel=""nofollow noreferrer"">https://azure.microsoft.com/blog/our-commitment-to-customers-and-microsoft-cloud-services-continuity/</a></p>

<p>All the free trials will not work as of now</p>
","1749403",2
891,61178682,2,61177696,2020-04-12 21:52:45,0,"<p>The Azure ML Studio (classic) webservice API is meant to be used to deploy ""models"". You give an input and a single prediction is returned. It sounds to me as if you'd like to take the output and also compare it to an intermediary table that's created in the pipeline.</p>

<p>I suggest:</p>

<ol>
<li>Determine if the webservice can return a string that is the concatenated result of both outputs you desire, then parse the output into two objects downstream.</li>
<li>Find a way to make the webservice return exactly what you want instead of something that is one step away from what you want.</li>
<li>Look into Azure Machine Learning service, as ML Studio classic has not been under active development for some time now. It supports using R in the control plane and the data plane.</li>
</ol>
","3842610",0
892,61199336,2,61179076,2020-04-14 01:25:31,2,"<p>Your <a href=""https://cloud.google.com/free/docs/gcp-free-tier"" rel=""nofollow noreferrer"">free trial credit</a> applies to all Google Cloud resources, with the following exceptions:</p>

<p>You can't have more than 8 cores (or virtual CPUs) running at the same time.</p>

<p>You can't add GPUs to your VM instances.</p>

<p>You can't request a quota increase. </p>

<p>You can't create VM instances that are based on Windows Server images.</p>

<p>You must <a href=""https://cloud.google.com/free/docs/gcp-free-tier#how-to-upgrade"" rel=""nofollow noreferrer"">upgrade your account</a> to perform any of the actions in the preceding list.</p>
","12264519",3
893,61213559,2,61212875,2020-04-14 17:21:05,1,"<p>You can access the bounding boxes during the onsubmit event like this:</p>

<pre><code>&lt;script&gt;
    document.querySelector('crowd-form').onsubmit = function(e) {
      const boundingBoxes = document.querySelector('crowd-bounding-box').value.boundingBoxes || document.querySelector('crowd-bounding-box')._submittableValue.boundingBoxes;
    }
&lt;/script&gt;
</code></pre>

<p><a href=""https://jsfiddle.net/ap56djgq/"" rel=""nofollow noreferrer"">Here's</a> a working jsfiddle.</p>

<p>Your use case sounds interesting. If you don't mind sharing, please email me at samhenry@amazon.com and I may be able to help further.</p>

<p>Thank you,</p>

<p>Amazon Mechanical Turk </p>
","5261029",0
894,61214016,2,61181955,2020-04-14 17:50:13,0,"<p>There may be some issue transferring the output data to S3 if the output is generated at a high rate and size is too large. </p>

<p>You can 1) try to slow down writing the output a bit or 2) call S3 from your algorithm container to upload the output directly using boto client (<a href=""https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/s3.html"" rel=""nofollow noreferrer"">https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/s3.html</a>).</p>
","4805806",0
895,61231952,2,61121418,2020-04-15 14:59:15,1,"<p>Posting the solution for anyone that is facing the same problem that I faced.</p>

<p>The reason that the model was not written in the filesystem was that the estimator needs a config argument to know where to write the model.</p>

<p>The following modification to the trainer_fn function should solve the problem.</p>

<pre><code>run_config = tf.estimator.RunConfig(save_checkpoints_steps=999, keep_checkpoint_max=1)  

run_config = run_config.replace(model_dir=trainer_fn_args.serving_model_dir)

estimator=tf.estimator.LinearClassifier(feature_columns=real_valued_columns,config=run_config)
</code></pre>
","13268867",0
896,61247086,2,61245284,2020-04-16 09:36:01,7,"<blockquote>
  <p>will you make it part of CI pipeline</p>
</blockquote>

<p>DVC often serves as a part of MLOps infrastructure. There is a popular <a href=""https://martinfowler.com/articles/cd4ml.html"" rel=""noreferrer"">blog post about CI/CD for ML</a> where DVC is used under the hood. <a href=""https://blog.codecentric.de/en/2020/01/remote-training-gitlab-ci-dvc/"" rel=""noreferrer"">Another example</a> but with GitLab CI/CD.</p>

<blockquote>
  <p>scenario where you will integrate dvc commit command with CI
  pipelines?</p>
</blockquote>

<p>If you mean <code>git commit</code> of DVC files (not <code>dvc commit</code>) then yes, you need to commit dvc-files into Git during CI/CD process. Auto-commit is not the best practice.</p>

<p>How to avoid Git commit in CI/CD:</p>

<ol>
<li>After ML model training in CI/CD, save changed dvc-files in external storage (for example GitLab artifact/releases), then get the files to a developer machine and commit there. Users usually write scripts to automate it.</li>
<li>Wait for DVC 1.0 release when <a href=""https://github.com/iterative/dvc/issues/1234"" rel=""noreferrer"">run-cache (like build-cache)</a> will be implemented. Run-cache makes dvc-files ephemeral and no additional Git commits will be required. Technically, run-cache is an associative storage <code>repo state --&gt; run results</code> outside of Git repo (in data remote).</li>
</ol>

<p>Disclaimer: I'm one of the creators of DVC.</p>
","3072174",5
897,61257546,2,61240809,2020-04-16 18:35:22,2,"<p>The following notebook illustrates how to leverage y_past, x_past, y_future, x_future, and fitted_model.forecast in the bottom half, 'Forecasting away from training data'. <a href=""https://github.com/Azure/MachineLearningNotebooks/blob/master/how-to-use-azureml/automated-machine-learning/forecasting-high-frequency/auto-ml-forecasting-function.ipynb"" rel=""nofollow noreferrer"">https://github.com/Azure/MachineLearningNotebooks/blob/master/how-to-use-azureml/automated-machine-learning/forecasting-high-frequency/auto-ml-forecasting-function.ipynb</a></p>

<p>The notebook will be a much better guide to grasping these concepts than perhaps a docstring doc. Should you have any more questions or need clarity, let us know!</p>
","12875447",0
898,61265123,2,61059996,2020-04-17 06:06:31,1,"<p>The bucket name `sagemaker-eu-west-1-\preprocessing-test-\input\code looks like a hardcoded string. In SageMaker Python SDK, the code upload function is <a href=""https://github.com/aws/sagemaker-python-sdk/blob/3bf569ece9e46a097d1ab69286ee89f762931e6c/src/sagemaker/processing.py#L463"" rel=""nofollow noreferrer"">here</a>:</p>
<p>Are you using a Windows environment? As Lauren noted in the comments, there have been some bug fixes there, so make sure to use the last version</p>
","11278913",1
899,61281810,2,61279914,2020-04-17 22:35:56,1,"<p>The <code>azureml-sdk</code> isn't available on conda, you need to install it with <code>pip</code>.</p>

<pre><code>myenv = Environment(name=""myenv"")
conda_dep = CondaDependencies().add_pip_package(""azureml-dataprep[pandas,fuse]"")
myenv.python.conda_dependencies=conda_dep
run_config.environment = myenv
</code></pre>

<p>For more information, about this error, the logs tab has a log named <code>20_image_build_log.txt</code> which Docker build logs. It contains the error where <code>conda</code> failed to failed to find <code>azureml-dataprep</code></p>

<p>EDIT:</p>

<p>Soon, you won't have to specify this dependency anymore. the Azure Data4ML team says <code>azureml-dataprep[pandas,fuse]</code> is getting added as a dependency for <code>azureml-defaults</code> which is automatically installed on all images. </p>
","3842610",4
900,61338052,2,61184895,2020-04-21 07:24:54,3,"<p>To reuse your model for TFX, a <strong>frozen graph</strong> needs to have a serving signature specified. Tried converting your model into <strong>savedmodel</strong> format using the code below which successfully created a <code>savedmodel.pb</code> file with a tag-set ""serve"".</p>

<pre><code>import tensorflow as tf
from tensorflow.python.saved_model import signature_constants
from tensorflow.python.saved_model import tag_constants

export_dir = './saved'
graph_pb = 'frozen_inference_graph.pb'

builder = tf.saved_model.builder.SavedModelBuilder(export_dir)

with tf.gfile.GFile(graph_pb, ""rb"") as f:
    graph_def = tf.GraphDef()
    graph_def.ParseFromString(f.read())

sigs = {}

with tf.Session(graph=tf.Graph()) as sess:
    # name="""" is important to ensure we don't get spurious prefixing
    tf.import_graph_def(graph_def, name="""")
    g = tf.get_default_graph()
    sess.graph.get_operations()
    inp = g.get_tensor_by_name(""image_tensor:0"")
    outputs = {}
    outputs[""detection_boxes""] = g.get_tensor_by_name('detection_boxes:0')
    outputs[""detection_scores""] = g.get_tensor_by_name('detection_scores:0')
    outputs[""detection_classes""] = g.get_tensor_by_name('detection_classes:0')
    outputs[""num_detections""] = g.get_tensor_by_name('num_detections:0')

    output_tensor = tf.concat([tf.expand_dims(t, 0) for t in outputs], 0)


    sigs[signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY] = \
        tf.saved_model.signature_def_utils.predict_signature_def(
            {""in"": inp}, {""out"": out})

    sigs[""predict_images""] = \
    tf.saved_model.signature_def_utils.predict_signature_def(
        {""in"": inp}, {""out"": output_tensor} )

    builder.add_meta_graph_and_variables(sess,
                                         [tag_constants.SERVING],
                                         signature_def_map=sigs)

builder.save().
</code></pre>

<p>We have tested the converted model by predicting the sample image that you provided. The result doesn't show any prediction which probably means the conversion method doesn't work as expected.</p>

<p><strong>As for your question:</strong></p>

<blockquote>
  <p><em>""Is there any mechanism in which I can reuse the model I currently have or will I have to retrain using Keras and deploy as shown in the reference?""</em></p>
</blockquote>

<p>With this result, it is way better to just retrain your model using <strong>Keras</strong> as the answer to your question because converting or reusing your frozen graph model isn't going to be the solution. Your model does not save variables that are required for serving the model and the model format is not suitable for a production environment.
And yes, it is the best way to follow the <a href=""https://www.tensorflow.org/tfx/tutorials/serving/rest_simple"" rel=""nofollow noreferrer"">official documentation</a> as you will be assured that this would work.</p>
","13071836",1
901,61352049,2,61351024,2020-04-21 19:52:17,2,"<p>Your <strong>mlflow-tracking-server</strong> service should have <em>ClusterIP</em> type, not <em>LoadBalancer</em>. </p>

<p>Both pods are inside the same Kubernetes cluster, therefore, there is no reason to use <em>LoadBalancer</em> Service type.</p>

<blockquote>
  <p>For some parts of your application (for example, frontends) you may want to expose a Service onto an external IP address, that’s outside of your cluster.
  Kubernetes ServiceTypes allow you to specify what kind of Service you want. The default is ClusterIP.</p>
  
  <p>Type values and their behaviors are:</p>
  
  <ul>
  <li><p><strong>ClusterIP</strong>: Exposes the Service on a cluster-internal IP. Choosing this
  value makes the Service only reachable from within the cluster. This
  is the default ServiceType. </p></li>
  <li><p><strong>NodePort</strong>: Exposes the Service on each Node’s IP at a static port (the NodePort). A > ClusterIP Service, to which the NodePort Service routes, is automatically created. You’ll > be able to contact the NodePort Service, from outside the cluster, by
  requesting :. </p></li>
  <li><strong>LoadBalancer</strong>: Exposes the Service
  externally using a cloud provider’s load balancer. NodePort and
  ClusterIP Services, to which the external load balancer routes, are
  automatically created. </li>
  <li><strong>ExternalName</strong>: Maps the Service to the contents
  of the externalName field (e.g. foo.bar.example.com), by returning a
  CNAME record with its value. No proxying of any kind is set up.</li>
  </ul>
  
  <p><a href=""https://kubernetes.io/docs/concepts/services-networking/service/#publishing-services-service-types"" rel=""nofollow noreferrer"">kubernetes.io</a></p>
</blockquote>
","886498",6
902,61364958,2,60577308,2020-04-22 12:19:12,5,"<p>From <a href=""https://github.com/tensorflow/tfx/issues/1622#issuecomment-616755799"" rel=""noreferrer"">git issue response</a>
Thanks <a href=""https://github.com/embr"" rel=""noreferrer"">Evan Rosen</a></p>

<p>Hi Folks,</p>

<p>The warnings you are seeing indicate that StatisticsGen is trying to treat your raw image features like a categorical string feature. The image bytes are being decoded just fine. The issue is that when the stats (including top K examples) are being written, the output proto is expecting a UTF-8 valid string, but instead gets the raw image bytes. Nothing is wrong with your setups from what I can tell, but this is just an unintended side-effect of a well-intentioned warning in the event that you have a categorical string feature which can't be serialized. We'll look into finding a better default that handles image data more elegantly.</p>

<p>In the meantime, to tell StatisticsGen that this feature is really an opaque blob, you can pass in a user-modified schema as described in the StatsGen docs. To generate this schema, you can run StatisticsGen and SchemaGen once (on a sample of data) and then modify the inferred schema to annotate that image features. Here is a modified version of the colab from @tall-josh:</p>

<p><a href=""https://colab.research.google.com/gist/embr/3827b641d8afc915ce1545a9956cc107/copy-of-tfx-dingocar.ipynb"" rel=""noreferrer"">Open In Colab</a></p>

<p>The additional steps are a bit verbose, but having a curated schema is often a good practice for other reasons. Here is the cell that I added to the notebook:</p>

<pre class=""lang-py prettyprint-override""><code>from google.protobuf import text_format
from tensorflow.python.lib.io import file_io
from tensorflow_metadata.proto.v0 import schema_pb2

# Load autogenerated schema (using stats from small batch)

schema = tfx.utils.io_utils.SchemaReader().read(
    tfx.utils.io_utils.get_only_uri_in_dir(
        tfx.types.artifact_utils.get_single_uri(schema_gen.outputs['schema'].get())))

# Modify schema to indicate which string features are images.
# Ideally you would persist a golden version of this schema somewhere rather
# than regenerating it on every run.
for feature in schema.feature:
  if feature.name == 'image/raw':
    feature.image_domain.SetInParent()

# Write modified schema to local file
user_schema_dir ='/tmp/user-schema/'
tfx.utils.io_utils.write_pbtxt_file(
    os.path.join(user_schema_dir, 'schema.pbtxt'), schema)

# Create ImportNode to make modified schema available to other components
user_schema_importer = tfx.components.ImporterNode(
    instance_name='import_user_schema',
    source_uri=user_schema_dir,
    artifact_type=tfx.types.standard_artifacts.Schema)

# Run the user schema ImportNode
context.run(user_schema_importer)
</code></pre>

<p>Hopefully you find this workaround is useful. In the meantime, we'll take a look at a better default experience for image-valued features.</p>
","7499546",0
903,61376902,2,61225077,2020-04-22 23:37:30,0,"<p>You cannot currently delete HumanTaskUis. That may be a capability added in the future.</p>
","2601671",0
904,61403202,2,61392212,2020-04-24 07:23:49,2,"<p>Currently the AML compute instance only allows the creator to access the CI.It's known bug, once it's fixed we will update you. We think it is related to MSA accounts.</p>
","11297406",2
905,61405122,2,61377643,2020-04-24 09:24:49,1,"<p>You cannot use pyfunc to store Any type object.</p>

<p>You should either specify one of loader_module as shown in the example below or you must write the wrapper that implements PythonModel interface and provides logic to deserialize your model from  previously-stored artifacts as described here 
 <a href=""https://www.mlflow.org/docs/latest/models.html#example-saving-an-xgboost-model-in-mlflow-format"" rel=""nofollow noreferrer"">https://www.mlflow.org/docs/latest/models.html#example-saving-an-xgboost-model-in-mlflow-format</a></p>

<p>example with loader:</p>

<pre><code>    model_uri = 'model.pkl'

    with open(model_uri, 'wb') as f:
        pickle.dump(model, f)

    mlflow.log_artifact(model_uri, 'model')

    mlflow.pyfunc.log_model(
        'model', loader_module='mlflow.sklearn', data_path='model.pkl', code_path=['src'], conda_env='environment.yml'
    )
</code></pre>

<p>I think PythonModel is the better way for you because of mlflow doesn't have a built-in loader for SageMaker DeepAR model.</p>

<p>Nonetheless, You must have the knowledge how to restore SageMaker model from artifacts, because I am not sure that is possible at all, cuz of some built-in SageMaker algorithms are blackboxes.</p>

<p>You can also may be interested in container that allow you to run any MLFlow projects inside Sagemaker: <a href=""https://github.com/odahu/sagemaker-mlflow-container"" rel=""nofollow noreferrer"">https://github.com/odahu/sagemaker-mlflow-container</a></p>
","9870162",1
906,61416003,2,61415793,2020-04-24 19:27:12,0,"<p>Check out the <a href=""https://learn.microsoft.com/en-us/azure/machine-learning/how-to-track-experiments#option-2-use-scriptrunconfig"" rel=""nofollow noreferrer"">ScriptRunConfig Section of the Monitor Azure ML experiment runs and metrics</a>. <code>ScriptRunConfig</code> works effectively the same as a <code>PythonScriptStep</code>.</p>

<p>The idiom is generally to have the following in your the script of your <code>PythonScriptStep</code>:</p>

<pre><code>from azureml.core.run import Run
run = Run.get_context()
run.log('foo_score', ""bar"")
</code></pre>

<p>Side note: You don't need to change your environment dependencies to use this because <code>PythonScriptStep</code>s have <code>azureml-defaults</code> installed automatically as a dependency.</p>
","3842610",2
907,61429860,2,60787646,2020-04-25 17:37:11,35,"<p>Building an MLOps platform is an action companies take in order to accelerate and manage the workflow of their data scientists in production. This workflow is reflected in ML pipelines, and includes the 3 main tasks of <code>feature engineering</code>, <code>training</code> and <code>serving</code>.</p>

<p>Feature engineering and model training are tasks which require a pipeline orchestrator, as they have dependencies of subsequent tasks and that makes the whole pipeline prone to errors.</p>

<p>Software building pipelines are different from data pipelines, which are in turn different from ML pipelines.</p>

<p>A <strong>software CI/CD flow</strong> compiles the code to deploy-able artifacts and accelerates the software delivery process. So, <em>code in, artifact out</em>. It's being achieved by the invocation of compilation tasks, execution of tests and deployment of the artifact. Dominant orchestrators for such pipelines are Jenkins, Gitlab-CI, etc.</p>

<p>A <strong>data processing flow</strong> gets raw data and performs transformation to create features, aggregations, counts, etc. So <em>data in, data out</em>. This is achieved by the invokation of remote distributed tasks, which perform data transformations by storing intermediate artifacts in data repositories. Tools for such pipelines are Airflow, Luigi and some hadoop ecosystem solutions.</p>

<p>In the <strong>machine learning flow</strong>, the ML engineer writes code to train models, uses the data to evaluate them and then observes how they perform in production in order to improve them. So <em>code <strong>and</strong> data in, model out</em>. Hence the implementation of such a workflow requires a combination of the orchestration technologies we've discussed above.</p>

<p><strong>TFX</strong> present this pipeline and proposes the use of components that perform these subsequent tasks. It defines a modern, complete ML pipeline, from building the features, to running the training, evaluating the results, deploying and serving the model in production</p>

<p><strong>Kubernetes</strong> is the most advanced system for orchestrating containers, the defacto tool to run workloads in production, the cloud-agnostic solution to save you from a cloud vendor lock-in and hence optimize your costs.</p>

<p><strong>Kubeflow</strong> is positioned as the way to do ML in Kubernetes, by implementing TFX. Eventually it handling the <em>code and data in, model out</em>. It provides a coding environment by implementing jupyter notebooks in the form of kubernetes resources, called <code>notebooks</code>. All cloud providers are onboard with the project and implement their data loading mechanisms across KF's components. The orchestration is implemented via <strong>KF pipelines</strong> and the serving of the model via <strong>KF serving</strong>. The metadata across its components are specified in the specs of the kubernetes resources throughout the platform.</p>

<p>In Kubeflow, the TFX components exist in the form of reusable tasks, implemented as containers. The management of the lifecycle of these components is achieved through Argo, the orchestrator of <strong>KF pipelines</strong>. Argo implements these workflows as kubernetes CRDs. In a <code>workflow</code> spec we define the dag tasks, the TFX components as containers, the metadata which will be written in the metadata store, etc. The execution of these workflows is happening nicely using standard kubernetes resources like pods, as well as <em>custom resource definitions</em> like <code>experiments</code>. That makes the implementation of the pipeline and the components language-agnostic, unline Airflow which implements the tasks in python only. These tasks and their lifecycle is then managed natively by kubernetes, without the need to use duct-tape solutions like Airflow's kubernetes-operator. Since everything is implemented as kubernetes resources, everything is a yaml and so the most Git friendly configuration you can find. Good luck trying to enforce version control in Airflow's dag directory.</p>

<p>The deployment and management of the model in production is done via <strong>KF serving</strong> using the CRD of <code>inferenceservice</code>. It utilizes <em>Istio</em>'s secure access to the models via its <code>virtualservices</code>, serverless resources using <em>Knative Serving</em>'s scale-from-zero <code>pods</code>, <code>revisions</code> for versioning, prometheus <code>metrics</code> for observability, <code>logs</code> in ELK for debugging and more. Running models in production could not be more SRE friendly than that.</p>

<p>On the topic of splitting training/serving between cloud and on-premise, the use of kubernetes is even more important, as it abstracts the custom infrastructure implementation of each provider, and so provides a unified environment to the developer/ml engineer.</p>
","3181539",1
908,61475577,2,61464960,2020-04-28 08:13:01,2,"<p>SageMaker Random Cut Forest is part of the <a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/algos.html"" rel=""nofollow noreferrer"">built-in algorithm library</a> and cannot be deployed in multi-model endpoint (MME). Built-in algorithms currently cannot be deployed to MME. XGboost is an exception, since it has an open-source container <a href=""https://github.com/aws/sagemaker-xgboost-container"" rel=""nofollow noreferrer"">https://github.com/aws/sagemaker-xgboost-container</a>.</p>

<p>If you really need to deploy a RCF to a multi-model endpoint, one option is to find a reasonably similar open-source implementation (for example <a href=""https://github.com/kLabUM/rrcf"" rel=""nofollow noreferrer""><code>rrcf</code></a> looks reasonably serious: based <a href=""http://proceedings.mlr.press/v48/guha16.pdf"" rel=""nofollow noreferrer"">on the same paper from Guha et al</a> and with 170+ github stars) and create a custom MME docker container. The <a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/build-multi-model-build-container.html"" rel=""nofollow noreferrer"">documentation is here</a> and there is <a href=""https://github.com/giuseppeporcelli/sagemaker-custom-serving-containers/blob/master/multi-model-server-container/notebook/multi-model-server-container.ipynb"" rel=""nofollow noreferrer"">an excellent tuto here</a></p>
","5331834",1
909,61476223,2,61389632,2020-04-28 08:50:49,2,"<p>A fix for this issue is being deployed to the official solution. In the meantime, you can make the changes described <a href=""https://github.com/awslabs/predictive-maintenance-using-machine-learning/pull/7/files"" rel=""nofollow noreferrer"">here</a> in your SageMaker environment by following the instructions below:</p>

<p>1) In the notebook, please change the <code>framework_version</code> to <code>1.6.0</code>.</p>

<pre><code>MXNet(entry_point='sagemaker_predictive_maintenance_entry_point.py',
          source_dir='sagemaker_predictive_maintenance_entry_point',
          py_version='py3',
          role=role, 
          train_instance_count=1, 
          train_instance_type=train_instance_type,
          output_path=output_location,
          hyperparameters={'num-datasets' : len(train_df),
                           'num-gpus': 1,
                           'epochs': 500,
                           'optimizer': 'adam',
                           'batch-size':1,
                           'log-interval': 100},
         input_mode='File',
         train_max_run=7200,
         framework_version='1.6.0')  &lt;- Change this to 1.6.0.
</code></pre>

<p>2) This will likely fix things, but just to be sure you don't have any stale packages, change the <code>requirements.txt</code> file as well.</p>

<p>You'll need to open up a terminal in SageMaker.
<a href=""https://i.stack.imgur.com/0Vn6l.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/0Vn6l.png"" alt=""enter image description here""></a>
image taken from <a href=""https://medium.com/swlh/jupyter-notebook-on-amazon-sagemaker-getting-started-55489f500439"" rel=""nofollow noreferrer"">https://medium.com/swlh/jupyter-notebook-on-amazon-sagemaker-getting-started-55489f500439</a></p>

<p>and run</p>

<pre><code>cd SageMaker/sagemaker_predictive_maintenance_entry_point/
sudo vim requirements.txt  # (or sudo nano requirements.txt)
</code></pre>

<p>Change the contents to:</p>

<pre><code>gluonnlp==0.9.1
pandas==0.22
</code></pre>

<p>Save it, and then run the example again.</p>

<p>Feel free to comment on the issue as well:
<a href=""https://github.com/awslabs/predictive-maintenance-using-machine-learning/issues/6"" rel=""nofollow noreferrer"">https://github.com/awslabs/predictive-maintenance-using-machine-learning/issues/6</a> </p>
","8274165",0
910,61550523,2,61550297,2020-05-01 20:43:38,1,"<p>Anytime you use the <code>await</code> keyword, you must use the <code>async</code> keyword before the function definition (which you have done). The thing is, any time an async function is called, it will always return a <code>Promise</code> object since it expects that some asynchronous task will take place within the function.</p>

<p>Therefore,you'll need to <code>await</code> the result of the imageslist function and make the surrounding function <code>async</code>.</p>

<pre class=""lang-js prettyprint-override""><code>&lt;script&gt;
    customElements.whenDefined( 'crowd-bounding-box' ).then( async () =&gt; {  
        var imgBox = document.getElementById('annotator');
        newImg = await imageslist();
        console.log(""Result of newImg is: "" + newImg);
        imgBox.src = ""https://my-images-bucket.s3.amazonaws.com/"" + newImg;
    } )
&lt;/script&gt;
</code></pre>
","7716202",1
911,61553326,2,61481147,2020-05-02 01:27:08,0,"<p>Yes, it's possible to have multiple instances of MLflow Tracker Service running behind a load balancer.</p>

<p>Because the Tracking server is stateless, you could have multiple instances log to a replicated primary DB as a store. A second hot standby can take over if the primary fails.</p>

<p>As for the documentation in how to set up replicated instances of your backend store will vary on which one you elect to use, we cannot definitely document all different scenarios and their configurations.</p>

<p>I would check the respective documentation of your backend DB and load balancer for how to federate requests to multiple instances of an MLflow tracking server, how to failover to a hot standby or replicated DB, or how to configure a hot-standby replicated DB instance.</p>

<p>The short of it: MLflow tracking server is stateless.</p>
","13272057",0
912,61586773,2,61380051,2020-05-04 07:02:18,4,"<p>ml EC2 instances do not appear in the EC2 console. You can find their metrics in Cloudwatch though, and create dashboards to monitor what you need:</p>

<p><a href=""https://i.stack.imgur.com/BgUkm.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/BgUkm.png"" alt=""enter image description here""></a>
<a href=""https://i.stack.imgur.com/wD4w0.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/wD4w0.png"" alt=""enter image description here""></a></p>
","840960",2
913,61610931,2,61258979,2020-05-05 10:25:59,1,"<p><code>CrashLoopBackError</code> can be related to these possible reasons:</p>

<ul>
<li><p>the application inside your pod is not starting due to an error;</p></li>
<li><p>the image your pod is based on is not present in the registry, or the
node where your pod has been scheduled cannot pull from the registry;</p></li>
<li><p>some parameters of the pod has not been configured correctly.</p></li>
</ul>

<p>In your case it seems an application error, inside the container.
Try to view the logs with:</p>

<pre><code>kubectl logs &lt;your_pod&gt; -n &lt;namespace&gt;
</code></pre>

<p>For more info on how to troubleshoot this kind of error refer to:</p>

<p><a href=""https://pillsfromtheweb.blogspot.com/2020/05/troubleshooting-kubernetes.html"" rel=""nofollow noreferrer"">https://pillsfromtheweb.blogspot.com/2020/05/troubleshooting-kubernetes.html</a></p>
","2554484",0
914,61620970,2,61615566,2020-05-05 19:01:35,2,"<p>Virtual Node Pool requires the usage of the Advance Networking configuration of AKS which brings in AZURE CNI network plugin. </p>

<p>The Default POD count per node on AKS when using AZURE CNI is 30 pods.</p>

<p><a href=""https://learn.microsoft.com/en-us/azure/aks/configure-azure-cni#maximum-pods-per-node"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/aks/configure-azure-cni#maximum-pods-per-node</a></p>

<p>This is the main reason why you are now getting 30 MAX pods per node. </p>

<p>This can be updated to a bigger number when using the AZ CLI to provision your cluster. </p>

<p><a href=""https://learn.microsoft.com/en-us/cli/azure/ext/aks-preview/aks?view=azure-cli-latest#ext-aks-preview-az-aks-create"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/cli/azure/ext/aks-preview/aks?view=azure-cli-latest#ext-aks-preview-az-aks-create</a></p>

<pre><code>--max-pods -m
The maximum number of pods deployable to a node.
</code></pre>
","2690525",4
915,61655193,2,61631687,2020-05-07 10:10:23,2,"<p>Adding volume size to the training job selecting ""additional storage volume per instance (gb)"" to 5GB on the creation seems to solve the problem. I still don't understand why, but problem seems solved.</p>
","4267439",5
916,61686291,2,61683506,2020-05-08 19:04:47,3,"<p>Try retrieving your service from the workspace directly </p>

<pre class=""lang-py prettyprint-override""><code>ws.webservices[service_name].get_logs()
</code></pre>

<p>Also, I found deploying an image as an endpoint to be easier than inference+deploy model (depending on your use case) </p>

<pre class=""lang-py prettyprint-override""><code>my_image = Image(ws, name='test', version='26')  
service = AksWebservice.deploy_from_image(ws, ""test1"", my_image, deployment_config, aks_target)
</code></pre>
","9182232",13
917,61740160,2,61740109,2020-05-11 22:01:03,1,"<p>This is not an available feature on Azure Cognitive Search. You'll need to work with some computer vision AI model.</p>
","1384539",3
918,61805863,2,61803031,2020-05-14 19:28:31,3,"<p>There are few ways to accomplish this:</p>

<ol>
<li><p>Put the additional file in the same folder as your model file, and <a href=""https://learn.microsoft.com/en-us/python/api/azureml-core/azureml.core.model.model?view=azure-ml-py#register-workspace--model-path--model-name--tags-none--properties-none--description-none--datasets-none--model-framework-none--model-framework-version-none--child-paths-none--sample-input-dataset-none--sample-output-dataset-none--resource-configuration-none-"" rel=""nofollow noreferrer"">register</a> the whole folder as the model. In this approach the file is stored alongside the model.</p></li>
<li><p>Put the file in a local folder, and specify that folder as source_directory in <a href=""https://learn.microsoft.com/en-us/python/api/azureml-core/azureml.core.model.inferenceconfig?view=azure-ml-py"" rel=""nofollow noreferrer"">InferenceConfig</a>. In this approach the file is re-uploaded every time you deploy a new endpoint.</p></li>
<li><p>Use custom base image in InferenceConfig to bake the file into Docker image itself.</p></li>
</ol>
","5784983",0
919,61812903,2,60720060,2020-05-15 06:15:03,0,"<p>Fixed after release new version of AzureML SDK in designer and update script to:</p>

<pre><code>os.system(f""pip install azureml-sdk[automl]==1.4.0 --upgrade"")
</code></pre>
","5109929",0
920,61848735,2,61751131,2020-05-17 08:03:45,1,"<p>By using the directions and comment by Jason, I've changed up the model part as tfx doesn't support the sequential model but the Keras functional API. </p>

<pre><code>def get_model(show_summary=True):

#one-hot categorical features
num_A = 4,
num_B = 3,
num_C = 2,
num_D = 8,
num_E = 12,
num_F = 4,
num_G = 16,
num_H = 26

input_A = tf.keras.Input(shape=(num_A,), name=""A_xf"")
input_B = tf.keras.Input(shape=(num_B,), name=""B_xf"")
input_C = tf.keras.Input(shape=(num_C,), name=""C_xf"")
input_D = tf.keras.Input(shape=(num_D,), name=""D_xf"")
input_E = tf.keras.Input(shape=(num_E,), name=""E_xf"")
input_F = tf.keras.Input(shape=(num_F,), name=""F_xf"")
input_G = tf.keras.Input(shape=(num_G,), name=""G_xf"")
input_H = tf.keras.Input(shape=(num_H,), name=""H_xf"")

inputs_con = tf.keras.layers.concatenate([
input_A,
input_B,
input_C,
input_D,
input_E,
input_F,
input_G,
input_H])

dense_1 = tf.keras.layers.Dense(50, activation = 'relu')(inputs_con)
dense_2 = tf keras.layers.Dense(25, activation = ""rely"") (dense_1)
output = tf.keras.laters.Dense(1, activation = ""sigmoid"") (dense_2)
model = keras.Model(inputs=inputs, outputs=outputs)

_inputs = [
input_A,
input_B,
input_C,
input_D,
input_E,
input_F,
input_G,
input_H]

model = tf.keras.models.Model(_inputs, output)

model.compile(optimizer='rmsprop',
          loss='binary_crossentropy',
          metrics=['accuracy'])

if show_summary:
    model.summary()

return model
</code></pre>
","13479017",0
921,61861467,2,61832086,2020-05-18 02:23:32,0,"<p>From <a href=""https://stackoverflow.com/questions/54736505/ideal-way-to-read-data-in-bucket-stored-batches-of-data-for-keras-ml-training-in"">Ideal way to read data in bucket stored batches of data for Keras ML training in Google Cloud Platform?</a> ""ImageDataGenerator.flow_from_directory() currently does not allow you to stream data directly from a GCS bucket. ""</p>

<p>I had to download the image from S3 first.  This is best for latency reasons as well. </p>
","12991264",1
922,61913117,2,61890322,2020-05-20 12:26:29,0,"<p>To make the trainers unique that the Kubeflow to pick up, I have to put in the instance_name to define the trainers. </p>

<pre><code>trainer2 = Trainer(
      run_fn=run_fn2,
      examples=transform.outputs['transformed_examples'],
      schema=schema_gen.outputs['schema'],
      transform_graph= transform.outputs['transform_graph'],
      train_args= train_args2,
      eval_args= eval_args2,
      custom_executor_spec= executor_spec.ExecutorClassSpec(trainer_executor.GenericExecutor),
      instance_name='trainer2'
  )
</code></pre>
","13479017",0
923,61916191,2,61870000,2020-05-20 14:53:01,4,"<p>I'm an engineer at AWS. In order to understand the ""active learning""/""automated data labeling"" feature, it will be helpful to start with a broader recap of how SageMaker Ground Truth works.</p>

<p>First, let's consider the workflow without the active learning feature. Recall that Ground Truth annotates data in batches [<a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/sms-batching.html]"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/sagemaker/latest/dg/sms-batching.html]</a>. This means that your dataset is submitted for annotation in ""chunks."" The size of these batches is controlled by the API parameter MaxConcurrentTaskCount [<a href=""https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_HumanTaskConfig.html#sagemaker-Type-HumanTaskConfig-MaxConcurrentTaskCount]"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_HumanTaskConfig.html#sagemaker-Type-HumanTaskConfig-MaxConcurrentTaskCount]</a>. This parameter has a default value of 1,000. You cannot control this value when you use the AWS console, so the default value will be used unless you alter it by submitting your job via the API instead of the console.</p>

<p>Now, let's consider how active learning fits into this workflow. Active learning runs <em>in between</em> your batches of manual annotation. Another important detail is that Ground Truth will partition your dataset into a validation set and an unlabeled set. For datasets smaller than 5,000 objects, the validation set will be 20% of your total dataset; for datasets largert than 5,000 objects, the validation set will be 10% of your total dataset. Once the validation set is collected, any data that is subsequently annotated manually consistutes the training set. The collection of the validation set and training set proceeds according to the batch-wise process described in the previous paragraph. A longer discussion of active learning is available in [<a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/sms-automated-labeling.html]"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/sagemaker/latest/dg/sms-automated-labeling.html]</a>.</p>

<p>That last paragraph was a bit of a mouthful, so I'll provide an example using the numbers you gave.</p>

<h1>Example #1</h1>

<ul>
<li>Default MaxConcurrentTaskCount (""batch size"") of 1,000</li>
<li>Total dataset size: 1,758 objects</li>
<li>Computed validation set size: 0.2 * 1758 = 351 objects</li>
</ul>

<p>Batch #</p>

<ol>
<li>Annotate 351 objects to populate the validation set (1407 remaining).</li>
<li>Annotate 1,000 objects to populate the first iteration of the training set (407 remaining).</li>
<li>Run active learning. This step may, depending on the accuracy of the model at this stage, result in the annotation of zero, some, or all of the remaining 407 objects.</li>
<li>(Assume no objects were automatically labeled in step #3) Annotate 407 objects. End labeling job.</li>
</ol>

<h1>Example #2</h1>

<ul>
<li>Non-default MaxConcurrentTaskCount (""batch size"") of 250</li>
<li>Total dataset size: 1,758 objects</li>
<li>Computed validation set size: 0.2 * 1758 = 351 objects</li>
</ul>

<p>Batch #</p>

<ol>
<li>Annotate 250 objects to begin populating the validation set (1508 remaining).</li>
<li>Annotate 101 objects to finish populating the validation set (1407 remaining).</li>
<li>Annotate 250 objects to populate the first iteration of the training set (1157 remaining).</li>
<li>Run active learning. This step may, depending on the accuracy of the model at this stage, result in the annotation of zero, some, or all of the remaining 1157 objects. All else being equal, we would expect the model to be less accurate than the model in example #1 at this stage, because our training set is only 250 objects here.</li>
<li>Repeat alternating steps of annotating batches of 250 objects and running active learning.</li>
</ol>

<p>Hopefully these examples illustrate the workflow and help you understand the process a little better. Since your dataset consists of 1,758 objects, the upper bound on the number of automated labels that can be supplied is 407 objects (assuming you use the default MaxConcurrentTaskCount).</p>

<p>Ultimately, 1,758 objects is still a relatively small dataset. We typically recommend at least 5,000 objects to see meaningful results [<a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/sms-automated-labeling.html]"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/sagemaker/latest/dg/sms-automated-labeling.html]</a>. Without knowing any other details of your labeling job, it's difficult to gauge why your job didn't result in more automated annotations. A useful starting point might be to inspect the annotations you received, and to determine the quality of the model that was trained during the Ground Truth labeling job.</p>

<p>Best regards from AWS! </p>
","13582686",0
924,61961397,2,61626392,2020-05-22 18:15:23,1,"<p>I`m not sure if you solved it already, but to use BigQuery as input you must have the --project-id flag setup like so:</p>

<pre><code>example_gen = components.BigQueryExampleGen(query='SELECT * except(day) FROM `gofind-datalake.data.temp_dist` where rand() &lt; 2800/30713393 limit 3000')
context.run(example_gen, beam_pipeline_args=[""--project=gofind-datalake""])
</code></pre>
","7105492",0
925,61984416,2,61984217,2020-05-24 10:09:54,0,"<p>The problem is that the body contents is being expected to be base 64 encoded, try base64 encoding the body before passing it to the invoke statement.</p>
","13460933",1
926,61987234,2,61987233,2020-05-24 14:13:56,8,"<p>if you start from the dataframe example</p>

<pre><code>df = pd.DataFrame({'A' : [2, 5], 'B' : [1, 7]})
</code></pre>

<p>you take a row</p>

<pre><code>df_1_record = df[:1]
</code></pre>

<p>and convert <code>df_1_record</code> to a csv like this:</p>

<pre><code>import io
from io import StringIO
csv_file = io.StringIO()
# by default sagemaker expects comma seperated
df_1_record.to_csv(csv_file, sep="","", header=False, index=False)
my_payload_as_csv = csv_file.getvalue()
</code></pre>

<p><code>my_payload_as_csv</code> looks like</p>

<pre><code>'2,1\n'
</code></pre>

<p>then you can invoke the sagemaker endpoint</p>

<pre><code>import boto3
client = boto3.client('sagemaker-runtime')
response = client.invoke_endpoint(
    EndpointName=""my-sagemaker-endpoint-name"",
    Body= my_payload_as_csv,
    ContentType = 'text/csv')
</code></pre>
","1771155",1
927,61998211,2,61958473,2020-05-25 08:02:45,0,"<p>Below is the code you can reuse
<a href=""https://github.com/microsoft/MLOpsPython/blob/master/diabetes_regression/evaluate/evaluate_model.py"" rel=""nofollow noreferrer"">https://github.com/microsoft/MLOpsPython/blob/master/diabetes_regression/evaluate/evaluate_model.py</a></p>

<p>Assuming in each previous experiment run, a model was registered with a tag that contains a metric of interest (test_mae for example), below is the code to retrieve the version with lowest mae.</p>

<pre><code>from azureml.core.model import Model

model_name = ""YOUR_MODEL_NAME""
model_path = ""LOCAL_PATH”
model_version_list = [(model.version,float(model.tags[""test_mae""])) for model in Model.list(workspace = ws,name =model_name)]
model_version_list.sort(key = lambda a: a[0])
lowest_mae_version =model_version_list[0][0]
print(""best version is {} with mae at {}"".format(lowest_mae_version,model_version_list[0][1]))
model = Model(name = model_name,workspace = ws, version =lowest_mae_version)
model.download(model_path, exist_ok=True)
</code></pre>

<p>when the model has not been registered,models in an automl run and  would like to get all the models and compare the results depending on featurization, method used, and metrics, also with other data sets. The models are all inside the workspace with the GUI you can see them and download them by hand. </p>
","11297406",1
928,62009529,2,62007961,2020-05-25 19:39:18,4,"<p>SageMaker team created a <a href=""https://github.com/aws/sagemaker-training-toolkit"" rel=""nofollow noreferrer"">python package <code>sagemaker-training</code></a> to install in your docker so that your customer container will be able to handle external <code>entry_point</code> scripts.
See here for an example using Catboost that does what you want to do :)</p>

<p><a href=""https://github.com/aws-samples/sagemaker-byo-catboost-container-demo"" rel=""nofollow noreferrer"">https://github.com/aws-samples/sagemaker-byo-catboost-container-demo</a></p>
","5331834",2
929,62021353,2,61997914,2020-05-26 11:41:48,5,"<p>Have a look at this example:</p>
<p><a href=""https://github.com/kubernetes-client/python/blob/master/examples/deployment_crud.py#L62-L70"" rel=""nofollow noreferrer"">https://github.com/kubernetes-client/python/blob/master/examples/deployment_crud.py#L62-L70</a></p>
<pre class=""lang-py prettyprint-override""><code>def update_deployment(api_instance, deployment):
    # Update container image
    deployment.spec.template.spec.containers[0].image = &quot;nginx:1.16.0&quot;
    # Update the deployment
    api_response = api_instance.patch_namespaced_deployment(
        name=DEPLOYMENT_NAME,
        namespace=&quot;default&quot;,
        body=deployment)
    print(&quot;Deployment updated. status='%s'&quot; % str(api_response.status))
</code></pre>
<p>The Labels are on the deployment object, from the App v1 API,</p>
<pre><code>kind: Deployment
metadata:
  name: deployment-example
spec:
  replicas: 3
  revisionHistoryLimit: 10
  template:
    metadata:
      labels:
        app: nginx
</code></pre>
<p>which means you need to update the following:</p>
<p><code>deployment.spec.template.metadata.labels.app = &quot;nginx&quot;</code></p>
","2690525",7
930,62037474,2,61994821,2020-05-27 07:18:11,0,"<p>Happened to me too. Contact support and ask them to delete the kernel behind the scenes.</p>
","121956",0
931,62140638,2,62140446,2020-06-01 20:37:38,1,"<p>I think this error isn't necessarily about Azure ML. I think the error has to do w/ the difference b/w using a hyphen and a period in your package name. But I'm a python packaging newb. 
In a new conda environment on my laptop, I ran the following</p>

<pre><code>&gt; conda create -n arcus python=3.6 -y
&gt; conda activate arcus
&gt; pip install arcus-ml
&gt; python
&gt;&gt;&gt; from arcus.ml import dataframes as adf
ModuleNotFoundError: No module named 'arcus'
</code></pre>

<p>When I look in the env's site packages folder, I didn't see the <code>arcus/ml</code> folder structure I was expecting. There's no arcus code there at all, only the <code>.dist-info</code> file</p>

<h3><code>~/opt/anaconda3/envs/arcus/lib/python3.6/site-packages</code></h3>

<p><a href=""https://i.stack.imgur.com/caExn.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/caExn.png"" alt=""enter image description here""></a></p>
","3842610",1
932,62152348,2,62137347,2020-06-02 12:45:32,1,"<p>Building off the answer given by @mokugo-devops, I was able to link my existing notebook to my GitHub account.</p>

<p>First, I followed the directions posted in the link provided in his answer to set up my GitHub repo with my AWS account on the CLI, then I used the following command to edit my existing notebook:</p>

<pre><code>aws sagemaker  update-notebook-instance \
--notebook-instance-name &lt;value&gt; \
--default-code-repository &lt;saved-github-repo-name-in-AWS&gt;
</code></pre>

<p>my notebook instance is now linked my GitHub repo.</p>
","9994799",0
933,62162559,2,62141490,2020-06-02 22:44:21,1,"<p>It seems nothing can be done in terms of the Cloud Registry quota limits because those are <strong>fixed</strong>. According to <a href=""https://cloud.google.com/container-registry/quotas"" rel=""nofollow noreferrer"">Container Registry &gt; Doc &gt; Resources &gt; Quotas and limits</a>:</p>
<blockquote>
<p>Any request sent to Container Registry has a 2 hour timeout limit.</p>
<p>The fixed rate limits per client IP address are:</p>
<ul>
<li>50,000 HTTP requests every 10 minutes</li>
<li>1,000,000 HTTP requests per day</li>
</ul>
</blockquote>
<p>Google provides support for GKE, but Kubeflow itself is not supported by Google. This question should be addressed to the Kubeflow support.</p>
<p>A Kubeflow issue with breaching quota limits and a question about how to get container pulls to use more IP addresses can be registered on the project page on GitHub:
<a href=""https://github.com/kubeflow/kubeflow/issues"" rel=""nofollow noreferrer"">https://github.com/kubeflow/kubeflow/issues</a></p>
<p>Other support options are available here:
<a href=""https://www.kubeflow.org/docs/other-guides/support/"" rel=""nofollow noreferrer"">https://www.kubeflow.org/docs/other-guides/support/</a></p>
<p>If you use CLI, you may try to customize Kubeflow configuration file before deployment or split it into separate deployments to overcome Cloud Registry quota limits. This approach helps for some complicated deployments. An important thing here is to take care of dependencies. First run</p>
<pre><code>kfctl build -v -f ${CONFIG_URI}
</code></pre>
<p>make changes in the file <code>${KF_DIR}/kfctl_gcp_iap.yaml</code>, and then run</p>
<pre><code>kfctl apply -v -f ${CONFIG_URI}
</code></pre>
","11602913",0
934,62176618,2,62171724,2020-06-03 15:25:55,15,"<p>Sure, try this:</p>

<pre><code>from azureml.core.run import Run
run = Run.get_context()
ws = run.experiment.workspace
</code></pre>
","8807730",1
935,62200269,2,62170192,2020-06-04 17:04:16,2,"<p>Your workaround is the best option as of now. But I know that the Azure ML product group has been working on exactly this problem, but I can't make any promises as to timeline.</p>

<p>I share your dream of an easily configurable data science cloud development environment that allows for Git repo cloning and environment creation w/ a conda yml. We're so close especially given all the press &amp; announcements around Visual Studio Codespaces!</p>
","3842610",1
936,62218361,2,62094204,2020-06-05 15:00:13,2,"<p>According to your code the input to the function serve_request_url_fn is a Dense Tensor, but maybe the input of your transform graph is a Sparse Tensor.  </p>

<p>The function tf.io.parse_example knows how to deserialise your tf.Example into a Sparse Tensor, but if you want to send a Tensor, without serialising it, then you should convert it manually to a Sparse Tensor and stop using the tf.io.parse function.</p>

<p>For example:</p>

<pre class=""lang-py prettyprint-override""><code>@tf.function(input_signature=[tf.TensorSpec(shape=(None), dtype=tf.string, name='examples')])
def serve_request_url_fn(self, request_url):
    request_url_sp_tensor = tf.sparse.SparseTensor(
        indices=[[0, 0]],
        values=request_url,
        dense_shape=(1, 1)
    )
    parsed_features = {
        'request_url': request_url_sp_tensor,
    }
    transformed_features = self.model.tft_example_layer(parsed_features)
    transformed_features.pop(_transformed_name(_LABEL_KEY))
    return self.model(transformed_features)
</code></pre>
","7381322",2
937,62239946,2,62235365,2020-06-07 01:37:03,1,"<p>Definitely empathize with the issue you're having. Every data scientist has struggled with this at some point.</p>

<p>The hard truth I have for you is that Azure ML Studio (classic) isn't really capable of  solving this ""works on my machine"" problem. However, the good news is that Azure ML Service is incredible at it. Studio classic doesn't let you define custom environments deterministically, only add and remove packages (and not so well even at that) </p>

<p>Because ML Service's execution is built on top of <code>Docker</code> containers and <code>conda</code> environments, you can feel more confident in repeated results. I highly recommend you take the time to learn it (and I'm also happy to debug any issues that come up). Azure's <a href=""https://github.com/Azure/MachineLearningNotebooks"" rel=""nofollow noreferrer"">MachineLearningNotebooks repo</a> has a lot of great tutorials for getting started.</p>

<p>I spent two hours making <a href=""https://github.com/swanderz/MachineLearningNotebooks/blob/SO_CPR/how-to-use-azureml/training/train-on-amlcompute/train-on-amlcompute.ipynb"" rel=""nofollow noreferrer"">a proof of concept</a> that demonstrate how ML Service solves the problem you're having by synthesizing:</p>

<ul>
<li>your code sample (before you shared your notebook),</li>
<li><a href=""https://scikit-learn.org/stable/auto_examples/manifold/plot_compare_methods.html#sphx-glr-auto-examples-manifold-plot-compare-methods-py"" rel=""nofollow noreferrer"">Jake Vanderplas's sklearn example</a>, and</li>
<li><a href=""https://github.com/Azure/MachineLearningNotebooks/blob/master/how-to-use-azureml/training/train-on-amlcompute/train-on-amlcompute.ipynb"" rel=""nofollow noreferrer"">this Azure ML tutorial</a> on remote training.</li>
</ul>

<p>I'm no T-SNE expert, but from the screenshot below, you can see that the t-sne outputs are the same when I run the script locally and remotely. This might be possible with Studio classic, but it would be hard to guarantee that it will always work.</p>

<p><a href=""https://i.stack.imgur.com/mhlg6.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/mhlg6.png"" alt=""Azure ML Experiment Results Page""></a></p>
","3842610",2
938,62245005,2,62189103,2020-06-07 12:02:00,2,"<p>Any specific reason you want to do this?</p>

<p>Since there are some heavy dependencies (<a href=""https://learn.microsoft.com/en-us/azure/machine-learning/concept-compute-instance#contents"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/machine-learning/concept-compute-instance#contents</a>), my guess is you have to try it yourself.</p>

<p>Create a new one and run:</p>

<pre><code>$ sudo apt update 
$ sudo apt upgrade
$ sudo apt dist-upgrade
</code></pre>

<p>Let us know what happened.</p>

<p>BTW: Are Compute Instance also Docker images? If so, the upgrade might be working, if not, there might be many drivers that need to be upgraded too. The ones from the GPU would be the easiest...</p>
","13518401",4
939,62247186,2,62180798,2020-06-07 14:56:47,0,"<p>What you are seeing in the test interface after the training is only a part of the total images because these metrics are calculated using k-fold cross validation.</p>
<p>You are not doing something wrong. It would not be logic to test all the images because it would mean testing with your training images.</p>
<p>To have a better accuracy, there's no magic: add more images, relevant to your use-case</p>
<p><a href=""https://learn.microsoft.com/en-us/azure/cognitive-services/custom-vision-service/getting-started-build-a-classifier#evaluate-the-classifier?WT.mc_id=AI-MVP-5003365"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/cognitive-services/custom-vision-service/getting-started-build-a-classifier#evaluate-the-classifier</a></p>
","3136339",2
940,62297970,2,62296590,2020-06-10 07:24:21,3,"<p>Thanks for reporting this. I was not aware of this issue.</p>

<p>This is because Hydra is changing your current working directory for each run.</p>

<p>I did some digging, this is what you can do:</p>

<ol>
<li>Set the MLFLOW_TRACKING_URI environment variable:</li>
</ol>

<pre><code>MLFLOW_TRACKING_URI=file:///$(pwd)/.mlflow  python3 hydra_temp.py
</code></pre>

<ol start=""2"">
<li>Call set_tracking_url() before hydra.main() starts:</li>
</ol>

<pre><code>import hydra
from omegaconf import DictConfig
from mlflow import log_metric, log_param, set_tracking_uri
import os

set_tracking_uri(f""file:///{os.getcwd()}/.mlflow"")

@hydra.main(config_name=""config"")
def my_app(cfg: DictConfig):
    log_param(""a"", 2)
    log_metric(""b"", 3)


if __name__ == ""__main__"":
    my_app()
</code></pre>

<ol start=""3"">
<li>Wait for my <a href=""https://github.com/facebookresearch/hydra/issues/664"" rel=""nofollow noreferrer"">new issue</a> to get resolved, then there will have a proper plugin to integrate with mlflow.
(This will probably take a while).</li>
</ol>

<p>By the way, Hydra 1.0 has new support for setting environment variables:</p>

<p>This <em>ALMOST</em> works:</p>

<pre class=""lang-yaml prettyprint-override""><code>hydra:
  job:
    env_set:
      MLFLOW_TRACKING_DIR: file://${hydra:runtime.cwd}/.mlflow
      MLFLOW_TRACKING_URI: file://${hydra:runtime.cwd}/.mlflow
</code></pre>

<p>Unfortunately Hydra is cleaning up the env variables when your function exits, and MLFlow is making the final save when the process exits so the env variable is no longer set.
MLFlow also keeps re-initializing the FileStore object used to store the experiments data. If they would have initialized it just once and reused the same object the above should would have worked.</p>
","1930838",1
941,62310475,2,62310010,2020-06-10 18:21:40,1,"<p>The code example is immensely helpful. Thanks for that. You're right that it can be confusing to get <code>PythonScriptStep -&gt; PipelineData</code>. Working initially even without the <code>DataTransferStep</code>.</p>

<p>I don't know 100% what's going on, but I thought I'd spitball some ideas:</p>

<ol>
<li>Does your <code>PipelineData</code>,  <code>export_blob</code>, contain the ""test.csv"" file? I would verify that before troubleshooting the <code>DataTransferStep</code>. You can verify this using the SDK, or more easily with the UI.

<ol>
<li>Go to the PipelineRun page, click on the <code>PythonScriptStep</code> in question.</li>
<li>On ""Outputs + Logs"" page, there's a ""Data Outputs"" Section (that is slow to load initially)</li>
<li>Open it and you'll see the output PipelineDatas then click on ""View Output""</li>
<li>Navigate to given path either in the Azure Portal or Azure Storage Explorer.
<a href=""https://i.stack.imgur.com/9LaEq.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/9LaEq.png"" alt=""enter image description here""></a>
<a href=""https://i.stack.imgur.com/XbnhC.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/XbnhC.png"" alt=""enter image description here""></a></li>
</ol></li>
<li>In <code>test_upload_stackoverflow.py</code> you are treating the <code>PipelineData</code> as a directory when call <code>.to_csv()</code> as opposed to a file which would be you just calling <code>df_data_all.to_csv(args.output_extract, index=False)</code>. Perhaps try defining the <code>PipelineData</code> with <code>is_directory=True</code>. Not sure if this is required though.</li>
</ol>
","3842610",7
942,62339171,2,62336002,2020-06-12 07:03:05,1,"<p>In real-life projects, everything can be much more complicated:</p>

<ul>
<li>the input data can be from the different sources: database, file system, third-party services. So we need to do classical ETL before we can start working with data.</li>
<li><p>you can use different technologies in the one pipeline. For instance, Spark as a preprocessing tool, after you can need to use an instance with GPU for the model training.</p></li>
<li><p>last, but not least - in production you need to care much more things.  For instance data validation, model evaluation, etc. I wrote <a href=""https://medium.com/@danil.baibak/machine-learning-in-production-using-apache-airflow-91d25a4d8152"" rel=""nofollow noreferrer"">a separate article</a> about how to organize this part using <a href=""https://airflow.apache.org/docs/stable/best-practices.html"" rel=""nofollow noreferrer"">Apache Airflow</a>.</p></li>
</ul>
","8609330",0
943,62342317,2,62314939,2020-06-12 10:26:02,0,"<p>After retest again, the test 1 has successfully executed. </p>
","13479017",1
944,62378949,2,62320331,2020-06-14 22:27:30,2,"<p>there is indeed a rather pythonic way in the SageMaker python SDK:</p>

<pre><code>tuner = sagemaker.tuner.HyperparameterTuner.attach('&lt; your tuning jobname&gt;')

results = tuner.analytics().dataframe()  # all your tuning metadata, in pandas!
</code></pre>

<p>See full example here <a href=""https://github.com/aws-samples/amazon-sagemaker-tuneranalytics-samples/blob/master/SageMaker-Tuning-Job-Analytics.ipynb"" rel=""nofollow noreferrer"">https://github.com/aws-samples/amazon-sagemaker-tuneranalytics-samples/blob/master/SageMaker-Tuning-Job-Analytics.ipynb</a></p>
","5331834",0
945,62382471,2,62189492,2020-06-15 06:23:03,1,"<p>Here is a response from Microsoft:</p>

<blockquote>
  <p>PipelineParameter is currently not supported for use with AutoMLConfig parameters inside of AutoMLStep.</p>
  
  <p>Then, the only workaround in order to use PipelineParameter with
  AutoMLConfig would be to use AutoML in a PythonScriptStep, which is a
  similar usage/approach when you use AutoMLConfig with
  ParallelRunConfig in pipelines (without using AutoMLStep), like the
  ‘Many Models’ solution accelerator does.</p>
</blockquote>
","5109929",1
946,62397535,2,62111580,2020-06-15 21:46:02,0,"<p>I ended up not using SageMaker for this, but for anybody else having similar problems, I solved this by opening the file using s3fs and writing it to a <code>tempfile.NamedTemporaryFile</code>. This gave me a file path that I could pass into either <code>torchaudio.load</code> or <code>librosa.core.load</code>. This was also important because I wanted the extra resampling functionality of <code>librosa.core.load</code>, but it doesn't accept file-like objects for loading mp3s.</p>
","13651024",0
947,62426945,2,62057838,2020-06-17 10:33:31,1,"<p>Somewhere you should have a mapping from label integers to label classes, e.g.</p>

<pre><code>label_map = {0: 'background', 1: 'motorbike', 2: 'train', ...}
</code></pre>

<p>If you are using the Pascal VOC dataset, that would be (1=aeroplane, 2=bicycle, 3=bird, 4=boat, 5=bottle, 6=bus, 7=car , 8=cat, 9=chair, 10=cow, 11=diningtable, 12=dog, 13=horse, 14=motorbike, 15=person, 16=potted plant, 17=sheep, 18=sofa, 19=train, 20=tv/monitor) - see here: <a href=""http://host.robots.ox.ac.uk/pascal/VOC/voc2012/segexamples/index.html"" rel=""nofollow noreferrer"">http://host.robots.ox.ac.uk/pascal/VOC/voc2012/segexamples/index.html</a></p>

<p>Then you can simply use that map:</p>

<pre><code>used_classes = np.unique(mask)
for cls in used_classes:
    print(""Found class: {}"".format(label_map[cls]))
</code></pre>
","1267056",1
948,62442849,2,62441146,2020-06-18 05:17:16,3,"<p>You should be safe (at least data is not gone) most likely. From the <code>dvc remove</code> <a href=""https://dvc.org/doc/command-reference/remove"" rel=""nofollow noreferrer"">docs</a>:</p>

<blockquote>
  <p>Note that it does not remove files from the DVC cache or remote storage (see dvc gc). However, remember to run <code>dvc push</code> to save the files you actually want to use or share in the future.</p>
</blockquote>

<p>So, if you created <code>training_data.dvc</code> as with <code>dvc add</code> and/or <code>dvc run</code> and <code>dvc remove -p</code> didn't ask/warn you about anything, means that data is cached similar to Git in the <code>.dvc/cache</code>. </p>

<p>There are ways to retrieve it, but I would need to know a little bit more details - how exactly did you add your dataset? Did you commit <code>training_data.dvc</code> or it's completely gone? Was it the only data you have added so far? (happy to help you in comments).</p>

<h2>Recovering a directory</h2>

<p>First of all, <a href=""https://dvc.org/doc/user-guide/dvc-files-and-directories#structure-of-cache-directory"" rel=""nofollow noreferrer"">here</a> is the document that describes briefly how DVC stores directories in the cache.</p>

<p>What we can do is to find all <code>.dir</code> files in the <code>.dvc/cache</code>:</p>

<p><code>find .dvc/cache -type f -name ""*.dir""</code></p>

<p>outputs something like:</p>

<pre><code>.dvc/cache/20/b786b6e6f80e2b3fcf17827ad18597.dir
.dvc/cache/00/db872eebe1c914dd13617616bb8586.dir
.dvc/cache/2d/1764cb0fc973f68f31f5ff90ee0883.dir
</code></pre>

<p>(if the local cache is lost and we are restoring data from the remote storage, the same logic applies, commands (e.g. to find files on S3 with .dir extension) look different)</p>

<p>Each <code>.dir</code> file is a JSON with a content of one version of a directory (file names, hashes, etc). It has all the information needed to restore it. The next thing we need to do is to understand which one do we need. There is no one single rule for that, what I would recommend to check (and pick depending on your use case):</p>

<ul>
<li>Check the date modified (if you remember when this data was added).</li>
<li>Check the content of those files - if you remember a specific file name that was present only in the directory you are looking for - just grep it.</li>
<li>Try to restore them one by one and check the directory content.</li>
</ul>

<p>Okay, now let's imagine we decided that we want to restore <code>.dvc/cache/20/b786b6e6f80e2b3fcf17827ad18597.dir</code>, (e.g. because content of it looks like:</p>

<pre><code>[
{""md5"": ""6f597d341ceb7d8fbbe88859a892ef81"", ""relpath"": ""test.tsv""}, {""md5"": ""32b715ef0d71ff4c9e61f55b09c15e75"", ""relpath"": ""train.tsv""}
]
</code></pre>

<p>and we want to get a directory with <code>train.tsv</code>).</p>

<p>The only thing we need to do is to create a <code>.dvc</code> file that references this directory:</p>

<pre class=""lang-yaml prettyprint-override""><code>outs:
- md5: 20b786b6e6f80e2b3fcf17827ad18597.dir
  path: my-directory
</code></pre>

<p>(note, that path /20/b786b6e6f80e2b3fcf17827ad18597.dir became a hash value: 20b786b6e6f80e2b3fcf17827ad18597.dir)</p>

<p>And run <code>dvc pull</code> on this file.</p>

<p>That should be it.</p>
","298182",4
949,62470874,2,62313532,2020-06-19 12:59:49,3,"<p>In Sagemaker notebooks  use the below steps </p>

<h3>a) If in Notebook</h3>

<p>i)  <code>!type python3</code></p>

<p>ii) Say the above is /home/ec2-user/anaconda3/envs/python3/bin/python3 for you </p>

<p>iii) <code>!/home/ec2-user/anaconda3/envs/python3/bin/python3 -m pip install  xgboost</code></p>

<p>iv)  <code>import xgboost</code></p>

<hr>

<h3>b) If using Terminal</h3>

<p>i) <code>conda activate conda_python3</code><br>
ii) <code>pip install xgboost</code></p>

<p>Disclaimer :  sometimes the installation would fail with gcc version ,in that case  update pip version before running install</p>
","4656721",0
950,62476220,2,62457880,2020-06-19 18:10:02,1,"<p>We checked the AKS networking configuration and realized it has an Azure CNI profile.</p>

<p>In order to test the webservice we need to do it from inside the created virtual network.
It worked well!</p>
","13155092",0
951,62477469,2,62433812,2020-06-19 19:39:12,1,"<p>Within a pipeline step, you can access the <code>Workspace</code> via:</p>

<pre><code>run = Run.get_context()
ws = run.experiment.workspace
</code></pre>
","10116",1
952,62477844,2,62407943,2020-06-19 20:06:47,2,"<p>For me, the killer feature of Azure ML is not having to worry about load balancing like this. Our team has a compute target with <code>max_nodes=100</code> for every feature branch and we have <code>Hyperdrive</code> pipelines that result in 130 runs for each pipeline.</p>
<p>We can submit multiple <code>PipelineRun</code>s back-to-back and the orchestrator does the heavy lifting of queuing, submitting, all the runs so that the <code>PipelineRun</code>s execute in the serial order I submitted them, and that the cluster is never overloaded. This works without issue for us 99% of the time.</p>
<p>If what you're looking for is that you'd like the <code>PipelineRun</code>s to be executed in parallel, then you should check out <a href=""https://learn.microsoft.com/en-us/azure/machine-learning/how-to-use-parallel-run-step#build-and-run-the-pipeline-containing-parallelrunstep"" rel=""nofollow noreferrer""><code>ParallelRunStep</code></a>.</p>
<p>Another option is to isolate your computes. You can have up to 200 <code>ComputeTarget</code>s per workspace. Two 50-node <code>ComputeTarget</code>s cost the same as one 100-node <code>ComputeTarget</code>.</p>
<p>On our team, we use <a href=""https://www.pygit2.org/"" rel=""nofollow noreferrer""><code>pygit2</code></a> to have a <code>ComputeTarget</code> created for each feature branch, so that, as data scientists, we can be confident that we're not stepping on our coworkers' toes.</p>
","3842610",3
953,62480976,2,61454181,2020-06-20 02:32:12,0,"<p>the issue might be related to your use of volumes. Have you tried to use the more supported data passing mechanisms?</p>

<p>For example, take this pipeline: <a href=""https://github.com/kubeflow/pipelines/blob/091316b8bf3790e14e2418843ff67a3072cfadc0/components/XGBoost/_samples/sample_pipeline.py"" rel=""nofollow noreferrer"">https://github.com/kubeflow/pipelines/blob/091316b8bf3790e14e2418843ff67a3072cfadc0/components/XGBoost/_samples/sample_pipeline.py</a></p>

<p>Apply the GPU-related customizations to the trainer:</p>

<pre><code>some_task.add_node_selector_constraint('cloud.google.com/gke-accelerator', 'nvidia-tesla-p100')
some_task.set_gpu_limit(1)
</code></pre>

<p>Put the trainer and predictor inside a <code>for _ in range(10):</code> loop so that you have 10 parallel copies.</p>

<p>Check whether the trainers run in parallel.</p>

<p>P.S. It's better to create issues in the official repo: <a href=""https://github.com/kubeflow/pipelines/issues"" rel=""nofollow noreferrer"">https://github.com/kubeflow/pipelines/issues</a></p>
","1497385",1
954,62525998,2,62422682,2020-06-23 01:37:54,-1,"<p>Solved it. The error I was getting is due to roles/permission of elastic inference attached to notebook. Once fixed these permissions by our devops team. It worked as expected.  See <a href=""https://github.com/aws/sagemaker-tensorflow-serving-container/issues/142"" rel=""nofollow noreferrer"">https://github.com/aws/sagemaker-tensorflow-serving-container/issues/142</a></p>
","845743",2
955,62535280,2,62515398,2020-06-23 12:56:43,0,"<p>Currently, there's no way to increase memory limit in Classic Studio. We encouraged customers to try <a href=""https://learn.microsoft.com/en-us/azure/machine-learning/concept-designer"" rel=""nofollow noreferrer"">Azure Machine Learning designer (preview)</a>, which provides similar drag and drop ML modules plus scalability, version control, and enterprise security. Furthermore, with Designer, the endpoints are deployed to AKS where no limit other than cluster resource is imposed.</p>
","11968855",0
956,62576453,2,62187748,2020-06-25 13:26:00,2,"<p>I just came across this same problem! Seems like you need to specify <code>CaptureContentTypeHeader</code> params to tell SageMaker which content type headers to treat as CSV (or JSON), versus the default which is to base64 encode the payload!</p>
<p>So e.g. adding the following to your <a href=""https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_CreateEndpointConfig.html"" rel=""nofollow noreferrer"">CreateEndpointConfig</a> call or boto3/sagemaker SDK equivalent should fix it:</p>
<pre><code>{
   &quot;DataCaptureConfig&quot;: { 
      &quot;CaptureContentTypeHeader&quot;: { 
         &quot;CsvContentTypes&quot;: [ &quot;text/csv&quot; ]
      },
   }
}
</code></pre>
<p>I guess this is to allow for non-standard Content-Type headers? Providing a layer of config to resolve e.g:</p>
<ul>
<li><code>application/x-mycoolmodel</code> -&gt; <code>JSON</code>, versus</li>
<li><code>application/x-secretsauce</code> -&gt; <code>BASE64</code></li>
</ul>
","13352657",0
957,62588813,2,62541587,2020-06-26 05:52:54,1,"<p>As answered here: <a href=""https://stackoverflow.com/a/19674648/5157515"">https://stackoverflow.com/a/19674648/5157515</a></p>
<p>There's no direct way to access python variables with <code>!</code> command.</p>
<p>But with magic command <code>%%bash</code> it is possible</p>
<pre><code>%%bash  -s &quot;$region&quot;
aws ecr get-login-password --region $1
</code></pre>
","5157515",0
958,62646350,2,62614143,2020-06-29 20:41:20,3,"<p>So, after to run on many pages, just found a clue <a href=""https://github.com/awslabs/amazon-sagemaker-examples/blob/master/introduction_to_amazon_algorithms/blazingtext_hosting_pretrained_fasttext/blazingtext_hosting_pretrained_fasttext.ipynb"" rel=""nofollow noreferrer"">here</a>. And I finally found out how to load the model and create the endpoint:</p>
<pre><code>def create_endpoint(self):
    sess = sagemaker.Session()
    training_image = get_image_uri(sess.boto_region_name, 'semantic-segmentation', repo_version=&quot;latest&quot;)        
    role = &quot;YOUR_ROLE_ARN_WITH_SAGEMAKER_EXECUTION&quot;
    model = &quot;s3://BUCKET/PREFIX/.../output/model.tar.gz&quot;

    sm_model = sagemaker.Model(model_data=model, image=training_image, role=role, sagemaker_session=sess)
    sm_model.deploy(initial_instance_count=1, instance_type='ml.p3.2xlarge')
</code></pre>
<p><strong>Please, do not forget to disable your endpoint after using. This is really important! Endpoints are charged by &quot;running&quot; not only by the use</strong></p>
<p>I hope it also can help you out!</p>
","4800905",0
959,62652238,2,62319753,2020-06-30 07:22:18,1,"<p>I found a simple solution to it. Env variables can be passed via Sagemaker sdk. It minimizes the dependencies.</p>
<p><a href=""https://sagemaker.readthedocs.io/en/stable/api/training/processing.html"" rel=""nofollow noreferrer"">https://sagemaker.readthedocs.io/en/stable/api/training/processing.html</a></p>
<p>As another answer suggested, paws can be used as well to get secrets from aws. This would be a better approach</p>
","5409192",0
960,62652613,2,61074798,2020-06-30 07:46:24,4,"<p>After a lot of searching and try &amp; error, I was able to solve this problem. In many cases, the problem arises because of the TensorFlow and Python versions.</p>
<p><strong>Cause of the problem:</strong>
To deploy the endpoints, I was using the <code>TensorflowModel</code> on TF 1.12 and python 3 and which exactly caused the problem.</p>
<blockquote>
<pre><code>sagemaker_model = TensorFlowModel(model_data = model_data,
                                  role = role,
                                  framework_version = '1.12',
                                  entry_point = 'train.py')
</code></pre>
</blockquote>
<p>Apparently, <code>TensorFlowModel</code> only allows python 2 on TF version 1.11, 1.12. 2.1.0.</p>
<p><strong>How I fixed it:</strong> There are two TensorFlow solutions that handle serving in the Python SDK. They have different class representations and documentation as shown here.</p>
<ol>
<li><strong>TensorFlowModel</strong> - <a href=""https://github.com/aws/sagemaker-python-sdk/blob/master/src/sagemaker/tensorflow/model.py#L47"" rel=""nofollow noreferrer"">https://github.com/aws/sagemaker-python-sdk/blob/master/src/sagemaker/tensorflow/model.py#L47</a></li>
</ol>
<ul>
<li>Doc:
<a href=""https://github.com/aws/sagemaker-python-sdk/tree/v1.12.0/src/sagemaker/tensorflow#deploying-directly-from-model-artifacts"" rel=""nofollow noreferrer"">https://github.com/aws/sagemaker-python-sdk/tree/v1.12.0/src/sagemaker/tensorflow#deploying-directly-from-model-artifacts</a></li>
<li>Key difference: Uses a proxy GRPC client to send requests</li>
<li>Container impl:
<a href=""https://github.com/aws/sagemaker-tensorflow-container/blob/master/src/tf_container/serve.py"" rel=""nofollow noreferrer"">https://github.com/aws/sagemaker-tensorflow-container/blob/master/src/tf_container/serve.py</a></li>
</ul>
<ol start=""2"">
<li><strong>Model</strong> - <a href=""https://github.com/aws/sagemaker-python-sdk/blob/master/src/sagemaker/tensorflow/serving.py#L96"" rel=""nofollow noreferrer"">https://github.com/aws/sagemaker-python-sdk/blob/master/src/sagemaker/tensorflow/serving.py#L96</a></li>
</ol>
<ul>
<li>Doc: <a href=""https://github.com/aws/sagemaker-python-sdk/blob/master/src/sagemaker/tensorflow/deploying_tensorflow_serving.rst"" rel=""nofollow noreferrer"">https://github.com/aws/sagemaker-python-sdk/blob/master/src/sagemaker/tensorflow/deploying_tensorflow_serving.rst</a></li>
<li>Key difference: Utilizes the TensorFlow serving rest API</li>
<li>Container impl: <a href=""https://github.com/aws/sagemaker-tensorflow-serving-container/blob/master/container/sagemaker/serve.py"" rel=""nofollow noreferrer"">https://github.com/aws/sagemaker-tensorflow-serving-container/blob/master/container/sagemaker/serve.py</a></li>
</ul>
<p>Python 3 isn't supported using the <code>TensorFlowModel</code> object, as the container uses the TensorFlow serving API library in conjunction with the GRPC client to handle making inferences, however, the TensorFlow serving API isn't supported in Python 3 officially, so there are only Python 2 versions of the containers when using the <code>TensorFlowModel</code> object.
If you need Python 3 then you will need to use the <code>Model</code> object defined in #2 above.</p>
<p>Finally, I used the <code>Model</code> with the TensorFlow version 1.15.1.</p>
<blockquote>
<pre><code>sagemaker_model = Model(model_data = model_data,
                        role = role,
                        framework_version='1.15.2',
                        entry_point = 'train.py')
</code></pre>
</blockquote>
<p>Also, here are the successful results.
<a href=""https://i.stack.imgur.com/OMsEf.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/OMsEf.png"" alt=""enter image description here"" /></a></p>
","7245808",0
961,62657388,2,62343056,2020-06-30 12:23:32,7,"<p>I eventually found a solution thanks to colleague of mine. I'm hence answering myself, in order to close the question and, maybe, help somebody else.</p>
<p>You can find the proper function in this link: <a href=""https://learn.microsoft.com/en-us/python/api/azureml-core/azureml.core.run.run?view=azure-ml-py#log-confusion-matrix-name--value--description----"" rel=""noreferrer"">https://learn.microsoft.com/en-us/python/api/azureml-core/azureml.core.run.run?view=azure-ml-py#log-confusion-matrix-name--value--description----</a>.</p>
<p>Anyway, you also have to consider that, apparently, Azure doesn't work with the standard confusion matrix format returned by sklearn. It accepts indeed ONLY list of list, instead of numpy array, populated with numpy.int64 elements. So you also have to transform the matrix in a simpler format (for the sake of simplicity I used the nested list comprehension in the command below:</p>
<pre><code>cmtx = metrics.confusion_matrix(y_test,(y_pred&gt;.5).astype(int))
cmtx = {

&quot;schema_type&quot;: &quot;confusion_matrix&quot;,
&quot;parameters&quot;: params,
 &quot;data&quot;: {&quot;class_labels&quot;: [&quot;0&quot;, &quot;1&quot;],
          &quot;matrix&quot;: [[int(y) for y in x] for x in cmtx]}
}
run.log_confusion_matrix('Confusion matrix - error rate', cmtx)
</code></pre>
","8683073",0
962,62660771,2,62602435,2020-06-30 15:18:34,1,"<p><code>SM_CHANNELS</code> returns a list of channel names. What you're looking for is <code>SM_CHANNEL_TRAIN_CHANNEL</code> (&quot;SM_CHANNEL&quot; + your channel name), which provides the filesystem location for the channel:</p>
<pre class=""lang-py prettyprint-override""><code>parser.add_argument('--train_channel', type=str, default=os.environ['SM_CHANNEL_TRAIN_CHANNEL'])
</code></pre>
<p>docs: <a href=""https://github.com/aws/sagemaker-training-toolkit/blob/master/ENVIRONMENT_VARIABLES.md#sm_channel_channel_name"" rel=""nofollow noreferrer"">https://github.com/aws/sagemaker-training-toolkit/blob/master/ENVIRONMENT_VARIABLES.md#sm_channel_channel_name</a></p>
","9074534",0
963,62663804,2,62462790,2020-06-30 18:19:11,4,"<p>For older containers using the deprecated <code>sagemaker_containers</code>, the approach you described is correct.</p>
<p>For newer containers that use <a href=""https://github.com/aws/sagemaker-training-toolkit"" rel=""nofollow noreferrer""><code>sagemaker-training-toolkit</code></a>, this is how you retrieve information about the environment: <a href=""https://github.com/aws/sagemaker-training-toolkit#get-information-about-the-container-environment"" rel=""nofollow noreferrer"">https://github.com/aws/sagemaker-training-toolkit#get-information-about-the-container-environment</a></p>
<pre class=""lang-py prettyprint-override""><code>from sagemaker_training import environment

env = environment.Environment()

job_name = env[&quot;job_name&quot;]
</code></pre>
<p>You can check the <a href=""https://docs.aws.amazon.com/deep-learning-containers/latest/devguide/dlc-release-notes.html"" rel=""nofollow noreferrer"">DLC Release Notes</a> to see what's installed in each version.</p>
","1344518",0
964,62664173,2,62623166,2020-06-30 18:43:45,2,"<p>This is commonly fixed by upgrading to the latest SDK. You can do this by running <code>pip install --upgrade azureml-sdk[explain,automl]</code>.</p>
<p>Thanks,
Sabina</p>
","12875447",4
965,62697100,2,62696966,2020-07-02 13:07:47,2,"<p>The known limitations of Machine Learning Studio (classic) are:</p>
<p>The Python runtime is sandboxed and does not allow access to the network or to the local file system in a persistent manner.</p>
<p>All files saved locally are isolated and deleted once the module finishes. The Python code cannot access most directories on the machine it runs on, the exception being the current directory and its subdirectories.</p>
<p>When you provide a zipped file as a resource, the files are copied from your workspace to the experiment execution space, unpacked, and then used. Copying and unpacking resources can consume memory.</p>
<p>The module can output a single data frame. It's not possible to return arbitrary Python objects such as trained models directly back to the Studio (classic) runtime. However, you can write objects to storage or to the workspace. Another option is to use pickle to serialize multiple objects into a byte array and then return the array inside a data frame.</p>
<p>Hope this helps!</p>
","13782447",1
966,62697848,2,61980244,2020-07-02 13:47:18,5,"<p>Had a similar issue. In my case, I solved it by running <code>mlflow ui</code> inside the <code>mlruns</code> directory of your experiment.</p>
<p>See the full discussion on Github <a href=""https://github.com/mlflow/mlflow/issues/3030"" rel=""nofollow noreferrer"">here</a></p>
<p>Hope it helps!</p>
","1380652",1
967,62719746,2,62701556,2020-07-03 16:57:00,4,"<p><code>TL;DR</code> You're right that normalization doesn't make sense for training gradient-boosted decision trees (<code>GBDT</code>s) on categorical data, but it won't have an adverse impact. AutoML is an automated framework for modeling. In exchange for calibration control, you get ease-of-use. It is still worth verifying first that AutoML is receiving data with the columns properly encoded as categorical.</p>
<p>Think of an AutoML model as effectively a <a href=""https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html"" rel=""nofollow noreferrer"">sklearn Pipeline</a>, which is a bundled set of pre-processing steps along with a predictive Estimator. AutoML will attempt to sample from a large swath of pre-configured Pipelines such that the most accurate Pipeline will be discovered. As <a href=""https://learn.microsoft.com/en-us/azure/machine-learning/concept-automated-ml#automatic-featurization-standard"" rel=""nofollow noreferrer"">the docs</a> say:</p>
<blockquote>
<p>In every automated machine learning experiment, your data is automatically scaled or normalized to help algorithms perform well. During model training, one of the following scaling or normalization techniques will be applied to each model.</p>
</blockquote>
<p>Too see this, you can called <code>.named_steps</code> on your fitted model. Also check out <a href=""https://learn.microsoft.com/en-us/azure/machine-learning/how-to-configure-auto-train#automated-feature-engineering"" rel=""nofollow noreferrer""><code>fitted_model.get_featurization_summary()</code></a></p>
<p>I especially empathize with your concern especially w.r.t. how <code>LightGBM</code> (MSFT's GBDT implementation) is levered by AutoML. <code>LightGBM</code> accepts categorical columns and instead of one-hot encoding, will bin them into two subsets whenever split. Despite this, AutoML will pre-process away the categorical columns by one-hot encoding, scaling, and/or normalization; so this unique categorical approach is never utilized in AutoML.</p>
<p>If you're interested in &quot;manual&quot; ML in Azure ML, I highly suggest looking into <a href=""https://learn.microsoft.com/en-us/azure/machine-learning/concept-train-machine-learning-model#estimators"" rel=""nofollow noreferrer""><code>Estimators</code></a> and <a href=""https://learn.microsoft.com/en-us/azure/machine-learning/concept-train-machine-learning-model#machine-learning-pipeline"" rel=""nofollow noreferrer""><code>Azure ML Pipelines</code></a></p>
","3842610",3
968,62734254,2,62734059,2020-07-04 20:43:32,2,"<p>If you want to perform any kind of upgrades or modification to the kernel of the notebook you can do this at launch by using <a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/notebook-lifecycle-config.html"" rel=""nofollow noreferrer"">lifecycle configuration</a>.</p>
","13460933",3
969,62749104,2,62711259,2020-07-06 04:08:45,1,"<p>After searching around, I found <a href=""https://github.com/mlflow/mlflow/issues/2390"" rel=""nofollow noreferrer"">this issue</a> related to my problem above. Actually, all my metrics just logged once each training (instead of each epoch as my intuitive thought). The reason is I didn't specify the <code>every_n_iter</code> parameter in <code>mlflow.tensorflow.autolog()</code>, which indicates how many 'iterations' must pass before MLflow logs metric executed (see the <a href=""https://mlflow.org/docs/latest/python_api/mlflow.tensorflow.html#mlflow.tensorflow.autolog"" rel=""nofollow noreferrer"">docs</a>). So, changing my code to:</p>
<p><code>mlflow.tensorflow.autolog(every_n_iter=1)</code></p>
<p>fixed the problem.</p>
<p>P/s: Remember that in TF 2.x, an 'iteration' is an epoch (in TF 1.x it's a batch).</p>
","6563277",0
970,62776775,2,62765658,2020-07-07 13:47:19,1,"<p>Your error is being raised in this function <code>load_img</code>:</p>
<pre><code>def load_img(path_to_img):
    max_dim = 512
    img = tf.io.read_file(path_to_img)
    img = tf.image.decode_image(img, channels=3)
    img = tf.image.convert_image_dtype(img, tf.float32)

    shape = tf.cast(tf.shape(img)[:-1], tf.float32)
    long_dim = max(shape)
    scale = max_dim / long_dim

    new_shape = tf.cast(shape * scale, tf.int32)

    img = tf.image.resize(img, new_shape)
    img = img[tf.newaxis, :]
    return img
</code></pre>
<p>Specifically, this line:</p>
<pre><code>    long_dim = max(shape)
</code></pre>
<p>You are passing a tensor to the <a href=""https://docs.python.org/3/library/functions.html#max"" rel=""nofollow noreferrer"">built-in Python max function</a> in graph execution mode. You can only iterate through tensors in eager-execution mode. You probably want to use <a href=""https://www.tensorflow.org/api_docs/python/tf/math/reduce_max"" rel=""nofollow noreferrer"">tf.reduce_max</a> instead:</p>
<pre><code>    long_dim = tf.reduce_max(shape)
</code></pre>
","2596715",0
971,62779433,2,62002183,2020-07-07 16:06:22,1,"<p>Looking through the example you posted, it looks as though the <code>model_dir</code> passed to the TensorFlow Object Detection package is configured to <code>/opt/ml/model</code>:</p>
<pre><code># These are the paths to where SageMaker mounts interesting things in your container.
prefix = '/opt/ml/'
input_path = os.path.join(prefix, 'input/data')
output_path = os.path.join(prefix, 'output')
model_path = os.path.join(prefix, 'model')
param_path = os.path.join(prefix, 'input/config/hyperparameters.json')
</code></pre>
<p>During the training process, tensorboard logs will be written to <code>/opt/ml/model</code>, and then uploaded to s3 as a final model artifact AFTER training: <a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/your-algorithms-training-algo-envvariables.html"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/sagemaker/latest/dg/your-algorithms-training-algo-envvariables.html</a>.</p>
<p>You <em>might</em> be able to side-step the SageMaker artifact upload step and point the <code>model_dir</code> of TensorFlow Object Detection API directly at an s3 location during training:</p>
<pre><code>model_path = &quot;s3://your-bucket/path/here
</code></pre>
<p>This means that the TensorFlow library within the SageMaker job is directly writing to S3 instead of the filesystem inside of it's container. Assuming the underlying TensorFlow Object Detection code can write directly to S3 (something you'll have to verify), you should be able to see the tensorboard logs and checkpoints there in realtime.</p>
","2596715",0
972,62783530,2,62737008,2020-07-07 20:37:11,2,"<p>You can do it with:</p>
<pre class=""lang-py prettyprint-override""><code>job_name = rf_transformer.latest_transform_job.name
rf_transformer.sagemaker_session.describe_transform_job(job_name)['TransformJobStatus']
</code></pre>
<p>You can also use the AWS SDK directly, if you wish:</p>
<pre class=""lang-py prettyprint-override""><code>import boto3

sagemaker_client = boto3.client('sagemaker')
sagemaker_client.describe_transform_job(job_name)['TransformJobStatus']
</code></pre>
<p>API documentation: <a href=""https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_DescribeTransformJob.html"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_DescribeTransformJob.html</a></p>
","9074534",0
973,62783682,2,62780977,2020-07-07 20:47:43,2,"<p>thanks for your question.</p>
<p>I'm assuming that HyperDriveStep is one of the steps in your Pipeline and that you want the remaining Pipeline steps to continue, when HyperDriveStep fails, is that correct?
Enabling continue_on_step_failure, should allow the rest of the pipeline steps to continue, when any single steps fails.</p>
<p>Additionally, the HyperDrive run consists of multiple child runs, controlled by the HyperDriveConfig. If the first 3 child runs explored by HyperDrive fail (e.g. with user script errors), the system automatically cancels the entire HyperDrive run, in order to avoid further wasting resources.</p>
<p>Are you looking to continue other Pipeline steps when the HyperDriveStep fails? or are you looking to continue other child runs within the HyperDrive run, when the first 3 child runs fail?</p>
<p>Thanks!</p>
","10808059",3
974,62802946,2,62557113,2020-07-08 20:14:36,3,"<p>The error is caused by your <code>sagemaker.tensorflow.serving.Model</code> not having a <code>sagemaker.session.Session</code> associated with it.</p>
<p>Add <code>sagemaker_session=sagemaker_session</code> to your Model instantiation:</p>
<pre class=""lang-py prettyprint-override""><code>from sagemaker.tensorflow.serving import Model
sagemaker_model = Model(model_data='s3://' + sagemaker_session.default_bucket() + '/Scikit-keras-NLP-pipeline-Boston-Housing-example-June08-test1/train/Bostonmodel.tar.gz',
                        role=role,
                        sagemaker_session=sagemaker_session)
</code></pre>
","9074534",2
975,62816770,2,62815426,2020-07-09 14:08:28,2,"<p>Each python script step runs on a single node even if you allocate multiple nodes in your cluster. I'm not sure whether training on different instances is possible off-the-shelf in AML, but there's definitely the possibility to use that single node more effectively (looking into using all your cores, etc.)</p>
","9182232",0
976,62827895,2,62734994,2020-07-10 05:34:20,1,"<p>You are not activating any conda environment such as <a href=""https://stackoverflow.com/questions/60036916/sagemaker-lifecycle-configuration-for-installing-pandas-not-working"">python3</a>.</p>
<pre><code>#!/bin/bash
sudo -u ec2-user -i &lt;&lt;'EOF'

# This will affect only the Jupyter kernel called &quot;conda_python3&quot;.
source activate python3

pip install pandas

conda update pandas

source deactivate

EOF
</code></pre>
","10600556",0
977,62851906,2,62844211,2020-07-11 16:38:36,3,"<p>If I am understanding the question correctly, you should be able to use <a href=""https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_CreateEndpointConfig.html"" rel=""nofollow noreferrer"">CreateEndpointConfig</a> near the end of the training job, then use <a href=""https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_UpdateEndpoint.html"" rel=""nofollow noreferrer"">UpdateEndpoint</a>:</p>
<p><code>Deploys the new EndpointConfig specified in the request, switches to using newly created endpoint, and then deletes resources provisioned for the endpoint using the previous EndpointConfig (there is no availability loss).</code></p>
<p>If the API Gateway / Lambda is routed via the endpoint ARN, that should not change after using <code>UpdateEndpoint</code>.</p>
","2596715",0
978,62860719,2,62860539,2020-07-12 11:50:03,4,"<p>I wouldn't know about SageMaker or AWS specifically, but something you can do is cast your input to <code>float32</code>, which takes less memory space. You can cast it like this:</p>
<pre><code>train_data = tf.cast(train_data, tf.float32)
</code></pre>
<p><code>float32</code> is the default value of Tensorflow weights so you don't need <code>float64</code> anyway. Proof:</p>
<pre><code>import tensorflow as tf
layer = tf.keras.layers.Dense(8)
print(layer(tf.random.uniform((10, 100), 0, 1)).dtype)
</code></pre>
<pre><code>&lt;dtype: 'float32'&gt;
</code></pre>
<p>My other suggestions are to get less words from your dataset, or to not one-hot encode them. If you're planning on training a recurrent model with an embedding layer, you won't need to anyway.</p>
","10908375",0
979,62861709,2,62816326,2020-07-12 13:31:06,0,"<p>Okay I ended up figuring this out myself. Turns out you can define a <code>persistent volume claim</code> on top of a <code>storage class</code>. The storage class can be specified to be an Azure File Share, which makes everything much more convenient.</p>
<p>sc.yaml:</p>
<pre><code>apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: &lt;NAME&gt;
provisioner: kubernetes.io/azure-file
mountOptions:
  - dir_mode=0777
  - file_mode=0777
  - uid=0
  - gid=0
  - mfsymlinks
  - cache=strict
parameters:
  storageAccount: &lt;STORAGE_ACC_NAME&gt;
</code></pre>
<p>pvc.yaml:</p>
<pre><code>apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: &lt;NAME&gt;
spec:
  accessModes:
    - ReadWriteMany
  storageClassName: &lt;NAME&gt;
  resources:
    requests:
      storage: 20Gi
</code></pre>
<p>The persistent volume can then be created and claimed by:</p>
<pre><code>kubectl apply -f sc.yaml
kubectl apply -f pvc.yaml
</code></pre>
<p>After this a share shows up in the specified storage account, and you can simply utilize Azure file share's systems to seamlessly upload data into it (like perhaps using azcopy to move data from your local machine or an existing share/container).</p>
","10229754",0
980,62880170,2,62856049,2020-07-13 16:28:50,3,"<p>A SageMaker training job in &quot;local&quot; is actually executing <em>inside of a Docker container</em> that is isolated from the Python kernel that is executing your notebook. Therefore, the <code>plt.show()</code> in the <code>train_cnn.py</code> script doesn't actually get routed to the notebook UI in the same way that executing that command directly from a notebook would.</p>
<p>Instead of using <code>plt.show()</code>, consider using <a href=""https://matplotlib.org/3.1.1/api/_as_gen/matplotlib.pyplot.savefig.html"" rel=""nofollow noreferrer"">plt.savefig()</a> to output the plot to an image:</p>
<pre><code>plt.savefig(&quot;training_results.png&quot;)  
</code></pre>
<p>Upon termination of the training container, SageMaker will zip up all the output artifacts (including the plot) and ship them to S3 in your training script. Alternatively, you could upload the plot straight to S3 -- see <a href=""https://stackoverflow.com/questions/31485660/python-uploading-a-plot-from-memory-to-s3-using-matplotlib-and-boto"">python - uploading a plot from memory to s3 using matplotlib and boto</a> for an example of this.</p>
<p>As a side note: have you considered using TensorBoard? It can offer a better experience for browsing results of a training script, and SageMaker should have a first class integration to make it easy to enable. Take a look at the <a href=""https://sagemaker.readthedocs.io/en/stable/frameworks/tensorflow/sagemaker.tensorflow.html#sagemaker.tensorflow.estimator.TensorFlow.fit"" rel=""nofollow noreferrer""><code>run_tensorboard_locally</code> argument</a>.</p>
","2596715",0
981,62881049,2,62841756,2020-07-13 17:24:30,0,"<p>Ok. So it looks like while the ARTIFACTS STORE does support hdfs, you have to use either file or a sql like for the BACKEND STORE.</p>
","3768204",1
982,62901468,2,62886435,2020-07-14 18:13:46,3,"<p>This should work :</p>
<pre><code>from azureml.core import Workspace
from azureml.core.environment import Environment
from azureml.train.estimator import Estimator
from azureml.core.conda_dependencies import CondaDependencies
from azureml.core import Experiment

ws = Workspace (...)
exp = Experiment(ws, 'test-so-exp')

myenv = Environment(name = &quot;myenv&quot;)
myenv.docker.enabled = True
dockerfile = r&quot;&quot;&quot;
FROM mcr.microsoft.com/azureml/base:intelmpi2018.3-ubuntu16.04
RUN apt-get update &amp;&amp; apt-get install -y libgl1-mesa-glx
RUN echo &quot;Hello from custom container!&quot;
&quot;&quot;&quot;
myenv.docker.base_image = None
myenv.docker.base_dockerfile = dockerfile

## You need to instead put your packages in the Environment definition instead... 
## see below for some changes too

myenv.python.conda_dependencies = CondaDependencies.create(pip_packages = ['scipy==1.1.0', 'torch==1.5.1'])
</code></pre>
<p>Finally you can build your estimator a bit differently :</p>
<pre><code>est = Estimator(
    source_directory = '.',
#     script_params = script_params,
#     use_gpu = True,
    compute_target = 'gpu-cluster-1',
#     pip_packages = ['scipy==1.1.0', 'torch==1.5.1'],
    entry_script = 'AzureEntry.py',
    environment_definition=myenv
    )
</code></pre>
<p>And submit it :</p>
<pre><code>run = exp.submit(config = est)
run.wait_for_completion(show_output=True)
</code></pre>
<p>Let us know if that works.</p>
","10435556",1
983,62926575,2,62919671,2020-07-16 02:37:49,1,"<p>Try increasing the ebs volume amount of your notebook ... this blog explains it well: <a href=""https://aws.amazon.com/blogs/machine-learning/customize-your-notebook-volume-size-up-to-16-tb-with-amazon-sagemaker/"" rel=""nofollow noreferrer"">https://aws.amazon.com/blogs/machine-learning/customize-your-notebook-volume-size-up-to-16-tb-with-amazon-sagemaker/</a></p>
<p>Also, best practice is to use lifecycle configuration scripts to build/add new dependencies ... official docs: <a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/notebook-lifecycle-config.html"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/sagemaker/latest/dg/notebook-lifecycle-config.html</a></p>
<p>This github page has some great template examples ... for example setting up specific configs like conda, etc: <a href=""https://github.com/aws-samples/amazon-sagemaker-notebook-instance-lifecycle-config-samples/tree/master/scripts"" rel=""nofollow noreferrer"">https://github.com/aws-samples/amazon-sagemaker-notebook-instance-lifecycle-config-samples/tree/master/scripts</a></p>
","11286588",0
984,62953400,2,62778020,2020-07-17 11:52:54,1,"<p>The short answer is yes, it's possible, but won't be exactly as easy as running a single mlflow command. You can paralelize single-node workflows using spark Python UDFs, a good example of this is this <a href=""https://pages.databricks.com/rs/094-YMS-629/images/Fine-Grained-Time-Series-Forecasting.html?_ga=2.64430959.1760852900.1593769579-972789996.1561118598"" rel=""nofollow noreferrer"">notebook</a></p>
<p>I'm not sure if this will work with pytorch, but there is hyperopt library that lets you parallelize search across parameters using Spark - it's integrated with mlflow and available in databricks ML runtime. I've been using it only with scikit-learn, but it may be <a href=""https://docs.databricks.com/applications/machine-learning/automl/hyperopt/hyperopt-model-selection.html"" rel=""nofollow noreferrer"">worth checking out</a></p>
","1946658",0
985,62958369,2,62949488,2020-07-17 16:45:39,6,"<p>this totally happens from time to time. it is certainly frustrating especially because the &quot;Cancel&quot; button it grayed out. You can use either the CLI or Python SDK  to cancel the run.</p>
<h2>SDK</h2>
<h3>&gt;= 1.16.0</h3>
<p>As of version <code>1.16.0</code> you no longer an <code>Experiment</code> object is no longer needed. Instead you can access using the <a href=""https://learn.microsoft.com/en-us/python/api/azureml-core/azureml.core.run(class)?view=azure-ml-py#get-workspace--run-id-&amp;WT.mc_id=AI-MVP-5003930"" rel=""nofollow noreferrer""><code>Run</code></a> or <a href=""https://learn.microsoft.com/en-us/python/api/azureml-core/azureml.core.workspace(class)?view=azure-ml-py#get-run-run-id-&amp;WT.mc_id=AI-MVP-5003930"" rel=""nofollow noreferrer""><code>Workspace</code></a> objects directly</p>
<pre class=""lang-py prettyprint-override""><code>from azureml.core import Workspace, Experiment, Run, VERSION
print(&quot;SDK version:&quot;, VERSION)

ws = Workspace.from_config()

run = ws.get_run('YOUR_RUN_ID')
run = Run().get(ws, 'YOUR_RUN_ID') # also works
run.cancel()
</code></pre>
<h3>&lt; 1.16.0</h3>
<pre class=""lang-py prettyprint-override""><code>from azureml.core import Workspace, Experiment, Run, VERSION
print(&quot;SDK version:&quot;, VERSION)

ws = Workspace.from_config()
exp = Experiment(workspace = ws, name = 'YOUR_EXP_NAME')

run = Run(exp, run_id='YOUR STEP RUN ID')

run.cancel() # or run.fail()
</code></pre>
<h1>CLI</h1>
<p><a href=""https://learn.microsoft.com/en-us/azure/machine-learning/reference-azure-machine-learning-cli#install-the-extension"" rel=""nofollow noreferrer"">More CLI details here</a></p>
<pre class=""lang-bash prettyprint-override""><code>az login
az ml run cancel --run YOUR_RUN_ID
</code></pre>
<hr>
<p>Updated CLI command on May 5th, 2023:</p>
<pre><code>az ml job cancel --name YOUR_JOB_NAME --resource-group YOUR_RG --workspace-name YOUR_WS
</code></pre>
","3842610",1
986,62980512,2,62980380,2020-07-19 12:49:45,9,"<p>This can be done by using the sagemaker library combined with the <a href=""https://sagemaker.readthedocs.io/en/stable/api/inference/model.html"" rel=""nofollow noreferrer"">Inference Model</a>.</p>
<pre><code>model = sagemaker.model.Model(
    image=image
    model_data='s3://bucket/model.tar.gz',
    role=role_arn)
</code></pre>
<p>The options you're passing in are:</p>
<ul>
<li><code>image</code> - This is the ECR image you're using for inference (which should be for the algorithm you're trying to use). Paths are available <a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-algo-docker-registry-paths.html"" rel=""nofollow noreferrer"">here</a>.</li>
<li><code>model_data</code> - This is the path of where your model is stored (in a <code>tar.gz</code> compressed archive).</li>
<li><code>role</code> - This is the arn of a role that is capable of both pulling the image from ECR and getting the s3 archive.</li>
</ul>
<p>Once you've successfully done this you will need to setup an endpoint, this can be done by performing the following in your notebook through the <a href=""https://sagemaker.readthedocs.io/en/stable/api/inference/model.html#sagemaker.model.Model.deploy"" rel=""nofollow noreferrer"">deploy function</a>.</p>
<pre><code>model.deploy(
   initial_instance_count=1,
   instance_type='ml.p2.xlarge'
)
</code></pre>
<p>Please note, the above is for a pre-trained model but will also work for BYOS (bring your own script). More information is available <a href=""https://aws.amazon.com/blogs/startups/how-startups-deploy-pretrained-models-on-amazon-sagemaker/"" rel=""nofollow noreferrer"">here</a>.</p>
","13460933",2
987,62988722,2,62889537,2020-07-20 04:33:43,0,"<p>There are different versions for the framework containers. Since the framework version I'm using is 1.15, the image I got had to be in a tensorflow-inference container. If I used versions &lt;= 1.13, then I would get sagemaker-tensorflow-serving images. The two aren't the same, but there's no 'correct' container type.</p>
","13850966",0
988,62998014,2,57544237,2020-07-20 14:48:59,0,"<p>I encountered the same problem when trying to predict on text classification with a BlazingText container. What worked for me was simply changing the key in the payload while keeping the ContentType as application/json:</p>
<pre><code>sentence = &quot;I'm selling my PS4, practically brand new&quot;

payload = {&quot;instances&quot;: [sentence]}

response = client.invoke_endpoint(
        EndpointName=&quot;text_classification&quot;,
        Body=json.dumps(payload),
        ContentType='application/json'
        
    )
</code></pre>
<p>After playing around a little with the payload it seems that blazing text models only accept payloads as a dictionary with &quot;instances&quot; as its key and a list containing your data you want to predict on as its value.</p>
<p>To get to your predictions simply :</p>
<pre><code>print(&quot;ResponseMetadata:&quot;, response[&quot;ResponseMetadata&quot;])
print()
print(&quot;Body:&quot;, response['Body'].read())
</code></pre>
","12119886",0
989,63133627,2,63128111,2020-07-28 11:58:39,0,"<p>2 ideas:</p>
<ol>
<li>Before increasing the GPU count, grow batch size so that a single
GPU is as busy as possible</li>
<li>Use P3 instances instead of P2. P3 is more recent, has more memory, more CUDA cores, faster memory bandwidth and NVLink inter-GPU connections. Though it's more
expensive by hour, your total training cost may be much smaller if
properly tuned</li>
</ol>
<p>Also, if your problem involves sparse updates, meaning if just a small fraction of all tokens appear in a given mini-batch, you can try using <code>token_embedding_storage_type = 'row_sparse'</code>, which I think refers to using sparse gradient updates like described here <a href=""https://medium.com/apache-mxnet/learning-embeddings-for-music-recommendation-with-mxnets-sparse-api-5698f4d7d8"" rel=""nofollow noreferrer"">https://medium.com/apache-mxnet/learning-embeddings-for-music-recommendation-with-mxnets-sparse-api-5698f4d7d8</a></p>
","5331834",1
990,63134065,2,63115867,2020-07-28 12:23:30,12,"<p>In part of my answers I'll assume you refer to Sklearn's Isolation Forest. I believe those are the 4 main differences:</p>
<ol>
<li><p><strong>Code availability:</strong> Isolation Forest has a popular open-source implementation in Scikit-Learn (<a href=""https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.IsolationForest.html"" rel=""noreferrer""><code>sklearn.ensemble.IsolationForest</code></a>), while both AWS implementation of Robust Random Cut Forest (RRCF) are closed-source, in <a href=""https://docs.aws.amazon.com/kinesisanalytics/latest/sqlref/sqlrf-random-cut-forest.html"" rel=""noreferrer"">Amazon Kinesis</a> and <a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/randomcutforest.html"" rel=""noreferrer"">Amazon SageMaker</a>. There is an interesting third party RRCF open-source implementation on GitHub though: <a href=""https://github.com/kLabUM/rrcf"" rel=""noreferrer"">https://github.com/kLabUM/rrcf</a> ; but unsure how popular it is yet</p>
</li>
<li><p><strong>Training design:</strong> RRCF can work on streams, as highlighted in the paper and as exposed in the streaming analytics service Kinesis Data Analytics. On the other hand, the absence of <code>partial_fit</code> method hints me that Sklearn's Isolation Forest is a batch-only algorithm that cannot readily work on data streams</p>
</li>
<li><p><strong>Scalability:</strong> SageMaker RRCF is more scalable. Sklearn's Isolation Forest is single-machine code, which can nonetheless be parallelized over CPUs with the <code>n_jobs</code> parameter. On the other hand, SageMaker RRCF can be used over <a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-algo-docker-registry-paths.html"" rel=""noreferrer"">one machine or multiple machines</a>. Also, it supports SageMaker Pipe mode (streaming data via unix pipes) which makes it able to learn on much bigger data than what fits on disk</p>
</li>
<li><p><strong>the way features are sampled</strong> at each recursive isolation: RRCF gives more weight to dimension with higher variance (according to <a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/rcf_how-it-works.html"" rel=""noreferrer"">SageMaker doc</a>), while I think isolation forest samples at random, which is one reason why RRCF is expected to perform better in high-dimensional space (picture from the RRCF paper)
<a href=""https://i.stack.imgur.com/3FXmE.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/3FXmE.png"" alt=""enter image description here"" /></a></p>
</li>
</ol>
","5331834",4
991,63138417,2,63114905,2020-07-28 16:11:01,0,"<p>Found the problem thanks to AWS support:</p>
<p>I was creating an endpoint that already had an endpoint configuration with the same name and the new configuration wasn't being utilized.</p>
","3740545",2
992,63156401,2,62261222,2020-07-29 14:52:50,2,"<p>Okay, here is the answer:</p>
<ul>
<li>You work for company A which is on Azure.</li>
<li>You get access to company B's subscription.</li>
<li>Problem is: You are associated to A's AAD in ML-Studio.</li>
<li>You need to specify the tenant ID in the <code>InteractiveLoginAuthentication</code> like so:</li>
</ul>
<pre><code>interactive_auth = InteractiveLoginAuthentication(tenant_id=tenant_id)

workspace = Workspace.get(name=workspace_name,
                          subscription_id=subscription_id,
                          resource_group=resource_group,
                          auth=interactive_auth)
</code></pre>
<ul>
<li>Now the <strong>important</strong> part: You need to use company B's <code>tenant_id</code> (I used company A's all the time since I thought that was my authentication point)</li>
<li>Of course, this is obvious while you read it...as it is to me now :)</li>
</ul>
<p>Hope this helps you. Took me some time but learned a lot ;)</p>
","13518401",1
993,63157967,2,63145277,2020-07-29 16:14:30,1,"<p>Can you verify from the training job logs that your training script is running? It doesn't look like your Docker image would respond to the command <code>train</code>, which is what SageMaker requires, and so I suspect that your model isn't actually getting trained/saved to <code>/opt/ml/model</code>.</p>
<p>AWS documentation about how SageMaker runs the Docker container: <a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/your-algorithms-training-algo-dockerfile.html"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/sagemaker/latest/dg/your-algorithms-training-algo-dockerfile.html</a></p>
<p>edit: summarizing from the comments below - the training script must also save the model to <code>/opt/ml/model</code> (the model isn't saved automatically).</p>
","9074534",6
994,63158069,2,63138835,2020-07-29 16:19:31,2,"<p>There is no way to have your local mode training jobs appear in the AWS console. The intent of local mode is to allow for faster iteration/debugging before using SageMaker for training your model.</p>
<p>You can create SageMaker Models from local model artifacts. Compress your model artifacts into a <code>.tar.gz</code> file, upload that file to S3, and then create the Model (with the SDK or in the console).</p>
<p>Documentation:</p>
<ul>
<li><a href=""https://sagemaker.readthedocs.io/en/stable/overview.html#using-models-trained-outside-of-amazon-sagemaker"" rel=""nofollow noreferrer"">https://sagemaker.readthedocs.io/en/stable/overview.html#using-models-trained-outside-of-amazon-sagemaker</a></li>
<li><a href=""https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_CreateModel.html"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_CreateModel.html</a></li>
</ul>
","9074534",0
995,63164910,2,63162310,2020-07-30 01:40:57,1,"<p>This is known issue as the models registered in workspace cannot be consumed in Designer without the new custom module capability (in private preview) available.</p>
<p>The models showing up in Designer today are these generated from Designer training -&gt; inference pipeline conversion and can only be used in Designer (not registered in the workspace).
We have an effort ongoing to reduce the confusion.</p>
","11297406",0
996,63171173,2,63127521,2020-07-30 10:23:55,0,"<p>It seems it was a bug, got corrected by itself today. Closing this question now</p>
","8595619",0
997,63182068,2,63179080,2020-07-30 22:07:25,10,"<p>SageMaker Notebooks home is on <code>/home/ec2-user/SageMaker</code></p>
<ul>
<li>Everything you send to <code>/home/ec2-user/SageMaker</code> will be visible in
the Jupyter home page</li>
<li>Everything you upload in the Jupyter home page
will be visible in the terminal via <code>ls /home/ec2-user/SageMaker</code></li>
<li>The content of <code>/home/ec2-user/SageMaker</code> is persisted in a storage volume called the &quot;ML Storage Volume&quot;, that is charged additionally to the
instance compute pricing and defaults at 5GB. It can be up to 16TB in
size. Content saved there stays persisted even when you switch off
the notebook instance. On the other hand, anything you save anywhere
else will be lost when you switch off the instance</li>
</ul>
","5331834",1
998,63199628,2,63199239,2020-07-31 22:01:21,1,"<p>The <code>/home/ec2-user/SageMaker</code> location is persisted even when you switch down the notebook instance, you can try saving things here to get them persisted. Things saved elsewhere will be lost when you switch off the instance</p>
<p>Regarding private git integration, you can use the <a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/nbi-git-repo.html"" rel=""nofollow noreferrer"">SageMaker git Notebook integration</a>, which uses Secrets Manager to safely handle your credentials</p>
<p>You can perform steps automatically when the notebook starts with a <a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/notebook-lifecycle-config.html"" rel=""nofollow noreferrer"">lifecycle configuration</a>. This is useful for example to standardise and automatise copying of data and environment customization</p>
","5331834",0
999,63234818,2,63204081,2020-08-03 18:16:11,1,"<p>A couple of things to check:</p>
<ol>
<li>Is your model registered in the workspace? AZUREML_MODEL_DIR only works for registered models. See <a href=""https://learn.microsoft.com/en-us/python/api/azureml-core/azureml.core.model.model?view=azure-ml-py#register-workspace--model-path--model-name--tags-none--properties-none--description-none--datasets-none--model-framework-none--model-framework-version-none--child-paths-none--sample-input-dataset-none--sample-output-dataset-none--resource-configuration-none-"" rel=""nofollow noreferrer"">this link</a> for information about registering a model</li>
<li>Are you specifying the same version of pyspark.ml.recommendation in your InferenceConfig as you use locally? This kind of error might be due to a difference in versions</li>
<li>Have you looked at the output of <code>print(service.get_logs())</code>? Check out our <a href=""https://review.learn.microsoft.com/en-us/azure/machine-learning/how-to-troubleshoot-deployment?branch=pr-en-us-124666"" rel=""nofollow noreferrer"">troubleshoot and debugging documentation here</a> for other things you can try</li>
</ol>
","10664974",1
1000,63250423,2,63243154,2020-08-04 15:43:49,0,"<p>The clue is in the final few lines of your stacktrace:</p>
<pre><code>  File &quot;/opt/ml/model/code/pytorch-model-reco.py&quot;, line 268, in predict_fn
    return torch.argsort(- final_matrix[input_data, :], dim = 1)
IndexError: tensors used as indices must be long, byte or bool tensors
</code></pre>
<p>In your <code>predict_fn</code> in <code>pytorch-model-reco.py</code> on line 268, you're trying to use <code>input_data</code> as indices for <code>final_matrix</code>, but <code>input_data</code> is the wrong type.</p>
<p>I would guess there is some type casting that your <code>predict_fn</code> should be doing when the input type is <code>text/csv</code>. This type casting is happening outside of the <code>predict_fn</code> when your input type is numpy data. Taking a look at the <a href=""https://github.com/aws/sagemaker-inference-toolkit/tree/master/src/sagemaker_inference"" rel=""nofollow noreferrer""><code>sagemaker_inference</code></a> source code might reveal more.</p>
","2596715",0
1001,63251861,2,61520346,2020-08-04 17:10:17,7,"<p>This indicates that there is a problem finding the Docker service.</p>
<p>By default, the Docker is not installed in the SageMaker Studio  (<a href=""https://github.com/aws/sagemaker-python-sdk/issues/656#issuecomment-632170943"" rel=""nofollow noreferrer"">confirming github ticket response</a>).</p>
","2687601",0
1002,63264076,2,63255631,2020-08-05 11:19:29,30,"<p>Mlflow required DB as datastore for Model Registry
So you have to run tracking server with DB as backend-store and log model to this tracking server.
The easiest way to use DB is to use SQLite.</p>
<pre><code>mlflow server \
    --backend-store-uri sqlite:///mlflow.db \
    --default-artifact-root ./artifacts \
    --host 0.0.0.0
</code></pre>
<p>And set MLFLOW_TRACKING_URI environment variable to <em>http://localhost:5000</em> or</p>
<pre><code>mlflow.set_tracking_uri(&quot;http://localhost:5000&quot;)
</code></pre>
<p>After got to http://localhost:5000 and you can register a logged model from UI or from the code.</p>
","12396281",5
1003,63269194,2,63268849,2020-08-05 16:08:25,1,"<p>This problem has stomped me for hours, but I was finally able to fix it. What I did was I opened a terminal and did a Jupyter lab rebuild &quot;jupyter lab build&quot;</p>
<p><a href=""https://imgur.com/aRB8GWS.png"" rel=""nofollow noreferrer""><img src=""https://imgur.com/aRB8GWS.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.stack.imgur.com/IceQO.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/IceQO.png"" alt=""enter image description here"" /></a></p>
","1465073",2
1004,63278264,2,63248562,2020-08-06 06:51:08,2,"<p><strong>Solution:</strong> The problem was solved by saving the dataframe as .csv using the arguments header=False, index=False. This makes the saved csv not include the dataframe indexing labels. TFS accepted a clean .csv with only float values (without labels). I assume the error message <em>Invalid argument: JSON Value: &quot;&quot; Type: String is not of expected type: float</em> refers to the first cell in the csv, which if the csv was exported with labels is just an empty cell. When it got an empty string instead of a float value it got confused.</p>
","14048343",1
1005,63296331,2,63296185,2020-08-07 06:24:13,1,"<p>I don’t think ADLS is supported for <code>PipelineData</code>. My suggestion is to use the workspace’s default blob store for the <code>PipelineData</code>, then use a <code>DataTransferStep</code> for after the <code>ParallelRunStep</code> is completed.</p>
","3842610",6
1006,63297039,2,63296520,2020-08-07 07:19:27,2,"<p>I'm not sure what you mean by <code>TFRecordDataset</code> is hard to read. But her is an example of how I would use my TFRecord data. <code>Feature_description</code> contains features that each sample in TFRecord holds (and their data type) Once you load the records this way, you can do all sort of stuff with it including batching, augmenting, shuffling in the pipeline or  accessing the individual files, converting them to numpy etc.</p>
<pre><code>import tensorflow as tf
import numpy as np
from PIL import Image

filenames = []
for i in range(128):
    name = &quot;./../result/validation-%.5d-of-%.5d&quot; % (i, 128)
    filenames.append(name)

def read_tfrecord(serialized_example):
    feature_description = {
            'image/height': tf.io.FixedLenFeature((), tf.int64),
            'image/width': tf.io.FixedLenFeature((), tf.int64),
            'image/colorspace': tf.io.FixedLenFeature((), tf.string),
            'image/channels': tf.io.FixedLenFeature((), tf.int64),
            'image/class/label': tf.io.FixedLenFeature((), tf.int64),
            'image/encoded': tf.io.FixedLenFeature((), tf.string),
    }

    parsed_features = tf.io.parse_single_example(serialized_example, feature_description)

    parsed_features['image/encoded'] = tf.io.decode_jpeg(
            parsed_features['image/encoded'], channels=3)

    return parsed_features



data = tf.data.TFRecordDataset(filenames)


parsed_dataset = data.shuffle(128).map(read_tfrecord).batch(128)


for sample in parsed_dataset.take(1):
        numpyed = sample['image/encoded'].numpy()
        img = Image.fromarray(numpyed, 'RGB')
        img.show()
        tf.print(sample['image/class/label'])
</code></pre>
","9347417",1
1007,63304097,2,63304005,2020-08-07 14:45:24,0,"<p>I managed to get sudo working in the &quot;System Terminal&quot; instead. The image terminals don't seem to have access to sudo.</p>
<p>Unrelated: But then when I tried to mount EFS onto the SageMaker studio app, it simply failed, saying mount target is not a directory. Looks like I'm not using Sagemaker Studio this year.</p>
","7365866",2
1008,63305570,2,63305569,2020-08-07 16:13:59,1,"<p>Update: I spoke to an AWS Solutions Architect, and he confirms that EFS is not supported on Sagemaker Studio.</p>
<hr />
<p><strong>Workaround:</strong></p>
<p>Instead of mounting your old EFS, you can mount the SageMaker studio EFS onto an EC2 instance, and copy over the data manually. You would need the correct EFS storage volume id, and you'll find your newly copied data available in Sagemaker Studio. <em>I have not actually done this though.</em></p>
<p>To find the EFS id, look at the section &quot;Manage your storage volume&quot; <a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/studio-tasks.html#manage-your-storage-volume"" rel=""nofollow noreferrer"">here</a>.</p>
","7365866",3
1009,63306249,2,62975940,2020-08-07 17:00:25,0,"<p>Throughput is reported in records per second (i.e. images per second). It shows how fast the algorithm can iterate over training or validation data. For example, with a throughput of 30 records/sec it would take a minute to iterate over 1800 images.</p>
","10208961",0
1010,63307133,2,63306816,2020-08-07 18:08:34,6,"<p>We've been working and recently released a <code>dask_cloudprovider.AzureMLCluster</code> that might be of interest to you: <a href=""https://github.com/dask/dask-cloudprovider"" rel=""noreferrer"">link to repo</a>. You can install it via <code>pip install dask-cloudprovider</code>.</p>
<p>The <code>AzureMLCluster</code> instantiates Dask cluster on AzureML service with elasticity of scaling up to 100s of nodes should you require that. The only required parameter is the <code>Workspace</code> object, but you can pass your own <code>ComputeTarget</code> should you choose to.</p>
<p>An example of how to use it you can <a href=""https://github.com/drabastomek/GTC/blob/master/SJ_2020/workshop/1_Setup/Setup.ipynb"" rel=""noreferrer"">found here</a>. In this example I use my custom GPU/RAPIDS docker image but you can use any images within the <code>Environment</code> class.</p>
","1157754",1
1011,63330748,2,63096583,2020-08-09 19:56:44,1,"<p>I resolved with <a href=""https://medium.com/swlh/upload-binary-files-to-s3-using-aws-api-gateway-with-aws-lambda-2b4ba8c70b8e"" rel=""nofollow noreferrer"">this</a> post:</p>
<p>Thank all</p>
<p>Finally the code in lambda function is:</p>
<pre><code>import os
import boto3
import json
import base64

ENDPOINT_NAME = os.environ['endPointName']
CLASSES = &quot;[&quot;chair&quot;, &quot;handbag&quot;, &quot;person&quot;, &quot;traffic light&quot;, &quot;clock&quot;]&quot;
runtime= boto3.client(&quot;runtime.sagemaker&quot;)

def lambda_handler(event, context):
    file_content = base64.b64decode(event['content'])

    payload = file_content
    response = runtime.invoke_endpoint(EndpointName=ENDPOINT_NAME, ContentType=&quot;application/x-image&quot;, Body=payload)

    result = json.loads(response[&quot;Body&quot;].read().decode())
    print(result)
    predicted_label=[]
    classes = CLASSES
    for idx, val in enumerate(classes):
       print(&quot;%s:%f &quot;%(classes[idx], result[idx]), end=&quot;&quot;)
       predicted_label += (classes[idx], result[idx])

    return {
      &quot;statusCode&quot;: 200,
      &quot;headers&quot;: { &quot;content-type&quot;: &quot;application/json&quot;},
      &quot;body&quot;:  predicted_label
    }
</code></pre>
","11908864",0
1012,63362864,2,63361229,2020-08-11 16:55:41,3,"<p>This is, indeed, to do with special character representation in different os' based on <a href=""https://github.com/aws-samples/amazon-sagemaker-notebook-instance-lifecycle-config-samples/issues/8"" rel=""nofollow noreferrer"">this</a> you can use notepad++ to easily convert the dos representation a unix representation, then just &quot;paste as plain text&quot;, and it works fine</p>
<ul>
<li>copy to notepad++ view</li>
<li>show symbol</li>
<li>show all symbols</li>
<li>replace &quot;/r&quot; with nothing CRLF should become LF which is valid in unix</li>
<li>copy and paste as plain text</li>
</ul>
<p>Doing this fixed the problem</p>
","10818367",0
1013,63363079,2,63328246,2020-08-11 17:07:57,1,"<p>If your image is binary-encoded, you could try this:</p>
<pre><code>import boto3 
import matplotlib.pyplot as plt 

# Define Bucket and Key 
s3_bucket, s3_key = 'YOUR_BUCKET', 'YOUR_IMAGE_KEY'

with BytesIO() as f:
    boto3.client(&quot;s3&quot;).download_fileobj(Bucket=s3_bucket, Key=s3_key, Fileobj=f)
    f.seek(0)
    img = plt.imread(f, format='png')
</code></pre>
<p>in other case, the following code works out (based on the <a href=""https://boto3.amazonaws.com/v1/documentation/api/1.9.42/guide/s3-example-download-file.html"" rel=""nofollow noreferrer"">documentation</a>):</p>
<pre><code>s3 = boto3.resource('s3')

img = s3.Bucket(s3_bucket).download_file(s3_key, 'local_image.jpg')
</code></pre>
<p>In both cases, you can visualize the image with <code>plt.imshow(img)</code>.</p>
<p>In your path example <code>path = 's3://iphone/Test/10.png'</code>, the bucket and key will be <code>s3_bucket = 'iphone'</code> and <code>s3_key=Test/10.png</code></p>
<p>Additional Resources: <a href=""https://boto3.amazonaws.com/v1/documentation/api/latest/guide/s3-example-download-file.html"" rel=""nofollow noreferrer"">https://boto3.amazonaws.com/v1/documentation/api/latest/guide/s3-example-download-file.html</a></p>
","12483346",0
1014,63374548,2,63187116,2020-08-12 10:22:56,1,"<p>After the good suggestion of @jean-sébastien and an answer to an isolated question for the <a href=""https://stackoverflow.com/questions/63357901/how-to-convert-a-dictionary-like-structure-in-azure-stream-analytics-to-a-mult/63373103#63373103"">array-parsing</a>, I finally was able to stitch everything together in a solution that builds.  (still have to get it to run at runtime, though).</p>
<p>So, the solution exists in using <code>CollectTop</code> to aggregate the latest rows of the entity you want to group by, including the specification of a Time Window.</p>
<p>And the next step was to create the javascript UDF to take that data structure and parse it into a multi-dimensional array.</p>
<p>This is the query I have right now:</p>
<pre class=""lang-sql prettyprint-override""><code>-- Taking relevant fields from the input stream
WITH RelevantTelemetry AS
(
    SELECT  engineid, tmp, hum, eventtime
    FROM    [engine-telemetry] 
    WHERE   engineid IS NOT NULL
),
-- Grouping by engineid in TimeWindows
TimeWindows AS
(
    SELECT engineid, 
        CollectTop(2) OVER (ORDER BY eventtime DESC) as TimeWindow
    FROM
        [RelevantTelemetry]
    WHERE engineid IS NOT NULL
    GROUP BY TumblingWindow(hour, 24), engineid
)
--Output timewindows for verification purposes
SELECT engineid, Udf.Predict(Udf.getTimeWindows(TimeWindow)) as Prediction
INTO debug
FROM TimeWindows
</code></pre>
<p>And this is the Javascript UDF:</p>
<pre class=""lang-js prettyprint-override""><code>    function getTimeWindows(input){
        var output = [];
        for(var x in input){
            var array = [];
            array.push(input[x].value.tmp);
            array.push(input[x].value.hum);
            output.push(array);
        }
        return output;
    }
</code></pre>
","2063902",0
1015,63397212,2,63374642,2020-08-13 14:20:31,0,"<p>I've managed to make it work, but not in the cleanest way.</p>
<p>The reason I got this message is that the TextVectorization layer in the model will only accept a tensor (a dense one it seems), a numpy array, a list or a dataset. So I give him what he want by adapting my code like this (this is the full function updated):</p>
<pre><code>def run_fn(fn_args: TrainerFnArgs):
  &quot;&quot;&quot;Train the model based on given args.

  Args:
    fn_args: Holds args used to train the model as name/value pairs.
  &quot;&quot;&quot;
  tf_transform_output = tft.TFTransformOutput(fn_args.transform_output)
  
  train_dataset = _input_fn(fn_args.train_files, tf_transform_output, 40)
  eval_dataset = _input_fn(fn_args.eval_files, tf_transform_output, 40)
  vectorize_dataset = train_dataset.map(lambda f, l: tf.sparse.to_dense(f[_transformed_name('Text')])).unbatch()
  
  vectorize_layer = TextVectorization(
    max_tokens=_max_features, 
    output_mode='int',
    output_sequence_length=500
  )
  vectorize_layer.adapt(vectorize_dataset.take(900))
  model = _build_keras_model(vectorize_layer)

  log_dir = os.path.join(os.path.dirname(fn_args.serving_model_dir), 'logs')
  tensorboard_callback = tf.keras.callbacks.TensorBoard(
      log_dir=log_dir, update_freq='batch')

  model.fit(
      train_dataset.map(lambda f, l: (tf.sparse.to_dense(f[_transformed_name('Text')]), l)),
      steps_per_epoch=fn_args.train_steps,
      validation_data=eval_dataset.map(lambda f, l: (tf.sparse.to_dense(f[_transformed_name('Text')]), l)),
      validation_steps=fn_args.eval_steps,
      callbacks=[tensorboard_callback])

  signatures = {
      'serving_default':
          _get_serve_tf_examples_fn(model,
                                    tf_transform_output).get_concrete_function(
                                        tf.TensorSpec(
                                            shape=[None],
                                            dtype=tf.string,
                                            name='examples')),
  }
  model.save(fn_args.serving_model_dir, save_format='tf', signatures=signatures)
</code></pre>
<p>Notice the map functions in the parameters of the fit function. The rest stayed the same (pretty much, I just adjusted the shape in the input layer and tweeked the model to get better results).</p>
<p>I wonder if there is an easier way to achieve this and still keep the benefits of SparseTensor.</p>
","4360683",0
1016,63400885,2,63380531,2020-08-13 18:06:10,1,"<p>It does look like these commands are currently in the CLI or the Python SDK. The CLI uses the Python SDK, so what's missing from one does tend to be missing from the other.</p>
<p>Fortunately, you can invoke the rest endpoints directly, either in Python or by using the <code>az rest</code> command in the CLI.</p>
<p>There are a few commands that may interest you:</p>
<p><a href=""https://learn.microsoft.com/en-us/rest/api/azureml/workspacesandcomputes/usages/list"" rel=""nofollow noreferrer"">Usage</a> and Quotas for a region:
<code>/subscriptions/{subscriptionId}/providers/Microsoft.MachineLearningServices/locations/{location}/usages?api-version=2019-05-01</code>
<code>/subscriptions/{subscriptionId}/providers/Microsoft.MachineLearningServices/locations/{location}/quotas?api-version=2020-04-01</code></p>
<p>The process for updating REST specs to the offical documentation is fairly lengthy so it isn't published yet, but if you are willing to use Swagger docs to explore what is available, the 2020-06-01 version of the API is on Github, which includes endpoints for updating quotas as well as retrieving them: <a href=""https://github.com/Azure/azure-rest-api-specs/tree/master/specification/machinelearningservices/resource-manager/Microsoft.MachineLearningServices/stable/2020-06-01"" rel=""nofollow noreferrer"">https://github.com/Azure/azure-rest-api-specs/tree/master/specification/machinelearningservices/resource-manager/Microsoft.MachineLearningServices/stable/2020-06-01</a></p>
","11002318",3
1017,63405081,2,63405080,2020-08-14 00:54:28,1,"<p>When running SageMaker in a local Jupyter notebook, it expects the Docker container to be running on the local machine as well.</p>
<p>The key to ensuring that SageMaker (running in a local notebook) uses the AWS hosted docker container, is to omit the <code>LocalSession</code> object when initializing the <code>Estimator</code>.</p>
<h2>Wrong</h2>
<pre><code>xgb_estimator = sagemaker.estimator.Estimator(
    container, role, train_instance_count=1, train_instance_type=instance_type,
    output_path=f's3://{session.default_bucket()}/{prefix}/output', sagemaker_session=session)
</code></pre>
<h2>Correct</h2>
<pre><code>xgb_estimator = sagemaker.estimator.Estimator(
    container, role, train_instance_count=1, train_instance_type=instance_type,
    output_path=f's3://{session.default_bucket()}/{prefix}/output')
</code></pre>
<p>  </p>
<h2>Additional info</h2>
<p>The SageMaker Python SDK source code provides the following helpful hints:</p>
<h1>File: <em>sagemaker/local/local_session.py</em></h1>
<pre><code>class LocalSagemakerClient(object):
    &quot;&quot;&quot;A SageMakerClient that implements the API calls locally.

    Used for doing local training and hosting local endpoints. It still needs access to
    a boto client to interact with S3 but it won't perform any SageMaker call.
    ...
</code></pre>
<h1>File: <em>sagemaker/estimator.py</em></h1>
<pre><code>class EstimatorBase(with_metaclass(ABCMeta, object)):
    &quot;&quot;&quot;Handle end-to-end Amazon SageMaker training and deployment tasks.

    For introduction to model training and deployment, see
    http://docs.aws.amazon.com/sagemaker/latest/dg/how-it-works-training.html

    Subclasses must define a way to determine what image to use for training,
    what hyperparameters to use, and how to create an appropriate predictor instance.
    &quot;&quot;&quot;

    def __init__(self, role, train_instance_count, train_instance_type,
                 train_volume_size=30, train_max_run=24 * 60 * 60, input_mode='File',
                 output_path=None, output_kms_key=None, base_job_name=None, sagemaker_session=None, tags=None):
        &quot;&quot;&quot;Initialize an ``EstimatorBase`` instance.

        Args:
            role (str): An AWS IAM role (either name or full ARN). ...
            
        ...

            sagemaker_session (sagemaker.session.Session): Session object which manages interactions with
                Amazon SageMaker APIs and any other AWS services needed. If not specified, the estimator creates one
                using the default AWS configuration chain.
        &quot;&quot;&quot;
</code></pre>
","1164465",0
1018,63424771,2,63297046,2020-08-15 10:00:20,1,"<p>Answering my own question since I've made some progress on them.</p>
<h2>I cannot update the ServiceRoleBinding even after I deleted the validating webhook</h2>
<p>That's because the ServiceRoleBinding is actually generated/monitored/managed by the <strong>profile controller</strong> in the kubeflow namespace instead of the <strong>validating webhook</strong>.</p>
<p>I'm having this rbac issue because based on the params.yaml in the profiles manifest folder the rule is generated as</p>
<pre><code>request.headers[]: roger.l.c.lei@XXXX.com
</code></pre>
<p>instead of</p>
<pre><code>request.headers[kubeflow-userid]: roger.l.c.lei@XXXX.com
</code></pre>
<p>Due to I mis-configed the value as blank instead of <em>userid-header=kubeflow-userid</em> in the params.yaml</p>
","1516376",0
1019,63442496,2,63403985,2020-08-16 22:27:27,3,"<p>The issue was an incompatible OS version in my virtual environment.</p>
<p>A huge thanks goes to <a href=""https://learn.microsoft.com/answers/users/111253/pramodvalavala-msft.html"" rel=""nofollow noreferrer"">PramodValavala-MSFT</a> for his idea to create a docker container! Following his suggestion, I suddenly got the following error message for the  <code>dataset = Dataset.Tabular.from_delimited_files(path = datastore_paths)</code> command:</p>
<blockquote>
<p>Exception: NotImplementedError: Unsupported Linux distribution debian 10.</p>
</blockquote>
<p>which reminded me of the following warning in the azure machine learning documentation:</p>
<blockquote>
<p>Some dataset classes have dependencies on the azureml-dataprep
package, which is only compatible with 64-bit Python. For Linux users,
these classes are supported only on the following distributions: Red
Hat Enterprise Linux (7, 8), Ubuntu (14.04, 16.04, 18.04), Fedora (27,
28), Debian (8, 9), and CentOS (7).</p>
</blockquote>
<p>Choosing the predefined docker image <code>2.0-python3.7</code> (running Debian 9) instead of  <code>3.0-python3.7</code> (running Debian 10) solved the issue (see <a href=""https://hub.docker.com/_/microsoft-azure-functions-python"" rel=""nofollow noreferrer"">https://hub.docker.com/_/microsoft-azure-functions-python</a>).</p>
<p>I suspect that the default virtual environment, which I was using originally, also ran on an incompatible OS.</p>
","14101480",0
1020,63450946,2,63415102,2020-08-17 12:35:44,2,"<p>I got it to work. My problem was that in the data set the label (the document category) is in a string format (e.g: &quot;sport&quot;, &quot;business&quot;,...). So to encode it as an integer I used the Transform component to preprocess it.</p>
<p>However, when building the evaluator component I passed the ExampleGen component where no processing were done on the data. So the evaluator was trying to cast the string from the ExampleGen to match the integer output from the model.</p>
<p>So, to fix this I simply did this:</p>
<pre><code>model_resolver = ResolverNode(
      instance_name='latest_blessed_model_resolver',
      resolver_class=latest_blessed_model_resolver.LatestBlessedModelResolver,
      model=Channel(type=Model),
      model_blessing=Channel(type=ModelBlessing))
context.run(model_resolver)

evaluator = Evaluator(
    examples=transform.outputs['transformed_examples'],
    model=trainer.outputs['model'],
    baseline_model=model_resolver.outputs['model'],
    eval_config=eval_config)
context.run(evaluator)
</code></pre>
<p>I used the examples from the transform component. Of course I also changed the label key in the config to match the label name of the transform component.</p>
<p>I don't know if there is a 'cleaner' way to perform this (or if I'm doing this all wrong please correct me!)</p>
","4360683",0
1021,63457041,2,63457040,2020-08-17 18:57:02,2,"<p>The original <a href=""https://dl.acm.org/doi/10.1145/3097983.3098021"" rel=""nofollow noreferrer"">ACM KDD '17 TFX paper</a> introduces the capabilities of TFX and how they enable deploying ML in production at scale. It is worth a read.
For a more recent coverage,
<a href=""https://www.oreilly.com/library/view/building-machine-learning/9781492053187/"" rel=""nofollow noreferrer""><strong>Building Machine Learning Pipelines</strong></a> by Hannes Hapke and Catherine Nelson, ISBN: 9781492053194, published by O'Reilly Media, Inc. in July 2020 covers the best practices quite well.</p>
<p>Here's the table of contents (courtesy of O'Reilly Media).</p>
<pre><code>Foreword
Preface
What Are Machine Learning Pipelines?
Who Is This Book For?
Why TensorFlow and TensorFlow Extended?
Overview of the Chapters
Conventions Used in This Book
Using Code Examples
O’Reilly Online Learning
How to Contact Us
Acknowledgments
1. Introduction
Why Machine Learning Pipelines?
When to Think About Machine Learning Pipelines
Overview of the Steps in a Machine Learning Pipeline
Data Ingestion and Data Versioning
Data Validation
Data Preprocessing
Model Training and Tuning
Model Analysis
Model Versioning
Model Deployment
Feedback Loops
Data Privacy
Pipeline Orchestration
Why Pipeline Orchestration?
Directed Acyclic Graphs
Our Example Project
Project Structure
Our Machine Learning Model
Goal of the Example Project
Summary
2. Introduction to TensorFlow Extended
What Is TFX?
Installing TFX
Overview of TFX Components
What Is ML Metadata?
Interactive Pipelines
Alternatives to TFX
Introduction to Apache Beam
Setup
Basic Data Pipeline
Executing Your Basic Pipeline
Summary
3. Data Ingestion
Concepts for Data Ingestion
Ingesting Local Data Files
Ingesting Remote Data Files
Ingesting Data Directly from Databases
Data Preparation
Splitting Datasets
Spanning Datasets
Versioning Datasets
Ingestion Strategies
Structured Data
Text Data for Natural Language Problems
Image Data for Computer Vision Problems
Summary
4. Data Validation
Why Data Validation?
TFDV
Installation
Generating Statistics from Your Data
Generating Schema from Your Data
Recognizing Problems in Your Data
Comparing Datasets
Updating the Schema
Data Skew and Drift
Biased Datasets
Slicing Data in TFDV
Processing Large Datasets with GCP
Integrating TFDV into Your Machine Learning Pipeline
Summary
5. Data Preprocessing
Why Data Preprocessing?
Preprocessing the Data in the Context of the Entire Dataset
Scaling the Preprocessing Steps
Avoiding a Training-Serving Skew
Deploying Preprocessing Steps and the ML Model as One Artifact
Checking Your Preprocessing Results in Your Pipeline
Data Preprocessing with TFT
Installation
Preprocessing Strategies
Best Practices
TFT Functions
Standalone Execution of TFT
Integrate TFT into Your Machine Learning Pipeline
Summary
6. Model Training
Defining the Model for Our Example Project
The TFX Trainer Component
run_fn() Function
Running the Trainer Component
Other Trainer Component Considerations
Using TensorBoard in an Interactive Pipeline
Distribution Strategies
Model Tuning
Strategies for Hyperparameter Tuning
Hyperparameter Tuning in TFX Pipelines
Summary
7. Model Analysis and Validation
How to Analyze Your Model
Classification Metrics
Regression Metrics
TensorFlow Model Analysis
Analyzing a Single Model in TFMA
Analyzing Multiple Models in TFMA
Model Analysis for Fairness
Slicing Model Predictions in TFMA
Checking Decision Thresholds with Fairness Indicators
Going Deeper with the What-If Tool
Model Explainability
Generating Explanations with the WIT
Other Explainability Techniques
Analysis and Validation in TFX
ResolverNode
Evaluator Component
Validation in the Evaluator Component
TFX Pusher Component
Summary
8. Model Deployment with TensorFlow Serving
A Simple Model Server
The Downside of Model Deployments with Python-Based APIs
Lack of Code Separation
Lack of Model Version Control
Inefficient Model Inference
TensorFlow Serving
TensorFlow Architecture Overview
Exporting Models for TensorFlow Serving
Model Signatures
Inspecting Exported Models
Setting Up TensorFlow Serving
Docker Installation
Native Ubuntu Installation
Building TensorFlow Serving from Source
Configuring a TensorFlow Server
REST Versus gRPC
Making Predictions from the Model Server
Getting Model Predictions via REST
Using TensorFlow Serving via gRPC
Model A/B Testing with TensorFlow Serving
Requesting Model Metadata from the Model Server
REST Requests for Model Metadata
gRPC Requests for Model Metadata
Batching Inference Requests
Configuring Batch Predictions
Other TensorFlow Serving Optimizations
TensorFlow Serving Alternatives
BentoML
Seldon
GraphPipe
Simple TensorFlow Serving
MLflow
Ray Serve
Deploying with Cloud Providers
Use Cases
Example Deployment with GCP
Model Deployment with TFX Pipelines
Summary
9. Advanced Model Deployments with TensorFlow Serving
Decoupling Deployment Cycles
Workflow Overview
Optimization of Remote Model Loading
Model Optimizations for Deployments
Quantization
Pruning
Distillation
Using TensorRT with TensorFlow Serving
TFLite
Steps to Optimize Your Model with TFLite
Serving TFLite Models with TensorFlow Serving
Monitoring Your TensorFlow Serving Instances
Prometheus Setup
TensorFlow Serving Configuration
Simple Scaling with TensorFlow Serving and Kubernetes
Summary
10. Advanced TensorFlow Extended
Advanced Pipeline Concepts
Training Multiple Models Simultaneously
Exporting TFLite Models
Warm Starting Model Training
Human in the Loop
Slack Component Setup
How to Use the Slack Component
Custom TFX Components
Use Cases of Custom Components
Writing a Custom Component from Scratch
Reusing Existing Components
Summary
11. Pipelines Part 1: Apache Beam and Apache Airflow
Which Orchestration Tool to Choose?
Apache Beam
Apache Airflow
Kubeflow Pipelines
Kubeflow Pipelines on AI Platform
Converting Your Interactive TFX Pipeline to a Production Pipeline
Simple Interactive Pipeline Conversion for Beam and Airflow
Introduction to Apache Beam
Orchestrating TFX Pipelines with Apache Beam
Introduction to Apache Airflow
Installation and Initial Setup
Basic Airflow Example
Orchestrating TFX Pipelines with Apache Airflow
Pipeline Setup
Pipeline Execution
Summary
12. Pipelines Part 2: Kubeflow Pipelines
Introduction to Kubeflow Pipelines
Installation and Initial Setup
Accessing Your Kubeflow Pipelines Installation
Orchestrating TFX Pipelines with Kubeflow Pipelines
Pipeline Setup
Executing the Pipeline
Useful Features of Kubeflow Pipelines
Pipelines Based on Google Cloud AI Platform
Pipeline Setup
TFX Pipeline Setup
Pipeline Execution
Summary
13. Feedback Loops
Explicit and Implicit Feedback
The Data Flywheel
Feedback Loops in the Real World
Design Patterns for Collecting Feedback
Users Take Some Action as a Result of the Prediction
Users Rate the Quality of the Prediction
Users Correct the Prediction
Crowdsourcing the Annotations
Expert Annotations
Producing Feedback Automatically
How to Track Feedback Loops
Tracking Explicit Feedback
Tracking Implicit Feedback
Summary
14. Data Privacy for Machine Learning
Data Privacy Issues
Why Do We Care About Data Privacy?
The Simplest Way to Increase Privacy
What Data Needs to Be Kept Private?
Differential Privacy
Local and Global Differential Privacy
Epsilon, Delta, and the Privacy Budget
Differential Privacy for Machine Learning
Introduction to TensorFlow Privacy
Training with a Differentially Private Optimizer
Calculating Epsilon
Federated Learning
Federated Learning in TensorFlow
Encrypted Machine Learning
Encrypted Model Training
Converting a Trained Model to Serve Encrypted Predictions
Other Methods for Data Privacy
Summary
15. The Future of Pipelines and Next Steps
Model Experiment Tracking
Thoughts on Model Release Management
Future Pipeline Capabilities
TFX with Other Machine Learning Frameworks
Testing Machine Learning Models
CI/CD Systems for Machine Learning
Machine Learning Engineering Community
Summary
A. Introduction to Infrastructure for Machine Learning
What Is a Container?
Introduction to Docker
Introduction to Docker Images
Building Your First Docker Image
Diving into the Docker CLI
Introduction to Kubernetes
Some Kubernetes Definitions
Getting Started with Minikube and kubectl
Interacting with the Kubernetes CLI
Defining a Kubernetes Resource
Deploying Applications to Kubernetes
B. Setting Up a Kubernetes Cluster on Google Cloud
Before You Get Started
Kubernetes on Google Cloud
Selecting a Google Cloud Project
Setting Up Your Google Cloud Project
Creating a Kubernetes Cluster
Accessing Your Kubernetes Cluster with kubectl
Using Your Kubernetes Cluster with kubectl
Persistent Volume Setups for Kubeflow Pipelines
C. Tips for Operating Kubeflow Pipelines
Custom TFX Images
Exchange Data Through Persistent Volumes
TFX Command-Line Interface
TFX and Its Dependencies
TFX Templates
Publishing Your Pipeline with TFX CLI

</code></pre>
","1397927",0
1022,63473731,2,63441299,2020-08-18 17:28:38,1,"<p>Okay, so after 2 days of continuous debugging was able to find out the root cause.
What I understood is Flair does not have any limitation on the sentence length, in the sense the word count, it is taking the highest length sentence as the maximum.
So there it was causing issue, as in my case there were few content with 1.5 lakh rows which is too much to load the embedding of into the memory, even a 16GB GPU.
So there it was breaking.</p>
<p><strong>To solve this</strong>: For content with this much lengthy words, you can take chunk of n words(10K in my case) from these kind of content from any portion(left/right/middle anywhere) and trunk the rest, or simply ignore those records for training if it is very minimal in comparative count.</p>
<p>After this I hope you will be able to progress with your training, as it happened in my case.</p>
<p>P.S.: If you are following this thread and face similar issue feel free to comment back so that I can explore and help on your case of the issue.</p>
","7132263",2
1023,63501047,2,63500377,2020-08-20 08:18:59,1,"<p>Try setting to <code>True</code> the <a href=""https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.get_dummies.html"" rel=""nofollow noreferrer""><code>sparse</code> parameter</a>:</p>
<blockquote>
<p>sparsebool, default False
Whether the dummy-encoded columns should be backed by a SparseArray (True) or a regular NumPy array (False).</p>
</blockquote>
<pre><code>sparse_onehot = pd.get_dummies(df[sp_features], columns = sp_features, sparse = True)
</code></pre>
<p>This will use a much more memory efficient (but somewhat slower) representation than the default one.</p>
","3510736",1
1024,63516582,2,63425902,2020-08-21 04:34:38,0,"<p>When you deploy a model, a Webservice object is returned with information about the service.</p>
<pre><code>from azureml.core.webservice import AciWebservice, Webservice
from azureml.core.model import Model

deployment_config = AciWebservice.deploy_configuration(cpu_cores = 3, memory_gb = 15, location = &quot;centralus&quot;)
service = Model.deploy(ws, &quot;aciservice&quot;, [model], inference_config, deployment_config)
service.wait_for_deployment(show_output = True)
print(service.state)
</code></pre>
<p><a href=""https://i.stack.imgur.com/AvVgY.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/AvVgY.png"" alt=""enter image description here"" /></a>
Please follow the below to Consume an Azure Machine Learning model deployed as a web service
<a href=""https://learn.microsoft.com/en-us/azure/machine-learning/how-to-consume-web-service"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/machine-learning/how-to-consume-web-service</a></p>
","11297406",0
1025,63609377,2,63568274,2020-08-27 04:48:39,17,"<p>in order to use the new serializers/deserializers, you will need to init them, for example:</p>
<pre><code>from sagemaker.deserializers import JSONDeserializer
from sagemaker.serializers import CSVSerializer

rcf_inference = rcf.deploy(
    initial_instance_count=1,
    instance_type='ml.m5.xlarge',
    serializer=CSVSerializer(),
    deserializer=JSONDeserializer()
)
</code></pre>
","11278913",2
1026,63926963,2,60202189,2020-09-16 19:42:13,4,"<p>In the meantime Microsoft has added a Terraform resource for ML Workspace in the Azure Provider. This should make any custom scripting obsolete.</p>
<p><a href=""https://www.terraform.io/docs/providers/azurerm/r/machine_learning_workspace.html"" rel=""nofollow noreferrer"">https://www.terraform.io/docs/providers/azurerm/r/machine_learning_workspace.html</a></p>
<pre><code>resource &quot;azurerm_machine_learning_workspace&quot; &quot;example&quot; {
  name                    = &quot;example-workspace&quot;
  location                = azurerm_resource_group.example.location
  resource_group_name     = azurerm_resource_group.example.name
  application_insights_id = azurerm_application_insights.example.id
  key_vault_id            = azurerm_key_vault.example.id
  storage_account_id      = azurerm_storage_account.example.id

  identity {
    type = &quot;SystemAssigned&quot;
  }
}
</code></pre>
","3260500",0
1027,63948720,2,63947132,2020-09-18 02:59:13,1,"<p>I fixed this by adding an inbound security rule enabled for the scoring endpoint in the NSG group that controls the virtual network.</p>
<p>This should be done so that the scoring endpoint can be called from outside the virtual network (see <a href=""https://learn.microsoft.com/en-us/azure/machine-learning/how-to-secure-inferencing-vnet"" rel=""nofollow noreferrer"">documentation</a>), but apparently Postman can figure out how to access the endpoint without this security rule!</p>
","13562335",1
1028,63960059,2,62012264,2020-09-18 17:09:51,0,"<p>I do not think the environment length of 1024 limit will be increased in a short time. To work around this, you could try to rebuild the spark ml container with the <code>SAGEMAKER_SPARKML_SCHEMA</code> env var:</p>
<p><a href=""https://github.com/aws/sagemaker-sparkml-serving-container/blob/master/README.md#running-the-image-locally"" rel=""nofollow noreferrer"">https://github.com/aws/sagemaker-sparkml-serving-container/blob/master/README.md#running-the-image-locally</a></p>
","11278913",0
1029,63978968,2,63920599,2020-09-20 12:28:40,0,"<p>Point number 2 is the way we solved this issue. Instead of using MLflow to deploy to a scoring service on Azure, we wrote a custom code which load MLflow model when container is initialised.</p>
<p>Scoring code is something like this:</p>
<pre class=""lang-py prettyprint-override""><code>import os
import json
from mlflow.pyfunc import load_model

from inference_schema.schema_decorators import input_schema, output_schema
from inference_schema.parameter_types.numpy_parameter_type import NumpyParameterType

def init():
    global model
    model = load_model(os.path.join(os.environ.get(&quot;AZUREML_MODEL_DIR&quot;), &quot;awesome_model&quot;))

@input_schema('data', NumpyParameterType(input_sample))
@output_schema(NumpyParameterType(output_sample))

def run(data):
    return model.predict(data)
</code></pre>
","455814",1
1030,63979294,2,63973530,2020-09-20 13:06:22,4,"<p>Have you tried wrapping up your model in custom class, logging and loading it using <code>mlflow.pyfunc.PythonModel</code>?
I put up a simple example and upon loading back the model it correctly shows <code>&lt;class 'xgboost.sklearn.XGBRegressor'&gt;</code> as a type.</p>
<p>Example:</p>
<pre><code>import xgboost as xgb
xg_reg = xgb.XGBRegressor(...)

class CustomModel(mlflow.pyfunc.PythonModel):
    def __init__(self, xgbRegressor):
        self.xgbRegressor = xgbRegressor

    def predict(self, context, input_data):
        print(type(self.xgbRegressor))
        
        return self.xgbRegressor.predict(input_data)

# Log model to local directory
with mlflow.start_run():
     custom_model = CustomModel(xg_reg)
     mlflow.pyfunc.log_model(&quot;custome_model&quot;, python_model=custom_model)


# Load model back
from mlflow.pyfunc import load_model
model = load_model(&quot;/mlruns/0/../artifacts/custome_model&quot;)
model.predict(X_test)
</code></pre>
<p>Output:</p>
<pre><code>&lt;class 'xgboost.sklearn.XGBRegressor'&gt;
[ 9.107417 ]
</code></pre>
","455814",0
1031,64009123,2,64005433,2020-09-22 11:48:36,1,"<p>For me this was fixed by updating azureml-core from 1.13.0 to 1.14.0.</p>
","8824165",0
1032,64041147,2,64033258,2020-09-24 07:04:57,0,"<p>I'm afraid you can't do this at the moment. The validation set is indeed built by Autopilot itself.</p>
","4686192",0
1033,64097907,2,64097278,2020-09-28 07:43:05,2,"<p>I found the answer. You must give a <strong>cluster</strong> compute instance to do data drift in Azure Machine Learning Studio. As it is not clear, I'm planning to add something in the documentation of Azure.</p>
","4553735",0
1034,64102874,2,64073769,2020-09-28 13:09:47,0,"<p>yes, as the <a href=""https://www.kubeflow.org/docs/gke/deploy/deploy-cli/"" rel=""nofollow noreferrer"">kubectl and kpt</a> says, the first step in getting prepared to install cluster is installing <a href=""https://cloud.google.com/sdk/"" rel=""nofollow noreferrer"">gcloud</a> that is CLI that manages authentication, local configuration, developer workflow, interactions with <a href=""https://cloud.google.com/apis"" rel=""nofollow noreferrer"">Google Cloud APIs</a>.
Without is you simply cant work with objects(in your case you need to enable <code>kpt anthoscli beta</code>) and perform tasks like</p>
<blockquote>
<p>creating a Compute Engine VM instance, managing a Google Kubernetes
Engine cluster, and deploying an App Engine application, either from
the command line or in scripts and other automations..</p>
</blockquote>
","9929015",0
1035,64103378,2,64084262,2020-09-28 13:41:33,2,"<p>You need to specify the module so that TFX knows where to find your MyMetric class. One way of doing this is to specify it as part of the metric specs:</p>
<p><code>from tensorflow_model_analysis import config</code></p>
<p><code>metric_config = [config.MetricConfig(class_name='MyMetric', module='mymodule.mymetric')]</code></p>
<p><code>metrics_specs = [config.MetricsSpec(metrics=metric_config)]</code></p>
<p>You will also need to create a module called <code>mymodule</code> and put your <code>MyMetric</code> class in in <code>mymetric.py</code> for this to work. Also make sure that the module is accessible from where you are executing the code (which should be the case if you have added it to your PYTHONPATH).</p>
","14355206",0
1036,64118398,2,64118186,2020-09-29 11:10:32,0,"<p>In your case, you'll want to get the standard deviation from getting the scores from historical stock volumes, and figuring out what your anomaly score is by calculating <code>3 * standard deviation</code></p>
<p>Update your code to do inference on <em>multiple</em> records at once</p>
<pre class=""lang-py prettyprint-override""><code>apple_stock_volumes = [123412, 465125, 237564, 238172]
def inference():
    client = boto3.client('sagemaker-runtime')
    
    body = &quot;\n&quot;.join(apple_stock_volumes). # New line for each record
    response = client.invoke_endpoint(
        EndpointName='apple-volume-endpoint',
        Body=body,
        ContentType='text/csv'
    )
    inference = json.loads(response['Body'].read())
    print(inference)

inference()
</code></pre>
<p>This will return a list of scores</p>
<p>Assuming <code>apple_stock_volumes_df</code> has your volumes and the scores (after running inference on each record):</p>
<pre class=""lang-py prettyprint-override""><code>score_mean = apple_stock_volumes_df['score'].mean()
score_std = apple_stock_volumes_df['score'].std()
score_cutoff = score_mean + 3*score_std
</code></pre>
<p>There is a great example <a href=""https://github.com/awslabs/amazon-sagemaker-examples/blob/master/introduction_to_amazon_algorithms/random_cut_forest/random_cut_forest.ipynb"" rel=""nofollow noreferrer"">here</a> showing this</p>
","1706504",6
1037,64120092,2,64115070,2020-09-29 12:54:38,1,"<p>Try to:</p>
<ul>
<li>Check for typo error: username or password.</li>
<li>Check the host name and compare it with mysql.user table host name</li>
<li>Check user exists or not.</li>
<li>Check whether host contains IP address or host name.</li>
<li>Check if mysql DB works.<br />
Run:  <code>mysql -D ${MYSQL_DATABASE} -u root -p${MYSQL_ROOT_PASSWORD} -e 'SELECT 1'</code>  on  <code>mysql</code>  pod</li>
<li>Check that PVC is properly bounded:  <code>kubectl get pvc mysql -n kubeflow</code></li>
<li>Check the volume are you using for the mysql PV -
If it is local hostPath, try to delete directory on the k8s cluster node</li>
</ul>
<p>See more: <a href=""https://stackoverflow.com/a/47279949/11300382"">mysql-errno-1045</a>.</p>
<p>Take a look: <a href=""https://stackoverflow.com/questions/10299148/mysql-error-1045-28000-access-denied-for-user-billlocalhost-using-passw"">access-denied-mysql</a>,  <a href=""https://github.com/kubeflow/kubeflow/issues/4610"" rel=""nofollow noreferrer"">kubeflow-mysql-pipeline</a>, <a href=""https://github.com/kubeflow/katib/issues/1212"" rel=""nofollow noreferrer"">kubeflow-mysql</a>.</p>
","11300382",2
1038,64142034,2,64137409,2020-09-30 16:39:04,2,"<p>Thanks for the feedback. You can use globing in path. e.g. path = '**/*.parquet' to select only the parquet files</p>
","12600227",1
1039,64166841,2,64164367,2020-10-02 06:05:48,0,"<p>You need to add the .htpasswd file inside your container's file system.</p>
<p>Generate the password file in your project's nginx folder.</p>
<pre><code>sudo htpasswd -c .htpasswd sammy
</code></pre>
<p>Copy the password file to the nginx container's directory. Add following line in nginx dockerfile.</p>
<pre><code>COPY .htpasswd /etc/nginx
</code></pre>
","10429189",0
1040,64171025,2,64170759,2020-10-02 11:48:43,18,"<p>Just figure out a way of boosting the queries:</p>
<p>Before I was trying:</p>
<pre><code>import pandas as pd
from pyathena import connect

conn = connect(s3_staging_dir=STAGIN_DIR,
             region_name=REGION)
pd.read_sql(QUERY, conn)
# takes 160s
</code></pre>
<p>Figured out that using a <em>PandasCursor</em> instead of a <em>connection</em> is way faster</p>
<pre><code>import pandas as pd
pyathena import connect
from pyathena.pandas.cursor import PandasCursor

cursor = connect(s3_staging_dir=STAGIN_DIR,
                 region_name=REGION,
                 cursor_class=PandasCursor).cursor()
df = cursor.execute(QUERY).as_pandas()
# takes 12s
</code></pre>
<p>Ref: <a href=""https://github.com/laughingman7743/PyAthena/issues/46"" rel=""noreferrer"">https://github.com/laughingman7743/PyAthena/issues/46</a></p>
","7519700",3
1041,64199301,2,64071574,2020-10-04 19:59:34,2,"<p>I'm also new to TFX. Your post about the <code>ExampleValidator</code> helped me out, hopefully this answers your question.</p>
<p><strong>Using components only to visualize schema</strong></p>
<pre><code> statistics_gen = StatisticsGen(
  examples=example_gen.outputs['examples'],
  exclude_splits=['eval']
)
context.run(statistics_gen)

schema_gen = SchemaGen(
    statistics=statistics_gen.outputs['statistics'],
    infer_feature_shape=True
)
context.run(schema_gen)

context.show(schema_gen.outputs['schema']) # this should allow you to to visualize your schema 
</code></pre>
<p><strong>Using components + TFDV to visualize schema</strong></p>
<p>It looks like we can't use the <code>StatisticsGen</code> directly. We'll need to know the location of where the statistics gen artifact is being saved to and then load that artifact using <code>tfdv.load_statistics</code></p>
<pre><code># get the stats artifact
stats_artifact = statistics_gen.outputs.statistics._artifacts[0]

# get base path 
base_path = stats_artifact.uri 

# get path to file 
train_stats_file = os.path.join(base_path, 'train/stats_tfrecord') #only showing training as an example

# load stats 
loaded_stats = tfdv.load_statistics(train_stats_file)

# generic and show schema
schema = tfdv.infer_schema(loaded_stats)

tfdv.display_schema(schema)
</code></pre>
","10438511",1
1042,64204348,2,64054587,2020-10-05 07:42:35,0,"<p>InferenceSchema used with Azure Machine Learning deployments, then the code for this package was recently published at <a href=""https://github.com/Azure/InferenceSchema"" rel=""nofollow noreferrer"">https://github.com/Azure/InferenceSchema</a> under an MIT license. So you could possibly use that to create a version specific to your needs.</p>
","11297406",0
1043,64206280,2,64169189,2020-10-05 09:51:09,6,"<p>Yes, under the hood the <code>model.deploy</code> creates a model, an endpoint configuration and an endpoint. When you call again the method from an already-deployed, trained estimator it will create an error because a similarly-configured endpoint is already deployed. What I encourage you to try:</p>
<ul>
<li><p>use the <code>update_endpoint=True</code> parameter. From the <a href=""https://sagemaker.readthedocs.io/en/stable/overview.html"" rel=""noreferrer"">SageMaker SDK doc</a>:
<em>&quot;Additionally, it is possible to deploy a different endpoint configuration, which links to your model, to an already existing
SageMaker endpoint. This can be done by specifying the existing
endpoint name for the <code>endpoint_name</code> parameter along with the
<code>update_endpoint</code> parameter as True within your <code>deploy()</code> call.&quot;</em></p>
</li>
<li><p>Alternatively, if you want to create a separate model you can specify a new <code>model_name</code> in your <code>deploy</code></p>
</li>
</ul>
","5331834",2
1044,64210262,2,64173739,2020-10-05 14:06:22,0,"<p>I found an error by myself. Problem was in different regions. For training and deploying model I used us-east-2 and for lambda I used us-east-1. Just creating all in same region fixed this issue!</p>
","13376024",0
1045,64242317,2,64073243,2020-10-07 10:38:51,4,"<p>Say, for example, you want to add the module file location as a runtime parameter that is passed to the transform component in your TFX pipeline.</p>
<p>Start by setting up your setup_pipeline.py and defining the module file parameter:</p>
<pre><code># setup_pipeline.py

from typing import Text
from tfx.orchestration import data_types, pipeline
from tfx.orchestration.kubeflow import kubeflow_dag_runner
from tfx.components import Transform

_module_file_param = data_types.RuntimeParameter(
    name='module-file',
    default=
    '/tfx-src/tfx/examples/iris/iris_utils_native_keras.py',
    ptype=Text,
)
</code></pre>
<p>Next, define a function that specifies the components used in your pipeline and pass along the parameter.</p>
<pre><code>def create_pipeline(..., module_file):
    # setup components:
    ...

    transform = Transform(
         ...
         module_file=module_file
      )
     ...

    components = [..., transform, ...]

    return pipeline.Pipeline(
          ...,
          components=components
    )
</code></pre>
<p>Finally, setup the Kubeflow DAG runner so that it passes the parameter along to the <code>create_pipeline</code> function. See <a href=""https://github.com/kubeflow/pipelines/tree/master/samples/core/iris"" rel=""nofollow noreferrer"">here</a> for a more complete example.</p>
<pre><code>if __name__ == &quot;__main__&quot;:

    # instantiate a kfp_runner
    ...

    kfp_runner = kubeflow_dag_runner.KubeflowDagRunner(
        ...
    )

    kfp_runner.run(
        create_pipeline(..., module_file=_module_file_param
      ))
</code></pre>
<p>Then you can run <code>python -m setup_pipeline</code> which will produce the yaml file that specifies the pipeline config, which you can then upload to the Kubeflow GCP interface.</p>
","14355206",0
1046,64293239,2,64246437,2020-10-10 12:08:26,7,"<p>It’s currently not possible to increase timeout—this is an open issue in GitHub. Looking through the issue and similar questions on SO, it seems like you may be able to use batch transforms in conjunction with inference.</p>
<h1>References</h1>
<p><a href=""https://stackoverflow.com/a/55642675/806876"">https://stackoverflow.com/a/55642675/806876</a></p>
<p>Sagemaker Python SDK timeout issue: <a href=""https://github.com/aws/sagemaker-python-sdk/issues/1119"" rel=""noreferrer"">https://github.com/aws/sagemaker-python-sdk/issues/1119</a></p>
","806876",1
1047,64301445,2,64300737,2020-10-11 07:04:51,0,"<p>Well, first check <code>{notebook_directory}/.ipynb_checkpoints/</code> for a snapshot.. the directory should contain copies of your notebooks from the point in time when they were last saved.</p>
<p>If that's gone, the IPython kernel saves commands issued to it in a sqlite db. found in <code>~/.ipython/profile_default</code>. There you should find <em><strong>history.sqlite</strong></em>.</p>
<p>history.sqlite should contain all commands issued to the ipython kernel. Tables of interest are <strong>sessions</strong> and <strong>history</strong>.</p>
<p>You need to query the sessions table by session start or end time to determine which sessions are related to the ones you're looking for:</p>
<pre><code>select 
    sessions.session, sessions.start, sessions.end, history.source
from sessions 
join history using (session)
where
    start &gt; 'your_start_date' and start &lt; 'your_end_date';
</code></pre>
<p>once you find your session:</p>
<pre><code>select '# @@ Cell '|| line || char(10) || source || char(10) from history where session = your_session_number;
</code></pre>
<p>Output your query to .py then use jupytext to convert to ipynb, then you have your notebook back:</p>
<pre><code>sqlite3 history.sqlite \
    &quot;select '# @@ Cell '|| line || char(10) || source || char(10)    from history where session = your_session_number;&quot; \
&gt; output.py
</code></pre>
","12520046",0
1048,64502765,2,64207678,2020-10-23 15:16:40,1,"<p>One needs to pass the <code>--workspace-name</code> argument to be able to run it on Azure's compute target and not on the local compute target:</p>
<pre class=""lang-sh prettyprint-override""><code>az ml run submit-script test.py --target compute-instance-test --experiment-name test_example --resource-group ex-test-rg --workspace-name test-ws
</code></pre>
","9698518",1
1049,64589653,2,64589652,2020-10-29 10:52:24,1,"<p>To fetch metrics from a run:</p>
<pre><code>client = kfp.Client(host=host)
client.get_run(run_id).run.metrics
</code></pre>
","5759881",0
1050,64622258,2,64486262,2020-10-31 13:34:49,0,"<p>You can use the <a href=""https://learn.microsoft.com/en-us/python/api/azureml-core/azureml.core.model.model?view=azure-ml-py&amp;preserve-view=true#delete--"" rel=""nofollow noreferrer"">delete</a> method in the <a href=""https://learn.microsoft.com/en-us/python/api/azureml-core/azureml.core.model.model?view=azure-ml-py"" rel=""nofollow noreferrer"">Model</a> class to delete a registered model.</p>
<p>This can also be done via the Azure CLI as:</p>
<pre><code>az ml model delete &lt;model id&gt;
</code></pre>
<p>Other commands can be found here: <a href=""https://learn.microsoft.com/en-us/cli/azure/ext/azure-cli-ml/ml/model?view=azure-cli-latest"" rel=""nofollow noreferrer"">az ml model</a></p>
","10805761",0
1051,64651933,2,64651724,2020-11-02 19:18:46,0,"<p>I have not yet used AWS Sagemaker notebooks, but they may be similar to GCP 'AI Platform notebooks', which I have used quite extensively. Additionally, if you're experiencing additional problems, could you describe how you're launching the notebooks (whether from command line or from GUI)?</p>
<p>In GCP, I defaulted to using <code>pip install</code> for my packages, as the conda environments were a bit finicky and didn't provide much support when creating notebooks sourced from my own created conda environments.</p>
<p>Assuming you're installing conda into your base directory, when you launch jupyter notebooks, this should be the default conda environment, else if you installed to a separate conda environment, you should be able to change this within jupyter notebooks using the <code>CONDA</code> tab and selecting which notebook uses which conda environment.
-Spencer</p>
","12749159",2
1052,64653403,2,64653042,2020-11-02 21:12:41,1,"<p>I think you are looking for the <a href=""https://dvc.org/doc/start/data-access"" rel=""nofollow noreferrer"">data access</a> set of commands.</p>
<p>In your particular case, <code>dvc import</code> makes sense:</p>
<pre><code>$ dvc import /raid/ml_data data
</code></pre>
<p>if you want to get the most recent version (HEAD). Then you will be able to update it with the <code>dvc update</code> command (if 2.0.0 is released, for example).</p>
<pre><code>$ dvc import /raid/ml_data data --rev 1.0.0
</code></pre>
<p>if you'd like to &quot;fix&quot; it to the specific version.</p>
<h3>Avoiding copies</h3>
<p>Make sure also, that <code>symlinks</code> are set for the second project, as described in the <a href=""https://dvc.org/doc/user-guide/large-dataset-optimization"" rel=""nofollow noreferrer"">Large Dataset Optimization</a>:</p>
<pre><code>$ dvc config cache.type reflink,hardlink,symlink,copy
</code></pre>
<p>(there are config modifiers <code>--global</code>, <code>--local</code>, <code>--system</code> to set this setting for everyone at once, or just for one project, etc)</p>
<p>Check the details instruction <a href=""https://dvc.org/doc/user-guide/large-dataset-optimization#configuring-dvc-cache-file-link-type"" rel=""nofollow noreferrer"">here</a>.</p>
<hr />
<p>Overall, it's a great setup, and looks like you got pretty much everything right. Please, don't hesitate to follow up and/or create other questions here- we'll help you with this.</p>
","298182",2
1053,64654646,2,64597526,2020-11-02 23:17:33,1,"<p>I was able to get this to work by first creating an AKS resource without an internal load balancer, then separately updating the load balancer following this code:</p>
<pre><code>import azureml.core
from azureml.core.compute.aks import AksUpdateConfiguration
from azureml.core.compute import AksCompute

# ws = workspace object. Creation not shown in this snippet
aks_target = AksCompute(ws,&quot;myaks&quot;)

# Change to the name of the subnet that contains AKS
subnet_name = &quot;default&quot;
# Update AKS configuration to use an internal load balancer
update_config = AksUpdateConfiguration(None, &quot;InternalLoadBalancer&quot;, subnet_name)
aks_target.update(update_config)
# Wait for the operation to complete
aks_target.wait_for_completion(show_output = True)
</code></pre>
<p>No network contributor role was required.</p>
","13562335",0
1054,64685794,2,60839279,2020-11-04 18:34:14,5,"<p>Difficult to debug what the exact root cause is in your case, but following steps worked for me. I started tensorboard inside the notebook instance manually.</p>
<ol>
<li><p>Followed guide on <a href=""https://sagemaker.readthedocs.io/en/stable/amazon_sagemaker_debugger.html#capture-real-time-tensorboard-data-from-the-debugging-hook"" rel=""noreferrer"">sagemaker debugging</a> to configure the <code>S3</code> output path for tensorboard logs.</p>
<pre><code>from sagemaker.debugger import TensorBoardOutputConfig

tensorboard_output_config = TensorBoardOutputConfig(
       s3_output_path = 's3://bucket-name/tensorboard_log_folder/'
)

estimator = TensorFlow(entry_point='train.py',
               source_dir='./',
               model_dir=model_dir,
               output_path= output_dir,
               train_instance_type=train_instance_type,
               train_instance_count=1,
               hyperparameters=hyperparameters,
               role=sagemaker.get_execution_role(),
               base_job_name='Testing-TrainingJob',
               framework_version='2.2',
               py_version='py37',
               script_mode=True,
               tensorboard_output_config=tensorboard_output_config)

estimator.fit(inputs)
</code></pre>
</li>
<li><p>Start the tensorboard with the <code>S3</code> location provided above via a terminal on the notebook instance.</p>
<pre><code>$ tensorboard --logdir 's3://bucket-name/tensorboard_log_folder/'
</code></pre>
</li>
<li><p>Access the board via URL with <code>/proxy/6006/</code>. You need to update the notebook instance details in the following URL.</p>
<pre><code>https://myinstance.notebook.us-east-1.sagemaker.aws/proxy/6006/
</code></pre>
</li>
</ol>
","3363978",1
1055,64708001,2,64703268,2020-11-06 02:23:45,0,"<p>yes u are right u can upload thousands of images using aws cli using $aws s3 cp  ; or $aws s3 sync</p>
","1312478",0
1056,64744223,2,62309772,2020-11-08 23:49:20,1,"<p><code>sagemaker.processing.ScriptProcessor</code> subclasses <code>sagemaker.processing.Processor</code>. <code>ScriptProcessor</code> can be used to write a custom processing script. <code>Processor</code> can be subclassed to create a <code>CustomProcessor</code> class for a more complex use case.</p>
","2089899",0
1057,64779562,2,63915711,2020-11-11 02:21:41,0,"<p><a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/input-data-limits.html"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/sagemaker/latest/dg/input-data-limits.html</a></p>
<p>In case someone is also looking for the answer of this one.</p>
<p>In short:</p>
<blockquote>
<p>40 MB</p>
</blockquote>
","3317548",0
1058,64789306,2,64779388,2020-11-11 15:26:02,3,"<p>that's a good question :) I agree, many of the official tutorials tend to show the full train-to-invoke pipeline and don't emphasize enough that each step can be done separately. In your specific case, when you want to invoke an already-deployed endpoint, you can either: (A) use the invoke API call in one of the numerous SDKs (example in <a href=""https://docs.aws.amazon.com/cli/latest/reference/sagemaker-runtime/invoke-endpoint.html"" rel=""nofollow noreferrer"">CLI</a>, <a href=""https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/sagemaker-runtime.html#SageMakerRuntime.Client.invoke_endpoint"" rel=""nofollow noreferrer"">boto3</a>) or (B) or instantiate a <code>predictor</code> with the high-level Python SDK, either the generic <a href=""https://sagemaker.readthedocs.io/en/stable/api/inference/model.html"" rel=""nofollow noreferrer""><code>sagemaker.model.Model</code></a> class or its XGBoost-specific child: <a href=""https://sagemaker.readthedocs.io/en/stable/frameworks/xgboost/xgboost.html#sagemaker.xgboost.model.XGBoostPredictor"" rel=""nofollow noreferrer""><code>sagemaker.xgboost.model.XGBoostPredictor</code></a> as illustrated below:</p>
<pre><code>from sagemaker.xgboost.model import XGBoostPredictor
    
predictor = XGBoostPredictor(endpoint_name='your-endpoint')
predictor.predict('&lt;payload&gt;')
</code></pre>
<p>similar question <a href=""https://stackoverflow.com/questions/56255154/how-to-use-a-pretrained-model-from-s3-to-predict-some-data/56277411#56277411"">How to use a pretrained model from s3 to predict some data?</a></p>
<p>Note:</p>
<ul>
<li>If you want the <code>model.deploy()</code> call to return a predictor, your model must be instantiated with a <a href=""https://sagemaker.readthedocs.io/en/stable/api/inference/model.html"" rel=""nofollow noreferrer""><code>predictor_cls</code></a>. This is optional, you can also first deploy a model, and then invoke it as a separate step with the above technique</li>
<li>Endpoints create charges even if you don't invoke them; they are charged per uptime. So if you don't need an always-on endpoint, don't hesitate to shut it down to minimize costs.</li>
</ul>
","5331834",4
1059,64818633,2,64816630,2020-11-13 09:42:37,4,"<p>This was actually a duplicate of <a href=""https://stackoverflow.com/questions/63891547/how-to-connect-amls-to-adls-gen-2"">How to connect AMLS to ADLS Gen 2?</a>.</p>
<p>The solution is to give the service principal that Azure ML uses to access the data lake the Storage Blob Data Reader access. And note you have to wait at least some minutes for this to have effect.</p>
","489090",0
1060,64907595,2,64875623,2020-11-19 08:04:28,0,"<p>This fixed it for me:
The Content Type was missing.</p>
<pre><code>import json
import boto3
import numpy as np
import io

client = boto3.client('runtime.sagemaker',aws_access_key_id=..., aws_secret_access_key=...,region_name=...)
endpoint_name = '...'

data = np.load(&quot;testValues.npy&quot;)


payload = json.dumps(data.tolist())
response = client.invoke_endpoint(EndpointName=endpoint_name,
                                  ContentType='application/json',
                                   Body=payload)
result = json.loads(response['Body'].read().decode())
res = result['predictions']
print(&quot;test&quot;)

</code></pre>
","10674521",0
1061,64910968,2,64758689,2020-11-19 11:41:43,1,"<p>There are 2 methods that you can achieve this depending on how you see the placement of your functionality in the TFX flow.</p>
<ol>
<li>Writing a Custom TFX Components -  which requires a lot of effort and you need to define quite a few things.</li>
<li>Reusing Existing Components - instead of writing a component for TFX entirely from scratch, we can inherit an existing component and customize it by overwriting the executor functionality.</li>
</ol>
<p>I would suggest the following blogs to begin with:<br />
<a href=""https://www.tensorflow.org/tfx/guide/understanding_custom_components"" rel=""nofollow noreferrer"">Anatomy of TFX Component</a><br />
<a href=""https://blog.tensorflow.org/2020/01/creating-custom-tfx-component.html"" rel=""nofollow noreferrer"">Creating custom Components</a></p>
","9879491",1
1062,64921903,2,64286191,2020-11-20 00:10:50,3,"<p>Could try something like:</p>
<pre><code>document.addEventListener('keydown', function(event) {
  if (event.shiftKey &amp;&amp; event.keyCode === 13) {
    document.getElementsByTagName('crowd-bounding-box')[0].shadowRoot.getElementById('nothing-to-adjust').querySelector('label').click();
  }
});
</code></pre>
","14672495",1
1063,64972341,2,64964435,2020-11-23 16:29:57,2,"<p>The AWS Step Functions Data Science SDK is an open source library that allows data scientists to easily create workflows that process and publish machine learning models using SageMaker and Step Functions.</p>
<p>The following <a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/howitworks-nbexamples.html"" rel=""nofollow noreferrer"">Example notebooks</a>, which are available in Jupyter notebook instances in the <a href=""https://console.aws.amazon.com/sagemaker/"" rel=""nofollow noreferrer"">SageMaker console</a> and the related <a href=""https://github.com/awslabs/amazon-sagemaker-examples/tree/master/step-functions-data-science-sdk"" rel=""nofollow noreferrer"">GitHub project</a>:</p>
<ul>
<li>hello_world_workflow.ipynb</li>
<li>machine_learning_workflow_abalone.ipynb</li>
<li>training_pipeline_pytorch_mnist.ipynb</li>
</ul>
","14302852",0
1064,64982645,2,64974572,2020-11-24 08:21:00,0,"<p>Is was because of CSV files in the same S3 folder as the RecordIO.</p>
","39242",0
1065,64994316,2,64979752,2020-11-24 20:27:00,1,"<p>So, after some trial and error, creating a fresh environment and then adding the packages solved the problem for me. I am still not clear on why this didn't work when I tried to use <a href=""http://from%20azureml.core%20import%20Workspace%20from%20azureml.core.model%20import%20Model%20from%20azureml.core.environment%20import%20Environment,%20DEFAULT_GPU_IMAGE%20from%20azureml.core.conda_dependencies%20import%20CondaDependencies%20from%20azureml.core.model%20import%20InferenceConfig%20from%20azureml.core.webservice%20import%20AksWebservice,%20LocalWebservice%20from%20azureml.core.compute%20import%20ComputeTarget%20%20%20#%201.%20Instantiate%20the%20workspace%20workspace%20=%20Workspace.from_config(path=%22config.json%22)%20%20#%202.%20Setup%20the%20environment%20env%20=%20Environment(%27sketchenv%27)%20with%20open(%27requirements.txt%27)%20as%20f:%20#%20Fetch%20all%20dependencies%20as%20a%20list%20%20%20%20%20dependencies%20=%20f.readlines()%20dependencies%20=%20%5Bx.strip()%20for%20x%20in%20dependencies%20if%20%27#%20%27%20not%20in%20x%5D%20env.docker.base_image%20=%20DEFAULT_GPU_IMAGE%20env.python.conda_dependencies%20=%20CondaDependencies.create(conda_packages=%5B%27numpy==1.17.4%27,%20%27Cython%27%5D,%20pip_packages=dependencies)%20%20#%203.%20Inference%20Config%20inference_config%20=%20InferenceConfig(entry_script=%27app.py%27,%20environment=env,%20source_directory=%27./ProcessImage%27)%20%20#%204.%20Compute%20target%20(using%20existing%20cluster%20from%20the%20workspacke)%20aks_target%20=%20ComputeTarget(workspace=workspace,%20name=%27sketch-ppt-vm%27)%20%20#%205.%20Deployment%20config%20deployment_config%20=%20AksWebservice.deploy_configuration(cpu_cores=6,%20memory_gb=100)%20%20#%206.%20Model%20deployment%20model%20=%20Model(workspace,%20%27sketch-inference%27)%20#%20Registered%20model%20(which%20contains%20model%20files/folders)%20service%20=%20Model.deploy(workspace,%20%22process-sketch-dev%22,%20%5Bmodel%5D,%20inference_config,%20deployment_config,%20deployment_target=aks_target,%20overwrite=True)%20service.wait_for_deployment(show_output%20=%20True)%20print(service.state)"" rel=""nofollow noreferrer"">Environment.from_pip_requirements()</a>. A detailed answer in this regard would be interesting to read.</p>
<p>My primary task was inference - object detection given an image, and we have our own model developed by our team. There are two types of imports I wanted to have:</p>
<p><strong>1. Standard python packages (installed through pip)</strong><br />
This was solved by creating conda dependencies and add it to env object (Step 2)</p>
<p><strong>2. Methods/vars from helper scripts</strong> (if you have pre/post processing to be done during model inference):<br />
This was done by mentioning <code>source_directory</code> in InferenceConfig (step 3)</p>
<p>Here is my updated script which combines Environment creation, Inference and Deployment configs and using existing compute in the workspace (created through portal).</p>
<pre><code>from azureml.core import Workspace
from azureml.core.model import Model
from azureml.core.environment import Environment, DEFAULT_GPU_IMAGE
from azureml.core.conda_dependencies import CondaDependencies
from azureml.core.model import InferenceConfig
from azureml.core.webservice import AksWebservice, LocalWebservice
from azureml.core.compute import ComputeTarget


# 1. Instantiate the workspace
workspace = Workspace.from_config(path=&quot;config.json&quot;)

# 2. Setup the environment
env = Environment('sketchenv')
with open('requirements.txt') as f: # Fetch all dependencies as a list
    dependencies = f.readlines()
dependencies = [x.strip() for x in dependencies if '# ' not in x]
env.docker.base_image = DEFAULT_GPU_IMAGE
env.python.conda_dependencies = CondaDependencies.create(conda_packages=['numpy==1.17.4', 'Cython'], pip_packages=dependencies)

# 3. Inference Config
inference_config = InferenceConfig(entry_script='app.py', environment=env, source_directory='./ProcessImage')

# 4. Compute target (using existing cluster from the workspacke)
aks_target = ComputeTarget(workspace=workspace, name='sketch-ppt-vm')

# 5. Deployment config
deployment_config = AksWebservice.deploy_configuration(cpu_cores=6, memory_gb=100)

# 6. Model deployment
model = Model(workspace, 'sketch-inference') # Registered model (which contains model files/folders)
service = Model.deploy(workspace, &quot;process-sketch-dev&quot;, [model], inference_config, deployment_config, deployment_target=aks_target, overwrite=True)
service.wait_for_deployment(show_output = True)
print(service.state)
</code></pre>
<hr />
","4825498",0
1066,65017792,2,63778066,2020-11-26 07:39:36,2,"<p>Spending time last one yar with Kubeflow wand echo system what I realize is following:</p>
<ol>
<li><p>Kubeflow-kale is open sourced however not completely free, there are issues running the kubeflow-kale on latest version, we have reported issues on github for the same, it works with Arrikito however Arrikito is licensed</p>
</li>
<li><p>You should be able to install kale locally step by step as mentioned in their documentation, however I have not tried it</p>
</li>
<li><p>If you are looking for pipeline editor Elyra is much better choice - free and they are adding support for other pipelines like TFX etc.</p>
</li>
</ol>
","4164124",0
1067,65026149,2,64909903,2020-11-26 16:45:18,1,"<p>Your tensorboard <code>logdir</code> is not <code>logs/fit</code>.. but there is the current date appended. Try using a <code>logs/fit</code> as <code>log_dir</code> and see if it's working.</p>
<p>EDIT</p>
<p>If you want to use tensorboard locally you have to send tensorboard logs to S3 and read from there. In order to do this you have to do what your third linked example does, so include sagemaker debugger:</p>
<blockquote>
<p>from sagemaker.debugger import TensorBoardOutputConfig</p>
<p>tensorboard_output_config = TensorBoardOutputConfig(
s3_output_path='s3://path/for/tensorboard/data/emission',
container_local_output_path='/local/path/for/tensorboard/data/emission'
)</p>
</blockquote>
<p>then your tensorboard command will be something like:</p>
<blockquote>
<p>AWS_REGION= &lt;your-region&gt; AWS_LOG_LEVEL=3 tensorboard --logdir
s3://path/for/tensorboard/data/emission</p>
</blockquote>
<p>Alternatively if you want to use tensorboard in the notebook you have to do what the second linked example does, so simply install in a cell and run tensorboard with something like:</p>
<p>https://&lt;notebook instance hostname&gt;/proxy/6006/</p>
","4267439",7
1068,65032470,2,64925353,2020-11-27 05:25:56,0,"<p>I have contacted AWS support and apparently this is a bug in <code>awscli</code>. Here is a revised excerpt from their long and detailed answer.</p>
<blockquote>
<p>This is an encoding issue with AWSCLI v2. For now, you can proceed
with AWSCLI v1.18 as a temporary solution.</p>
</blockquote>
<p>I also verified it works with aws-cli/1.18.185:</p>
<pre><code>$ aws --version
aws-cli/1.18.185 Python/3.8.3 Linux/4.19.104-microsoft-standard botocore/1.19.25

$ aws sagemaker-runtime invoke-endpoint \
&gt;     --region eu-west-1 \
&gt;     --endpoint-name DEMO-XGBoostEndpoint-2020-11-20-06-26-30 \
&gt;     --body $(seq 784 | xargs echo | sed 's/ /,/g') \
&gt;     &gt;(cat) \
&gt;     --content-type text/csv &gt; /dev/null
8.0%
</code></pre>
<p>In AWS cli v2.1.21 amazon added the <code>--cli-binary-format raw-in-base64-out</code> option and this should work as it worked with AWS cli v1.18:</p>
<pre><code>aws sagemaker-runtime invoke-endpoint \
&gt;     --region &lt;aws-region&gt; \
&gt;     --endpoint-name &lt;you-endpoint-name&gt; \
&gt;     --cli-binary-format raw-in-base64-out \
&gt;     --body $(seq 784 | xargs echo | sed 's/ /,/g') \
&gt;     &gt;(cat) \
&gt;     --content-type text/csv &gt; /dev/null
</code></pre>
","1031417",0
1069,65151039,2,62142825,2020-12-04 21:33:00,0,"<p>SageMaker Autopilot doesn't support SageMaker Debugger out of the box currently (as of Dec 2020). You can hack the Hyperparameter Tuning job to pass in a debug parameter.</p>
<p>However, there is a way to use SHAP with Autopilot models. Take a look at this blog post explaining how to use SHAP with SageMaker Autopilot: <a href=""https://aws.amazon.com/blogs/machine-learning/explaining-amazon-sagemaker-autopilot-models-with-shap/"" rel=""nofollow noreferrer"">https://aws.amazon.com/blogs/machine-learning/explaining-amazon-sagemaker-autopilot-models-with-shap/</a>.</p>
","11540319",0
1070,65276071,2,65276017,2020-12-13 13:18:12,12,"<p>The instances with the <code>ml</code> prefix are instance classes specifically for use in Sagemaker.</p>
<p>In addition to being used within the Sagemaker service, the instance will be running an AMI with all the necessary libraries and packages such as Jupyter.</p>
","13460933",7
1071,65301516,2,65259702,2020-12-15 07:16:53,0,"<p>Are you using an Amazon SageMaker Notebook? When I run your code above in a new <code>conda_python3</code> Amazon SageMaker notebook, I don't get any errors at all.</p>
<p>Example screenshot output showing no errors:</p>
<p><a href=""https://i.stack.imgur.com/ZTP5D.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ZTP5D.png"" alt=""enter image description here"" /></a></p>
<p>If you're getting something like <code>NameError: name 'ModelQualityMonitor' is not defined</code> then I suspect you are running in a Python environment that doesn't have the Amazon SageMaker SDK installed in it. Perhaps try running <code>pip install sagemaker</code> and then see if this resolves your error.</p>
","30632",0
1072,65316587,2,65316586,2020-12-16 02:45:27,21,"<p>we can get the experiment id from the experiment name and we can use python API to get the best runs.</p>
<pre><code>experiment_name = &quot;experiment-1&quot;
current_experiment=dict(mlflow.get_experiment_by_name(experiment_name))
experiment_id=current_experiment['experiment_id']
</code></pre>
<p>By using the experiment id, we can get all the runs and we can sort them based on metrics like below. In the below code, rmse is my metric name (so it may be different for you based on metric name)</p>
<pre><code>df = mlflow.search_runs([experiment_id], order_by=[&quot;metrics.rmse DESC&quot;])
best_run_id = df.loc[0,'run_id']
</code></pre>
","7160346",1
1073,65326382,2,65285203,2020-12-16 15:34:33,2,"<p>It was likely created as a default bucket by the SageMaker Python SDK. Note that the code you wrote about is not <code>boto3</code> (AWS python SDK), but <code>sagemaker</code> (<a href=""https://sagemaker.readthedocs.io/en/stable/index.html"" rel=""nofollow noreferrer"">link</a>), the SageMaker-specific Python SDK, that is higher-level than boto3.</p>
<p>The SageMaker Python SDK uses S3 at multiple places, for example to stage training code when using a Framework Estimator, and to stage inference code when deployment with a Framework Model (your case). It gives you control of the S3 location to use, but if you don't specify it, it may use an automatically generated bucket, if it has the permissions to do so.</p>
<p>To control code staging S3 location, you can use the parameter <code>code_location</code> in either your <code>PyTorchEstimator</code> (training) or your <code>PyTorchModel</code> (serving)</p>
","5331834",3
1074,65332317,2,64684503,2020-12-16 22:55:11,1,"<p>I am on the AWS SageMaker team.  For &quot;Real-Time Inference&quot; you are only charged for:</p>
<ol>
<li>usage of the instance types you choose (instance hours)</li>
<li>storage attached to those instance (GB storage hours)</li>
<li>data in and out of your Endpoint (Bytes in/out)</li>
</ol>
<p>See &quot;Pricing Example #6: Real-Time Inference&quot; as well.</p>
","16318",0
1075,65358482,2,65322286,2020-12-18 14:09:00,5,"<p>AutoScaling requires a cloudwatch alarm to trigger to scale in.  Sagemaker doesn't push 0 value metrics when there's no activity (it just doesn't push anything).  This leads to the alarm being put into insufficient data and not triggering the autoscaling scale in action when your workload suddenly ends.</p>
<p>Workarounds are either:</p>
<ol>
<li>Have a step scaling policy using the cloudwatch metric math FILL() function for your scale in.  This way you can tell CloudWatch &quot;if there's no data, pretend this was the metric value when evaluating the alarm.  This is only possible with step scaling since target tracking creates the alarms for you (and AutoScaling will periodically recreate them, so if you make manual changes they'll get deleted)</li>
<li>Have scheduled scaling set the size back down to 1 every evening</li>
<li>Make sure traffic continues at a low level for some times</li>
</ol>
","7097192",1
1076,65633926,2,65627039,2021-01-08 17:48:17,2,"<p>At the moment, you have to query your run again in MLflow to get the run with all the info that you logged. In the example below, I call <code>mlflow.get_run(&lt;run_id&gt;)</code> to achieve this.</p>
<pre><code>import mlflow


with mlflow.start_run() as active_run:
  tags = { &quot;my_tag&quot;: 1}
  mlflow.set_tags(tags)            
  # Keep track of the run ID of the active run
  run_id = active_run.info.run_id

run = mlflow.get_run(run_id)
print(&quot;The tags are &quot;, run.data.tags)
</code></pre>
","4461048",0
1077,65703266,2,65699980,2021-01-13 13:47:25,2,"<p>You can use parameter <code>output_path</code> when you define the estimator. If you use the
<code>model_dir</code> I guess you have to create that bucket beforehand, but you have the advantage that artifacts can be saved in real time during the training (if the instance has rights on S3). You can take a look at my <a href=""https://github.com/roccopietrini/TFSagemakerDetection"" rel=""nofollow noreferrer"">repo</a> for this specific case.</p>
","4267439",0
1078,65719943,2,65719292,2021-01-14 13:30:20,2,"<p>You need to use the conda_pytorch_36 kernel (this is the one I used) and tensorboard is not installed by default so you need to run</p>
<pre><code>!pip install tensorboard
</code></pre>
<p>Then you will get a blank screen when you run.</p>
<pre><code>%load_ext tensorboard
%tensorboard --logdir &quot;./runs&quot;
</code></pre>
<p>You can connect to tensorboard using your URL with notebook or lab replaced with proxy/6006</p>
<pre><code>https://YOUR_NOTEBOOK_INSTANCE_NAME.notebook.ap-northeast-1.sagemaker.aws/proxy/6006/
</code></pre>
","4453351",3
1079,65721638,2,65721061,2021-01-14 15:13:35,3,"<p>So as per the <a href=""https://docs.aws.amazon.com/step-functions/latest/dg/sample-train-model.html#sample-train-model-code-examples"" rel=""nofollow noreferrer"">documentation</a>, we should be passing the parameters in the following format</p>
<pre><code>        &quot;Parameters&quot;: {
            &quot;ModelName.$&quot;: &quot;$$.Execution.Name&quot;,  
            ....
        },
</code></pre>
<p>If you take a close look this is something missing from your definition, So your step function definition should be something like below:</p>
<p>either</p>
<pre><code>      &quot;TransformJobName.$&quot;: &quot;$$.Execution.Id&quot;,
</code></pre>
<p>OR</p>
<pre><code>      &quot;TransformJobName.$: &quot;States.Format('mytransformjob{}', $$.Execution.Id)&quot;
</code></pre>
<p>full State machine definition:</p>
<pre><code>    {
        &quot;Comment&quot;: &quot;Defines the statemachine.&quot;,
        &quot;StartAt&quot;: &quot;Generate Random String&quot;,
        &quot;States&quot;: {
            &quot;Generate Random String&quot;: {
                &quot;Type&quot;: &quot;Task&quot;,
                &quot;Resource&quot;: &quot;arn:aws:lambda:eu-central-1:1234567890:function:randomstring&quot;,
                &quot;ResultPath&quot;: &quot;$.executionid&quot;,
                &quot;Parameters&quot;: {
                &quot;executionId.$&quot;: &quot;$$.Execution.Id&quot;
                },
                &quot;Next&quot;: &quot;SageMaker CreateTransformJob&quot;
            },
        &quot;SageMaker CreateTransformJob&quot;: {
            &quot;Type&quot;: &quot;Task&quot;,
            &quot;Resource&quot;: &quot;arn:aws:states:::sagemaker:createTransformJob.sync&quot;,
            &quot;Parameters&quot;: {
            &quot;BatchStrategy&quot;: &quot;SingleRecord&quot;,
            &quot;DataProcessing&quot;: {
                &quot;InputFilter&quot;: &quot;$&quot;,
                &quot;JoinSource&quot;: &quot;Input&quot;,
                &quot;OutputFilter&quot;: &quot;xxx&quot;
            },
            &quot;Environment&quot;: {
                &quot;SAGEMAKER_MODEL_SERVER_TIMEOUT&quot;: &quot;300&quot;
            },
            &quot;MaxConcurrentTransforms&quot;: 100,
            &quot;MaxPayloadInMB&quot;: 1,
            &quot;ModelName&quot;: &quot;${model_name}&quot;,
            &quot;TransformInput&quot;: {
                &quot;DataSource&quot;: {
                    &quot;S3DataSource&quot;: {
                        &quot;S3DataType&quot;: &quot;S3Prefix&quot;,
                        &quot;S3Uri&quot;: &quot;${s3_input_path}&quot;
                    }
                },
                &quot;ContentType&quot;: &quot;application/jsonlines&quot;,
                &quot;CompressionType&quot;: &quot;Gzip&quot;,
                &quot;SplitType&quot;: &quot;Line&quot;
            },
            &quot;TransformJobName.$&quot;: &quot;$.executionid&quot;,
            &quot;TransformOutput&quot;: {
                &quot;S3OutputPath&quot;: &quot;${s3_output_path}&quot;,
                &quot;Accept&quot;: &quot;application/jsonlines&quot;,
                &quot;AssembleWith&quot;: &quot;Line&quot;
            },    
            &quot;TransformResources&quot;: {
                &quot;InstanceType&quot;: &quot;xxx&quot;,
                &quot;InstanceCount&quot;: 1
            }
        },
            &quot;End&quot;: true
        }
        }
    }
</code></pre>
<p>In the above definition the lambda could be a function which parses the execution id arn which I am passing via the parameters section:</p>
<pre><code> def lambda_handler(event, context):
    return(event.get('executionId').split(':')[-1])
</code></pre>
<p>Or if you dont wanna pass the execution id , it can simply return the random string like</p>
<pre><code> import string
 def lambda_handler(event, context):
    return(string.ascii_uppercase + string.digits)
</code></pre>
<p>you can generate all kinds of random string or do generate anything in the lambda and pass that to the transform job name.</p>
","2246345",23
1080,65733660,2,64849557,2021-01-15 09:37:28,2,"<p>Came across the same issue.</p>
<p>Check the number of rows returned after prediction in your serving code. In my case, my prediction output didn't have a column header.</p>
<p>e.g. As a text/csv response, using batch transform with join will post join the input &amp; output.</p>
<p>A single input record would be [[&quot;feature_1&quot;, &quot;feature_2&quot;],[0, 1]], while my model predicted output returned [1].</p>
<p>add column name to predicted output like this [&quot;result&quot;, 1] then returning the csv result will yield [[&quot;result&quot;],[1]] matching input.</p>
<p>P.S. you may need to find a scalable way of doing this for multi-row  batch. Not sure.</p>
","15011512",0
1081,65883615,2,65882686,2021-01-25 11:12:07,0,"<p>I can answer my question.. it seems to be a new feature as highlighted <a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/use-debugger-built-in-rules.html"" rel=""nofollow noreferrer"">here</a>. You can turn it off as suggested in the doc:</p>
<pre><code>To disable both monitoring and profiling, include the disable_profiler parameter to your estimator and set it to True. 
</code></pre>
","4267439",0
1082,65888860,2,65887231,2021-01-25 16:41:54,10,"<p>MLflow supports <a href=""https://mlflow.org/docs/latest/models.html#custom-python-models"" rel=""noreferrer"">custom models</a> of mlflow.pyfunc flavor.  You can create a custom  class  inherited from the <code>mlflow.pyfunc.PythonModel</code>, that needs to provide function <code>predict</code> for performing predictions, and optional <code>load_context</code> to load the necessary artifacts, like this (adopted from the docs):</p>
<pre class=""lang-py prettyprint-override""><code>class MyModel(mlflow.pyfunc.PythonModel):

    def load_context(self, context):
        # load your artifacts

    def predict(self, context, model_input):
        return my_predict(model_input.values)
</code></pre>
<p>You can log to MLflow whatever artifacts you need for your models, define Conda environment if necessary, etc.<br />
Then you can use <code>save_model</code> with your class to save your implementation, that could be loaded with <code>load_model</code> and do the <code>predict</code> using your model:</p>
<pre class=""lang-py prettyprint-override""><code>mlflow.pyfunc.save_model(
        path=mlflow_pyfunc_model_path, 
        python_model=MyModel(), 
        artifacts=artifacts)

# Load the model in `python_function` format
loaded_model = mlflow.pyfunc.load_model(mlflow_pyfunc_model_path)
</code></pre>
","18627",3
1083,65905254,2,65889143,2021-01-26 16:30:05,0,"<p>According to the <a href=""https://sagemaker.readthedocs.io/en/stable/api/utility/session.html#sagemaker.session.Session.upload_data"" rel=""nofollow noreferrer"">documentation</a> <code>train_data</code> is the local path of the file to upload to S3, so you need this file locally where you are launching the training job. If you are using a notebook this is not the way to do. You have instead to manually upload your dataset in a S3 bucket. I suggest to preprocess your dataset in a single file (tfrecord for example if you are using TF) and upload that file to S3. You can do it using the AWS web console or using the AWS-CLI with the <code>aws s3 cp yourfile s3://your-bucket </code>command.</p>
","4267439",0
1084,66034048,2,65819978,2021-02-03 19:01:31,1,"<p>yes, it is possible to deploy the built in image classification models as a SageMaker multi model endpoint. The key is that the image classification uses <a href=""https://mxnet.apache.org/versions/1.7.0/"" rel=""nofollow noreferrer"">Apache MXNet</a>. You can extract the model artifacts (SageMaker stores them in a zip file named model.tar.gz in S3), then load them in to MXNet. The SageMaker MXNet container supports multi model endpoints, so you can use that to deploy the model.</p>
<p>If you unzip the model.tar.gz from this algorithm, you'll find three files:</p>
<p>image-classification-****.params</p>
<p>image-classification-symbol.json</p>
<p>model-shapes.json</p>
<p>The MxNet container expects these files to be named <strong>image-classification-0000.params, model-symbol.json, and model-shapes.json</strong>. So I unzipped the zip file, renamed the files and rezipped them. For more information on the MXNet container check out the <a href=""https://github.com/aws/sagemaker-mxnet-inference-toolkit"" rel=""nofollow noreferrer"">GitHub repository</a>.</p>
<p>After that you can deploy the model as a single MXNet endpoint using the SageMaker SDK with the following code:</p>
<pre><code>from sagemaker import get_execution_role
from sagemaker.mxnet.model import MXNetModel

role = get_execution_role()

mxnet_model = MXNetModel(model_data=s3_model, role=role, 
                         entry_point='built_in_image_classifier.py', 
                         framework_version='1.4.1',
                         py_version='py3')

predictor = mxnet_model.deploy(instance_type='ml.c4.xlarge', initial_instance_count=1)
</code></pre>
<p>The entry point Python script can be an empty Python file for now. We will be using the default inference handling provided by the MXNet container.</p>
<p>The default MXNet container only accepts JSON, CSV, and Numpy arrays as valid input. So you will have to format your input in to one of these three formats. The code below demonstrates how I did it with Numpy arrays:</p>
<pre><code>import cv2
import io

np_array = cv2.imread(filename=img_filename)
np_array = np_array.transpose((2,0,1))
np_array = np.expand_dims(np_array, axis=0)

buffer = io.BytesIO()
np.save(buffer, np_array)

response = sm.invoke_endpoint(EndpointName='Your_Endpoint_name', Body=buffer.getvalue(), ContentType='application/x-npy')
</code></pre>
<p>Once you have a single endpoint working with MXNet container, you should be able to get it running in multi model endpoint using the <a href=""https://sagemaker.readthedocs.io/en/stable/api/inference/multi_data_model.html"" rel=""nofollow noreferrer"">SageMaker MultiDataModel constructor</a>.</p>
<p>If you want to use a different input data type so you don't have to do the preprocessing in your application code, you can overwrite the input_fn method in the MxNet container by providing it in the entry_point script. <a href=""https://sagemaker.readthedocs.io/en/stable/frameworks/mxnet/using_mxnet.html"" rel=""nofollow noreferrer"">See here</a> for more information. If you do this, you could pass the image bytes directly to SageMaker, without formatting the numpy arrays.</p>
","12633603",3
1085,66035378,2,66020144,2021-02-03 20:43:11,7,"<p>I agree that this could probably be better documented, but fortunately, it's a simple implementation.</p>
<p>this is how you get a run object for an already submitted run for <code>azureml-sdk&gt;=1.16.0</code> (for the older approach <a href=""https://stackoverflow.com/questions/62949488/amls-experiment-run-stuck-in-status-running/62958369#62958369"">see my answer here</a>)</p>
<pre class=""lang-py prettyprint-override""><code>from azureml.core import Workspace

ws = Workspace.from_config()
run = ws.get_run('YOUR_RUN_ID')
</code></pre>
<p>once you have the <code>run</code> object, you can call methods like</p>
<ul>
<li><code>.get_file_names()</code> to see what files are available (the logs in <code>azureml-logs/</code> and <code>logs/azureml/</code> will also be listed)</li>
<li><code>.download_file()</code> to download an individual file</li>
<li><code>.download_files()</code> to download all files that match a given prefix (or all the files)</li>
</ul>
<p>See the <a href=""https://learn.microsoft.com/en-us/python/api/azureml-core/azureml.core.run(class)?view=azure-ml-py&amp;WT.mc_id=AI-MVP-5003930"" rel=""noreferrer"">Run object docs</a> for more details.</p>
","3842610",3
1086,66055048,2,66016802,2021-02-04 22:52:48,1,"<p>One way to do this is to have two separate clusters. One large cluster for training and another smaller cluster for serving. You could use Kubeflow Pipelines on the larger cluster, train the model and then place the model file in distributed storage. On the smaller cluster you could just run <a href=""https://github.com/kubeflow/kfserving#standalone-kfserving-installation"" rel=""nofollow noreferrer"">KFServing standalone</a> and load the model binary from distributed storage into your Inference service.</p>
","1197913",1
1087,66056741,2,65795579,2021-02-05 02:26:04,0,"<p>The issue was in the YAML file. <strong>The dependencies/libraries in the YAML should be according to conda environment</strong>. So, I changed everything accordingly, and it worked.</p>
<p>Modified YAML file:</p>
<pre><code>name: gender_prediction
dependencies:
- python=3.7
- numpy
- scikit-learn
- pip:
    - azureml-defaults
    - pandas
    - pickle4
    - regex
    - inference-schema[numpy-support]   
</code></pre>
","12930433",0
1088,66071542,2,66062015,2021-02-05 22:37:51,2,"<p>AFAIK you'll have to create your own Runtime as <code>R</code> isn't supported natively.</p>
<p>Have you already tried <a href=""https://learn.microsoft.com/en-us/azure/azure-functions/functions-create-function-linux-custom-image?tabs=bash%2Cportal&amp;pivots=programming-language-other"" rel=""nofollow noreferrer"">&quot;Create a function on Linux using a custom container&quot;</a>? Interestingly they have given <code>R</code> as the example of custom runtime, so hopefully that answers your question.</p>
","496289",1
1089,66075896,2,64700093,2021-02-06 10:35:49,0,"<p>I fixed this problem.</p>
<p>The error was that the Sagemaker ipynb notebook was opened in conda_python2.7 or so. Just re-wrote the script in conda_python3 and then everything worked fine :)</p>
","11371807",0
1090,66079679,2,62673468,2021-02-06 17:18:16,1,"<p>The most seamless way is to do a <a href=""https://kubernetes.io/docs/tutorials/kubernetes-basics/update/update-intro/"" rel=""nofollow noreferrer"">rolling update</a> via k8s/kubectl. This will require a new container, however, this is considered a best practice, as each container stays atomic and reproducible. Updating the model via threads would be difficult to debug.</p>
<p>Another scenario you could do is <a href=""https://semaphoreci.com/blog/continuous-blue-green-deployments-with-kubernetes"" rel=""nofollow noreferrer"">blue-green deployment</a> using Istio, and slowly move traffic between the old and new model, although this would require a bit more overhead.</p>
","1197913",0
1091,66080135,2,64784040,2021-02-06 17:58:59,2,"<p>You could use <a href=""https://github.com/jupyterhub/jupyterhub"" rel=""nofollow noreferrer"">Jupyterhub</a> and install it on your private cloud. This should give you both isolation and authentication.</p>
","1197913",0
1092,66093400,2,66086605,2021-02-07 21:41:32,0,"<p>indeed you're right, <strong>SageMaker Random Cut Forest cannot be trained and deployed locally. The 18 Amazon SageMaker Built-in algorithms are designed to be trained and deployed on Amazon SageMaker.</strong> There are 2 exceptions: SageMaker BlazingText and SageMaker XGBoost, which can be read with their open-source counterparts (fastText and XGBoost) and used for  inference out of SageMaker (eg EC2, Lambda, on-prem or on your laptop - as long as you can install those libraries)</p>
<p>There is an open-source attempt to implement the Random Cut Forest here <a href=""https://github.com/kLabUM/rrcf"" rel=""nofollow noreferrer"">https://github.com/kLabUM/rrcf</a> ; I don't think it has any connection to SageMaker RCF codebase so results, speed and scalability may differ.</p>
","5331834",1
1093,66096811,2,66095293,2021-02-08 06:27:35,3,"<p>As you thought you are right in case of using VM and that be easy to test it out.</p>
<p>Instead of setting up Kubernetes on docker you can use Linux base container for development testing.</p>
<p>There is linux container available name as LXC container. Docker is kind of application container while in simple words LXC is like VM for local development testing. you can install the stuff into rather than docker setting up application inside image.</p>
<p>read some details about lxc : <a href=""https://medium.com/@harsh.manvar111/lxc-vs-docker-lxc-101-bd49db95933a"" rel=""nofollow noreferrer"">https://medium.com/@harsh.manvar111/lxc-vs-docker-lxc-101-bd49db95933a</a></p>
<p>you can also run it on windows and try it out at : <a href=""https://linuxcontainers.org/"" rel=""nofollow noreferrer"">https://linuxcontainers.org/</a></p>
<p>If you have read the documentation of <strong>Kubeflow</strong> there is also one option <a href=""https://www.kubeflow.org/docs/started/workstation/getting-started-linux/#multipass"" rel=""nofollow noreferrer"">multipass</a></p>
<blockquote>
<p>Multipass creates a Linux virtual machine on Windows, Mac or Linux
systems. The VM contains a complete Ubuntu operating system which can
then be used to deploy Kubernetes and Kubeflow.</p>
</blockquote>
<p>Learn more about Multipass : <a href=""https://multipass.run/#install"" rel=""nofollow noreferrer"">https://multipass.run/#install</a></p>
","5525824",2
1094,66116970,2,65991587,2021-02-09 10:17:20,1,"<p>There is something weird happening on your code, you are getting the data from a first workspace (<code>workspace = Workspace(subscription_id, resource_group, workspace_name)</code>), then using the resources from a second one (<code>ws = Workspace.from_config()</code>). I would suggest avoiding having code relying on two different workspaces, especially when you know that an underlying datasource can be registered (linked) to multiple workspaces (<a href=""https://learn.microsoft.com/en-us/azure/machine-learning/how-to-access-data#create-and-register-datastores"" rel=""nofollow noreferrer"">documentation</a>).</p>
<p>In general using a <code>config.json</code> file when instantiating a <code>Workspace</code> object will result in an interactive authentication. When your code will be processed and you will have a log asking you to reach a specific URL and enter a code. This will use your Microsoft account to verify that you are authorized to access the Azure resource (in this case your <code>Workspace('mysubid', 'myrg', 'mlplayground')</code>). This has its limitations when you start deploying the code onto virtual machines or agents, you will not always manually check the logs, access the URL and authenticate yourself.</p>
<p>For this matter it is strongly recommended setting up more advanced authentication methods and personally I would suggest using the service principal one since it is simple, convinient and secure if done properly.
You can follow Azure's official documentation <a href=""https://learn.microsoft.com/en-us/azure/machine-learning/how-to-setup-authentication#configure-a-service-principal"" rel=""nofollow noreferrer"">here</a>.</p>
","6518873",0
1095,66239529,2,65871038,2021-02-17 09:55:20,0,"<p>Not at the moment maybe in an upcoming version.
if you check the link that you've mentionned you'll find that features support the following types (dates are not included):</p>
<pre><code>enum FeatureType {
  TYPE_UNKNOWN = 0;
  BYTES = 1;
  INT = 2;
  FLOAT = 3;
  STRUCT = 4;
}
</code></pre>
","14414262",1
1096,66240292,2,66239966,2021-02-17 10:40:57,1,"<p>The path should be:</p>
<pre><code>data_location = 's3://{}/{}'.format(bucket, data_key)
</code></pre>
<p>where <code>bucket</code> is <code>&lt;bucket-name&gt;</code> <strong>not ARN</strong>. For example <code>bucket=my-bucket-333222</code>.</p>
","248823",1
1097,66342031,2,66320831,2021-02-23 22:28:27,2,"<p>I have three solutions to choose from depending on your deployment requirements. In order of difficulty:</p>
<ol>
<li>Use the Spark &quot;uber jar&quot; job server. This starts an embedded job server inside the Spark master, instead of using a standalone job server in a container. This would simplify your deployment a lot, since you would not need to start the <code>beam_spark_job_server</code> container at all.</li>
</ol>
<pre class=""lang-sh prettyprint-override""><code>python -m apache_beam.examples.wordcount \
--output ./data_test/ \
--runner=SparkRunner \
--spark_submit_uber_jar \
--spark_master_url=spark://spark-master:7077 \
--environment_type=LOOPBACK
</code></pre>
<ol start=""2"">
<li><p>You can pass the properties through a Spark configuration file. Create the Spark configuration file, and add <code>spark.driver.host</code> and whatever other properties you need. In the <code>docker run</code> command for the job server, mount that configuration file to the container, and set the <code>SPARK_CONF_DIR</code> environment variable to point to that directory.</p>
</li>
<li><p>If that neither of those work for you, you can alternatively build your own customized version of the job server container. Pull Beam source from Github. Check out the release branch you want to use (e.g. <code>git checkout origin/release-2.28.0</code>). Modify the entrypoint <a href=""https://github.com/apache/beam/blob/92ddb5c0bf61174e2153eef2a294a61bc924156f/runners/spark/job-server/container/spark-job-server.sh#L28"" rel=""nofollow noreferrer"">spark-job-server.sh</a> and set <code>-Dspark.driver.host=x</code> there. Then build the container using <code>./gradlew :runners:spark:job-server:container:docker -Pdocker-repository-root=&quot;your-repo&quot; -Pdocker-tag=&quot;your-tag&quot;</code>.</p>
</li>
</ol>
","4400443",8
1098,66576996,2,66576663,2021-03-11 05:33:26,2,"<p>I'm not sure why it has to be one csv file. There are many Python-based libraries for working with a dataset spread across multiple csvs.</p>
<ul>
<li><a href=""https://docs.dask.org/en/latest/dataframe.html"" rel=""nofollow noreferrer"">Dask DataFrame API</a></li>
<li><a href=""https://stackoverflow.com/questions/37639956/how-to-import-multiple-csv-files-in-a-single-load""><code>pyspark</code></a></li>
<li><a href=""https://learn.microsoft.com/en-us/azure/machine-learning/how-to-create-register-datasets?WT.mc_id=AI-MVP-5003930#create-a-tabulardataset"" rel=""nofollow noreferrer"">Azure Machine Learning Tabular Dataset</a></li>
</ul>
<p>In all of the examples, you pass a <a href=""https://en.wikipedia.org/wiki/Glob_%28programming%29"" rel=""nofollow noreferrer""><code>glob</code> pattern</a>, that will match multiple files. This pattern works very naturally with Azure ML Dataset which you can use as your input. See this excerpt from the docs link above.</p>
<pre class=""lang-py prettyprint-override""><code>from azureml.core import Workspace, Datastore, Dataset

datastore_name = 'your datastore name'

# get existing workspace
workspace = Workspace.from_config()
    
# retrieve an existing datastore in the workspace by name
datastore = Datastore.get(workspace, datastore_name)

# create a TabularDataset from 3 file paths in datastore
datastore_paths = [(datastore, 'weather/2018/11.csv'),
                   (datastore, 'weather/2018/12.csv'),
                   (datastore, 'weather/2019/*.csv')] # here's the glob pattern

weather_ds = Dataset.Tabular.from_delimited_files(path=datastore_paths)
</code></pre>
<p>Assuming that all the csvs can fit into memory, you can turn these datasets easily into <code>pandas</code> dataframes. <a href=""https://learn.microsoft.com/en-us/azure/machine-learning/how-to-train-with-datasets#access-dataset-in-training-script"" rel=""nofollow noreferrer"">with Azure ML Datasets,</a> you call</p>
<pre class=""lang-py prettyprint-override""><code># get the input dataset by name
dataset = Dataset.get_by_name(ws, name=dataset_name)
# load the TabularDataset to pandas DataFrame
df = dataset.to_pandas_dataframe()
</code></pre>
<p>With Dask Dataframe, <a href=""https://github.com/dask/dask/issues/1651#issuecomment-253187846"" rel=""nofollow noreferrer"">this GitHub issue</a> says you can call</p>
<pre class=""lang-py prettyprint-override""><code>df = my_dask_df.compute()
</code></pre>
<p>As far as output datasets, you can control this by reading in the output CSV as a dataframe, appending data to it then overwriting it to the same location.</p>
","3842610",5
1099,66710387,2,66561959,2021-03-19 14:47:12,2,"<p>I believe that Tarun might on the right path. The BrokenPipeError that you got is thrown when the connection is abruptly closed. See <a href=""https://docs.python.org/3/library/exceptions.html#BrokenPipeError"" rel=""nofollow noreferrer"">the python docs for BrokenPipeError</a>.
The SageMaker endpoint probably drops the connection as soon as you go over the limit of 5MB. I suggest you try a smaller dataset. Also the data you send might get enlarged because of how sagemaker.tensorflow.model.TensorFlowPredictor encodes the data according to <a href=""https://github.com/aws/sagemaker-python-sdk/issues/799#issuecomment-492698717"" rel=""nofollow noreferrer"">this comment</a> on a similar issue.</p>
<p>If that doesn't work I've also seen a couple of people having problems with their networks in general. Specifically firewall/antivirus (<a href=""https://github.com/aws/aws-cli/issues/3999#issuecomment-531151161"" rel=""nofollow noreferrer"">for example this comment</a>) or network timeout.</p>
<p>Hope this points you in the right direction.</p>
","11260936",0
1100,66720288,2,65434323,2021-03-20 10:03:52,4,"<p>Here is the solution that worked for me. Simply follow the following steps.</p>
<p>1 - Load your model in the SageMaker's jupyter environment with the help of</p>
<pre><code>from keras.models import load_model

model = load_model (&lt;Your Model name goes here&gt;) #In my case it's model.h5
</code></pre>
<p>2 - Now that the model is loaded convert it into the <code>protobuf format</code> that is required by <code>AWS</code> with the help of</p>
<pre><code>def convert_h5_to_aws(loaded_model):

from tensorflow.python.saved_model import builder
from tensorflow.python.saved_model.signature_def_utils import predict_signature_def
from tensorflow.python.saved_model import tag_constants

model_version = '1'
export_dir = 'export/Servo/' + model_version
# Build the Protocol Buffer SavedModel at 'export_dir'
builder = builder.SavedModelBuilder(export_dir)
# Create prediction signature to be used by TensorFlow Serving Predict API
signature = predict_signature_def(
    inputs={&quot;inputs&quot;: loaded_model.input}, outputs={&quot;score&quot;: loaded_model.output})
from keras import backend as K

with K.get_session() as sess:
    # Save the meta graph and variables
    builder.add_meta_graph_and_variables(
        sess=sess, tags=[tag_constants.SERVING], signature_def_map={&quot;serving_default&quot;: signature})
    builder.save()
import tarfile
with tarfile.open('model.tar.gz', mode='w:gz') as archive:
    archive.add('export', recursive=True)
import sagemaker

sagemaker_session = sagemaker.Session()
inputs = sagemaker_session.upload_data(path='model.tar.gz', key_prefix='model')
convert_h5_to_aws(model):
</code></pre>
<p>3 - And now you can deploy your model with the help of</p>
<pre><code>!touch train.py
from sagemaker.tensorflow.model import TensorFlowModel
sagemaker_model = TensorFlowModel(model_data = 's3://' + sagemaker_session.default_bucket() + '/model/model.tar.gz',
                                  role = role,
                                  framework_version = '1.15.2',
                                  entry_point = 'train.py')
%%timelog
predictor = sagemaker_model.deploy(initial_instance_count=1,
                                   instance_type='ml.m4.xlarge')
</code></pre>
<p>This will generate the endpoint which can be seen in the Inference section of the Amazon SageMaker and with the help of that endpoint you can now make predictions from the jupyter notebook as well as from web and mobile applications.
This <a href=""https://www.youtube.com/watch?v=RPnvfxR5DY8"" rel=""nofollow noreferrer"">Youtube tutorial</a> by Liam and <a href=""https://aws.amazon.com/blogs/machine-learning/deploy-trained-keras-or-tensorflow-models-using-amazon-sagemaker/"" rel=""nofollow noreferrer"">AWS blog</a> by Priya helped me alot.</p>
","9920934",2
1101,66790512,2,66785273,2021-03-24 22:47:56,3,"<p>two big ideas here -- let's start with the main one.</p>
<h2>main ask</h2>
<blockquote>
<p>With an Azure ML Pipeline, how can I access the output data of a <code>PythonScriptStep</code> outside of the context of the pipeline?</p>
</blockquote>
<h3>short answer</h3>
<p>Consider using <code>OutputFileDatasetConfig</code> (<a href=""https://learn.microsoft.com/en-us/python/api/azureml-core/azureml.data.output_dataset_config.outputdatasetconfig?view=azure-ml-py&amp;viewFallbackFrom=experimental&amp;WT.mc_id=AI-MVP-5003930"" rel=""nofollow noreferrer"">docs</a> <a href=""http://%20https://learn.microsoft.com/en-us/azure/machine-learning/how-to-move-data-in-out-of-pipelines?WT.mc_id=AI-MVP-5003930#use-outputfiledatasetconfig-for-intermediate-data"" rel=""nofollow noreferrer"">example</a>), instead of <code>DataReference</code>.</p>
<p>To your example above, I would just change your last two definitions.</p>
<pre class=""lang-py prettyprint-override""><code>data_ref = OutputFileDatasetConfig(
    name='data_ref',
    destination=(ds, '/data')
).as_upload()


data_prep_step = PythonScriptStep(
    name='data_prep',
    script_name='pipeline_steps/data_prep.py',
    source_directory='/.',
    arguments=[
        '--main_path', main_ref,
        '--data_ref_folder', data_ref
                ],
    inputs=[main_ref, data_ref],
    outputs=[data_ref],
    runconfig=arbitrary_run_config,
    allow_reuse=False
)
</code></pre>
<p>some notes:</p>
<ul>
<li>be sure to check out how <code>DataPath</code>s work. Can be tricky at first glance.</li>
<li>set <code>overwrite=False</code> in the `.as_upload() method if you don't want future runs to overwrite the first run's data.</li>
</ul>
<h3>more context</h3>
<p><code>PipelineData</code> used to be the defacto object to pass data ephemerally between pipeline steps. The idea was to make it easy to:</p>
<ol>
<li>stitch steps together</li>
<li>get the data after the pipeline runs if need be (<code>datastore/azureml/{run_id}/data_ref</code>)</li>
</ol>
<p>The downside was that you have no control over <em>where</em> the pipeline is saved. If you wanted to data for more than just as a baton that gets passed between steps, you could have a <code>DataTransferStep</code> to land the <code>PipelineData</code> wherever you please after the <code>PythonScriptStep</code> finishes.</p>
<p>This downside is what motivated <code>OutputFileDatasetConfig</code></p>
<h2>auxilary ask</h2>
<blockquote>
<p>how might I programmatically test the functionality of my Azure ML pipeline?</p>
</blockquote>
<p>there are not enough people talking about data pipeline testing, IMHO.</p>
<p>There are three areas of data pipeline testing:</p>
<ol>
<li>unit testing (the code in the step works?</li>
<li>integration testing (the code works when submitted to the Azure ML service)</li>
<li>data expectation testing (the data coming out of the meets my expectations)</li>
</ol>
<p>For #1, I think it should be done outside of the pipeline perhaps as part of a package of helper functions
For #2, Why not just see if the whole pipeline completes, I think get more information that way. That's how we run our CI.</p>
<p>#3 is the juiciest, and we do this in our pipelines with the <a href=""https://greatexpectations.io/"" rel=""nofollow noreferrer"">Great Expectations (GE)</a> Python library. The GE community calls these &quot;expectation tests&quot;. To me you have two options for including expectation tests in your Azure ML pipeline:</p>
<ol>
<li>within the <code>PythonScriptStep</code> itself, i.e.
<ol>
<li>run whatever code you have</li>
<li>test the outputs with GE before writing them out; or,</li>
</ol>
</li>
<li>for each functional <code>PythonScriptStep</code>, hang a downstream <code>PythonScriptStep</code> off of it in which you run your expectations against the output data.</li>
</ol>
<p>Our team does #1, but either strategy should work. What's great about this approach is that you can run your expectation tests by just running your pipeline (which also makes integration testing easy).</p>
","3842610",1
1102,66794905,2,66152375,2021-03-25 07:27:52,3,"<p>That is primarily because you have started a run with <code>default experiment name</code> and then you are trying to set the <code>experiment_name</code> as &quot;TNF_EXP&quot;.</p>
<p>Will suggest you to make use of <code>mlflow.run(..., experiment_name=&quot;TNF_EXP&quot;)</code> python method then running it from the <code>CLI</code>.</p>
<p>You can find more information <a href=""https://www.mlflow.org/docs/latest/python_api/mlflow.html#mlflow.run"" rel=""nofollow noreferrer"">here</a>.</p>
","14876608",0
1103,66795908,2,66782040,2021-03-25 08:47:21,0,"<p>Since you can't manually terminate a sagemaker instance, run an Amazon SageMaker Managed Spot training for a small number of epochs, Amazon SageMaker would have backed up your checkpoint files to S3. Check that checkpoints are there. Now run a second training run, but this time provide the first jobs’ checkpoint location to <code>checkpoint_s3_uri</code>. Reference is <a href=""https://towardsdatascience.com/a-quick-guide-to-using-spot-instances-with-amazon-sagemaker-b9cfb3a44a68"" rel=""nofollow noreferrer"">here</a>, this also answer your second question.</p>
","4267439",4
1104,66899132,2,66899120,2021-04-01 06:00:09,0,"<p>You have <strong>space</strong> in the name. It should be:</p>
<pre><code>role ='arn:aws:iam::94911111111542:role/SageMaker-Full-Access'
</code></pre>
","248823",1
1105,66981008,2,66923216,2021-04-07 06:57:37,2,"<p>There is no possible way to change the compute disk type if you use the Azure ML compute cluster and compute instance. Only when you use the extra computer, you can manage the separate resources such as the disk, network, and so on. For example, you attach a VM as the target computer to the Azure ML. Then when you create the VM you can set the disk type with HDD.</p>
","9773937",0
1106,66990097,2,66989018,2021-04-07 16:20:59,2,"<p>There was one step missing which is not mentioned in the tutorial, which is, I have to install docker. I've installed docker, rebooted the machine, and now everything works fine.</p>
","4534466",0
1107,66990449,2,63775893,2021-04-07 16:42:26,4,"<p>The newer versions of SageMaker SDK have a more centralized API for getting the URIs:</p>
<pre class=""lang-py prettyprint-override""><code>import sagemaker 
sagemaker.image_uris.retrieve(&quot;linear-learner&quot;, &quot;eu-central-1&quot;)
</code></pre>
<p>which gives the expected result:</p>
<pre><code>664544806723.dkr.ecr.eu-central-1.amazonaws.com/linear-learner:1
</code></pre>
","4118756",0
1108,66995166,2,66978458,2021-04-07 22:54:26,1,"<p>We are currently developing test-set ingestion in the UI. However, currently there is no way to upload test data through the UI to populate these graphs. This experience can only be accessed by kicking off an explanation through the SDK with the test data. We refer to this as &quot;Interpretability at inference time&quot; and have some documentation on how to do this here: <a href=""https://learn.microsoft.com/en-us/azure/machine-learning/how-to-machine-learning-interpretability-aml#interpretability-at-inference-time"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/machine-learning/how-to-machine-learning-interpretability-aml#interpretability-at-inference-time</a></p>
<p>Test-set ingestion is scoped to land for private preview before end of June. Let's keep in touch to ensure you get early access here.</p>
<p>Thanks,
Sabina</p>
","12875447",0
1109,67093568,2,67093041,2021-04-14 14:21:31,1,"<p>Make sure your estimator has</p>
<ul>
<li>framework_version = '2.3',</li>
<li>py_version = 'py37',</li>
</ul>
","11222526",0
1110,67119340,2,67104752,2021-04-16 04:58:58,3,"<p>We need to use the special <code>remote://</code> URL format to <code>add --external</code> data using existing remote configurations:</p>
<pre><code>dvc add --external remote://s3remote/wine-quality.csv
</code></pre>
<p>wine-quality.csv is in location s3://dvc-example/wine-quality.csv, here <strong>remote://s3remote</strong> is replaced by <code>url</code> string in config file.</p>
<p>I got answer after posting question in DVC forum.
<a href=""https://discuss.dvc.org/t/dvc-add-external-s3-mybucket-data-csv-is-failing-with-access-error-even-after-giving-correct-remote-cache-configurations/726/5"" rel=""nofollow noreferrer"">Link to answer</a></p>
","8089726",0
1111,67122976,2,67122683,2021-04-16 09:48:37,3,"<blockquote>
<p>In all cases, files are added to dvc with the command dvc add %filename%.</p>
</blockquote>
<p>It seems like there is a high chance that one of the dvc files created in newer versions of dvc and you are trying to operate with an older version. Are all of your colleagues use the same dvc version when adding new files?</p>
","15330941",4
1112,67123409,2,67123040,2021-04-16 10:18:25,2,"<p>Based on the comments.</p>
<p>If no new resources are to be created (no CloudWatch Event rules, lambda functions) nor any changes to existing Step Function are allowed, then <strong>pooling iteratively</strong> <code>list_executions</code> would be the best solution.</p>
<p>AWS CLI and boto3 have implemented similar solutions (not for Step Functions), but for some other services. They are called <code>waiters</code> (e.g. <a href=""https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/ec2.html#waiters"" rel=""nofollow noreferrer"">ec2 waiters</a>). So basically you would have to create your own <strong>waiter for Step Function</strong>, as AWS does not provide one for that. AWS uses <strong>15 seconds</strong> sleep time from what I recall for its waiters.</p>
","248823",0
1113,67166354,2,66226685,2021-04-19 17:05:44,1,"<p>I uploaded and registered the files with this script and everything works as expected.</p>
<pre class=""lang-py prettyprint-override""><code>from azureml.core import Datastore, Dataset, Workspace

import logging

logger = logging.getLogger(__name__)
logging.basicConfig(
    level=logging.INFO,
    format=&quot;%(asctime)s.%(msecs)03d %(levelname)s %(module)s - %(funcName)s: %(message)s&quot;,
    datefmt=&quot;%Y-%m-%d %H:%M:%S&quot;,
)

datastore_name = &quot;mydatastore&quot;
dataset_path_on_disk = &quot;./data/images_greyscale&quot;
dataset_path_in_datastore = &quot;images_greyscale&quot;

azure_dataset_name = &quot;images_grayscale&quot;
azure_dataset_description = &quot;dataset transformed into the coco format and into grayscale images&quot;


workspace = Workspace.from_config()
datastore = Datastore.get(workspace, datastore_name=datastore_name)

logger.info(&quot;Uploading data...&quot;)
datastore.upload(
    src_dir=dataset_path_on_disk, target_path=dataset_path_in_datastore, overwrite=False
)
logger.info(&quot;Uploading data done.&quot;)

logger.info(&quot;Registering dataset...&quot;)
datastore_path = [(datastore, dataset_path_in_datastore)]
dataset = Dataset.File.from_files(path=datastore_path)
dataset.register(
    workspace=workspace,
    name=azure_dataset_name,
    description=azure_dataset_description,
    create_new_version=True,
)
logger.info(&quot;Registering dataset done.&quot;)

</code></pre>
","2577416",0
1114,67167050,2,67161293,2021-04-19 17:59:29,2,"<p>The files should've been mounted at path &quot;https%3A/vladiliescu.net/images/deploying-models-with-azure-ml-pipelines.jpg&quot; and &quot;https%3A/vladiliescu.net/images/reverse-engineering-automated-ml.jpg&quot;.</p>
<p>We retain the directory structure following the url structure to avoid potential conflicts.</p>
","12600227",0
1115,67175474,2,67131617,2021-04-20 08:45:21,0,"<p>You can use <a href=""https://docs.aws.amazon.com/secretsmanager/latest/userguide/intro.html"" rel=""nofollow noreferrer"">AWS Secrets Manager</a> to store and access credentials. Your Sagemaker execution role should have permission to read from Secrets Manager (AFAIK AWS managed-role does have it). This is the same mechanism that's used by Sagemaker notebooks to get access to github repo, for example</p>
","5242656",0
1116,67184181,2,67051900,2021-04-20 18:08:52,0,"<p>You can generate forecast quantiles. See the following notebook for more details: <a href=""https://github.com/Azure/MachineLearningNotebooks/blob/master/how-to-use-azureml/automated-machine-learning/forecasting-forecast-function/auto-ml-forecasting-function.ipynb"" rel=""nofollow noreferrer"">https://github.com/Azure/MachineLearningNotebooks/blob/master/how-to-use-azureml/automated-machine-learning/forecasting-forecast-function/auto-ml-forecasting-function.ipynb</a></p>
","12875447",4
1117,67186379,2,67176637,2021-04-20 21:09:13,2,"<p>I am answering my own question after searching and reading forums,</p>
<p>The easiest way to get bearer token is to install AWS CLI and configure it, using <code>aws configure</code> command.
For configuring, we must need to know access key, secret key, region of user. These things can be get by AWS users section.
After configuration by running this command, <code>aws ecr get-authorization-token</code>, we can get authorizationToken. <a href=""https://docs.aws.amazon.com/cli/latest/reference/ecr/get-authorization-token.html"" rel=""nofollow noreferrer"">here</a> This token can be fed into bearer token, along with aws signature (access key and secret key) in authorization menu in Postman app.</p>
","12035438",0
1118,67187871,2,67186515,2021-04-21 00:31:28,1,"<p>At the beginning of inference.py add these lines:</p>
<pre><code>from subprocess import check_call, run, CalledProcessError
import sys
import os

# Since it is likely that you're going to run inference.py multiple times, this avoids reinstalling the same package:
if not os.environ.get(&quot;INSTALL_SUCCESS&quot;):
    
    try:
        check_call(
        [ sys.executable, &quot;pip&quot;, &quot;install&quot;, &quot;pycocotools&quot;,]
        )
    except CalledProcessError:
        run(
        [&quot;pip&quot;, &quot;install&quot;, &quot;pycocotools&quot;,]
        )
    os.environ[&quot;INSTALL_SUCCESS&quot;] = &quot;True&quot;
</code></pre>
","10653507",2
1119,67206331,2,67205469,2021-04-22 03:29:38,2,"<p>Yes you can use S3 as storage for your training datasets.</p>
<p>Refer diagram in this link describing how everything works together: <a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/how-it-works-training.html"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/sagemaker/latest/dg/how-it-works-training.html</a></p>
<p>You may also want to checkout following blogs that details about File mode and Pipe mode, two mechanisms for transferring training data:</p>
<ol>
<li><a href=""https://aws.amazon.com/blogs/machine-learning/accelerate-model-training-using-faster-pipe-mode-on-amazon-sagemaker/"" rel=""nofollow noreferrer"">https://aws.amazon.com/blogs/machine-learning/accelerate-model-training-using-faster-pipe-mode-on-amazon-sagemaker/</a></li>
</ol>
<blockquote>
<p>In File mode, the training data is downloaded first to an encrypted EBS volume attached to the training instance prior to commencing the training. However, in Pipe mode the input data is streamed directly to the training algorithm while it is running.</p>
</blockquote>
<ol start=""2"">
<li><a href=""https://aws.amazon.com/blogs/machine-learning/using-pipe-input-mode-for-amazon-sagemaker-algorithms/"" rel=""nofollow noreferrer"">https://aws.amazon.com/blogs/machine-learning/using-pipe-input-mode-for-amazon-sagemaker-algorithms/</a></li>
</ol>
<blockquote>
<p>With Pipe input mode, your data is fed on-the-fly into the algorithm container without involving any disk I/O. This approach shortens the lengthy download process and dramatically reduces startup time. It also offers generally better read throughput than File input mode. This is because your data is fetched from Amazon S3 by a highly optimized multi-threaded background process. It also allows you to train on datasets that are much larger than the 16 TB Amazon Elastic Block Store (EBS) volume size limit.</p>
</blockquote>
<p>The blog also contains python code snippets using Pipe input mode for reference.</p>
","1391837",0
1120,67208910,2,67196775,2021-04-22 07:42:57,1,"<p>I found a workaround, not the cleanest one though.</p>
<p>I import mlflow at the beginning even if it's not useful this way:</p>
<pre class=""lang-py prettyprint-override""><code>import mlflow
import logging

from opencensus.ext.azure.log_exporter import AzureLogHandler

logger = logging.getLogger(__name__)
logger.addHandler(AzureLogHandler(connection_string='InstrumentationKey=&lt;your-key&gt;'))
logger.warning('Hello, World!')

import mlflow
</code></pre>
","4553735",0
1121,67210005,2,67209146,2021-04-22 08:56:43,4,"<p>There are 3 steps in this process:</p>
<ol>
<li>Create a CSV dump. Many DBs have these tools but DVC does not support this natively.</li>
<li>Version the CSV dump and move it to some storage. DVC does this job.</li>
<li>Schedule periodical dump. You can use Cron (easy), AirFlow (not easy) or <a href=""https://docs.github.com/en/actions/reference/events-that-trigger-workflows"" rel=""nofollow noreferrer"">periodical jobs in GitHub Actions</a>/<a href=""https://docs.gitlab.com/ee/ci/pipelines/schedules.html"" rel=""nofollow noreferrer"">GitLab CI/CD</a>. Another project from the DVC team can help with CI/CD option: <a href=""https://cml.dev"" rel=""nofollow noreferrer"">https://cml.dev</a>.</li>
</ol>
","3072174",0
1122,67211018,2,67210677,2021-04-22 09:58:14,2,"<p>If you are training directly in the notebook the answer is yes.
However the best practice is not to train directly with the notebook.
Use instead the notebook (you can choose a very cheap instance for the notebook) to launch your training job (in the instance type you desire) adapting you code to be the entrypoint of the estimator. In that way, you can close the notebook after launching the training job and monitor the training job using cloudwatch. You can also define some regex to capture metrics from the stout and cloudwatch will automatically plot for you, which is very useful!
As a quick example.. in my notebook I have this cell:</p>
<pre><code>import sagemaker from sagemaker.tensorflow import TensorFlow from sagemaker import get_execution_role

bucket = 'mybucket'

train_data = 's3://{}/{}'.format(bucket,'train')

validation_data = 's3://{}/{}'.format(bucket,'test')

s3_output_location = 's3://{}'.format(bucket)

hyperparameters = {'epochs': 70, 'batch-size' : 32, 'learning-rate' :
0.01}

metrics = [{'Name': 'Loss', 'Regex': 'loss: ([0-9\.]+)'},
           {'Name': 'Accuracy', 'Regex': 'acc: ([0-9\.]+)'},
           {'Name': 'Epoch', 'Regex': 'Epoch ([0-9\.]+)'},
           {'Name': 'Validation_Acc', 'Regex': 'val_acc: ([0-9\.]+)'},
           {'Name': 'Validation_Loss', 'Regex': 'val_loss: ([0-9\.]+)'}]

tf_estimator = TensorFlow(entry_point='training.py', 
                          role=get_execution_role(),
                          train_instance_count=1, 
                          train_instance_type='ml.p2.xlarge',
                          train_max_run=172800,
                          output_path=s3_output_location,
                          framework_version='1.12',
                          py_version='py3',
                          metric_definitions = metrics,
                          hyperparameters = hyperparameters)

inputs = {'train': train_data, 'test': validation_data}

myJobName = 'myname'

tf_estimator.fit(inputs=inputs, job_name=myJobName)
</code></pre>
<p>My training script training.py is something like this:</p>
<pre><code>if __name__ =='__main__':

    parser = argparse.ArgumentParser()

    # input data and model directories
    parser.add_argument('--gpu-count', type=int, default=os.environ['SM_NUM_GPUS'])
    parser.add_argument('--model-dir', type=str, default=os.environ.get('SM_MODEL_DIR'))
    parser.add_argument('--train', type=str, default=os.environ.get('SM_CHANNEL_TRAIN'))
    parser.add_argument('--test', type=str, default=os.environ.get('SM_CHANNEL_TEST'))
    parser.add_argument('--learning-rate', type=float, default=0.0001)
    parser.add_argument('--batch-size', type=int, default=32)
    parser.add_argument('--epochs', type=int, default=1)
....
</code></pre>
","4267439",0
1123,67221065,2,67215839,2021-04-22 21:34:02,3,"<p>I think you don't need the templating feature in this case. As shown in this <a href=""https://dvc.org/doc/command-reference/params#examples-python-parameters-file"" rel=""nofollow noreferrer"">example</a>:</p>
<pre class=""lang-yaml prettyprint-override""><code>stages:
  train:
    cmd: python train.py
    deps:
      - users.csv
    params:
      - params.py:
          - BOOL
          - INT
          - TrainConfig.EPOCHS
          - TrainConfig.layers
    outs:
      - model.pkl
</code></pre>
<p>The way to redefine the default <code>params.yaml</code> is to specify the file name explicitly in the <code>params:</code> section:</p>
<pre class=""lang-yaml prettyprint-override""><code>params:
  - preproc.yaml:
    - important_parameter
</code></pre>
<p>Also, when you create a stage either with <a href=""https://dvc.org/doc/command-reference/run"" rel=""nofollow noreferrer""><code>dvc run</code></a> (not recommended) or <a href=""https://dvc.org/doc/command-reference/stage/add"" rel=""nofollow noreferrer""><code>dvc stage add</code></a>, you can provide the params file name explicitly as a prefix:</p>
<pre class=""lang-sh prettyprint-override""><code>dvc run -n train -d train.py -d logs/ -o users.csv -f \
          -p parse_params.yaml:threshold,classes_num \
          python train.py
</code></pre>
<p>Here ^^ <code>parse_params.yaml</code> is a custom params file.</p>
<p>Please, let me know if it solves the problem and if you have any other questions :)</p>
","298182",1
1124,67265148,2,67241248,2021-04-26 10:41:01,1,"<p>You cannot do this in ContainerOp, that was one of the reasons ContainerOp has been deprecated, refer to <a href=""https://github.com/kubeflow/pipelines/pull/4166"" rel=""nofollow noreferrer"">https://github.com/kubeflow/pipelines/pull/4166</a>.</p>
<p>Suggestions:</p>
<ol>
<li>Following <a href=""https://www.kubeflow.org/docs/components/pipelines/reference/component-spec/"" rel=""nofollow noreferrer"">https://www.kubeflow.org/docs/components/pipelines/reference/component-spec/</a> to build your reusable component yaml.</li>
<li>if you prefer inlining component yaml for one-off components, you can load it via <code>kfp.components.load_component_from_text</code> method refer to <a href=""https://github.com/kubeflow/pipelines/blob/f757a8842fd6d1ed3d37ab38c0707d127ea2048c/samples/core/output_a_directory/output_a_directory.py#L49"" rel=""nofollow noreferrer"">this example pipeline</a>.</li>
</ol>
","8745218",3
1125,67305496,2,67301583,2021-04-28 17:55:54,0,"<p>Nevermind, the userpool ARN has a typo.</p>
","12121648",0
1126,67315835,2,67258917,2021-04-29 10:46:49,0,"<p>Answer from python azure sdk: <em>In Studio, if you go to the step's Metrics tab, you will be able to see a chart/table of execution progress, including remaining items, remaining mini batches, failed items, etc.</em></p>
<p><a href=""https://github.com/Azure/azure-sdk-for-python/issues/18357"" rel=""nofollow noreferrer"">https://github.com/Azure/azure-sdk-for-python/issues/18357</a></p>
","1568781",0
1127,67316171,2,67258465,2021-04-29 11:08:05,0,"<p>Apparently you need to specify a local mount path to use side_inputs in more than one node:</p>
<pre class=""lang-py prettyprint-override""><code>bg_file_named = bg_file_ds.as_named_input(f&quot;bg&quot;)
bg_file_mnt = bg_file_named.as_mount(f&quot;/tmp/{str(uuid.uuid4())}&quot;)

...

parallelrun_step = ParallelRunStep(
    name='batch-inference',
    parallel_run_config=parallel_run_config,
    inputs=[frames_data_named.as_download()],
    arguments=[&quot;--bg_folder&quot;, bg_file_mnt],
    side_inputs=[bg_file_mnt],
    output=inference_frames_ds,
    allow_reuse=True
)
</code></pre>
<p>Sources:</p>
<ul>
<li><a href=""https://learn.microsoft.com/en-us/azure/machine-learning/how-to-debug-parallel-run-step"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/machine-learning/how-to-debug-parallel-run-step</a></li>
<li><a href=""https://github.com/Azure/azure-sdk-for-python/issues/18355"" rel=""nofollow noreferrer"">https://github.com/Azure/azure-sdk-for-python/issues/18355</a></li>
</ul>
","1568781",0
1128,67347467,2,67277764,2021-05-01 14:44:33,2,"<p>Solution:</p>
<p>Install openjdk through conda but specify conda-forge as the channel to install the package from.</p>
<pre><code>name: venv
channels:
  - defaults
  - conda-forge
dependencies:
  - conda-forge::openjdk=11.0.9.1
</code></pre>
<p><a href=""https://i.stack.imgur.com/AHCye.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/AHCye.png"" alt=""Conda Forge"" /></a></p>
","11612543",0
1129,67353446,2,67339965,2021-05-02 06:04:04,1,"<p>You would like to explore MLMD
<a href=""https://www.tensorflow.org/tfx/tutorials/mlmd/mlmd_tutorial"" rel=""nofollow noreferrer"">https://www.tensorflow.org/tfx/tutorials/mlmd/mlmd_tutorial</a></p>
<p>Edit: Interesting article: <a href=""https://containerjournal.com/kubeconcnc/the-most-crucial-component-in-an-ml-pipeline-is-invisible/"" rel=""nofollow noreferrer"">https://containerjournal.com/kubeconcnc/the-most-crucial-component-in-an-ml-pipeline-is-invisible/</a></p>
","260826",0
1130,67364783,2,67361483,2021-05-03 07:08:02,2,"<p>Can you check your Pandas versions? This error typically occurs when the pickled file was written in an old Pandas version. Your Sagemaker notebook probably runs Pandas &gt; 1.1 where as the Pandas in which the dataframe was pickled is probably &lt; 1.1</p>
","12638118",0
1131,67375742,2,67375607,2021-05-03 20:58:28,0,"<p>No actually I was missing the &quot;<code>!</code>&quot; in front of <code>sm-docker build .</code>
So <code>!sm-docker build .</code> is working.</p>
","6660372",0
1132,67384940,2,53154542,2021-05-04 12:38:54,1,"<p>AWS SageMaker is a robust machine learning service in AWS that manages every major aspect of machine learning implementation, including data preparation, model construction, training and fine-tuning, and deployment.</p>
<p><strong>Preparation</strong></p>
<p>SageMaker uses a range of resources to make it simple to prepare data for machine learning models, even though it comes from many sources or is in a variety of formats.</p>
<p>It's simple to mark data, including video, images, and text, that's automatically processed into usable data, with SageMaker Ground Truth. GroundWork will process and merge this data using auto-segmentation and a suite of tools to create a single data label that can be used in machine learning models. AWS, in conjunction with SageMaker Data Wrangler and SageMaker Processing, reduces a data preparation phase that may take weeks or months to a matter of days, if not hours.</p>
<p><strong>Build</strong></p>
<p>SageMaker Studio Notebooks centralize everything relevant to your machine learning models, allowing them to be conveniently shared along with their associated data. You can choose from a variety of built-in, open-source algorithms to start processing your data with SageMaker JumpStart, or you can build custom parameters for your machine learning model.</p>
<p>Once you've chosen a model, SageMaker starts processing data automatically and offers a simple, easy-to-understand interface for tracking your model's progress and performance.</p>
<p><strong>Training</strong></p>
<p>SageMaker provides a range of tools for training your model from the data you've prepared, including a built-in debugger for detecting possible errors.</p>
<p>Machine Learning
The training job's results are saved in an Amazon S3 bucket, where they can be viewed using other AWS services including AWS Quicksight.</p>
<p><strong>Deployment</strong></p>
<p>It's pointless to have strong machine learning models if they can't be easily deployed to your hosting infrastructure. Fortunately, SageMaker allows deploying machine learning models to your current services and applications as easy as a single click.</p>
<p>SageMaker allows for real-time data processing and prediction after installation. This has far-reaching consequences in a variety of areas, including finance and health. Businesses operating in the stock market, for example, may make real-time financial decisions about stock and make more attractive acquisitions by pinpointing the best time to buy.</p>
<p>Incorporation with Amazon Comprehend, allows for natural language processing, transforming human speech into usable data to train better models, or provide a chatbot to customers through Amazon Lex.</p>
<p><strong>In conclusion…</strong></p>
<p>Machine Learning is no longer a niche technological curiosity; it now plays a critical role in the decision-making processes of thousands of companies around the world. There has never been a better time to start your Machine Learning journey than now, with virtually unlimited frameworks and simple integration into the AWS system.</p>
","15397939",0
1133,67393472,2,67393339,2021-05-04 23:40:51,5,"<p>Short answer: there is no way to do that.</p>
<p>Long answer:
Dvc remote is a content-based storage, so names are not preserved. Dvc creates metafiles (*.dvc files) in your workspace that contain names and those files are usually tracked by git, so you need to use git remote and dvc remote together to have both filenames and their contents. Here is a more detailed explanation about the format of local and remote storage <a href=""https://dvc.org/doc/user-guide/project-structure/internal-files#structure-of-the-cache-directory"" rel=""noreferrer"">https://dvc.org/doc/user-guide/project-structure/internal-files#structure-of-the-cache-directory</a> . Also, checkout <a href=""https://dvc.org/doc/use-cases/sharing-data-and-model-files"" rel=""noreferrer"">https://dvc.org/doc/use-cases/sharing-data-and-model-files</a></p>
","2628602",0
1134,67413572,2,67398763,2021-05-06 07:26:37,0,"<p>According to the <a href=""https://cloud.google.com/ai-platform/prediction/docs/custom-prediction-routines"" rel=""nofollow noreferrer"">documentation</a>, you can only deploy a <em><strong>Custom prediction routine</strong></em> when using a <a href=""https://cloud.google.com/ai-platform/prediction/docs/machine-types-online-prediction#differences"" rel=""nofollow noreferrer"">legacy (MLS1) machine type</a> for your model version. However, you can not use a regional endpoint with this type of machine, as stated <a href=""https://cloud.google.com/ai-platform/prediction/docs/regional-endpoints#understanding_regional_endpoints"" rel=""nofollow noreferrer"">here</a>,</p>
<blockquote>
<ul>
<li>Regional endpoints only support Compute Engine (N1) machine types. You cannot use legacy (MLS1) machine types on regional endpoints.</li>
</ul>
</blockquote>
<p>As I can see, you have specified a regional endpoint with  the <code>--region</code> flag, which does not support the machine type you required for your use case. Thus, you need to change the model and its version to a global endpoint, so you won't face the error anymore.</p>
<p>In addition, when you specify a regional endpoint within <code>gcloud create model --region</code>, you need to specify the same region when creating the model's version. On the other hand, when creating a model in the global endpoint <code>gcloud create model --regions</code>, you can omit the region flag in the command <code>gcloud ai-platform versions create</code>. <em><strong>Note that the <code>--regions</code> command is used only for the global endpoint</strong></em></p>
<p><em>Lastly, I must point out that, as per <a href=""https://cloud.google.com/ai-platform/prediction/docs/regional-endpoints#global-endpoint-region"" rel=""nofollow noreferrer"">documentation</a>, when selecting a region for the global endpoint, using the <code>--regions</code> flag when creating the model, your prediction nodes run in the specified region. Although, the AI Platform Prediction infrastructure managing your resources might not necessarily run in the same region.</em></p>
","12571387",1
1135,67415158,2,67407702,2021-05-06 09:13:21,5,"<p>Usually, the reason for this error is the DVC version.</p>
<p>If the dvc.lock file has a DVC 2.* schema and you are using a lower version, it will throw this error.</p>
<p>Upgrade your DVC version, and it should work.</p>
","15832700",0
1136,67420097,2,67387249,2021-05-06 14:23:42,14,"<p>The <code>ScriptRunConfig</code> class now accepts a <code>docker_runtime_config</code> argument, which is where you pass the <code>DockerConfiguration</code> object.</p>
<p>So, the code would look something like this:</p>
<pre><code>from azureml.core import Environment
from azureml.core import ScriptRunConfig
from azureml.core.runconfig import DockerConfiguration

# Other imports and code...

# Specify VM and Python environment:
vm_env = Environment.from_conda_specification(name='my-test-env', file_path=PATH_TO_YAML_FILE)
vm_env.docker.base_image = 'mcr.microsoft.com/azureml/openmpi3.1.2-cuda10.2-cudnn7-ubuntu18.04'

docker_config = DockerConfiguration(use_docker=True)

# Finally, use the environment in the ScriptRunConfig:
src = ScriptRunConfig(source_directory=DEPLOY_CONTAINER_FOLDER_PATH,
                      script=SCRIPT_FILE_TO_EXECUTE,
                      arguments=EXECUTE_ARGUMENTS,
                      compute_target=compute_target,
                      environment=vm_env,
                      docker_runtime_config=docker_config)
</code></pre>
","9964109",3
1137,67432142,2,67380942,2021-05-07 09:15:26,1,"<p>In the Dockerfile, there was a reference to another file that was not present in the directory from where the command <code>sm-docker build .</code> was launched.</p>
","6660372",0
1138,67433983,2,67433776,2021-05-07 11:16:48,1,"<p>As always with questions asking for &quot;best&quot; or &quot;recommended&quot; method, the primary answer is: &quot;it depends&quot;.</p>
<p>However, there are certain considerations worth spelling out in your case.</p>
<ol>
<li><p><strong>Saving to S3 in between pipeline steps.</strong>
This stores intermediate result of the pipeline and as long as the steps take long time and are restartable it may be worth doing that. What &quot;long time&quot; means is dependent on your use case though.</p>
</li>
<li><p><strong>Passing the data directly from component to component.</strong> This saves you storage throughput and very likely the not insignificant time to store and retrieve the data to / from S3. The downside being: if you fail mid-way in the pipeline, you have to start from scratch.</p>
</li>
</ol>
<p>So the questions are:</p>
<ul>
<li>Are the steps <a href=""https://stackoverflow.com/questions/1077412/what-is-an-idempotent-operation"">idempotent</a> (restartable)?</li>
<li>How often the pipeline fails?</li>
<li>Is it easy to restart the processing from some mid-point?</li>
<li>Do you care about the processing time more than the risk of loosing some work?</li>
<li>Do you care about the incurred cost of S3 storage/transfer?</li>
</ul>
","6573902",5
1139,67462013,2,67454531,2021-05-09 20:19:38,4,"<p><strong>Short answer</strong></p>
<p>Do not use <code>ask_password</code>.
Instead, save your token in the local config by running once:</p>
<pre><code>dvc remote modify origin --local --unset ask_password
dvc remote modify origin --local password &lt;--access token--&gt;
</code></pre>
<p><code>dvc push -r origin</code> should work then.</p>
<p><strong>Long answer</strong></p>
<p><a href=""https://www.atlassian.com/git/tutorials/git-bash#:%7E:text=What%20is%20Git%20Bash%3F,operating%20system%20through%20written%20commands."" rel=""nofollow noreferrer"">Git Bash</a> is not running the regular Windows command prompt but an emulated Unix-style bash prompt. From the information in your question, I cannot know for sure, but this is probably causing the <code>msvcrt</code> package used by DVC to prompt the password on windows machines to fail/hang.</p>
<p>There are potentially 3 ways to deal with the issue:</p>
<ol>
<li>Run <code>dvc pull</code> from the regular Windows cmd prompt.</li>
<li>Find a way to make Git Bash wrap Python calls with <code>winpty</code> - I am not 100% positive about how to do this, but not using <code>winpty</code> seems to be the reason <code>msvcrt</code> fails at prompting for your password.</li>
<li>The simplest solution - Do not use <code>ask_password</code>.
Instead, save your token in the local config by running once:
<pre><code>dvc remote modify origin --local --unset ask_password
dvc remote modify origin --local password &lt;--access token--&gt;
</code></pre>
You can get your access token by clicking on the question mark beside the DVC
remote of your DAGsHub repository, then click on &quot;Reveal my token&quot;.</li>
</ol>
","4741053",0
1140,67465607,2,67456172,2021-05-10 06:19:06,1,"<p>Run MLflow server such was that it will use your machine IP instead of <code>localhost</code>.  Then point the <code>mlflow run</code> to that IP instead of <code>http://localhost:5000</code>.   The main reason is that <code>localhost</code> of Docker process is its own, not your machine.</p>
","18627",2
1141,66926238,2,66925614,2021-04-02 23:07:02,2,"<p>I'm not 100% sure that I understand the question (it would be great to expand it a bit on the actual use case you are trying to solve with this database), but I can share a few thoughts.</p>
<p>When we talk about DVC, I think you need to specify a few things to identify the file/directory:</p>
<ol>
<li>Git commit + path (actual path like <code>data/data/xml</code>). Commit (or to be precise any Git revision) is needed to identify the version of the data file.</li>
<li>Or path in the DVC storage (<code>/mnt/shared/storage/00/198493ef2343ao</code> ...<code>) + actual name of this file. This way you would be saving info that </code>.dvc` files have.</li>
</ol>
<p>I would say that second way is <em>not</em> recommended since to some extent it's an implementation detail - how does DVC store files internally. The public interface to DVC organized data storage is its repository URL + commit + file name.</p>
<p>Edit (example):</p>
<pre class=""lang-py prettyprint-override""><code>with dvc.api.open(
        'activity.log',
        repo='location/of/dvc/project',
        remote='my-s3-bucket'
        ) as fd:
    for line in fd:
        match = re.search(r'user=(\w+)', line)
        # ... Process users activity log
</code></pre>
<p><code>location/of/dvc/project</code> this path must point to an actual Git repo. This repo should have a <code>.dvc</code> or <code>dvc.lock</code> file that has <code>activity.log</code> name in it + its hash in the remote storage:</p>
<pre class=""lang-yaml prettyprint-override""><code>outs:
  - md5: a304afb96060aad90176268345e10355
    path: activity.log
</code></pre>
<p>By reading this Git repo and analyzing let's say <code>activity.log.dvc</code> DVC will be able to create the right path <code>s3://my-bucket/storage/a3/04afb96060aad90176268345e10355</code></p>
<p><code>remote='my-s3-bucket'</code> argument is optional. By default it will use the one that is defined in the repo itself.</p>
<p>Let's take another real example:</p>
<pre class=""lang-py prettyprint-override""><code>with dvc.api.open(
        'get-started/data.xml',
        repo='https://github.com/iterative/dataset-registry'
        ) as fd:
    for line in fd:
        match = re.search(r'user=(\w+)', line)
        # ... Process users activity log
</code></pre>
<p>In the <code>https://github.com/iterative/dataset-registry</code> you could find the <a href=""https://github.com/iterative/dataset-registry/blob/master/get-started/data.xml.dvc"" rel=""nofollow noreferrer""><code>.dvc</code> file</a> that is enough for DVC to create a path to the file by also analyzing its <a href=""https://github.com/iterative/dataset-registry/blob/master/.dvc/config"" rel=""nofollow noreferrer"">config</a></p>
<pre><code>https://remote.dvc.org/dataset-registry/a3/04afb96060aad90176268345e10355
</code></pre>
<p>you could run <code>wget</code> on this file to download it</p>
","298182",4
1142,66911626,2,66911321,2021-04-01 21:13:30,1,"<p>You can update the Dockerfile after it install Python3.8 using <code>apt-get</code> with the following <code>RUN</code> commands</p>
<pre><code>RUN update-alternatives --install /usr/bin/python python /usr/bin/python3.8 1
RUN update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.8 1
</code></pre>
<p>The first <code>RUN</code> command will link <code>/usr/bin/python</code> to <code>/usr/bin/python3.8</code> and the second one will link <code>/usr/bin/python3</code> to <code>/usr/bin/python3.8</code></p>
","7375347",2
1143,66889105,2,66887340,2021-03-31 13:45:19,0,"<p>If you don't want your data to be persistent across multiple notebook runs, just store them in <code>/tmp</code> which is not persistent. You have at least 10GB. More details <a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/howitworks-create-ws.html"" rel=""nofollow noreferrer"">here</a>.</p>
","4267439",3
1144,66874329,2,66817781,2021-03-30 15:54:38,1,"<p><code>sagemaker</code> is the SageMaker Python SDK. It calls SageMaker-related AWS service APIs on your behalf. You don't need to use it, but it can make life easier</p>
<blockquote>
<ol>
<li>Is using sagemaker the equivalent of running a training job without taking advantage of the distributed computing capabilities of AWS? I assume it is, if not, why have they implemented sagemaker_pyspark?</li>
</ol>
</blockquote>
<p>No. You can run distributed training jobs using <code>sagemaker</code> (see <code>instance_count</code> parameter)</p>
<p><code>sagemaker_pyspark</code> facilitates calling SageMaker-related AWS service APIs from Spark. Use it if you want to use SageMaker services from Spark</p>
<blockquote>
<ol start=""2"">
<li>Is it normal for something like model = xgboost_estimator.fit(training_data) to take 4 minutes to run with sagemaker_pyspark for a small set of test data?</li>
</ol>
</blockquote>
<p>Yes, it takes a few minutes for an EC2 instance to spin-up. Use <a href=""https://sagemaker.readthedocs.io/en/stable/overview.html#local-mode"" rel=""nofollow noreferrer"">Local Mode</a> if you want to iterate more quickly locally. Note: Local Mode won't work with SageMaker built-in algorithms, but you can prototype with (non AWS) XGBoost/SciKit-Learn</p>
<blockquote>
<ol start=""3"">
<li>Does sagemaker_pyspark support custom algorithms? Or does it only allow you to use the predefined ones in the library?</li>
</ol>
</blockquote>
<p>Yes, but you'd probably want to extend <a href=""https://sagemaker-pyspark.readthedocs.io/en/latest/api.html#sagemakerestimator"" rel=""nofollow noreferrer"">SageMakerEstimator</a>. Here you can provide the <code>trainingImage</code> URI</p>
<blockquote>
<ol start=""4"">
<li>Do you know if sagemaker_pyspark can perform hyperparameter optimization?</li>
</ol>
</blockquote>
<p>It does not appear so. It'd probably be easier just to do this from SageMaker itself though</p>
<blockquote>
<p>can you deploy models with and without containers?</p>
</blockquote>
<p>You can certainly host your own models any way you want. But if you want to use SageMaker model inference hosting, then containers are required</p>
<blockquote>
<p>why would you use model containers?</p>
</blockquote>
<blockquote>
<p>Do you always need to define the model externally with the entry_point script?</p>
</blockquote>
<p>The whole Docker thing makes bundling dependencies easier, and also makes things language/runtime-neutral. SageMaker doesn't care if your algorithm is in Python or Java or Fortran. But it needs to know how to &quot;run&quot; it, so you tell it a working directory and a command to run. This is the entry point</p>
<blockquote>
<p>It is also confusing that the class AlgorithmEstimator allows the input argument algorithm_arn; I see there are three different ways of passing a model as input, why? which one is better?</p>
</blockquote>
<p>Please clarify which &quot;three&quot; you are referring to</p>
<p>6 is not a question, so no answer required :)</p>
<blockquote>
<ol start=""7"">
<li>What is the difference between this and what sagemaker_pyspark offers?</li>
</ol>
</blockquote>
<p>sagemaker_pyspark lets you call SageMaker services from Spark, whereas SparkML Serving lets you use Spark ML services from SageMaker</p>
","223478",0
1145,66798509,2,66792575,2021-03-25 11:24:29,2,"<p>You can use <a href=""https://mlflow.org/docs/latest/python_api/mlflow.html#mlflow.log_param"" rel=""nofollow noreferrer"">log_param/log_params</a> for that. For long texts maybe it's better to use <a href=""https://mlflow.org/docs/latest/python_api/mlflow.html#mlflow.log_text"" rel=""nofollow noreferrer"">log_text</a> instead...</p>
","18627",2
1146,66748193,2,66704314,2021-03-22 14:35:49,0,"<p>If we wish to obtain the model-id for the latest model, instead of using <code>az ml model</code> list with <code>-l</code> flag, using <code>az model show</code> will return the details for the latest model. The syntax to get a string for model-id will be:</p>
<pre><code>az ml model show --model-id $(TRN_MODEL_ID) --resource-group $(AML_TRN_RG) --subscription-id $(AML_TRN_SUB_ID) --workspace-name $(AML_TRN_WS) --query name -o tsv
</code></pre>
","14384792",0
1147,66694541,2,65847574,2021-03-18 15:52:36,1,"<p>In the end, the problem was that I indeed was pulling from the wrong remote (I had multiple remotes, their configuration was tricky, and local configurations differed on different machines).</p>
","1888471",0
1148,66694049,2,66692579,2021-03-18 15:24:06,2,"<p>You need to add (or modify) an IAM policy to grant access to the key the bucket uses for its encryption:</p>
<pre><code>{
  &quot;Sid&quot;: &quot;KMSAccess&quot;,
  &quot;Action&quot;: [
    &quot;kms:Decrypt&quot;
  ],
  &quot;Effect&quot;: &quot;Allow&quot;,
  &quot;Resource&quot;: &quot;arn:aws:kms:example-region-1:123456789098:key/111aa2bb-333c-4d44-5555-a111bb2c33dd&quot;
}
</code></pre>
<p>Alternatively you can change the key policy of the KMS key directly to grant the sagemaker role access directly. <a href=""https://aws.amazon.com/premiumsupport/knowledge-center/s3-bucket-access-default-encryption/"" rel=""nofollow noreferrer"">https://aws.amazon.com/premiumsupport/knowledge-center/s3-bucket-access-default-encryption/</a></p>
","2442804",1
1149,66633353,2,66601526,2021-03-15 06:37:57,1,"<p>Maybe you need to use an internal load balancer, then it will use a private IP address. <a href=""https://learn.microsoft.com/en-us/azure/machine-learning/how-to-create-attach-kubernetes?tabs=python#create-or-attach-an-aks-cluster-to-use-internal-load-balancer-with-private-ip"" rel=""nofollow noreferrer"">Here</a> is the example code for Python:</p>
<pre><code>from azureml.core.compute.aks import AksUpdateConfiguration
from azureml.core.compute import AksCompute, ComputeTarget

# When you create an AKS cluster, you can specify Internal Load Balancer to be created with provisioning_config object
provisioning_config = AksCompute.provisioning_configuration(load_balancer_type = 'InternalLoadBalancer')

# when you attach an AKS cluster, you can update the cluster to use internal load balancer after attach
aks_target = AksCompute(ws,&quot;myaks&quot;)

# Change to the name of the subnet that contains AKS
subnet_name = &quot;default&quot;
# Update AKS configuration to use an internal load balancer
update_config = AksUpdateConfiguration(None, &quot;InternalLoadBalancer&quot;, subnet_name)
aks_target.update(update_config)
# Wait for the operation to complete
aks_target.wait_for_completion(show_output = True)
</code></pre>
","9773937",0
1150,66598017,2,66566031,2021-03-12 10:16:46,2,"<p>The problem seems to be that by default the systemd service is run by root.
Specifying a user and creating a ssh key pair for that user to access the same remote machine worked.</p>
<pre><code>[Unit]

Description=MLflow server

After=network.target 

[Service]

Restart=on-failure

RestartSec=20

User=_user_

Group=_group_

ExecStart=/bin/bash -c 'PATH=_yourpath_/anaconda3/envs/mlflow_server/bin/:$PATH exec mlflow server --backend-store-uri postgresql://mlflow:mlflow@localhost/mlflow --default-artifact-root sftp://_user_@192.168.1.245:_yourotherpath_/MLFLOW_SERVER/mlruns -h 0.0.0.0 -p 8000' 

[Install]

WantedBy=multi-user.target
</code></pre>
<p><code>_user_</code> and <code>_group_</code> should be the same listed by <code>ls -la</code> in the <code>mlruns</code> directory.</p>
","3751931",0
1151,66558206,2,66497968,2021-03-10 04:05:15,0,"<p>Figured out my issue, the RCF can't handle dates and strings.  There's this page for the Kenesis offering from AWS that covers the same Random Cut Forest algorithm <a href=""https://docs.aws.amazon.com/kinesisanalytics/latest/sqlref/sqlrf-random-cut-forest.html"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/kinesisanalytics/latest/sqlref/sqlrf-random-cut-forest.html</a>  It says the function only supports &quot;The algorithm accepts the DOUBLE, INTEGER, FLOAT, TINYINT, SMALLINT, REAL, and BIGINT data types.&quot;</p>
<p>The gotcha part that AWS does with the NYC Taxi example is they use .value which is referring to only the value column of the data.  They are basically dropping the dates from the RCF as a feature.  It doesn't help that .values on the array does work and looks very similar to .value</p>
","1652528",0
1152,66505356,2,66504979,2021-03-06 11:54:11,1,"<p>The <code>to_pandas_dataframe()</code>method <em>returns</em> a pandas DataFrame, so you need to assign it back your variable:</p>
<pre><code>orders_df = orders_df.to_pandas_dataframe()
</code></pre>
","8222791",0
1153,66479786,2,66423449,2021-03-04 17:13:20,3,"<p>So, in the end, what I did was write a get-data-components.yaml to create my component and wrote the function below and it worked:</p>
<pre><code>def build_get_data():
    component = kfp.components.load_component_from_file(os.path.join(COMPONENTS_PATH, 'get-data-component.yaml'))()
    component.add_volume(k8s_client.V1Volume(
        name=&quot;get-data-volume&quot;,
        secret=k8s_client.V1SecretVolumeSource(secret_name=&quot;pipeline-secrets&quot;))
    )
    envs = [
        (&quot;MYSQL_DB_NAME&quot;, &quot;mysql_db_name&quot;),
        (&quot;MYSQL_USER_NAME&quot;, &quot;mysql_username&quot;), 
        (&quot;MYSQL_PASSWORD&quot;, &quot;mysql_password&quot;), 
        (&quot;MYSQL_ENDPOINT&quot;, &quot;mysql_endpoint&quot;)
    ]
    for name, key in envs:
        component.add_env_variable(
            V1EnvVar(
                name=name,
                value_from=k8s_client.V1EnvVarSource(secret_key_ref=k8s_client.V1SecretKeySelector(
                    name=&quot;pipeline-secrets&quot;,
                    key=key
                    )
                )
            )
        )
    return component
</code></pre>
","4534466",0
1154,66478612,2,66477468,2021-03-04 15:58:09,4,"<p>Can you try to install <code>google-api-python-client==1.12.8</code> and test in that way?</p>
<p>Edit:</p>
<p>It appears to be that, this was a bug in the 2.0.0-2.0.1 of google-api-client and resolved in 2.0.2. So this should also work <code>google-api-python-client&gt;=2.0.2</code></p>
","15330941",1
1155,66463802,2,64315239,2021-03-03 19:39:04,1,"<p>The Microsoft <a href=""https://learn.microsoft.com/en-us/azure/machine-learning/how-to-deploy-advanced-entry-script#power-bi-compatible-endpoint"" rel=""nofollow noreferrer"">documentation</a> say's: &quot;In order to generate conforming swagger for automated web service consumption, scoring script run() function must have API shape of:</p>
<blockquote>
<p>A first parameter of type &quot;StandardPythonParameterType&quot;, named
<strong>Inputs</strong> and nested.</p>
<p>An optional second parameter of type &quot;StandardPythonParameterType&quot;,
named GlobalParameters.</p>
<p>Return a dictionary of type &quot;StandardPythonParameterType&quot; named
<strong>Results</strong> and nested.&quot;</p>
</blockquote>
<p>I've already test this and it is case sensitive
So it will be like this:</p>
<pre><code>import numpy as np
import pandas as pd
import joblib

from azureml.core.model import Model
from inference_schema.schema_decorators import input_schema, output_schema
from inference_schema.parameter_types.standard_py_parameter_type import 
    StandardPythonParameterType
from inference_schema.parameter_types.numpy_parameter_type import NumpyParameterType
from inference_schema.parameter_types.pandas_parameter_type import PandasParameterType

def init():
    global model
    # Model name is the name of the model registered under the workspace
    model_path = Model.get_model_path(model_name = 'databricksmodelpowerbi2')
    model = joblib.load(model_path)

# Provide 3 sample inputs for schema generation for 2 rows of data
numpy_sample_input = NumpyParameterType(np.array([[2400.0, 78.26086956521739, 11100.0, 
3.612565445026178, 3.0, 0.0], [368.55, 96.88311688311687, 709681.1600000012, 
73.88059701492537, 44.0, 0.0]], dtype = 'float64'))

pandas_sample_input = PandasParameterType(pd.DataFrame({'value': [2400.0, 368.55], 
'delayed_percent': [78.26086956521739, 96.88311688311687], 'total_value_delayed': 
[11100.0, 709681.1600000012], 'num_invoices_per30_dealing_days': [3.612565445026178, 
73.88059701492537], 'delayed_streak': [3.0, 44.0], 'prompt_streak': [0.0, 0.0]}))

standard_sample_input = StandardPythonParameterType(0.0)

# This is a nested input sample, any item wrapped by `ParameterType` will be described 
by schema
sample_input = StandardPythonParameterType({'input1': numpy_sample_input, 
                                         'input2': pandas_sample_input, 
                                         'input3': standard_sample_input})

sample_global_parameters = StandardPythonParameterType(1.0) #this is optional

numpy_sample_output = NumpyParameterType(np.array([1.0, 2.0]))

# 'Results' is case sensitive
sample_output = StandardPythonParameterType({'Results': numpy_sample_output})

# 'Inputs' is case sensitive
@input_schema('Inputs', sample_input)
@input_schema('global_parameters', sample_global_parameters) #this is optional
@output_schema(sample_output)
def run(Inputs, global_parameters):
    try:
        data = inputs['input1']
        # data will be convert to target format
        assert isinstance(data, np.ndarray)
        result = model.predict(data)
        return result.tolist()
    except Exception as e:
        error = str(e)
        return error
</code></pre>
<p>`</p>
","15146404",0
1156,66451448,2,66348756,2021-03-03 05:48:37,1,"<p>Looks like you are trying to run the experiment remotely, AFAIK and as per the doc <a href=""https://learn.microsoft.com/en-us/azure/machine-learning/how-to-configure-auto-train#data-source-and-format?WT.mc_id=AI-MVP-5003930"" rel=""nofollow noreferrer"">here</a> :</p>
<p><a href=""https://i.stack.imgur.com/CXYyE.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/CXYyE.png"" alt=""enter image description here"" /></a></p>
<p>You could refer this article to understand creating <a href=""https://learn.microsoft.com/en-us/azure/machine-learning/how-to-create-register-datasets?WT.mc_id=AI-MVP-5003930"" rel=""nofollow noreferrer"">Azure ML TabularDataset</a></p>
","13755246",0
1157,66447901,2,65233943,2021-03-02 22:12:52,1,"<p>Mohamed, this means that Sagemaker Edge Manager was unable to use the RoleAlias you provided to take the necessary actions when creating a DeviceFleet. It needs to have the AmazonSageMakerEdgeDeviceFleetPolicy attached (or have similar permissions granted) and it needs to trust both SageMaker and IoT Core.</p>
","15317145",1
1158,66412578,2,66409283,2021-02-28 18:34:51,1,"<p>When you <code>import</code> (or <code>add</code>) something into your project, a .dvc file is created with that lists that something (in this case the <code>raw/</code> dir) as an &quot;output&quot;.</p>
<p>DVC doesn't allow overlapping outputs among .dvc files or dvc.yaml stages, meaning that your &quot;menu_items&quot; stage shouldn't write to <code>raw/</code> since it's already under the control of <code>raw.dvc</code>.</p>
<p>Can you make a separate directory for the pipeline outputs? E.g. use <code>processed/menu_items/restaurant.jsonl</code></p>
","761963",2
1159,66403555,2,66388294,2021-02-27 21:16:54,5,"<p>tl;dr
You can use <a href=""https://eksctl.io/usage/autoscaling/"" rel=""noreferrer"">taints</a>.</p>
<blockquote>
<p>Which pods need to be assigned to our GPU nodes?</p>
</blockquote>
<p>The pods of the jobs that require GPU.</p>
<p>If your <strong>training</strong> job requires GPU you need to assign it using the <code>nodeSelector</code> and <code>tolerations</code> in the spec of your training/deployment deployment, see a nice example <a href=""https://alexei-led.github.io/post/eks_gpu_spot/"" rel=""noreferrer"">here</a>.</p>
<p>If your model is CV/NLP (many matrix multiplications), you might want to have the <strong>inferenceservice</strong> in the GPU as well, in that case you need to have it requested in its spec as described <a href=""https://github.com/kubeflow/kfserving/blob/master/docs/samples/v1alpha2/pytorch/pytorch_gpu.yaml"" rel=""noreferrer"">here</a>.</p>
<blockquote>
<p>Do we only need our argo workflow pod to be assigned and repel the
rest?</p>
</blockquote>
<p>Yes, if your inferenceservice does not require GPU.</p>
<blockquote>
<p>Are there other kfserving components needed within the GPU node to work right?</p>
</blockquote>
<p>No, the only kfserving component is the <code>kfserving-controller</code> and does not require a gpu as it's only orchestrating the creation of the istio&amp;knative resources for your inferenceservice.</p>
<p>If there are inferenceservices running in your gpu nodegroup without having the GPU requested in the spec, it means that the nodegroup is not configured to have the taint effect <code>NoSchedule</code>. Make sure that the gpu nodegroup in the eksctl configuration has the taint as described in the <a href=""https://eksctl.io/usage/autoscaling/"" rel=""noreferrer"">doc</a>.</p>
","3181539",0
1160,66402028,2,66196815,2021-02-27 18:21:16,2,"<p>I once developed a so called Genetic Time Series in R. I used a Genetic Algorithm which sorted out the best solutions from multivariate data, which were fitted on a VAR in differences or a VECM. Your data seems more macro economic or financial than user-centric and VAR or VECM seems appropriate. (Surely it is possible to treat time-series data in the same way so that we can use LSTM or other approaches, but these are very common) However, I do not know if VAR in differences or VECM works with binary classified labels. Perhaps if you would calculate a metric outcome, which you later label encode to a categorical feature (or label it first to a categorical) than VAR or VECM may also be appropriate.</p>
<p>However you may add all yearly data points to one data points per firm to forecast its survival, but you would loose a lot of insight. If you are interested in time series ML which works a little bit different than for neural networks or elastic net (which could also be used with time series) let me know. And we can work something out. Or I'll paste you some sources.</p>
<p>Summary:
1.)
It is possible to use LSTM, elastic NEt (time points may be dummies or treated as cross sectional panel) or you use VAR in differences and VECM with a slightly different out come variable</p>
<p>2.)
It is possible but you will loose information over time.</p>
<p>All the best,
Patrick</p>
","13994262",2
1161,66387852,2,66320435,2021-02-26 14:45:10,4,"<p>I think you need an MLflow &quot;run&quot; for every new batch of data, so that your parameters are logged independently for each new training.</p>
<p>So, try the following in your consumer:</p>
<pre><code>if tweet_counter == train_every:
            update_df()
            data_path = 'data/updated_tweets.csv'
            with mlflow.start_run() as mlrun:
               train(data_path)
            print(&quot;\nTraining with new data is completed!\n&quot;)
            tweet_counter = 0
</code></pre>
","8921612",0
1162,66318522,2,65464181,2021-02-22 15:24:19,1,"<p>Let me clarify some confusion here. TPUs are only offered on Google Cloud and the <code>TPUClusterResolver</code> implementation queries GCP APIs to get the cluster config for the TPU node. Thus, no you can't use <code>TPUClusterResolver</code> with AWS sagemaker, but you should try it out with TPUs on GCP instead or try find some other documentation on Sagemaker's end on how they enable cluster resolving on their end (if they do).</p>
","10199571",0
1163,66277557,2,66277250,2021-02-19 12:26:47,1,"<pre><code>s3 = boto3.resource('s3')
bucket = s3.Bucket('testbucket')

prefix_objs = bucket.objects.filter(Prefix=&quot;extracted/abc&quot;)

prefix_df = []

for obj in prefix_objs:
    try:
        key = obj.key
        body = obj.get()['Body'].read()
        temp = pd.read_csv(io.BytesIO(body),header=None, encoding='utf8',sep=',')        
        prefix_df.append(temp)
    except:
        continue
</code></pre>
","14490579",0
1164,66274681,2,66264795,2021-02-19 09:12:25,0,"<p>According to the example <a href=""https://learn.microsoft.com/en-us/azure/machine-learning/how-to-train-with-custom-image#define-your-environment"" rel=""nofollow noreferrer"">here</a>, I think you need to configure the environment variables for the docker images stored in the Azure Container Registry:</p>
<pre><code>batch_env = Environment(name='batch_environment')
batch_env.python.conda_dependencies = cd
batch_env.docker.enabled = True
# Set the container registry information.
batch_env.docker.base_image_registry.address = &quot;myregistry.azurecr.io&quot;
batch_env.docker.base_image_registry.username = &quot;username&quot;
batch_env.docker.base_image_registry.password = &quot;password&quot;
batch_env.docker.base_image = &quot;myregistry.azurecr.io/DEFAULT_CPU_IMAGE&quot;
</code></pre>
","9773937",0
1165,66270765,2,66246693,2021-02-19 02:00:53,0,"<p>Through other avenues, I was pointed to this <a href=""https://fabiouechi.medium.com/using-google-cloud-build-for-kubeflow-pipelines-ci-cd-b1459b08b158"" rel=""nofollow noreferrer"">article</a>, which contains an example of how to port-forward KFP's ml-pipeline service in Cloud Build.  I had to make one minor modification, which was to remove the <code>&quot;-n&quot;</code> and <code>&quot;kubeflow&quot;</code> arguments to the <code>kubectl port-forward</code> command.  This specifies to <code>kubectl</code> to use the <code>&quot;kubeflow&quot;</code> namespace.  GCP's AI Platform Pipelines, however, appears to create a <code>&quot;default&quot;</code> namespace when it deploys your KFP instance.</p>
","15222427",0
1166,66267130,2,65619881,2021-02-18 19:48:18,0,"<p>There is a limitation in our account so we had to request for using the instances and increasing the available resource from AWS.</p>
","8060154",0
1167,66162150,2,65937786,2021-02-11 20:16:43,7,"<p>My team has recently added integration between MLflow and our open source data monitoring library called <a href=""https://github.com/whylabs/whylogs-python"" rel=""noreferrer"">whylogs</a>. This lets you log statistical profiles of the data passing through the model and/or the output of the model. You can then collect these profiles from MLflow run artifacts and analyze them for drift.</p>
<p>We have a <a href=""https://github.com/whylabs/whylogs-examples/blob/mainline/python/MLFlow%20Integration%20Example.ipynb"" rel=""noreferrer"">notebook</a> that walks you through the integration process and a <a href=""https://whylabs.ai/blog/posts/on-model-lifecycle-and-monitoring"" rel=""noreferrer"">blog post</a> to go along with it. Lmk if you have any questions or additional feature requests!</p>
","8600523",0
1168,66022019,2,65608355,2021-02-03 05:56:27,0,"<p>So for running the above python code I mentioned as pipeline component.
I added the environment variables using the os library and this individual piece was able to connect to external networks.
Updated python code-</p>
<pre><code>def basic():
    import urllib.request
    import os
    proxy = 'http://proxy-path:port'

    os.environ['http_proxy'] = proxy 
    os.environ['HTTP_PROXY'] = proxy
    os.environ['https_proxy'] = proxy
    os.environ['HTTPS_PROXY'] = proxy

    print(&quot;inside basic funtion&quot;)
    with urllib.request.urlopen('http://python.org/') as response:
        html = response.read()
        print(html)
</code></pre>
<p>And if the docker image is created from scratch without taking help of pipeline library function then we need to just add the env details into our dockerfile the usual way after the base image call-</p>
<pre><code>ENV HTTP_PROXY http://proxy-path:port
ENV HTTPS_PROXY http://proxy-path:port
</code></pre>
","6821697",0
1169,66018566,2,54461730,2021-02-02 22:32:47,3,"<p>This is the filesystems for my notebook instance:</p>
<pre><code>sh-4.2$ df -h
Filesystem      Size  Used Avail Use% Mounted on
devtmpfs         16G   76K   16G   1% /dev
tmpfs            16G     0   16G   0% /dev/shm
/dev/nvme0n1p1   94G   76G   19G  81% /
/dev/nvme1n1     99G   40G   55G  43% /home/ec2-user/SageMaker
</code></pre>
<p>Note that one pointing to <code>/home/ec2-user/SageMaker</code> is the only one which is saved between reboots. Since ssh keys are stored in <code>/home/ec2-user/.ssh</code>, they are lost after reboot.</p>
<p>The way I make it work is:</p>
<ol>
<li>Create the folder <code>/home/ec2-user/SageMaker/.ssh</code></li>
<li>Run <code>ssh-keygen</code> and set the location <code>/home/ec2-user/SageMaker/.ssh/id_rsa</code></li>
<li>Clone repo with <code>GIT_SSH_COMMAND=&quot;ssh -i ~/SageMaker/.ssh/id_rsa -F /dev/null&quot; git clone git@domain:account/repo.git</code></li>
<li>cd repo</li>
<li>Set your repo to use the custom location with <code>git config core.sshCommand &quot;ssh -i ~/SageMaker/.ssh/id_rsa -F /dev/null&quot;</code></li>
</ol>
<p>Based on <a href=""https://superuser.com/a/912281"">https://superuser.com/a/912281</a></p>
","5883471",1
1170,66006427,2,65997961,2021-02-02 08:58:02,1,"<p>Assumptions:</p>
<ol>
<li>An AzureML Pipeline is published and the REST endpoint is ready- To be referred to in this answer as &lt;AML_PIPELINE_REST_URI&gt;. And Published Pipeline ID is also ready- To be referred to in this answer as &lt;AML_PIPELINE_ID&gt;</li>
<li>You have the Azure Machine Learning Extension installed: <a href=""https://marketplace.visualstudio.com/items?itemName=ms-air-aiagility.vss-services-azureml&amp;ssr=false#review-details"" rel=""nofollow noreferrer"">Azure Machine Learning Extension</a></li>
</ol>
<p>To Invoke the Azure Machine Learning Pipeline we use the <code>Invoke ML Pipeline</code> step available in Azure DevOps. It is available when running an Agentless Job.</p>
<p>To trigger it the workflow is as follows:</p>
<ol>
<li>Create a New Pipeline. Using the Classic Editor, delete the default Agent Job 1 stage.
<a href=""https://i.stack.imgur.com/phzL3.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/phzL3.png"" alt=""enter image description here"" /></a></li>
</ol>
<p><a href=""https://i.stack.imgur.com/QkiPY.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/QkiPY.png"" alt=""enter image description here"" /></a></p>
<ol start=""2"">
<li><p>Add an agentless job:
<a href=""https://i.stack.imgur.com/0PXwg.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/0PXwg.png"" alt=""enter image description here"" /></a></p>
</li>
<li><p>Add a task to this Agentless Job:
<a href=""https://i.stack.imgur.com/trW7j.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/trW7j.png"" alt=""enter image description here"" /></a></p>
</li>
<li><p>Use AzureML Published Pipeline Task:
<a href=""https://i.stack.imgur.com/3rl4z.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/3rl4z.png"" alt=""enter image description here"" /></a></p>
</li>
<li><p>Use the Service Connection Mapped to the AML Workspace. You can find more on this at the <a href=""https://learn.microsoft.com/en-us/azure/devops/pipelines/library/service-endpoints?view=azure-devops&amp;tabs=yaml"" rel=""nofollow noreferrer"">official documentation</a>
<a href=""https://i.stack.imgur.com/mnV36.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/mnV36.png"" alt=""enter image description here"" /></a></p>
</li>
<li><p>Choose the Pipeline to trigger using the &lt;AML_PIPELINE_ID&gt;:
<a href=""https://i.stack.imgur.com/fbpQW.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/fbpQW.png"" alt=""enter image description here"" /></a></p>
</li>
<li><p>Give The experiment name and Pipeline Parameters if any:
<a href=""https://i.stack.imgur.com/og1kx.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/og1kx.png"" alt=""enter image description here"" /></a></p>
</li>
<li><p>That's it, you can Save and Queue:
<a href=""https://i.stack.imgur.com/iCwdl.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/iCwdl.png"" alt=""enter image description here"" /></a></p>
</li>
</ol>
<p>Alternatively, you can simply use the following jobs:</p>
<pre><code>- job: Job_2
  displayName: Agentless job
  pool: server
  steps:
  - task: MLPublishedPipelineRestAPITask@0
    displayName: Invoke ML pipeline
    inputs:
      connectedServiceName: &lt;REDACTED-AML-WS-Level-Service_Connection-ID&gt;
      PipelineId: &lt;AML_PIPELINE_ID&gt;
      ExperimentName: experimentname
      PipelineParameters: ''
</code></pre>
","14384792",1
1171,65979056,2,65525944,2021-01-31 12:24:54,3,"<p>(Copied from the related issue for greater visibility)</p>
<p>After some digging, here is an alternative approach, which assumes no knowledge of the <code>feature_spec</code> before-hand.  Do the following:</p>
<ul>
<li>Set the <code>BulkInferrer</code> to write to <code>output_examples</code> rather than <code>inference_result</code> by adding a <a href=""https://github.com/tensorflow/tfx/blob/3aeb6d1b19f1b6a8090f8cda22b45bb81aafdd1a/tfx/proto/bulk_inferrer.proto#L43"" rel=""nofollow noreferrer"">output_example_spec</a> to the component construction.</li>
<li>Add a  <code>StatisticsGen</code> and a <code>SchemaGen</code> component in the main pipeline right after the <code>BulkInferrer</code> to generate a schema for the aforementioned <code>output_examples</code></li>
<li>Use the artifacts from <code>SchemaGen</code> and <code>BulkInferrer</code> to read the TFRecords and do whatever is neccessary.</li>
</ul>
<pre class=""lang-py prettyprint-override""><code>bulk_inferrer = BulkInferrer(
     ....
     output_example_spec=bulk_inferrer_pb2.OutputExampleSpec(
         output_columns_spec=[bulk_inferrer_pb2.OutputColumnsSpec(
             predict_output=bulk_inferrer_pb2.PredictOutput(
                 output_columns=[bulk_inferrer_pb2.PredictOutputCol(
                     output_key='original_label_name',
                     output_column='output_label_column_name', )]))]
     ))

 statistics = StatisticsGen(
     examples=bulk_inferrer.outputs.output_examples
 )

 schema = SchemaGen(
     statistics=statistics.outputs.output,
 )
</code></pre>
<p>After that, one can do the following:</p>
<pre class=""lang-py prettyprint-override""><code>import tensorflow as tf
from tfx.utils import io_utils
from tensorflow_transform.tf_metadata import schema_utils

# read schema from SchemaGen
schema_path = '/path/to/schemagen/schema.pbtxt'
schema_proto = io_utils.SchemaReader().read(schema_path)
spec = schema_utils.schema_as_feature_spec(schema_proto).feature_spec

# read inferred results
data_files = ['/path/to/bulkinferrer/output_examples/examples/examples-00000-of-00001.gz']
dataset = tf.data.TFRecordDataset(data_files, compression_type='GZIP')

# parse dataset with spec
def parse(raw_record):
    return tf.io.parse_example(raw_record, spec)

dataset = dataset.map(parse)
</code></pre>
<p>At this point, the dataset is like any other parsed dataset, so its trivial to write a CSV, or to a BigQuery table or whatever from there. It certainly helped us in <a href=""https://github.com/maiot-io/zenml/t"" rel=""nofollow noreferrer"">ZenML</a> with our <a href=""https://github.com/maiot-io/zenml/blob/newpipelines/zenml/core/pipelines/infer_pipeline.py#L150"" rel=""nofollow noreferrer"">BatchInferencePipeline</a>.</p>
","1561232",2
1172,65977591,2,65421005,2021-01-31 09:33:25,1,"<p>I ended up using the checkpoint path that is by default being synced with the specified S3 path in an uncompressed manner.</p>
","11487739",0
1173,65943075,2,65941675,2021-01-28 18:22:51,1,"<p><strong>Short Answer</strong> : No</p>
<p><strong>Long Answer</strong>:
The recommended way to use Sagemaker with very large datasets is to use the Pipe API (as opposed to the File Api) which streams data to the training image rather than downloading the data. To take advantage of the Pipe API the data will need to be in one of the supported file types: <strong>text records, TFRecord or Protobuf</strong></p>
<p>The benefits are</p>
<ol>
<li>reducing delay when the container is launched</li>
<li>not needing to scale the instance storage to the size of the training data</li>
<li>increasing throughput by moving most preprocessing before model training</li>
</ol>
<p>References:</p>
<ol>
<li><a href=""https://aws.amazon.com/blogs/machine-learning/using-pipe-input-mode-for-amazon-sagemaker-algorithms/"" rel=""nofollow noreferrer"">https://aws.amazon.com/blogs/machine-learning/using-pipe-input-mode-for-amazon-sagemaker-algorithms/</a></li>
<li><a href=""https://julsimon.medium.com/making-amazon-sagemaker-and-tensorflow-work-for-you-893365184233"" rel=""nofollow noreferrer"">https://julsimon.medium.com/making-amazon-sagemaker-and-tensorflow-work-for-you-893365184233</a> (This is a fantastic resource which answers a lot of questions regarding using Sagemaker on very large datasets)</li>
<li><a href=""https://julsimon.medium.com/deep-dive-on-tensorflow-training-with-amazon-sagemaker-and-amazon-s3-12038828075c"" rel=""nofollow noreferrer"">https://julsimon.medium.com/deep-dive-on-tensorflow-training-with-amazon-sagemaker-and-amazon-s3-12038828075c</a></li>
</ol>
","7049567",0
1174,65941460,2,65881699,2021-01-28 16:41:09,2,"<p>I had a similar issue as well, along with a similar fix to Bas (per comment above).</p>
<p>I was finding I wasn't necessarily having issues with the .tar.gz step, this command does work fine:</p>
<p><code>tar -czf &lt;filename&gt; ./&lt;directory-with-files&gt;</code></p>
<p>but rather with the uploading step.</p>
<p>Manually uploading to S3 should take care of this, however, if you're doing this step programmatically, you might need to double check the steps taken. Bas appears to have had filename issues, mine were around using boto properly. Here's some code that works (Python only here, but watch for similar issues with other libraries):</p>
<pre><code>bucket = 'bucket-name'
key = 'directory-inside-bucket'
file = 'the file name of the .tar.gz'

s3_client = boto3.client('s3')
s3_client.upload_file(file, bucket, key)
</code></pre>
<p>Docs: <a href=""https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/s3.html#S3.Client.upload_file"" rel=""nofollow noreferrer"">https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/s3.html#S3.Client.upload_file</a></p>
","2179871",0
1175,65939477,2,65939058,2021-01-28 14:48:14,1,"<p>I've found the problem just after posting the question.
I had forgotten to install the <code>google-cloud-storage</code> library on the GCP VM. Everything works as expected now.</p>
","15099357",0
1176,65938083,2,65937623,2021-01-28 13:30:03,2,"<p>From your error traceback, the model artifact can't be located. In your code, you are executing the 'mlflow' command from within a Jupyter Notebook. I would suggest trying the following:</p>
<ol>
<li>Check if your models artifacts are on the path you are using Home/miniconda3/envs/mlruns/0/baa40963927a49258c845421e3175c06/artifacts/model</li>
<li>Try opening a terminal, then <code>cd /Home/miniconda3/envs</code> and  execute <code>mlflow models serve -m ./mlruns/0/baa40963927a49258c845421e3175c06/artifacts/model -p 8001</code></li>
<li>MLFlow offers different solutions to serve a model, you can try to register your model and refer to it as &quot;models:/{model_name}/{stage}&quot; as mentioned in the Model Registry <a href=""https://mlflow.org/docs/latest/model-registry.html#serving-an-mlflow-model-from-model-registry"" rel=""nofollow noreferrer"">docs</a></li>
</ol>
","9738443",2
1177,65935665,2,65902366,2021-01-28 10:54:28,1,"<p>It's not really clear to all the test are you doing, but that error usually means that there is not enough disk space on the instance you are using for the training job. You can try to increase the additional storage for the instance (you can do in the estimator parameters if you are using the sagemaker SDK in a notebook).</p>
","4267439",1
1178,65933775,2,65846689,2021-01-28 08:50:32,0,"<p>It seams that this is not possible at the time.
See:
<a href=""https://github.com/kubeflow/kubeflow/issues/1583"" rel=""nofollow noreferrer"">https://github.com/kubeflow/kubeflow/issues/1583</a></p>
","2035457",0
1179,65858994,2,65807148,2021-01-23 12:06:26,2,"<p>The SageMaker Inference instance has a directory called <code>/temp/</code> which is writeable, and can be used for non-persistent storage. I used the S3Downloader utility in the SageMaker SDK to download data to this directory. For instance:</p>
<pre><code>from sagemaker.s3 import S3Downloader
from sagemaker.session import Session

sagemaker_session = Session()

def download_data_from_s3(s3_uri):
    S3Downloader.download(s3_uri=s3_uri,
                          local_path='/tmp/',
                          sagemaker_session=sagemaker_session)

</code></pre>
","7104347",0
1180,65838878,2,65822290,2021-01-22 03:21:48,2,"<p>Here is a slightly simplified version of your pipeline that I tested and which works.
It doesn't matter what class type you pass to <code>OutputTextFile</code> and <code>InputTextFile</code>. It'll be read and written as <code>str</code>. So this is what you should change:</p>
<ul>
<li>While writing to <code>OutputTextFile</code>: cast <code>sum_</code> from <code>float to str</code></li>
<li>While reading from <code>InputTextFile</code>: cast <code>f.read()</code> value from <code>str to float</code></li>
</ul>
<pre><code>import kfp
from kfp import dsl
from kfp import components as comp


def add(a: float, b: float, f: comp.OutputTextFile()):
    '''Calculates sum of two arguments'''
    sum_ = a + b
    f.write(str(sum_)) # cast to str
    return sum_


def multiply(c: float, d: float, f: comp.InputTextFile()):
    '''Calculates the product'''
    in_ = float(f.read()) # cast to float
    product = c * d * in_
    print(product)
    return product


add_op = comp.func_to_container_op(add,
                                   output_component_file='add_component.yaml')
product_op = comp.create_component_from_func(
    multiply, output_component_file='multiple_component.yaml')


@dsl.pipeline(
    name='Addition-pipeline',
    description='An example pipeline that performs addition calculations.')
def my_pipeline(a, b='7', c='4', d='1'):

    first_add_task = add_op(a, b)
    second_add_task = product_op(c, d, first_add_task.output)


if __name__ == &quot;__main__&quot;:
    compiled_name = __file__ + &quot;.yaml&quot;
    kfp.compiler.Compiler().compile(my_pipeline, compiled_name)
</code></pre>
","8298316",0
1181,65835932,2,65832799,2021-01-21 21:23:30,1,"<p>There are different aspects to consider here:</p>
<p>There is a need to perform a custom deployment to enable Kubeflow Pipelines to integrate/use a Tekton environment. As you mentioned the steps are described <a href=""https://github.com/kubeflow/kfp-tekton/blob/master/guides/kfp_tekton_install.md#standalone-kubeflow-pipelines-with-tekton-backend-deployment"" rel=""nofollow noreferrer"">here</a>.</p>
<p>As for Elyra support, Elyra recent releases starting with Elyra 2.x has incorporated support for KFP using Tekton and further documentation is available in the <a href=""https://elyra.readthedocs.io/en/latest/user_guide/runtime-conf.html#kubeflow-pipelines-engine-engine"" rel=""nofollow noreferrer"">Elyra user guide</a></p>
<p>If you are still using Elyra 1.x or lower, the below still applies:</p>
<p>Now, related to compiling and executing the Kubeflow Pipeline on a Tekton environment, Elyra is currently using the KFP SDK python package and the regular compiler to compile and generate the default ARGO YAML (see code <a href=""https://github.com/elyra-ai/elyra/blob/9de4c89a0e35f13dedd671ce36cb3f6286593fea/elyra/pipeline/processor_kfp.py#L71"" rel=""nofollow noreferrer"">here</a>). In order to support Tekton, we will need to use a different python package (kfp-tekton==0.3.0) and there is also a need for a different code path during compilation (see <a href=""https://github.com/kubeflow/kfp-tekton/tree/master/guides/kfp-user-guide#1-compile-pipelines-using-the-kfp_tektoncompilertektoncompiler-in-python"" rel=""nofollow noreferrer"">example</a>).</p>
<p>Another aspect is that it seems that there are some incompatibilities between the two pipelines as described in the <a href=""https://github.com/kubeflow/kfp-tekton/tree/master/guides/kfp-user-guide#migration-from-argo-backend"" rel=""nofollow noreferrer"">migration path</a>.</p>
<p>Regarding support, the <a href=""https://github.com/elyra-ai/elyra"" rel=""nofollow noreferrer"">Elyra project</a> would welcome contributions.</p>
","5326410",1
1182,65835703,2,65794703,2021-01-21 21:04:52,0,"<p>I got a very nice explanation of it in here, so I'll leave it as the answer:
<a href=""https://github.com/aws-samples/amazon-forecast-samples/issues/104#issuecomment-764896502"" rel=""nofollow noreferrer"">https://github.com/aws-samples/amazon-forecast-samples/issues/104#issuecomment-764896502</a></p>
","11483674",0
1183,65833489,2,65832720,2021-01-21 18:19:48,0,"<p>This issue has been solved.</p>
<p>The directory that I'm looking for was created with <code>os.makedirs()</code>. On closer inspection, I can see that it created the filepath from <code>os.getcwd()</code> exactly as it was entered.</p>
<pre class=""lang-py prettyprint-override""><code>&gt;&gt;&gt; import os
&gt;&gt;&gt; os.getcwd()
'/ec2-user/SageMaker/path/to/current/'

&gt;&gt;&gt; os.listdir('./results') # Should show me 5 different folders
[]

&gt;&gt;&gt; os.listdir('./~') # Uh oh
['SageMaker']
</code></pre>
<p>So what happened was that the full file path was created from the original working directory, contrary to what was expected.</p>
<pre><code>sh-4.2 $ ls ~/SageMaker/path/to/current/~/SageMaker/path/to/current/results
folder01 folder02 folder03 folder04 folder05
</code></pre>
<p><strong>TL;DR</strong>
I did not confirm the location of the directory was being created from root as expected, and it was created in the wrong location. <code>os.listdir()</code> still showed the files in the &quot;correct location&quot; because it wasn't starting in root, but in the current working directory.</p>
","12046750",1
1184,65826304,2,65824766,2021-01-21 11:20:29,2,"<p>You could use key-based auth for SSH instead instead of password auth so that your Jenkins user can access your SSH DVC remote without needing to specify a password.</p>
","1538451",2
1185,65799418,2,65776789,2021-01-19 20:56:13,1,"<p>Short answer: you can't export the models from Forecast</p>
<p>Ref: <a href=""https://github.com/aws-samples/amazon-forecast-samples/issues/104#issuecomment-763119541"" rel=""nofollow noreferrer"">https://github.com/aws-samples/amazon-forecast-samples/issues/104#issuecomment-763119541</a></p>
","11483674",0
1186,65790102,2,65789715,2021-01-19 10:55:13,1,"<p>This is a known issue with MLFlow package, in which a hotfix has been raised.</p>
<p>See here: <a href=""https://github.com/mlflow/mlflow/pull/3978"" rel=""nofollow noreferrer"">https://github.com/mlflow/mlflow/pull/3978</a></p>
<p><strong>Description of fault</strong></p>
<p>In MLflow 1.13.0 and 1.13.1, the following Python event logging message is emitted when a patched ML training function begins execution within a preexisting MLflow run.</p>
<p>Unfortunately, for patched ML training routines that make child calls to other patched ML training routines (e.g. sklearn random forests that call fit() on a collection of sklearn DecisionTree instances), this event log is printed to stdout every time a child is called.</p>
<p>This can produce hundreds of redundant event logging calls that don't provide value to the user.</p>
","14259378",1
1187,65787840,2,60695933,2021-01-19 08:31:17,0,"<p>MLflow's Artifacts are typically ML models, i.e. relatively large binary files. On the other hand, run data are typically a couple of floats.</p>
<p>In the end it is not a question of what is possible or not (many things are possible if you put enough effort into it), but rather to follow good practices:</p>
<ul>
<li>storing large binary artifacts in an SQL database is possible but is bound the degrade the performance of the database sooner or later, and this in turn will degrade your user experience.</li>
<li>storing a couple of floats from a SQL database for quick retrieval for display in a front-end or via command line is a robust industry-proven classic</li>
</ul>
<p>It remains true that the documentation of MLflow on the architecture design rationale could be improved (as of 2020)</p>
","3929660",1
1188,65782342,2,65770913,2021-01-18 21:32:53,2,"<p>Maybe, you are having this issue because this notebook was designed to run when you have an EMR cluster. I suggest you start a notebook with conda_python3 kernel on Sagemaker instead of the SparkMagic kernel. You will need to install <code>pyspark</code> and <code>sagemaker_pyspark</code> using pip, but it should work with the code you posted.</p>
","7691940",1
1189,65771061,2,65769868,2021-01-18 08:30:55,3,"<p><a href=""https://learn.microsoft.com/en-us/azure/container-instances/container-instances-overview"" rel=""nofollow noreferrer"">Azure Container Instances</a> itself is the compute platform. It spins up a container in a serverless-fashion.</p>
","1537195",0
1190,65755792,2,65753455,2021-01-16 23:29:35,0,"<p>From a notebook cell you should be able to just ask Pip which version is being used</p>
<pre><code>!pip list | grep gremlinpython
</code></pre>
","5442034",1
1191,65744174,2,61191412,2021-01-15 22:07:51,2,"<p>Answering my question, looks like I was just missing the line <code>jupyter contrib nbextension install --user</code> to copy the JS/CSS files into Jupyter's search directory and some config updates (<a href=""https://github.com/ipython-contrib/jupyter_contrib_nbextensions"" rel=""nofollow noreferrer"">https://github.com/ipython-contrib/jupyter_contrib_nbextensions</a>).</p>
<p>Corrected statement</p>
<pre><code>#!/bin/bash

set -e
sudo -u ec2-user -i &lt;&lt;'EOF'

source /home/ec2-user/anaconda3/bin/activate JupyterSystemEnv

pip install jupyter_contrib_nbextensions
jupyter contrib nbextension install --user
jupyter nbextension enable toc2/main

source /home/ec2-user/anaconda3/bin/deactivate


EOF

##Below may be unnecessary, but other user needed to run to see success
initctl restart jupyter-server --no-wait
</code></pre>
","6011629",2
1192,65727301,2,65710812,2021-01-14 21:40:39,1,"<p>I opened <a href=""https://github.com/kubeflow/kubeflow/issues/5516"" rel=""nofollow noreferrer"">same question</a> in official kubeflow project in Github.</p>
<p>The issue was answered by DavidSpek</p>
<p>To generate the image with the interface I required, I just have to change last line from <code>Dockerfile</code>.</p>
<pre><code>CMD [&quot;sh&quot;,&quot;-c&quot;, &quot;jupyter lab --notebook-dir=/home/${NB_USER} --ip=0.0.0.0 --no-browser --allow-root --port=8888 --NotebookApp.token='' --NotebookApp.password='' --NotebookApp.allow_origin='*' --NotebookApp.base_url=${NB_PREFIX}&quot;]
</code></pre>
<p>And it solved my problem.</p>
","4598175",0
1193,65611073,2,65609804,2021-01-07 10:58:21,1,"<p>The var you are trying to use terraform doesn't know about it</p>
<blockquote>
<p>jobname-$$.Execution.Id.</p>
</blockquote>
<p>That's something specific to the Step function and available within state machine not available for terraform.</p>
","2246345",17
1194,65590861,2,65577286,2021-01-06 05:53:03,0,"<p>The reason behind the abnormal result is due to improper pre-processing method was applied. Here is the complete inference code for Mobilenet-ssd model.</p>
<pre><code>def transform(img):
    # Normalize
    mean_vec = np.array([0.485, 0.456, 0.406])
    stddev_vec = np.array([0.229, 0.224, 0.225])
    image = (img / 255 - mean_vec) / stddev_vec

    # Transpose
    if len(image.shape) == 2:  # for greyscale image
        image = np.expand_dims(image, axis=2)

    image = np.rollaxis(image, axis=2, start=0)[np.newaxis, :]
    return image

model = DLRModel(model_dir, 'gpu')


for file_name in image_folder:
    image = PIL.Image.open(file_name)
    image = np.asarray(image.resize((512, 512)))
    image = transform(image)
    # flatten within a input array
    input_data = {'data': image}
    outputs = model.run(input_data)
    objects = outputs[0]
    scores = outputs[1]
    bounding_boxes = outputs[2]
    result = [objects.tolist(), scores.tolist(), bounding_boxes.tolist()]
    print(json.dumps(result))
</code></pre>
","4684861",0
1195,65588226,2,65587939,2021-01-05 23:51:20,0,"<p>Yes, this is possible you are looking for <a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/sms-custom-templates.html"" rel=""nofollow noreferrer"">Custom Labelling worklflows</a> you can also apply either Majority Voting (MV) or MDS to evaluate the accuracy of the job</p>
","4453351",0
1196,65572260,2,65569634,2021-01-05 02:03:57,0,"<blockquote>
<p>I don't understand the difference between RDS which is the one I think I have to use for this and Aurora or S3</p>
</blockquote>
<p>RDS and Aurora are <a href=""https://en.wikipedia.org/wiki/Relational_database"" rel=""nofollow noreferrer"">relational databases</a> fully managed by AWS. &quot;Regular&quot; RDS allows you to launch the existing popular databases such as MySQL, PostgreSQSL and other which you can launch at home/work as well.</p>
<p>Aurora is <strong>in-house, cloud-native</strong> implementation databases compatible with MySQL and PosrgreSQL. It can store the same data as RDS MySQL or PosrgreSQL, but provides a number of features not available for RDS, such as more read replicas, distributed storage, global databases and more.</p>
<p>S3 is <strong>not a database</strong>, but an <a href=""https://en.wikipedia.org/wiki/Object_storage"" rel=""nofollow noreferrer"">object storage</a>, where you can store your files, such as images, csv, excels, similarly like you would store them on your computer.</p>
<blockquote>
<p>I understand this connect my local postgress to AWS but I can't find the data in the amazon page, only creates an instance??</p>
</blockquote>
<p>You can migrate your data from your local postgress to RDS or Aurora if you wish. But RDS nor Aurora will not connect to your existing local database, as they are databases themselfs.</p>
<blockquote>
<p>My final goal is to run the notebooks in the cloud and connect to my postgres in the cloud.</p>
</blockquote>
<p>I don't see a reason why you wouldn't be able to connect to the database. You can try to make it work, and if you encounter difficulties you can make new question on SO with RDS/Aurora setup details.</p>
","248823",0
1197,65569348,2,65494496,2021-01-04 20:25:59,2,"<p>You'll have to make use of <code>mlflow.&lt;model_flavor&gt;.load_model()</code> to load a given model from the Model Registry. For example:</p>
<pre><code>import mlflow.pyfunc

model = mlflow.pyfunc.load_model(
          model_uri=&quot;models:/&lt;model_name&gt;/&lt;model_version&gt;&quot;
          )

model.predict(...)
</code></pre>
<p>With <code>mlflow.tracking.client.MlflowClient</code> you can retrieve metadata about a model from the model registry, but for retrieving the actual model you will need to use <code>mlflow.&lt;model_flavor&gt;.load_model</code>. For example, you could use the MlflowClient to get the download URI for a given model, and then use <code>mlflow.&lt;flavor&gt;.load_model</code> to retrieve that model.</p>
<pre><code>model_uri = client.get_model_version_download_uri(&quot;&lt;model_name&gt;&quot;, &lt;version&gt;)
model = mlflow.pyfunc.load_model(model_uri)

model.predict(...)
</code></pre>
","4461048",1
1198,65561538,2,65561432,2021-01-04 10:58:52,2,"<p>You can only call <code>read</code> on a Spark Session, not on a Spark Context.</p>
<pre><code>from pyspark import SparkContext, SparkConf
from pyspark.sql import SparkSession

conf = SparkConf().setAppName(&quot;app&quot;)
spark = SparkSession.builder.config(conf=conf)
</code></pre>
<p>Or you can convert the Spark context to a Spark session</p>
<pre><code>conf = SparkConf().setAppName(&quot;app&quot;)
sc = SparkContext(conf=conf)
spark = SparkSession(sc)
</code></pre>
","14165730",0
1199,65557986,2,65556574,2021-01-04 05:20:26,2,"<p>great question -- I also had the same misconception starting out. The missing piece is that there's a difference between model 'registration' and model 'deployment'. Registration is simply for tracking and for easy downloading at a later place and time. Deployment is what you're after, making a model available to be scored against.</p>
<p>There's a <a href=""https://learn.microsoft.com/en-us/azure/machine-learning/how-to-deploy-and-where?tabs=python&amp;WT.mc_id=AI-MVP-5003930"" rel=""nofollow noreferrer"">whole section in the docs about deployment</a>. My suggestion would be to deploy it locally first for testing.</p>
","3842610",3
1200,65538791,2,65332452,2021-01-02 11:52:26,3,"<p>KFServing is injecting a second container in the pod of the predictor, SKLearn in your case, which is called <code>storage_initializer</code>. Its role is to download and copy of the model file from the <code>storageUri</code> to a location in the pod to offload the predictor from such a task.</p>
<p>Using the <code>file://</code> in the <code>storageUri</code> can be comfortable for tests when building KFServing, but it would require the pod to have the file already mounted locally.</p>
<p>If you don't have access to a cloud-based storage like <code>gs://</code> and <code>s3://</code>, you can use one of the alternative solutions like <code>uri://</code> or <code>pvc://</code>, serving the model file from your local kubernetes cluster. You can find <a href=""https://github.com/kubeflow/kfserving/tree/master/docs/samples/storage"" rel=""nofollow noreferrer"">examples here</a>.</p>
","3181539",0
1201,65525729,2,65521556,2020-12-31 21:36:05,7,"<p>Using <a href=""https://sagemaker.readthedocs.io/en/stable/api/utility/inputs.html#inputs"" rel=""noreferrer""><code>sagemaker.inputs.TrainingInput</code></a> instead of <code>sagemaker.s3_inputs</code> worked to get that code cell functioning. It is an appropriate solution, though there may be another approach.</p>
<p>Step 4.b also had code which needed updating</p>
<pre><code>sess = sagemaker.Session()
xgb = sagemaker.estimator.Estimator(containers[my_region],role, train_instance_count=1, train_instance_type='ml.m4.xlarge',output_path='s3://{}/{}/output'.format(bucket_name, prefix),sagemaker_session=sess)
xgb.set_hyperparameters(max_depth=5,eta=0.2,gamma=4,min_child_weight=6,subsample=0.8,silent=0,objective='binary:logistic',num_round=100)
</code></pre>
<p>uses parameters <code>train_instance_count</code> and <code>train_instance_type</code> which have been changed in a later version (<a href=""https://sagemaker.readthedocs.io/en/stable/v2.html#parameter-and-class-name-changes"" rel=""noreferrer"">https://sagemaker.readthedocs.io/en/stable/v2.html#parameter-and-class-name-changes</a>).</p>
<p>Making these changes resolved the errors for the tutorial using <code>conda_python3</code> kernel.</p>
","2118138",1
1202,65518989,2,65517560,2020-12-31 09:26:04,2,"<p>Add extensions: *.parquet:</p>
<pre><code>from azureml.data.datapath import DataPath
datastore_path = [DataPath(datastore, 'churn/*.parquet')]
tabular_dataset = Dataset.Tabular.from_parquet_files(path=datastore_path)
</code></pre>
<p>Other ways to not read all data into memory at once would be to use <code>skip()</code> and <code>take()</code> on the TabularDataset to only request portions of source data at a time.
Or to mount the Parquet files as a FileDataset and then construct separate TabularDataset for subsets of the files in your training script.</p>
<p>Here’s a sample notebook for your reference: <a href=""https://github.com/Azure/MachineLearningNotebooks/blob/master/how-to-use-azureml/machine-learning-pipelines/parallel-run/tabular-dataset-inference-iris.ipynb"" rel=""nofollow noreferrer"">https://github.com/Azure/MachineLearningNotebooks/blob/master/how-to-use-azureml/machine-learning-pipelines/parallel-run/tabular-dataset-inference-iris.ipynb</a></p>
","11297406",0
1203,65517812,2,65465114,2020-12-31 07:17:48,8,"<p>If you just remove it then the prediction will work. Therefore, recommend removing this code line.
xgb_predictor.content_type = 'text/csv'</p>
","11753702",0
1204,65417940,2,65416056,2020-12-23 01:27:49,2,"<p>By default, DVC will run your stage command from the same directory as the <a href=""https://dvc.org/doc/user-guide/dvc-files#dvcyaml-file"" rel=""nofollow noreferrer"">dvc.yaml</a> file. If you need to run the command from a different location, you can specify an alternate working directory via <code>wdir</code>, which should be a path relative to <code>dvc.yaml</code>'s location.</p>
<p>Paths for everything else in your stage (like <code>params.yaml</code>) should be specified as relative to <code>wdir</code> (or relative to <code>dvc.yaml</code> if <code>wdir</code> is not provided).</p>
<p>Looking at your example, there also seems to be a bit of confusion on parameters in DVC. In a DVC stage, <code>params</code> is for specifying <a href=""https://dvc.org/doc/command-reference/params"" rel=""nofollow noreferrer"">parameter dependencies</a>, not used for specifying command-line flags. The full command including flags/options should be included  the <code>cmd</code> section for your stage. If you wanted to make sure that your stage was rerun every time certain values in <code>cfg.json</code> have changed, your stage's <code>params</code> section would look something like:</p>
<pre><code>params:
  &lt;relpath from dvc.yaml&gt;/cfg.json:
    - param1
    - param2
    ...
</code></pre>
<p>So your example <code>dvc.yaml</code> would look something like:</p>
<pre><code>stages:
  foodoo:
    cmd: &lt;relpath from dvc.yaml&gt;/foo.py do-this --with &lt;relpath from dvc.yaml&gt;/cfg.json --dest &lt;relpath from dvc.yaml&gt;/...
    deps:
      &lt;relpath from dvc.yaml&gt;/foo.py
    params:
      &lt;relpath from dvc.yaml&gt;/cfg.json:
        ...
    ...
</code></pre>
<p>This would make the command <code>dvc repro</code> rerun your stage any time that the code in foo.py has changed, or the specified parameters in <code>cfg.json</code> have changed.</p>
<p>You may also want to refer to the docs for <a href=""https://dvc.org/doc/command-reference/run#run"" rel=""nofollow noreferrer"">dvc run</a>, which can be used to generate or update a <code>dvc.yaml</code> stage (rather than writing <code>dvc.yaml</code> by hand)</p>
","1538451",1
1205,65412745,2,64630198,2020-12-22 16:57:22,5,"<p>With a <a href=""https://github.com/aws/sagemaker-python-sdk/blob/master/src/sagemaker/session.py#L70"" rel=""noreferrer""><code>sagemaker.session.Session</code></a> instance, you can <a href=""https://github.com/aws/sagemaker-python-sdk/blob/master/src/sagemaker/session.py#L1519"" rel=""noreferrer"">describe training jobs</a>:</p>
<pre><code>import sagemaker


sagemaker_session = sagemaker.session.Session()
sagemaker_session.describe_training_job(&quot;Job...&quot;)
</code></pre>
","13622560",1
1206,65407525,2,65407274,2020-12-22 11:02:18,0,"<p>So the solution is to include a VPC endpoint for the sagemaker API (api.sagemaker...) as well as STS.</p>
","14132588",0
1207,65241252,2,65239565,2020-12-10 19:42:31,5,"<p>It appears what you want here is the ARN of the S3 bucket, which is provided by <a href=""https://www.terraform.io/docs/configuration/blocks/resources/behavior.html#accessing-resource-attributes"" rel=""noreferrer"">exported resource attributes</a>. Specifically, you probably want the <code>arn</code> resource attribute.</p>
<p>Updating your policy like:</p>
<pre><code> 144:             &quot;${aws_s3_bucket.xx_xxxxxxxxxx_xxx_bucket.arn}&quot;,
 146:             &quot;${aws_s3_bucket.xx_xxxxxxxxxx_xxx_bucket.arn}/*&quot;,
</code></pre>
<p>will provide you with the String that you need by accessing the <code>arn</code> attribute. The currently written policy is accessing <code>aws_s3_bucket.xx_xxxxxxxxxx_xxx_bucket</code>, which is a Map (possibly Object) of every argument and attribute for that resource, and will not interpolate within the string of your policy.</p>
","5343387",4
1208,65202042,2,65035488,2020-12-08 15:40:42,5,"<p>Try adding the venv's bin path to the environment that systemd runs in:</p>
<pre><code>[Service]
...
Environment=&quot;PATH=/home/praxasense/.miniconda3/envs/mlflow-server/bin&quot;
...
</code></pre>
<p>I also recommend setting <code>KillMode=mixed</code>, since MLFlow will spawn gunicorn instances that won't be terminated if you terminate the service otherwise. <code>mixed</code> means that child processes will also be terminated.</p>
","4885170",1
1209,65173663,2,65168915,2020-12-06 22:17:54,8,"<p>Yes you can. AWS documentation focuses on end-to-end from training to deployment in SageMaker which makes the impression that training has to be done on sagemaker. AWS documentation and examples should have clear separation among Training in Estimator, Saving and loading model, and Deployment model to SageMaker Endpoint.</p>
<h2>SageMaker Model</h2>
<p>You need to create the <a href=""https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-sagemaker-model.html"" rel=""nofollow noreferrer"">AWS::SageMaker::Model</a> resource which refers to the &quot;model&quot; you have trained <strong>and more</strong>. AWS::SageMaker::Model is in CloudFormation document but it is only to explain what AWS resource you need.</p>
<p><a href=""https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_CreateModel.html"" rel=""nofollow noreferrer"">CreateModel</a> API creates a SageMaker model resource. The parameters specifie the docker image to use, model location in S3, IAM role to use, etc. See <a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/your-algorithms-inference-code.html#your-algorithms-inference-code-load-artifacts"" rel=""nofollow noreferrer"">How SageMaker Loads Your Model Artifacts</a>.</p>
<h3>Docker image</h3>
<p>Obviously you need the framework e.g. ScikitLearn, TensorFlow, PyTorch, etc that you used to train your model to get inferences. You need a docker image that has the framework, and HTTP front end to respond to the prediction calls. See <a href=""https://github.com/aws/sagemaker-inference-toolkit"" rel=""nofollow noreferrer"">SageMaker Inference Toolkit</a> and <a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/amazon-sagemaker-toolkits.html"" rel=""nofollow noreferrer"">Using the SageMaker Training and Inference Toolkits</a>.</p>
<p>To build the image is not easy. Hence AWS provides pre-built images called <a href=""https://docs.aws.amazon.com/deep-learning-containers/latest/devguide/deep-learning-containers-images.html"" rel=""nofollow noreferrer"">AWS Deep Learning Containers</a> and available images are in <a href=""https://github.com/aws/deep-learning-containers/blob/master/available_images.md"" rel=""nofollow noreferrer"">Github</a>.</p>
<p>If your framework and the version is listed there, you can use it as the image. Otherwise you need to build by yourself. See <a href=""https://github.com/awslabs/amazon-sagemaker-mlops-workshop/blob/master/lab/01_CreateAlgorithmContainer/01_Creating%20a%20Classifier%20Container.ipynb"" rel=""nofollow noreferrer"">Building a docker container for training/deploying our classifier</a>.</p>
<h2>SageMaker Python SDK for Frameworks</h2>
<p>AWS SageMaker Python SDK provids utilities to create the SageMaker models for several frameworks. See <a href=""https://sagemaker.readthedocs.io/en/stable/frameworks/index.html"" rel=""nofollow noreferrer"">Frameworks</a> for available frameworks. If it is not there, you may still be able to use <a href=""https://sagemaker.readthedocs.io/en/stable/api/inference/model.html#sagemaker.model.FrameworkModel"" rel=""nofollow noreferrer"">sagemaker.model.FrameworkModel</a> and <a href=""https://sagemaker.readthedocs.io/en/stable/api/inference/model.html"" rel=""nofollow noreferrer"">Model</a> to load your trained model. For your case, see <a href=""https://sagemaker.readthedocs.io/en/stable/frameworks/sklearn/using_sklearn.html"" rel=""nofollow noreferrer"">Using Scikit-learn with the SageMaker Python SDK</a>.</p>
<h3>model.tar.gz</h3>
<p>The saved model artifact needs to be packaged into model.tar.gz. Each framework (e.g. TensorFlow, PyTorch, etc) has its own structure to comply with.</p>
<p>Suppose you used PyTorch and saved the model as model.pth, and the inference.py is the code for prediction, then the structure inside the model.tar.gz is explained <a href=""https://sagemaker.readthedocs.io/en/stable/frameworks/pytorch/using_pytorch.html#model-directory-structure"" rel=""nofollow noreferrer"">Model Directory Structure</a> section in the SDK documentation for PyTorch framework. See <a href=""https://sagemaker.readthedocs.io/en/stable/frameworks/pytorch/using_pytorch.html#create-the-directory-structure-for-your-model-files"" rel=""nofollow noreferrer"">Create the directory structure for your model files</a>.</p>
<pre><code>|- model.pth        # model file is inside / directory.
|- code/            # Code artefacts must be inside /code
  |- inference.py   # Your inference code for the framework
  |- requirements.txt  # only for versions 1.3.1 and higher. Name must be &quot;requirements.txt&quot;
</code></pre>
<p>If you use TensorFlow, you need to look at <a href=""https://github.com/aws/sagemaker-tensorflow-serving-container"" rel=""nofollow noreferrer"">SageMaker TensorFlow Serving Container</a>. As common with AWS, the documentation is not clear and not always up-to-date.</p>
<p>If you use Windows, beware of the CRLF to LF as AWS SageMaker runs in *NIX environment.</p>
<p>Save the model.tar.gz file in S3. Make sure that SageMaker execution role has the IAM permission to access the S3 bucket and objects.</p>
<h3>Loading model and get inference</h3>
<p>See <a href=""https://sagemaker.readthedocs.io/en/stable/frameworks/pytorch/using_pytorch.html#create-a-pytorchmodel-object"" rel=""nofollow noreferrer"">Create a PyTorchModel object</a>. When instantiating the PyTorchModel class, SageMaker automatically selects the AWS Deep Learning Container image for PyTorch for the version specified in <strong>framework_version</strong>. If the image for the version does not exist, then it fails. This has not been documented in AWS but need to be aware of. SageMaker then internally calls the CreateModel API with the S3 model file location and the AWS Deep Learning Container image URL.</p>
<pre><code>import sagemaker
from sagemaker import get_execution_role
from sagemaker.pytorch import PyTorchModel
from sagemaker.predictor import RealTimePredictor, json_serializer, json_deserializer

role = sagemaker.get_execution_role()  # IAM role to run SageMaker, access S3 and ECR
model_file = &quot;s3://YOUR_BUCKET/YOUR_FOLDER/model.tar.gz&quot;   # Must be &quot;.tar.gz&quot; suffix


class AnalysisClass(RealTimePredictor):
    def __init__(self, endpoint_name, sagemaker_session):
        super().__init__(
            endpoint_name,
            sagemaker_session=sagemaker_session,
            serializer=json_serializer,
            deserializer=json_deserializer,   # To be able to use JSON serialization
            content_type='application/json'   # To be able to send JSON as HTTP body
        )

model = PyTorchModel(
    model_data=model_file,
    name='YOUR_MODEL_NAME_WHATEVER',
    role=role,
    entry_point='inference.py',
    source_dir='code',              # Location of the inference code
    framework_version='1.5.0',      # Availble AWS Deep Learning PyTorch container version must be specified
    predictor_cls=AnalysisClass     # To specify the HTTP request body format (application/json)
)

predictor = model.deploy(
    initial_instance_count=1,
    instance_type='ml.m5.xlarge'
)

test_data = {&quot;body&quot;: &quot;YOUR PREDICTION REQUEST&quot;}
prediction = predictor.predict(test_data)
</code></pre>
<p>By default, SageMaker uses NumPy as the serialization format. To be able to use JSON, need to specify the serializer and content_type. Instead of using RealTimePredictor class, you can specify them to predictor.</p>
<pre><code>predictor.serializer=json_serializer
predictor.predict(test_data)
</code></pre>
<p>Or</p>
<pre><code>predictor.serializer=None # As the serializer is None, predictor won't serialize the data
serialized_test_data=json.dumps(test_data) 
predictor.predict(serialized_test_data)
</code></pre>
<h3>Inference code sample</h3>
<p>See <a href=""https://sagemaker.readthedocs.io/en/stable/frameworks/pytorch/using_pytorch.html#process-model-input"" rel=""nofollow noreferrer"">Process Model Input</a>, <a href=""https://sagemaker.readthedocs.io/en/stable/frameworks/pytorch/using_pytorch.html#get-predictions-from-a-pytorch-model"" rel=""nofollow noreferrer"">Get Predictions from a PyTorch Model</a> and <a href=""https://sagemaker.readthedocs.io/en/stable/frameworks/pytorch/using_pytorch.html#process-model-output"" rel=""nofollow noreferrer"">Process Model Output</a>. The prediction request is sent as JSON in HTTP request body in this example.</p>
<pre><code>import os
import sys
import datetime
import json
import torch
import numpy as np

CONTENT_TYPE_JSON = 'application/json'

def model_fn(model_dir):
    # SageMaker automatically load the model.tar.gz from the S3 and 
    # mount the folders inside the docker container. The  'model_dir'
    # points to the root of the extracted tar.gz file.

    model_path = f'{model_dir}/'
    
    # Load the model
    # You can load whatever from the Internet, S3, wherever &lt;--- Answer to your Question
    # NO Need to use the model in tar.gz. You can place a dummy model file.
    ...

    return model


def predict_fn(input_data, model):
    # Do your inference
    ...

def input_fn(serialized_input_data, content_type=CONTENT_TYPE_JSON):
    input_data = json.loads(serialized_input_data)
    return input_data


def output_fn(prediction_output, accept=CONTENT_TYPE_JSON):
    if accept == CONTENT_TYPE_JSON:
        return json.dumps(prediction_output), accept
    raise Exception('Unsupported content type') 
</code></pre>
<h2>Related</h2>
<ul>
<li><p><a href=""https://sagemaker.readthedocs.io/en/stable/overview.html#using-models-trained-outside-of-amazon-sagemaker"" rel=""nofollow noreferrer"">Using Models Trained Outside of Amazon SageMaker
</a></p>
</li>
<li><p><a href=""https://github.com/aws/amazon-sagemaker-examples/blob/main/advanced_functionality/tensorflow_iris_byom/tensorflow_BYOM_iris.ipynb"" rel=""nofollow noreferrer"">TensorFlow Bring Your Own Model: Train locally and deploy on SageMaker.</a><br />
Example to deploy a TensorFlow trained model to the SageMaker Endpoint.</p>
</li>
</ul>
<h2>Note</h2>
<p>SageMaker team keeps changing the implementations and the documentations are not updated accordingly or too disorganized to understand. When you are sure you did follow the documents and it does not work, obsolete or incorrect documentations are quite likely. In such cases, need to clarify with the AWS support, or open an issue in the Github.</p>
","4281353",6
1210,65167028,2,65002633,2020-12-06 10:34:54,0,"<p>I transitioned over to Ubuntu and from there the problem was fixed.</p>
<p>What I did:</p>
<ul>
<li><p>Installed Google Cloud SDK and configured it: <a href=""https://cloud.google.com/sdk/docs/quickstart"" rel=""nofollow noreferrer"">https://cloud.google.com/sdk/docs/quickstart</a></p>
<p>Also install gsutil and kubectl using <code>gcloud components install component_id</code> command</p>
</li>
<li><p>Install KFP SDK: Run the following commands in terminal:</p>
<pre><code> apt-get update; apt-get install -y wget bzip2
 wget https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86_64.sh
 bash Miniconda3-latest-Linux-x86_64.sh

 conda create --name mlpipeline python=3.7   
 conda activate mlpipeline

 pip3 install kfp --upgrade
</code></pre>
</li>
</ul>
","13435688",0
1211,65149662,2,65148768,2020-12-04 19:35:42,0,"<p>Please refer to the sample code provided in this <a href=""https://learn.microsoft.com/en-us/azure/machine-learning/tutorial-1st-experiment-bring-data"" rel=""nofollow noreferrer"">tutorial</a>. Specifically, where it explains how to <a href=""https://learn.microsoft.com/en-us/azure/machine-learning/tutorial-1st-experiment-bring-data#upload"" rel=""nofollow noreferrer"">upload data to Azure</a>.</p>
<pre><code>from azureml.core import Workspace
ws = Workspace.from_config()
datastore = ws.get_default_datastore()
datastore.upload(src_dir='./data',
                 target_path='datasets/cifar10',
                 overwrite=True)
</code></pre>
","11968855",1
1212,65146060,2,65145994,2020-12-04 15:14:14,8,"<p>Put <code>plt.show()</code> after <code>plt.savefig()</code> - <code>plt.show()</code> will remove your plot because it is shown already.</p>
","14165730",4
1213,65137632,2,61340117,2020-12-04 03:42:43,1,"<p>without 'kfp' is pretty messy and hard to automate.</p>
<p>anyway, <a href=""https://github.com/kubeflow/kfctl/issues/140#issuecomment-578837304"" rel=""nofollow noreferrer"">here</a> is one way</p>
<pre><code>
0) curl -v http://$SERVICE:$PORT

Response:
&gt;&gt; &lt;a href=&quot;/dex/auth?client_id=kubeflow-oidc-authservice&amp;amp;redirect_uri=%2Flogin%2Foidc&amp;amp;response_type=code&amp;amp;scope=profile+email+groups+openid&amp;amp;state=STATE_VALUE&quot;&gt;Found&lt;/a&gt;.

STATE=STATE_VALUE

1) curl -v &quot;http://$SERVICE:$PORT/dex/auth?client_id=kubeflow-oidc-authservice&amp;redirect_uri=%2Flogin%2Foidc&amp;response_type=code&amp;scope=profile+email+groups+openid&amp;amp;state=$STATE_VALUE&quot;
Response:
&gt;&gt; &lt;a href=&quot;/dex/auth/local?req=REQ_VALUE&quot;&gt;Found&lt;/a&gt;

REQ=REQ_VALUE

2) curl -v 'http://$SERVICE:$PORT/dex/auth/local?req=REQ_VALUE' -H 'Content-Type: application/x-www-form-urlencoded' --data 'login=admin%40kubeflow.org&amp;password=12341234'

3) curl -v 'http://$SERVICE:$PORT/dex/approval?req=$REQ_VALUE'

Response:
&gt;&gt; &lt;a href=&quot;/login/oidc?code=CODE_VALUE&amp;amp;state=STATE_VALUE&quot;&gt;See Other&lt;/a&gt;.

CODE=CODE_VALUE

4) curl -v 'http://$SERVICE:$PORT/login/oidc?code=$CODE_VALUE&amp;amp;state=$STATE_VALUE'

Response:
&gt;&gt; set cookie authservice_session=SESSION

5) curl -v 'http://$SERVICE:$PORT/pipeline/apis/v1beta1/pipelines' -H 'Cookie: authservice_session=SESSION'

Response:
&gt;&gt; 200 OK { ... }
</code></pre>
","5778044",1
1214,65112717,2,65112585,2020-12-02 17:02:08,12,"<p>Workarounds:</p>
<p>Local environment:
Downgrade pip to &lt; 20.3</p>
<p>Conda environment created from yaml:
This will be seen only if conda-forge is highest priority channel, anaconda channel doesn't have pip 20.3 (as of now). To mitigate the issue please explicitly specify pip&lt;20.3 (!=20.3 or =20.2.4 pin to other version) as a conda dependency in the conda specification file</p>
<p>AzureML experimentation:
Follow the case above to make sure pinned pip resulted as a conda dependency in the environment object, either from yml file or programmatically</p>
","12126562",2
1215,65110356,2,64209196,2020-12-02 14:45:49,14,"<p>To add or correct a parameter, metric or artifact of an existing run, pass run_id instead of experiment_id to mlflow.start_run function</p>
<pre><code>with mlflow.start_run(run_id=&quot;your_run_id&quot;) as run:
    mlflow.log_param(&quot;p1&quot;,&quot;your_corrected_value&quot;)
    mlflow.log_metric(&quot;m1&quot;,42.0) # your corrected metrics
    mlflow.log_artifact(&quot;data_sample.html&quot;) # your corrected artifact file
</code></pre>
<p>You can correct, add to, or delete any MLflow run any time after it is complete. Get the run_id either from the UI or by using <a href=""https://www.mlflow.org/docs/latest/python_api/mlflow.html#mlflow.search_runs"" rel=""noreferrer"">mlflow.search_runs</a>.</p>
<p>Source: <a href=""https://towardsdatascience.com/5-tips-for-mlflow-experiment-tracking-c70ae117b03f"" rel=""noreferrer"">https://towardsdatascience.com/5-tips-for-mlflow-experiment-tracking-c70ae117b03f</a></p>
","2176211",3
1216,65102411,2,60616430,2020-12-02 04:51:47,7,"<p>I ran into this same problem and was able to do get all of the values for the metric by using using <a href=""https://mlflow.org/docs/latest/python_api/mlflow.tracking.html#mlflow.tracking.MlflowClient.get_metric_history"" rel=""noreferrer""><code>mlflow.tracking.MlflowClient().get_metric_history</code></a>. This will return every value you logged using <code>mlflow.log_metric(key, value)</code>.</p>
<p>Quick example (untested)</p>
<pre class=""lang-py prettyprint-override""><code>import mlflow
trackingDir = 'file:///....'
registryDir = 'file:///...'
runID = 'my run id'
metricKey = 'loss'

client = mlflow.tracking.MlflowClient(
            tracking_uri=trackingDir,
            registry_uri=registryDir,
        )

metrics = client.get_metric_history(runID, metricKey)
</code></pre>
<p><a href=""https://mlflow.org/docs/latest/python_api/mlflow.tracking.html#mlflow.tracking.MlflowClient.get_metric_history"" rel=""noreferrer"">From the docs</a></p>
<blockquote>
<p>get_metric_history(run_id, key)[source] Return a list of metric
objects corresponding to all values logged for a given metric.</p>
<p>Parameters run_id – Unique identifier for run</p>
<p>key – Metric name within the run</p>
<p>Returns A list of mlflow.entities.Metric entities if logged, else
empty list</p>
<pre class=""lang-py prettyprint-override""><code>from mlflow.tracking import MlflowClient

def print_metric_info(history):
    for m in history:
        print(&quot;name: {}&quot;.format(m.key))
        print(&quot;value: {}&quot;.format(m.value))
        print(&quot;step: {}&quot;.format(m.step))
        print(&quot;timestamp: {}&quot;.format(m.timestamp))
        print(&quot;--&quot;)

# Create a run under the default experiment (whose id is &quot;0&quot;). Since this is low-level
# CRUD operation, the method will create a run. To end the run, you'll have
# to explicitly end it. 
client = MlflowClient() 
experiment_id = &quot;0&quot; 
run = client.create_run(experiment_id) 
print(&quot;run_id:{}&quot;.format(run.info.run_id))
print(&quot;--&quot;)

# Log couple of metrics, update their initial value, and fetch each
# logged metrics' history. 
for k, v in [(&quot;m1&quot;, 1.5), (&quot;m2&quot;, 2.5)]:
    client.log_metric(run.info.run_id, k, v, step=0)
    client.log_metric(run.info.run_id, k, v + 1, step=1)
    print_metric_info(client.get_metric_history(run.info.run_id, k))
client.set_terminated(run.info.run_id) 
</code></pre>
</blockquote>
","6720145",0
1217,65100257,2,65099376,2020-12-01 23:48:58,2,"<p>I fixed the issue by changing the pytorch version from 1.4.0 to 1.6.0.
So the requirements.txt looks like this:</p>
<pre><code>torch==1.6.0
sentence-transformers
</code></pre>
<p>At first I tried one of the older versions of sentence-transformers which was compatible with pytorch 1.4.0. But the older version doesn't support &quot;xml-roberta-base&quot; model, so I tried to upgrade the pytorch version.</p>
","5711976",0
1218,65092191,2,65091602,2020-12-01 14:14:57,2,"<p>No, according to the <a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/sms-text-classification-multilabel.html"" rel=""nofollow noreferrer"">documentation</a> the maximum is 50.</p>
","12638118",0
1219,65085612,2,65075228,2020-12-01 06:21:04,2,"<blockquote>
<p>To resolve this issue, I would request you to install azureml library from PyPi packages.</p>
</blockquote>
<p>To make third-party or locally-built code available to notebooks and jobs running on your clusters, you can install a library. Libraries can be written in Python, Java, Scala, and R. You can upload Java, Scala, and Python libraries and point to external packages in PyPI, Maven, and CRAN repositories.</p>
<p><strong>Steps to install third-party libraries:</strong></p>
<p><strong>Step1:</strong> Create Databricks Cluster.</p>
<p><strong>Step2:</strong> Select the cluster created.</p>
<p><strong>Step3:</strong> Select Libraries =&gt; Install New =&gt; Select Library Source = &quot;PYPI&quot; =&gt; Package = &quot;azureml-sdk[databricks]&quot;.</p>
<p><a href=""https://i.stack.imgur.com/DafqR.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/DafqR.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.stack.imgur.com/wCjUG.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/wCjUG.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.stack.imgur.com/HD6VC.gif"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/HD6VC.gif"" alt=""enter image description here"" /></a></p>
<p><strong>Reference:</strong> <a href=""https://learn.microsoft.com/en-us/azure/machine-learning/how-to-configure-databricks-automl-environment"" rel=""nofollow noreferrer"">Set up a development environment with Azure Databricks and autoML in Azure Machine Learning</a></p>
<p>For different methods to install packages in Azure Databricks: <a href=""https://stackoverflow.com/questions/60543850/how-to-install-a-library-on-a-databricks-cluster-using-some-command-in-the-noteb/60557852#60557852"">How to install a library on a databricks cluster using some command in the notebook?</a></p>
","8188433",1
1220,65054932,2,65054187,2020-11-28 22:05:11,3,"<p>The Sagemaker Python SDK is <a href=""https://github.com/aws/sagemaker-python-sdk"" rel=""nofollow noreferrer"">open source and on GitHub</a>, as well as published on <a href=""https://pypi.org/project/sagemaker/"" rel=""nofollow noreferrer"">Pypi</a>.</p>
<p>You can install it by running:</p>
<pre class=""lang-sh prettyprint-override""><code>pip install sagemaker
</code></pre>
","2304421",2
1221,64980410,2,61979691,2020-11-24 04:47:08,1,"<p>You can use SageMaker input channels:</p>
<pre><code>
train_data = sagemaker.inputs.TrainingInput(
    ""s3://my-bucket/path/to/train"",
    distribution=""FullyReplicated"",
    content_type=""text/csv"",
    s3_data_type=""S3Prefix"",
    record_wrapping=None,
    compression=None
)

validation_data = sagemaker.inputs.TrainingInput(
    ""s3://my-bucket/path/to/validation"",
    distribution=""FullyReplicated"",
    content_type=""text/csv"",
    s3_data_type=""S3Prefix"",
    record_wrapping=None,
    compression=None
)

linear.fit({""train"": train_data, ""validation"": validation_data})
</pre></code>
<p><a href=""https://github.com/aws/amazon-sagemaker-examples/blob/master/introduction_to_amazon_algorithms/linear_learner_abalone/Linear_Learner_Regression_csv_format.ipynb"" rel=""nofollow noreferrer"">See this example</a></p>
","7740929",0
1222,64603612,2,64594361,2020-10-30 06:27:26,1,"<p>According to your error message: error: One or more 'categories' must be provided. You need to add categories to your vss-extension.json file. This file contains required attributes. Categories is an Array of strings representing the categories your extension belongs to. At least one category must be provided and there's no limit to how many categories you may include.
For example:</p>
<pre><code>&quot;categories&quot;: [
        &quot;Azure Boards&quot;
],
</code></pre>
<p>You can find more information about <a href=""https://learn.microsoft.com/en-us/azure/devops/extend/develop/manifest?view=azure-devops#required-attributes"" rel=""nofollow noreferrer"">required attributes</a> in the document.</p>
","13464420",1
1223,64600395,2,63960011,2020-10-29 23:01:49,1,"<p>If you don't want to keep an inference endpoint up, one option is to use SageMaker Processing to run a job that takes your trained model and test dataset as input, performs inference and computes evaluation metrics, and saves them to S3 in a JSON file.</p>
<p><a href=""https://github.com/aws/amazon-sagemaker-examples/blob/master/sagemaker_processing/scikit_learn_data_processing_and_model_evaluation/scikit_learn_data_processing_and_model_evaluation.ipynb"" rel=""nofollow noreferrer"">This Jupyter notebook example</a> steps through (1) preprocessing training and test data, (2) training a model, then (3) evaluating the model</p>
","4941254",0
1224,64599405,2,64211755,2020-10-29 21:22:55,0,"<p><code>2. Use a bash script to copy the packages from s3 using the CLI. The bash script I used is based off this post. But the packages never get copied, and an error message is not thrown.</code></p>
<p>This approach seems sound.</p>
<p>You may be better off overriding the <code>command</code> field on the <code>SKLearnProcessor</code> to <code>/bin/bash</code>, run a bash script like <code>install_and_run_my_python_code.sh</code> that installs the wheel containing your python dependencies, then runs your main python entry point script.</p>
<p>Additionally, instead of using AWS S3 calls to download your code in a script, you could use a <a href=""https://sagemaker.readthedocs.io/en/stable/api/training/processing.html#sagemaker.processing.ProcessingInput"" rel=""nofollow noreferrer"">ProcessingInput</a> to download your code rather than doing this with AWS CLI calls in a bash script, which is what the <code>SKLearnProcessor</code> does to download your entry point <code>script.py</code> code across all the instances.</p>
","4941254",1
1225,64597989,2,64593327,2020-10-29 19:29:17,1,"<p>The following link has an example of how to call a stored model in SageMaker to run Batch Transform job.</p>
<p><a href=""https://github.com/YiranJing/amazon-sagemaker-examples/blob/master/introduction_to_applying_machine_learning/xgboost_customer_churn/customised_xgboost_customer_churn_batch_transform.ipynb"" rel=""nofollow noreferrer"">Batch Transform Reference</a></p>
","1002903",0
1226,64584469,2,64584295,2020-10-29 03:15:58,2,"<p>Based on the Sagemaker developer documentation, <a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/xgboost-tuning.html"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/sagemaker/latest/dg/xgboost-tuning.html</a>, the hyperparameter <code>scale_pos_weight</code> is NOT tunable. The only parameters that you can tune are given in the link.</p>
","3780426",0
1227,64580845,2,64546521,2020-10-28 20:31:26,1,"<p>One of my colleagues discovered that the managed identities were preventing the preview functionality. Once this aspect of the identities was modified, we could examine the data and use it for a data labelling project.</p>
","14525146",0
1228,64575515,2,64554615,2020-10-28 14:58:13,2,"<p>This might go wrong at some point, but my workaround was installing opencv-python-headless instead of opencv.</p>
<p>In the environment .yaml file, just replace:</p>
<pre><code>- opencv-python&gt;=4.1.2
</code></pre>
<p>with:</p>
<pre><code>- opencv-python-headless
</code></pre>
","6805177",0
1229,64569905,2,64513552,2020-10-28 09:26:02,3,"<p>I found a solution, maybe it will be useful for someone else. You can see details with code examples here: <a href=""https://github.com/mlflow/mlflow/issues/3592"" rel=""nofollow noreferrer"">https://github.com/mlflow/mlflow/issues/3592</a></p>
","9215579",0
1230,64569605,2,64561968,2020-10-28 09:07:36,1,"<p>The notebook instance was created in ap-south-1 and the S3 bucket was in us-east-1. Creating another notebook instance from the same region as the S3 bucket resolved the issue.</p>
","1002903",0
1231,64551290,2,64537150,2020-10-27 09:16:19,2,"<p>Here are some ideas:</p>
<p><em><strong>1. does anyone have any ideas how to go about implementing a system in AWS Sagemaker whereby for hundreds of thousands of units, we can have a separate trained model artifact for each unit? Is there a way to output multiple model artifacts for 1 sagemaker training job with the use of an SKLearn estimator?</strong></em></p>
<p>I don't know if the 30-training job concurrency is a hard limit, if it is a blocker you should try and open a support ticket to ask if it is and try and get it raised. Otherwise as you can point out, you can try and train multiple models in one job, and produce multiple artifacts that you can either (a) send to S3 manually, or (b) save to <code>opt/ml/model</code> so that they all get sent to the model.tar.gz artifact in S3. Note that if this artifact gets too big this could get impractical though</p>
<p><em><strong>2. how does Sagemaker make use of multiple CPUs when a training script is submitted? Does this have to be specified in the training script/estimator object or is this handled automatically?</strong></em></p>
<p>This depends on the type of training container you are using. SageMaker built-in containers are developed by Amazon teams and designed to efficiently use available resources. If you use your own code such as custom python in the Sklearn container, you are responsible for making sure that your code is efficiently written and uses available hardware. Hence framework choice is quite important :) for example, some sklearn models support explicitly using multiple CPUs (eg the <code>n_jobs</code> parameter in the <a href=""https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html"" rel=""nofollow noreferrer"">random forest</a>), but I don't think that Sklearn natively supports GPU, multi-GPU or multi-node training.</p>
","5331834",3
1232,64546723,2,59375896,2020-10-27 00:21:55,1,"<hr />
<p>Did you setup SageMaker Studio to use AWS SSO or IAM for the authentication method?</p>
<p>From what I have gathered, the SageMaker Studio users, when setup using IAM for the authentication method are not actually users. They just provide partitions within Studio for different work environments. You can then control access to these partitions using IAM policies for your IAM users / roles for federated users.</p>
<p>Each Studio user has it's own URL to access that environment.</p>
<p>Here is the SageMaker developer guide: <a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-dg.pdf#%5B%7B%22num%22%3A14340%2C%22gen%22%3A0%7D%2C%7B%22name%22%3A%22XYZ%22%7D%2C72%2C533.986%2Cnull%5D"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-dg.pdf#%5B%7B%22num%22%3A14340%2C%22gen%22%3A0%7D%2C%7B%22name%22%3A%22XYZ%22%7D%2C72%2C533.986%2Cnull%5D</a></p>
<p>Page 36 discusses on-boarding with IAM.</p>
","10209701",0
1233,64540670,2,64508625,2020-10-26 15:57:51,0,"<p>Found a way to do this:</p>
<p>Pass your params to PythonScriptStep</p>
<pre><code>param_1 = PipelineParameter(name='min_id', default_value=5)
param_2 = PipelineParameter(name='max_id', default_value=10)
sql_datastore = &quot;sql_datastore&quot;
step = PythonScriptStep(script_name='script.py', arguments=[param_1, param_2, 
sql_datastore])
</code></pre>
<p>In script.py</p>
<pre><code>min_id_param = sys.argv[1]
max_id_param = sys.argv[2]
sql_datastore_name = sys.argv[3]
query = &quot;&quot;&quot;
SELECT id, foo, bar FROM baz
WHERE baz.id BETWEEN {} AND {}
&quot;&quot;&quot;.format(min_id_param, max_id_param)
run = Run.get_context()
sql_datastore = Datastore.get(run.experiment.workspace, sql_datastore_name)
dataset = Dataset.Tabular.from_sql_query((sql_datastore, query))
</code></pre>
","4898269",0
1234,64535242,2,64448720,2020-10-26 10:22:46,1,"<p>To anyone that comes across this issue in future, the problem has been solved.</p>
<p>The issue was nothing to do with the training, but with invalid characters in directory names being sent to S3. So the script would produce the artifacts correctly, but sagemaker would throw an exception when trying to save them to S3</p>
","13756183",0
1235,64481643,2,64215998,2020-10-22 11:51:57,0,"<p>The issue turned out to be related to the css styling that was being applied to the canvas portion of my site that was loading the labeling tools.</p>
","7369248",0
1236,64471736,2,64456396,2020-10-21 21:04:50,3,"<p>If we talk about unit tests, I think it's indeed better to do a mock. It's best to have unit tests small, testing actual logic of the unit, etc. It's good to have other tests though that would pull the model and run some logic on top of that - I would call them integration tests.</p>
<p>It's not black and white though. If you for some reason see that it's easier to use an actual model (e.g. it changes a lot and it is easier to use it instead of maintaining and updating stubs/fixtures), you could potentially cache it.</p>
<p>I think, to help you with the mock, you would need to share some technical details- how does the function look like, what have you tried, what breaks, etc.</p>
<blockquote>
<p>to do this because downloading models every time you want to run push some code will quickly eat up my usage minutes for CI and is an expensive option.</p>
</blockquote>
<p>I think you can potentially utilize CI systems cache to avoid downloading it over and over again. This is the GitHub Actions related <a href=""https://github.com/actions/cache#cache-limits"" rel=""nofollow noreferrer"">repository</a>, this is <a href=""https://circleci.com/docs/2.0/caching"" rel=""nofollow noreferrer"">CircleCI</a>. The idea is the same across all common CI providers. Which one are considering to use, btw?</p>
<blockquote>
<p>Just put the model files in the git remote and be done with it. Only use DVC to track data.</p>
</blockquote>
<p>This can be the way, but if models are large enough you will pollute Git history significantly. On some CI systems it can become even slower since they will be fetching this with regular <code>git clone</code>. Effectively, downloading models anyway.</p>
<p>Btw, if you use DVC or not take a look at another open-source project that is made specifically to do CI/CD for ML - <a href=""https://cml.dev"" rel=""nofollow noreferrer"">CML</a>.</p>
","298182",1
1237,64455227,2,64447654,2020-10-21 01:01:30,1,"<p>TFX <a href=""https://github.com/tensorflow/tfx/releases/tag/v0.23.0"" rel=""nofollow noreferrer"">0.23.0</a> has added support for TF.ExampleSequence in some components.</p>
<p>You can also make use of TF.Example using the list in the way you describe. If you need to feed a sequence to your model based on your TF.Example you will need to use TF.transform to stack and reshape the values read in.</p>
<pre><code>float32 = tf.reshape(
        tf.stack(...),
        [-1, timesteps, features)])
</code></pre>
","4141845",0
1238,64438643,2,64415787,2020-10-20 04:49:36,1,"<p>To run Grafana with kubeflow, follow the steps:</p>
<ol>
<li>create namespace</li>
</ol>
<blockquote>
<p>kubectl create namespace knative-monitoring</p>
</blockquote>
<ol start=""2"">
<li>setup monitoring components</li>
</ol>
<blockquote>
<p>kubectl apply --filename
<a href=""https://github.com/knative/serving/releases/download/v0.13.0/monitoring-metrics-prometheus.yaml"" rel=""nofollow noreferrer"">https://github.com/knative/serving/releases/download/v0.13.0/monitoring-metrics-prometheus.yaml</a></p>
</blockquote>
<ol start=""3"">
<li>Launch grafana board via port forwarding</li>
</ol>
<blockquote>
<p>kubectl port-forward --namespace knative-monitoring $(kubectl get pod
--namespace knative-monitoring --selector=&quot;app=grafana&quot; --output jsonpath='{.items[0].metadata.name}') 8080:3000</p>
</blockquote>
<p>Access the grafana dashboard on http://localhost:8080.</p>
","7031716",0
1239,64414765,2,64407452,2020-10-18 14:58:51,2,"<p>Scheduling notebook execution is a bit of a SageMaker anti-pattern, because (1) you would need to manage data I/O (training set, trained model) yourself, (2) you would need to manage metadata tracking yourself, (3) you cannot run on distributed hardware and (4) you cannot use Spot. Instead, it is recommended for scheduled task to leverage the various SageMaker long-running, background job APIs: SageMaker Training, SageMaker Processing or SageMaker Batch Transform (in the case of a batch inference).</p>
<p>That being said, if you still want to schedule a notebook to run, you can do it in a variety of ways:</p>
<ul>
<li>in the <a href=""https://www.youtube.com/watch?v=6EYEoAqihPg"" rel=""nofollow noreferrer"">SageMaker CICD Reinvent 2018 Video</a>, Notebooks are launched as Cloudformation templates, and their execution is automated via a SageMaker lifecycle configuration.</li>
<li>AWS released <a href=""https://aws.amazon.com/blogs/machine-learning/scheduling-jupyter-notebooks-on-sagemaker-ephemeral-instances/"" rel=""nofollow noreferrer"">this blog post</a> to document how to launch Notebooks from within Processing jobs</li>
</ul>
<p>But again, my recommendation for scheduled tasks would be to remove them from Jupyter, turn them into scripts and run them in SageMaker Training</p>
<p>No matter your choices, all those tasks can be launched as API calls from within a Lambda function, as long as the function role has appropriate permissions</p>
","5331834",0
1240,64411597,2,64302986,2020-10-18 09:15:33,1,"<p>I used the AWS documentation of the <a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/a2i-crowd-textract-detection.html"" rel=""nofollow noreferrer"">a2i-crowd-textract-detection human task element</a> to generate the value of the <code>initialValue</code> attribute. It appears the doc for that attribute is incorrect. While the the doc shows that the value should be in the same format as the output of Textract, namely:</p>
<pre><code>[
        {
            &quot;BlockType&quot;: &quot;KEY_VALUE_SET&quot;,
            &quot;Confidence&quot;: 38.43309020996094,
            &quot;Geometry&quot;: { ... }
            &quot;Id&quot;: &quot;8c97b240-0969-4678-834a-646c95da9cf4&quot;,
            &quot;Relationships&quot;: [
                { &quot;Type&quot;: &quot;CHILD&quot;, &quot;Ids&quot;: [...]},
                { &quot;Type&quot;: &quot;VALUE&quot;, &quot;Ids&quot;: [...]}
            ],
            &quot;EntityTypes&quot;: [&quot;KEY&quot;],
            &quot;Text&quot;: &quot;Foo bar&quot;
        },
]
</code></pre>
<p>the <code>a2i-crowd-textract-detection</code> expects the input to have lowerCamelCase attribute names (rather than UpperCamelCase). For example:</p>
<pre><code>[
        {
            &quot;blockType&quot;: &quot;KEY_VALUE_SET&quot;,
            &quot;confidence&quot;: 38.43309020996094,
            &quot;geometry&quot;: { ... }
            &quot;id&quot;: &quot;8c97b240-0969-4678-834a-646c95da9cf4&quot;,
            &quot;relationships&quot;: [
                { &quot;Type&quot;: &quot;CHILD&quot;, &quot;ids&quot;: [...]},
                { &quot;Type&quot;: &quot;VALUE&quot;, &quot;ids&quot;: [...]}
            ],
            &quot;entityTypes&quot;: [&quot;KEY&quot;],
            &quot;text&quot;: &quot;Foo bar&quot;
        },
]
</code></pre>
<p>I opened a support case about this documentation error to AWS.</p>
","1860089",0
1241,64410652,2,64394661,2020-10-18 07:07:46,3,"<p><a href=""https://learn.microsoft.com/en-us/azure/machine-learning/how-to-configure-auto-train#data-source-and-format?WT.mc_id=AI-MVP-5003930"" rel=""nofollow noreferrer"">Configure AutoML Doc</a> says:</p>
<blockquote>
<p>For remote experiments, training data must be accessible from the remote compute. AutoML only accepts Azure Machine Learning TabularDatasets when working on a remote compute.</p>
</blockquote>
<p>It looks as if your <code>dataset</code> object is a Pandas DataFrame, when it should really be an Azure ML <code>Dataset</code>. Check out <a href=""https://learn.microsoft.com/en-us/azure/machine-learning/how-to-create-register-datasets?WT.mc_id=AI-MVP-5003930"" rel=""nofollow noreferrer"">this doc</a> on creating Datasets.</p>
","3842610",1
1242,64371023,2,64263330,2020-10-15 11:53:13,1,"<p>After some research, this is the solution I've followed:</p>
<ul>
<li>First I have created a <code>SKLearn</code> sagemaker model to do all the preprocess setup (I've built a Scikit-Learn custom class to handle all the preprocess steps, following this <a href=""https://github.com/aws/amazon-sagemaker-examples/blob/master/sagemaker-python-sdk/scikit_learn_inference_pipeline/Inference%20Pipeline%20with%20Scikit-learn%20and%20Linear%20Learner.ipynb"" rel=""nofollow noreferrer"">AWS code</a>)</li>
<li>Trained this preprocess model on my training data. My model, in specific, didn't need to be trained (it does not have any standardization or anything that would need to store training data parameters), but sagemaker requires the model to be trained.</li>
<li>Loaded the trained legacy model that we had using the <code>Model</code> parameter.</li>
<li>Created a <code>PipelineModel</code> with the preprocessing model and legacy model in cascade:</li>
</ul>
<pre class=""lang-py prettyprint-override""><code>pipeline_model = PipelineModel(name=model_name,
                               role=role,
                               models=[
                                    preprocess_model,
                                    trained_model
                               ])
</code></pre>
<ul>
<li>Create a new endpoint, calling the <code>PipelineModel</code> and then changed the <code>Lambda</code> function to call this new endpoint. With this I could send the <strong>raw data</strong> directly for the same <code>API Gateway</code> and it would call only <strong>one</strong> endpoint, without needing to pay two endpoints 24/7 to perform the entire process.</li>
</ul>
<p>I've found this to be a good and &quot;<em>economic</em>&quot; way to perform the preprocess outside the trained model, without having to do hard processing jobs on a <code>Lambda</code> function.</p>
","3835691",0
1243,64360437,2,64359321,2020-10-14 19:40:06,2,"<p>You do not need to use sagemaker to deploy your flask application to AWS. AWS has a nice documentation to deploy a Flask Application to an <a href=""https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/create-deploy-python-flask.html"" rel=""nofollow noreferrer"">AWS Elastic Beanstalk</a> environment.</p>
<p>Other than that you can also deploy the application using two methods.</p>
<ol>
<li>via EC2</li>
<li>via Lambda</li>
</ol>
<h3>EC2 Instances</h3>
<p>You can launch the ec2 instance with public IP with SSH enabled from your IP address. Then SSH into the instance and install the python, it's libraries and your application.</p>
<h3>Lambda</h3>
<p>AWS lambda is the perfect solution. It scales automatically, depends upon the requests your application will receive.
As lambda needs your dependencies be available in the package, so you need to install them using <code>--target</code> parameter, zip the python code along with the installed packages and then upload to the Lambda.</p>
<pre class=""lang-sh prettyprint-override""><code>pip install --target ./package Flask
cd package
zip -r9 function.zip . # Create a ZIP archive of the dependencies.
cd .. &amp;&amp; zip -g function.zip lambda_function.py # Add your function code to the archive.
</code></pre>
<p>For more detailed instructions you can read these documentations</p>
<ul>
<li><a href=""https://docs.aws.amazon.com/lambda/latest/dg/lambda-python.html"" rel=""nofollow noreferrer"">Lambda</a></li>
</ul>
","14401399",0
1244,64359469,2,64338898,2020-10-14 18:31:40,0,"<p>I believe it is that way.</p>
<pre><code>from azureml.core import Experiment, Workspace
Experiment = ws.experiments[&quot;teste2-Monitor-Runs&quot;]
</code></pre>
","9584025",0
1245,64213834,2,64139290,2020-10-05 17:58:05,3,"<p>Thanks for reaching out to us.</p>
<p>In Azure Machine Learning Studio, you would need to setup partition format similar to python SDK, as follows, assuming your data path is &quot;timeseries/timestamp=2020-01-01/data.parquet&quot;:
<a href=""https://i.stack.imgur.com/HwYfF.png"" rel=""nofollow noreferrer"">Set up partition format when creating time series dataset</a></p>
","14396244",1
1246,64212742,2,64080209,2020-10-05 16:37:47,1,"<p>Thanks @Lorin S., for sharing the solution reference. For the benefit of community I am providing solution here (answer section) given by 1025KB in <a href=""https://github.com/tensorflow/tfx/issues/2582#issuecomment-700865479"" rel=""nofollow noreferrer"">github</a>.</p>
<blockquote>
<p>Added split in TFX 0.23 version, but Colab is not updated in 0.23.
Colab is fixed in 0.24 <a href=""https://github.com/tensorflow/tfx/blob/b01482442891a49a1487c67047e85ab971717b75/tfx/orchestration/experimental/interactive/standard_visualizations.py#L55"" rel=""nofollow noreferrer"">here</a></p>
</blockquote>
<p>Issue was resolved by upgrading tfx to 0.24</p>
","user11530462",0
1247,63923931,2,63457857,2020-09-16 16:04:09,0,"<p>when the training container runs, your entry_point script will be executed.</p>
<p>Since your notebook file and entry_point script are under the same directory, your <code>source_dir</code> should just be &quot;.&quot;</p>
<p>Does your entry_point script import any modules that are not installed by the tensorflow training container by default? Also could you share your stacktrace of the error?</p>
","11278913",0
1248,63920220,2,63781356,2020-09-16 12:35:22,0,"<p>This seems to work:</p>
<p><code>return json.dumps({&quot;inputs&quot;: df.tolist() }).</code></p>
","184925",2
1249,63909876,2,60157184,2020-09-15 21:09:11,1,"<blockquote>
<p>My questions are:</p>
<p>How does Tensorflow decide the distribution of variables on the PS's?
In the example code, there is no explicit reference to devices.
Somehow the distribution is done automatically.</p>
</blockquote>
<p>The TensorFlow image provided by SageMaker has the code to setup TF_CONFIG and launching parameter server for multi work training. See the code [here][1] The setup is for each node in the cluster there is a PS and a worker thread configured.</p>
<p>It's not using any DistributionStrategy so the default strategy is used. See [here][2].</p>
<p>If you would like to use a different DistributionStrategy or different TF_CONFIG you will need to disable <code>parameter_server</code> option when launching the SageMaker training job and set everything up in your training script.</p>
<blockquote>
<p>Is it possible to see which parameters have been assigned to each PS?
Or to see what the communication between PS's looks like? If so, how?</p>
</blockquote>
<p>You should be able to get some information from the output log which can be found in CloudWatch. The link is available on the Training Job console page.
[1]: <a href=""https://github.com/aws/sagemaker-tensorflow-training-toolkit/blob/master/src/sagemaker_tensorflow_container/training.py#L37"" rel=""nofollow noreferrer"">https://github.com/aws/sagemaker-tensorflow-training-toolkit/blob/master/src/sagemaker_tensorflow_container/training.py#L37</a>
[2]: <a href=""https://www.tensorflow.org/guide/distributed_training#default_strategy"" rel=""nofollow noreferrer"">https://www.tensorflow.org/guide/distributed_training#default_strategy</a></p>
","10481182",0
1250,63897261,2,63891547,2020-09-15 07:41:56,11,"<p>According to this <a href=""https://learn.microsoft.com/en-us/azure/machine-learning/how-to-access-data#azure-data-lake-storage-generation-2"" rel=""noreferrer"">documentation</a>,you need to enable the service principal.</p>
<p>1.you need to register your application and grant the service principal with <strong>Storage Blob Data Reader access</strong>.</p>
<p><a href=""https://i.stack.imgur.com/FZl8O.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/FZl8O.png"" alt=""enter image description here"" /></a></p>
<p>2.try this code:</p>
<pre><code>adlsgen2_datastore = Datastore.register_azure_data_lake_gen2(workspace=ws,
                                                             datastore_name=adlsgen2_datastore_name,
                                                             account_name=account_name,
                                                             filesystem=file_system,
                                                             tenant_id=tenant_id,
                                                             client_id=client_id,
                                                             client_secret=client_secret
                                                             )

adls_ds = Datastore.get(ws, datastore_name=adlsgen2_datastore_name)
dataset = Dataset.Tabular.from_delimited_files((adls_ds,'sample.csv'))
print(dataset.to_pandas_dataframe())
</code></pre>
<p><strong>Result:</strong></p>
<p><a href=""https://i.stack.imgur.com/50mit.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/50mit.png"" alt=""enter image description here"" /></a></p>
","13367638",0
1251,63881845,2,59487643,2020-09-14 09:42:09,0,"<p>The best way to overcome this problem in sagemaker is to use lifecycle configuration.
Rather than doing pip install inside the notebook, write all your requirements.txt installs inside the lifecycle configurations. The notebook will take more time to spawn but the code but the libraries will be pre-installed.</p>
","7965340",0
1252,63846406,2,63844663,2020-09-11 11:42:57,0,"<p>The following code uses <a href=""https://www.geeksforgeeks.org/backtracking-algorithms/"" rel=""nofollow noreferrer"">backtracking algorithm</a> to find a solution in ~3 minutes on Windows 10 PC with i7 CPU 920 @ 2.67 MHz</p>
<p><strong>Code</strong></p>
<pre><code>def condition(a):
    ' Apply conditions individually to allow immediate backtracking when a condition is not met '
    if len(a)==4:
        return (a[0] + a[1] - a[2] + a[3]) == 19
    elif len(a) == 8:
        return (a[4] - a[5] - a[6] - a[7]) == -31
    elif len(a) == 11:
        return (a[6] % a[10]) == 0 and (a[9] % a[10]) == 0
    elif len(a)==12:
        return (a[8] - a[9] // a[10] + a[11]) == 8
    elif len(a) == 13:
        return (a[0] + a[4] + a[8] + a[12]) == 23
    elif len(a) == 14:
        return (a[1] - a[5] + a[9] - a[13]) == -3
    elif len(a) == 15:
        return (a[2] - a[6] // a[10] + a[14]) == 5 and (a[13] % a[14]) == 0
    elif len(a) == 16:
        return (a[3] + a[7] - a[11] + a[15]) == 22 and (a[12] - a[13] // a[14] + a[15]) == 1
    
    elif len(a) &gt; 16:
        return False  # array exceeds max length
    else:
        return True   # not one of the lengths to try conditions

def solve(answer = None):
    ' Uses backtracking to find solve 4x4 math grid problem '
    if answer is None:
        answer = ()
        
    if condition(answer):
        # satisfies conditions so far
        if len(answer) == 16:
            # satisfies all conditions
            yield answer
        else:
            # Expand on solution since satisfies conditions so far
            for i in range(1, 17):
                # Try adding one of the numbers 1 to 17 to current answer
                yield from solve(answer + (i,))
        
from time import time

tstart = time()
print(f'Solution: {next(solve(), None))}') # get first solution
                                           # use list(solve()) to get all solutions
print(f'Elapsed time {time()-tstart}')
</code></pre>
<p><strong>Output</strong></p>
<pre><code>Solution: (1, 6, 1, 13, 6, 14, 14, 9, 15, 16, 2, 1, 1, 11, 11, 1)
Elapsed time 189.32917761802673
</code></pre>
<p><strong>Explanation</strong></p>
<p>Trying all multiset_permutations of numbers of length 16 is infeasible since there are too many (i.e. 16^16 = 2^64 ~ 18e18).</p>
<p>Idea is to create arrays of increasing size (i.e. 0 to 16 length), but abort early if the array will not satisfy conditions (i.e. backtracking).</p>
<p>To be able to abort early (i.e. backtracking) we:</p>
<ul>
<li>Split conditions up so we can apply based upon the size of the array (i.e. condition function)</li>
<li>We add the condition that one number will be divisible by another for the division (i.e. if we have x/y then we need x % y == 0)</li>
<li>We use integer division throughout (i.e. x // y)</li>
</ul>
","3066077",2
1253,63824703,2,63813624,2020-09-10 07:22:57,5,"<p>For Scikit Learn for example, you can get inspiration from this public demo <a href=""https://github.com/awslabs/amazon-sagemaker-examples/blob/master/sagemaker-python-sdk/scikit_learn_randomforest/Sklearn_on_SageMaker_end2end.ipynb"" rel=""noreferrer"">https://github.com/awslabs/amazon-sagemaker-examples/blob/master/sagemaker-python-sdk/scikit_learn_randomforest/Sklearn_on_SageMaker_end2end.ipynb</a></p>
<p>Step 1: Save your artifact (eg the joblib) compressed in S3 at <code>s3://&lt;your path&gt;/model.tar.gz</code></p>
<p>Step 2: Create an inference script with the deserialization function <code>model_fn</code>. (Note that you could also add custom inference functions <code>input_fn</code>, <code>predict_fn</code>, <code>output_fn</code> but for scikit the defaults function work fine)</p>
<pre><code>%%writefile inference_script.py. # Jupiter command to create file in case you're in Jupiter

import joblib
import os

def model_fn(model_dir):
    clf = joblib.load(os.path.join(model_dir, &quot;model.joblib&quot;))
    return clf
</code></pre>
<p>Step 3: Create a model associating the artifact with the right container</p>
<pre><code>from sagemaker.sklearn.model import SKLearnModel

model = SKLearnModel(
    model_data='s3://&lt;your path&gt;/model.tar.gz',
    role='&lt;your role&gt;',
    entry_point='inference_script.py',
    framework_version='0.23-1')
</code></pre>
<p>Step 4: Deploy!</p>
<pre><code>model.deploy(
    instance_type='ml.c5.large',  # choose the right instance type
    initial_instance_count=1)
</code></pre>
","5331834",2
1254,63820436,2,63740792,2020-09-09 22:37:06,3,"<p>As you said, and is indicated in the <a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/your-algorithms-batch-code.html"" rel=""nofollow noreferrer"">AWS documentation</a>, Sagemaker will run your container with the following command:</p>
<pre class=""lang-sh prettyprint-override""><code>docker run image serve
</code></pre>
<p>By issuing this command Sagemaker will overwrite any <code>CMD</code> that you provide in your container Dockerfile, so you cannot use <code>CMD</code> to provide dynamic arguments to your program.</p>
<p>We can think in use the Dockerfile <code>ENTRYPOINT</code> to consume some environment variables, but the documentation of AWS dictates that it is preferable use the <code>exec</code> form of the <code>ENTRYPOINT</code>. Somethink like:</p>
<pre><code>ENTRYPOINT [&quot;/usr/bin/Rscript&quot;, &quot;/opt/ml/mars.R&quot;, &quot;--no-save&quot;]
</code></pre>
<p>I think that, for analogy with <a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/your-algorithms-training-algo-dockerfile.html"" rel=""nofollow noreferrer"">model training</a>, they need this kind of container execution to enable the container to receive termination signals:</p>
<blockquote>
<p>The exec form of the <code>ENTRYPOINT</code> instruction starts the executable directly, not as a child of <code>/bin/sh</code>. This enables it to receive signals like <code>SIGTERM</code> and <code>SIGKILL</code> from SageMaker APIs.</p>
</blockquote>
<p>To allow variable expansion, we need to use the <code>ENTRYPOINT</code> <code>shell</code> form. Imagine:</p>
<pre><code>ENTRYPOINT [&quot;sh&quot;, &quot;-c&quot;, &quot;/usr/bin/Rscript&quot;, &quot;/opt/ml/mars.R&quot;, &quot;--no-save&quot;, &quot;$ENV_VAR1&quot;]
</code></pre>
<p>If you try to do the same with the <code>exec</code> form the variables provided will be treated as a literal and will not be sustituited for their actual values.</p>
<p>Please, see the approved answer of <a href=""https://stackoverflow.com/questions/37904682/how-do-i-use-docker-environment-variable-in-entrypoint-array"">this</a> stackoverflow question for a great explanation of this subject.</p>
<p>But, one thing you can do is obtain the value of these variables in your R code, similar as when you process <code>commandArgs</code>:</p>
<pre class=""lang-r prettyprint-override""><code>ENV_VAR1 &lt;- Sys.getenv(&quot;ENV_VAR1&quot;)
</code></pre>
<p>To pass environment variables to the container, as indicated in the <a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/your-algorithms-batch-code.html"" rel=""nofollow noreferrer"">AWS documentation</a>, you can use the <a href=""https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_CreateModel.html"" rel=""nofollow noreferrer""><code>CreateModel</code></a> and <a href=""https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_CreateTransformJob.html"" rel=""nofollow noreferrer""><code>CreateTransformJob</code></a> requests on your container.</p>
<p>You probably will need to include in your Dockerfile <code>ENV</code> definitions for every required environment variable on your container, and provide for these definitions default values with <code>ARG</code>:</p>
<pre><code>ARG ENV_VAR1_DEFAULT_VALUE=VAL1
ENV_VAR1=$ENV_VAR1_DEFAULT_VALUE
</code></pre>
","13942448",4
1255,63816880,2,63811726,2020-09-09 17:43:38,2,"<p>This might be a neat feature to have, but for now it's not a thing - at least not directly in the YAML like this.</p>
<p>Instead, the unit of computation in Conda is the <em>package</em>. That is, if you need to run additional scripts or commands at environment creation, it can be achieved by building a custom package and including this package in the YAML as a dependency. The package itself could be pretty much empty, but whatever code one needs to run would be included via <a href=""https://docs.conda.io/projects/conda-build/en/latest/resources/link-scripts.html"" rel=""nofollow noreferrer"">some installation scripts</a>.</p>
","570918",1
1256,63814062,2,63667022,2020-09-09 14:42:45,1,"<p>The problem is actually coming from smdebug version 0.9.1
Downgrading to 0.8.1 solves the issue</p>
","14181507",0
1257,63811516,2,63807950,2020-09-09 12:17:08,1,"<p>This error occurs when there is a mismatch of number of columns of the two dataset you are appending.</p>
<p>Looking at the error :</p>
<p>The output of one model is returning rows with 3 columns and other one is having either more or less than 3 columns.</p>
<p>Before this step &quot;Add Rows&quot; step -&gt; Do quick Visualize</p>
<p><a href=""https://i.stack.imgur.com/PsYQT.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/PsYQT.png"" alt=""enter image description here"" /></a></p>
<p>This will give a view of the dataset that you are planning to append.</p>
<p><a href=""https://i.stack.imgur.com/x442d.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/x442d.png"" alt=""![enter image description here"" /></a></p>
<p>Ensure for both, the columns numbers are same.</p>
","13755246",1
1258,63806070,2,63698011,2020-09-09 06:39:35,0,"<p>I assume the content type you specified is <code>text/csv</code>, so try out:</p>
<pre><code>{&quot;data&quot;: [&quot;1,1,1,1,1,1&quot;]}
</code></pre>
","12638118",0
1259,63805812,2,63805114,2020-09-09 06:21:38,0,"<p>You could use the <a href=""https://docs.aws.amazon.com/cli/latest/userguide/cli-services-s3-commands.html#using-s3-commands-managing-objects-move"" rel=""nofollow noreferrer"">AWS-CLI</a> or the <a href=""https://docs.aws.amazon.com/AWSJavaScriptSDK/latest/AWS/S3.html"" rel=""nofollow noreferrer"">AWS-S3 SDK</a> (JS in this example).</p>
","12638118",0
1260,63798310,2,63796488,2020-09-08 16:49:29,3,"<p>Yes.You are right. TabularDataset does not support download or mount. You can create&amp;register a Filedataset and the code sample will work.
Learn more about dataset type <a href=""https://learn.microsoft.com/en-us/azure/machine-learning/how-to-create-register-datasets#dataset-types"" rel=""nofollow noreferrer"">here</a></p>
","12600227",3
1261,63771268,2,63770171,2020-09-07 04:17:19,2,"<p>I just did the following and wasn't able to reproduce your error:</p>
<ol>
<li>make a new compute instance</li>
<li>open it up using JupyterLab</li>
<li>open a new terminal</li>
<li><code>conda activate azureml_py36</code></li>
<li><code>conda install seaborn -y</code></li>
<li>open a new notebook and run <code>import seaborn as sns</code></li>
</ol>
<h3>Spitballing</h3>
<ol>
<li>Are you using the kernel, <code>Python 3.6 - AzureML</code> (i.e. the <code>azureml_py36</code> conda env)?</li>
<li>Have you tried restarting the kernel and/or creating a new compute instance?</li>
</ol>
<p><a href=""https://i.stack.imgur.com/kPbLH.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/kPbLH.png"" alt=""enter image description here"" /></a></p>
","3842610",1
1262,63769857,2,63766714,2020-09-06 23:48:42,8,"<h2>Short Answer</h2>
<h3>Option 1: create child runs within run</h3>
<p><code>run = Run.get_context()</code> assigns the run object of the run that you're currently in to <code>run</code>. So in every iteration of the hyperparameter search, you're logging to the same run. To solve this, you need to create child (or sub-) runs for each hyperparameter value. You can do this with <code>run.child_run()</code>. Below is the template for making this happen.</p>
<pre class=""lang-py prettyprint-override""><code>run = Run.get_context()

for hyperparam_alpha in alphas:
    # Get the experiment run context
    run_child = run.child_run()
    print(&quot;Started run: &quot;, run_child.id)
    run_child.log(&quot;train_split_size&quot;, x_train.size)
</code></pre>
<p>On the <code>diabetes-local-script-file</code> Experiment page, you can see that Run <code>9</code> was the parent run and Runs <code>10-19</code> were the child runs if you click &quot;Include child runs&quot; page. There is also a &quot;Child runs&quot; tab on Run 9 details page.</p>
<p><a href=""https://i.stack.imgur.com/5IMcz.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/5IMcz.png"" alt=""enter image description here"" /></a></p>
<h2>Long answer</h2>
<p>I highly recommend abstracting the hyperparameter search away from the data plane (i.e. <code>train.py</code>) and into the control plane (i.e. &quot;authoring code&quot;). This becomes especially valuable as training time increases and you can arbitrarily parallelize and also choose Hyperparameters more intelligently by using Azure ML's <a href=""https://learn.microsoft.com/en-us/azure/machine-learning/how-to-tune-hyperparameters"" rel=""noreferrer""><code>Hyperdrive</code></a>.</p>
<h3>Option 2 Create runs from control plane</h3>
<p>Remove the loop from your code, add the code like below (<a href=""https://gist.github.com/swanderz/f5c0dc1aefc796d37f6bc3600f2ae3cd"" rel=""noreferrer"">full data and control here</a>)</p>
<pre class=""lang-py prettyprint-override""><code>import argparse
from pprint import pprint

parser = argparse.ArgumentParser()
parser.add_argument('--alpha', type=float, default=0.5)
args = parser.parse_args()
print(&quot;all args:&quot;)
pprint(vars(args))

# use the variable like this
model = Ridge(args.alpha)
</code></pre>
<p>below is how to submit a single run using a script argument. To submit multiple runs, just use a loop in the control plane.</p>
<pre class=""lang-py prettyprint-override""><code>alphas = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0] # Define hyperparameters

list_rcs = [ScriptRunConfig(
    source_directory =  os.path.join(os.getcwd(), 'code'),
    script = 'train.py',
    arguments=['--alpha',a],
    run_config = run_local_config) for a in alphas]

list_runs = [exp.submit(rc) for rc in list_rcs]

</code></pre>
<h3>Option 3 Hyperdrive (IMHO the recommended approach)</h3>
<p>In this way you outsource the hyperparameter source to <code>Hyperdrive</code>. The UI will also report results exactly how you want them, and via the API you can easily download the best model.  Note you can't use this locally anymore and must use <code>AMLCompute</code>, but to me it is a worthwhile trade-off.<a href=""https://learn.microsoft.com/en-us/azure/machine-learning/how-to-tune-hyperparameters#configure-experiment"" rel=""noreferrer"">This is a great overview</a>. Excerpt below (<a href=""https://gist.github.com/swanderz/f5c0dc1aefc796d37f6bc3600f2ae3cd#file-hyperdrive-ipynb"" rel=""noreferrer"">full code here</a>)</p>
<pre class=""lang-py prettyprint-override""><code>param_sampling = GridParameterSampling( {
        &quot;alpha&quot;: choice(0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0)
    }
)

estimator = Estimator(
    source_directory =  os.path.join(os.getcwd(), 'code'),
    entry_script = 'train.py',
    compute_target=cpu_cluster,
    environment_definition=Environment.get(workspace=ws, name=&quot;AzureML-Tutorial&quot;)
)

hyperdrive_run_config = HyperDriveConfig(estimator=estimator,
                          hyperparameter_sampling=param_sampling, 
                          policy=None,
                          primary_metric_name=&quot;rmse&quot;, 
                          primary_metric_goal=PrimaryMetricGoal.MAXIMIZE,
                          max_total_runs=10,
                          max_concurrent_runs=4)

run = exp.submit(hyperdrive_run_config)
run.wait_for_completion(show_output=True)
</code></pre>
<p><a href=""https://i.stack.imgur.com/q1AhJ.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/q1AhJ.png"" alt=""enter image description here"" /></a></p>
","3842610",1
1263,63692153,2,63691515,2020-09-01 16:53:06,2,"<p>is the datastore behind vnet? where are you running the registration code above? On a compute instance behind the same vnet?
here is the doc that describe what you need to do to connect to data behind vnet:
<a href=""https://learn.microsoft.com/en-us/azure/machine-learning/how-to-enable-virtual-network#use-datastores-and-datasets"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/machine-learning/how-to-enable-virtual-network#use-datastores-and-datasets</a></p>
","12600227",1
1264,63680556,2,63642175,2020-09-01 02:38:29,1,"<p>You need to remove the trailing comma after  <code>ContentType='application/json',</code> and try below snippet for passing JSON to body field.</p>
<pre><code>import json 
json.dumps(request_body) 
test=json.dumps(request_body).encode()
</code></pre>
<p>This will also validate the JSON that you are passing.Now pass test to body for invoking the endopoint.</p>
","4326922",0
1265,63680081,2,63679503,2020-09-01 01:21:18,1,"<p>AWS has currently <a href=""https://status.aws.amazon.com/"" rel=""nofollow noreferrer"">issues</a> with Sagemaker:</p>
<blockquote>
<p>Increased Error Rates and Latencies for Multiple API operations</p>
</blockquote>
<blockquote>
<p>5:33 PM PDT We are investigating increased error rates and latencies for CreateTrainingJob, CreateHyperParameterTuningJob, and CreateEndpoint API operations in the US-EAST-1 Region. Previously created jobs and endpoints are unaffected.</p>
</blockquote>
<blockquote>
<p>6:04 PM PDT We are continuing to investigate increased error rates and latencies for CreateTrainingJob, CreateHyperParameterTuningJob, and CreateEndpoint API operations in the US-EAST-1 Region. Previously created jobs and endpoints are unaffected.</p>
</blockquote>
<p><a href=""https://i.stack.imgur.com/rQHQC.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/rQHQC.png"" alt=""enter image description here"" /></a></p>
","248823",0
1266,63662028,2,62453292,2020-08-30 20:51:14,1,"<p>In MXNet, we automatically test for this through examining the garbage collection records. You can find how it's implemented here: <a href=""https://github.com/apache/incubator-mxnet/blob/c3aff732371d6177e5d522c052fb7258978d8ce4/tests/python/conftest.py#L26-L79"" rel=""nofollow noreferrer"">https://github.com/apache/incubator-mxnet/blob/c3aff732371d6177e5d522c052fb7258978d8ce4/tests/python/conftest.py#L26-L79</a></p>
","3062895",0
1267,63652848,2,63571552,2020-08-30 00:01:57,0,"<p>Fixed this problem by adding my AMLS workspace to a 'storage blob data contributor' role in the AMLS default storage account. It seemly like usually this role is added by default, but it didn't happen in my case.</p>
","13562335",0
1268,63637671,2,63637178,2020-08-28 16:28:48,1,"<p>Referred to this <a href=""https://aws.amazon.com/blogs/big-data/install-python-libraries-on-a-running-cluster-with-emr-notebooks/"" rel=""nofollow noreferrer"">link</a> and updated the python version of spark context to python3. This fixes the issue:</p>
<pre><code>%%configure -f
{ &quot;conf&quot;:{
          &quot;spark.pyspark.python&quot;: &quot;python3&quot;,
          &quot;spark.pyspark.virtualenv.enabled&quot;: &quot;true&quot;,
          &quot;spark.pyspark.virtualenv.type&quot;:&quot;native&quot;,
          &quot;spark.pyspark.virtualenv.bin.path&quot;:&quot;/usr/bin/virtualenv&quot;
         }
}
</code></pre>
","5003459",0
1269,63636399,2,63617788,2020-08-28 15:05:14,0,"<p>The 54.06 is likely the average over time when GPU VM was allocated. If the VM gets deallocated, the Azure Monitor gets no data. These missing values get interpolated as zeros on the chart.</p>
<p>To get a better estimate of utilization, you could check when the VM was stopped, and exclude that time interval from the average.</p>
","5784983",0
1270,63628252,2,63600068,2020-08-28 05:59:03,1,"<p>Sorry, I was looking at a wrong namespace. I'm closing the question.</p>
","10690874",0
1271,63545267,2,62725467,2020-08-23 09:12:52,1,"<ol>
<li><p>Can I upload my existing local jupyter notebook in AWS Sagemaker directly? or
Yes you can upload from local disk or computer</p>
</li>
<li><p>Do I need to type the entire code again in instance of AWS Sagemaker jupyter notebook?
No</p>
</li>
</ol>
","11760970",0
1272,63459278,2,63458904,2020-08-17 22:08:41,1,"<p>One concept that took me a while to get was the bifurcation of registering and using an Azure ML <code>Environment</code>. If you have already registered your env, <code>myenv</code>, and none of the details of the your environment have changed, there is no need re-register it with <code>myenv.register()</code>. You can simply get the already register env using <a href=""https://learn.microsoft.com/en-us/python/api/azureml-core/azureml.core.environment.environment?view=azure-ml-py#get-workspace--name--version-none-"" rel=""nofollow noreferrer""><code>Environment.get()</code></a> like so:</p>
<pre class=""lang-py prettyprint-override""><code>myenv = Environment.get(ws, name='myenv', version=11)
</code></pre>
<p>My recommendation would be to name your environment something new: like <code>&quot;model_scoring_env&quot;</code>. Register it once, then pass it to the <code>InferenceConfig</code>.</p>
","3842610",9
1273,63350094,2,63350039,2020-08-11 01:07:54,2,"<p>You need to shut the instance down, then you can edit it. Then, if you use your eyes (which I neglected to do) you can see the &quot;Additional Configurations&quot; section contains lifecycle configurations</p>
","10818367",0
1274,63339006,2,63338577,2020-08-10 11:09:52,0,"<p>I was actually using AWS SageMaker, and restarting the kernel from the toolbar was not enough. I needed to restart the kernel session, by pressing 'shut down' in the &quot;Running terminals and kernels&quot; section on the left navigation.</p>
<p><a href=""https://i.stack.imgur.com/934Fe.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/934Fe.png"" alt=""Shut down button seen on left navigation of JupterLab"" /></a></p>
<p>They are currently <a href=""https://github.com/jupyterlab/jupyterlab/issues/5989"" rel=""nofollow noreferrer"">discussing</a> warning users about the need to restart the kernel when a notebook is moved.</p>
","7365866",0
1275,63172707,2,63170715,2020-07-30 12:00:16,0,"<p>Solved by deleting the vdi file (minikf-user-data.vdi) and running these commands again:</p>
<pre><code>vagrant init arrikto/minikf
vargant up
</code></pre>
","501560",0
1276,63081451,2,63024900,2020-07-24 21:06:01,2,"<p>I recommend <code>AWS Step Functions</code>.  Been using it to schedule <code>SageMaker Batch Transform</code> and preprocessing jobs since it integrates with <code>CloudWatch</code> event rules.  It can also train models, perform hpo tuning, and integrates with <code>lambda</code>.  There is a SageMaker/Step Functions SDK as well as you can use Step Functions directly by creating state machines. Some examples and documentation:</p>
<p><a href=""https://aws.amazon.com/about-aws/whats-new/2019/11/introducing-aws-step-functions-data-science-sdk-amazon-sagemaker/"" rel=""nofollow noreferrer"">https://aws.amazon.com/about-aws/whats-new/2019/11/introducing-aws-step-functions-data-science-sdk-amazon-sagemaker/</a></p>
<p><a href=""https://docs.aws.amazon.com/step-functions/latest/dg/connect-sagemaker.html"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/step-functions/latest/dg/connect-sagemaker.html</a></p>
","11286588",1
1277,63063386,2,63035151,2020-07-23 21:28:32,2,"<p>You should use one of on-demand ML hosting instances supported as detailed at <a href=""https://aws.amazon.com/sagemaker/pricing/instance-types/"" rel=""nofollow noreferrer"">this link</a>. I think non-valid <code>instance_type='a1.small'</code> is replaced by a valid one (ml.m5.2xlarge), and that is not in your AWS service quota. The weird part is that seeing <code>instance_type='a1.small'</code> was generated by SageMaker Autopilot.</p>
","8386985",0
1278,63063217,2,61630914,2020-07-23 21:14:30,1,"<p>If anyone faces the same issue, open port 6443 from master to nodes.
I had to check out our very restricted network and find out.Creating a firewall rule for that port will fix it.</p>
","13444168",0
1279,63047205,2,62664183,2020-07-23 05:03:25,3,"<pre><code>import mlflow 
client = mlflow.tracking.MlflowClient()
runs = client.search_runs(&quot;my_experiment_id&quot;, &quot;&quot;, order_by=[&quot;metrics.rmse DESC&quot;], max_results=1)
best_run = runs[0]
</code></pre>
<p><a href=""https://mlflow.org/docs/latest/python_api/mlflow.tracking.html#mlflow.tracking.MlflowClient.search_runs"" rel=""nofollow noreferrer"">https://mlflow.org/docs/latest/python_api/mlflow.tracking.html#mlflow.tracking.MlflowClient.search_runs</a></p>
","13980177",0
1280,63045544,2,63045395,2020-07-23 01:35:57,1,"<p>Totally empathize with your confusion. Our team has been working with Azure ML pipelines for quite some time but <code>PublishedPipelines</code> still confused me initially because:</p>
<ul>
<li>what the SDK calls a <code>PublishedPipeline</code> is called as a <code>Pipeline Endpoint</code> in the Studio UI, and</li>
<li>it is semi-related to <code>Dataset</code> and <code>Model</code>'s <code>.register()</code> method, but fundamentally different.</li>
</ul>
<p><code>TL;DR</code>: all <code>Pipeline.publish()</code> does is create an endpoint that you can use to:</p>
<ul>
<li><a href=""https://github.com/Azure/MachineLearningNotebooks/blob/master/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-setup-schedule-for-a-published-pipeline.ipynb"" rel=""nofollow noreferrer"">schedule</a> and <a href=""https://github.com/Azure/MachineLearningNotebooks/blob/master/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-setup-versioned-pipeline-endpoints.ipynb"" rel=""nofollow noreferrer"">version</a> Pipelines, and</li>
<li>re-run the pipeline from other services via a REST API call (e.g. <a href=""https://learn.microsoft.com/en-us/azure/data-factory/transform-data-machine-learning-service"" rel=""nofollow noreferrer"">via Azure Data Factory</a>).</li>
</ul>
<p>You can see <code>PublishedPipelines</code> in the Studio UI in two places:</p>
<ul>
<li>Pipelines page :: Pipeline Endpoints tab</li>
<li>Endpoints page :: Pipeline Endpoints tab</li>
</ul>
<p><a href=""https://i.stack.imgur.com/UQ6RS.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/UQ6RS.png"" alt=""enter image description here"" /></a></p>
","3842610",0
1281,67073907,2,67064180,2021-04-13 11:21:42,0,"<p>From what I can see, setting pod resource requests and limits for each task in the kubeflow pipeline is not possible with Kale. The best alternative is to set <a href=""https://kubernetes.io/docs/tasks/administer-cluster/manage-resources/memory-default-namespace/"" rel=""nofollow noreferrer"">default resources for the namespace</a>, which will set the pod requests and limits for all tasks in the kubeflow pipeline.</p>
","4237005",0
1282,67056852,2,67052295,2021-04-12 10:33:23,1,"<p>When your notebook stops the run gets the status finished. However, if you want to continue logging metrics or artifacts to that run, you just need to use <code>mlflow.start_run(run_id=&quot;YourRunIDYouCanGetItFromUI&quot;)</code>. This is explained in the <a href=""https://www.mlflow.org/docs/latest/python_api/mlflow.html#mlflow.start_run"" rel=""nofollow noreferrer"">documentation</a></p>
","9593712",0
1283,67051991,2,67021176,2021-04-12 02:34:47,1,"<p>From <a href=""https://learn.microsoft.com/en-us/azure/machine-learning/how-to-manage-workspace?tabs=python#limitations"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/machine-learning/how-to-manage-workspace?tabs=python#limitations</a></p>
<blockquote>
<p>If you want to use <strong>existing services from a different Azure
subscription</strong> than the workspace, you must register the Azure Machine
Learning namespace in the subscription that contains those services.</p>
</blockquote>
<p>So, in order to use the ACR in that different subscription, you need to register resource provider <code>Microsoft.MachineLearningServices</code> in that subscription contains ACR. For information on how to see if it is registered and how to register it, see the <a href=""https://learn.microsoft.com/en-us/azure/azure-resource-manager/management/resource-providers-and-types"" rel=""nofollow noreferrer"">Azure resource providers and types</a> article.</p>
<p>To register a resource provider, use:</p>
<pre><code>Register-AzResourceProvider -ProviderNamespace Microsoft.MachineLearningServices
</code></pre>
<p>To see information for a particular resource provider, use:</p>
<pre><code>Get-AzResourceProvider -ProviderNamespace Microsoft.MachineLearningServices
</code></pre>
","9833314",0
1284,67050334,2,67015185,2021-04-11 21:26:49,0,"<p>My Microsoft contact says -</p>
<p>&quot;For this, their best  bet is probably to see what the training env was pinned to and install those same pins. They can get that env by running child_run.get_environment() and then pip install all the pkgs listed in there with the pins listed there.&quot;</p>
<p>A useful code snippet.</p>
<pre><code>for run in experiment.get_runs():
    tags_dictionary = run.get_tags()
    best_run = AutoMLRun(experiment, tags_dictionary['automl_best_child_run_id'])
    env = best_run.get_environment()
    print(env.python.conda_dependencies.serialize_to_string())
</code></pre>
","1107074",0
1285,67042281,2,66814885,2021-04-11 07:04:00,1,"<p>I think that you should be able to do that by implementing the custom python model or custom flavor, as it's described in the <a href=""https://mlflow.org/docs/latest/models.html#model-customization"" rel=""nofollow noreferrer"">documentation</a>.  In this case you need to create a class that is inherited from <code>mlflow.pyfunc.PythonModel</code>, and implement the <code>predict</code> method, and inside that method you're free to do anything.  Here is just simple example from documentation:</p>
<pre class=""lang-py prettyprint-override""><code>class AddN(mlflow.pyfunc.PythonModel):

    def __init__(self, n):
        self.n = n

    def predict(self, context, model_input):
        return model_input.apply(lambda column: column + self.n)
</code></pre>
<p>and this model is then could be saved &amp; loaded again just as normal models:</p>
<pre class=""lang-py prettyprint-override""><code># Construct and save the model
model_path = &quot;add_n_model&quot;
add5_model = AddN(n=5)
mlflow.pyfunc.save_model(path=model_path, python_model=add5_model)

# Load the model in `python_function` format
loaded_model = mlflow.pyfunc.load_model(model_path)
</code></pre>
","18627",3
1286,67033636,2,66950948,2021-04-10 11:05:04,0,"<p>I was missing one thing which was causing this error. After receiving the image data I used python list and then <code>json.dump</code> that list (of lists). Below is the code for reference.</p>
<pre><code>import os
import sys

CWD = os.path.dirname(os.path.realpath(__file__))
sys.path.insert(0, os.path.join(CWD, &quot;lib&quot;))

import json
import base64
import boto3
import numpy as np
import io
from io import BytesIO
from skimage.io import imread
from skimage.transform import resize

# grab environment variable of Lambda Function
ENDPOINT_NAME = os.environ['ENDPOINT_NAME']
runtime= boto3.client('runtime.sagemaker')

def lambda_handler(event, context):
    s3 = boto3.client(&quot;s3&quot;)
    
    # retrieving data from event.
    get_file_content_from_postman = event[&quot;content&quot;]
    
    # decoding data.
    decoded_file_name = base64.b64decode(get_file_content_from_postman)
    
    data = np.array([resize(imread(io.BytesIO(decoded_file_name)), (137, 310, 3))])
    
    payload = json.dumps(data.tolist())
    
    response = runtime.invoke_endpoint(EndpointName=ENDPOINT_NAME, ContentType='application/json', Body=payload)
        
    result = json.loads(response['Body'].read().decode())
    
    return result
        
</code></pre>
","9920934",0
1287,67006317,2,66884848,2021-04-08 14:39:45,1,"<p>Turns out it can be done through minio UI. To access minio remotly use configured <code>kubectl</code>:</p>
<p><code>kubectl port-forward -n kubeflow svc/minio-service 9000:9000</code></p>
<p>And then in web browser go to <code>localhost:9000</code>.</p>
<p>Also each bucket can be assigned a lifecycle rule, which will give objects added under some prefix expiration date
<a href=""https://docs.min.io/docs/python-client-api-reference.html"" rel=""nofollow noreferrer"">https://docs.min.io/docs/python-client-api-reference.html</a>.</p>
","10874583",2
1288,66932914,2,66915920,2021-04-03 15:36:02,2,"<p>Using AWS Sagemaker you don't need to worry about the GPU, you simply select an instance type with GPU ans Sagemaker will use it. Specifically <code>ml.t2.medium</code> doesn't have a GPU but it's anyway not the right way to train a model.
Basically you have 2 canonical ways to use Sagemaker (look at the documentation and examples please), the first is to use a notebook with a limited computing resource to spin up a training job using a prebuilt image, in that case when you call the estimator you simply specify what <a href=""https://aws.amazon.com/ec2/instance-types/"" rel=""nofollow noreferrer"">instance type</a> you want (you'll choose one with GPU, looking at the costs). The second way is to use your own container, push it to ECR and launch a training job from the console, where you specify the instance type.</p>
","4267439",2
1289,66915402,2,66745404,2021-04-02 06:43:35,0,"<p>To consume this from the AZ ML CLI we use the following syntax:</p>
<pre><code>    curl -X POST [Pipeline_REST_Endpoint] -H &quot;Authorization: Bearer $(az account get-access-token --query accessToken -o tsv)&quot; -H &quot;Content-Type: application/json&quot; --data-binary @- &lt;&lt;DATA
{&quot;ExperimentName&quot;: &quot;[ExperimentName]&quot;,
                               &quot;RunSource&quot;: &quot;SDK&quot;,
                               &quot;DataSetDefinitionValueAssignments&quot;: {&quot;tabular_ds_param&quot;: 
                                                                     {&quot;SavedDataSetReference&quot;: 
                                                                      {&quot;Id&quot;:&quot;[Dataset_ID]&quot;}
                                                                     }
                                                                    }
                              }
DATA
</code></pre>
<p>We use the simple REST call because <code>az ml run submit-pipeline</code> does not have the dataset parameter and datapath does not achieve the desired result.</p>
","14384792",0
1290,66915380,2,66711458,2021-04-02 06:41:28,0,"<p>For the inputs we create Dataset class instances:</p>
<pre><code>tabular_ds1 = Dataset.Tabular.from_delimited_files('some_link')
tabular_ds2 = Dataset.Tabular.from_delimited_files('some_link')
</code></pre>
<p>ParallelRunStep produces an output file, we use the PipelineData class to create a folder which will store this output:</p>
<pre><code>from azureml.pipeline.core import Pipeline, PipelineData

output_dir = PipelineData(name=&quot;inferences&quot;, datastore=def_data_store)
</code></pre>
<p>The ParallelRunStep depends on ParallelRunConfig Class to include details about the environment, entry script, output file name and other necessary definitions:</p>
<pre><code>from azureml.pipeline.core import PipelineParameter
from azureml.pipeline.steps import ParallelRunStep, ParallelRunConfig

parallel_run_config = ParallelRunConfig(
    source_directory=scripts_folder,
    entry_script=script_file,
    mini_batch_size=PipelineParameter(name=&quot;batch_size_param&quot;, default_value=&quot;5&quot;),
    error_threshold=10,
    output_action=&quot;append_row&quot;,
    append_row_file_name=&quot;mnist_outputs.txt&quot;,
    environment=batch_env,
    compute_target=compute_target,
    process_count_per_node=PipelineParameter(name=&quot;process_count_param&quot;, default_value=2),
    node_count=2
)
</code></pre>
<p>The input to ParallelRunStep is created using the following code</p>
<pre><code>tabular_pipeline_param = PipelineParameter(name=&quot;tabular_ds_param&quot;, default_value=tabular_ds1)
tabular_ds_consumption = DatasetConsumptionConfig(&quot;tabular_dataset&quot;, tabular_pipeline_param)
</code></pre>
<p>The PipelineParameter helps us run the pipeline for different datasets.
ParallelRunStep consumes this as an input:</p>
<pre><code>parallelrun_step = ParallelRunStep(
    name=&quot;some-name&quot;,
    parallel_run_config=parallel_run_config,
    inputs=[ tabular_ds_consumption ],
    output=output_dir,
    allow_reuse=False
)
</code></pre>
<p>To consume with another dataset:</p>
<pre><code>pipeline_run_2 = experiment.submit(pipeline, 
                                   pipeline_parameters={&quot;tabular_ds_param&quot;: tabular_ds2}
)
</code></pre>
<p>There is an error currently: <a href=""https://github.com/Azure/MachineLearningNotebooks/issues/1312"" rel=""nofollow noreferrer"">DatasetConsumptionConfig and PipelineParameter cannot be reused</a></p>
","14384792",0
1291,66915200,2,66888622,2021-04-02 06:19:10,0,"<p>To raise exceptions to let the end-user get proper feedback if their API call is unsuccessful, we use the <a href=""https://learn.microsoft.com/en-us/python/api/azureml-contrib-services/azureml.contrib.services.aml_response.amlresponse?view=azure-ml-py"" rel=""nofollow noreferrer""><code>azureml.contrib.services.aml_response.AMLResponse</code> Class</a>.</p>
<p>Example of use in <code>score.py</code>:</p>
<pre><code>if [some-condition]:    
    return AMLResponse(&quot;bad request&quot;, 500)
</code></pre>
<p>Documentation Link can be found <a href=""https://learn.microsoft.com/en-us/azure/machine-learning/how-to-deploy-advanced-entry-script#binary-data"" rel=""nofollow noreferrer"">here</a>.</p>
","14384792",0
1292,66851021,2,65884046,2021-03-29 08:35:16,0,"<p>After a lot of experiments I can answer myself that Sagemaker generates multiple events with the same payload, except for the field <code>LastModifiedTime</code>. I don't know is this is a bug, but should not happen in my opinion. These are rules defined by AWS itself, so nothing I can customize. The situation is even worse if you enable the profiler.
There is nothing I can do, since I already posted on the official AWS forum multiple times without any luck.</p>
","4267439",0
1293,66827617,2,66801546,2021-03-27 02:56:54,0,"<p>Sorry for your loss! First, I'd make absolutely sure that you can't recover the deleted workspace. Definitely worthwhile to open an priority support ticket with Azure.</p>
<p>Another thing you might try is:</p>
<ol>
<li>create a new workspace (which will create a new storage account for you for the new workspace's logs)</li>
<li>copy your old workspace's data into the new workspace's storage account.</li>
</ol>
","3842610",2
1294,66819462,2,66819026,2021-03-26 15:05:38,1,"<p>If you are not using a built-in algorithm, like in the example you linked, you have to define your metrics when you create the training job. You have to define regex expressions to grab from the logs the metric values, then cloudwatch will plot for you. The x axis will be the timestamp, you cannot change it.
Basically just run your traning job and observe how the metrics are outputted, then you can build the appropriate regex. For example, since I am using coco metrics in tensorflow which periodically produce this:</p>
<pre><code>INFO:tensorflow:Saving dict for global step 1109: DetectionBoxes_Precision/mAP = 0.111895345, DetectionBoxes_Precision/mAP (large) = 0.12102994, DetectionBoxes_Precision/mAP (medium) = 0.050807837, DetectionBoxes_Precision/mAP (small) = -1.0, DetectionBoxes_Precision/mAP@.50IOU = 0.33130914, DetectionBoxes_Precision/mAP@.75IOU = 0.03787096, DetectionBoxes_Recall/AR@1 = 0.18493989, DetectionBoxes_Recall/AR@10 = 0.36792925, DetectionBoxes_Recall/AR@100 = 0.48543888, DetectionBoxes_Recall/AR@100 (large) = 0.5131599, DetectionBoxes_Recall/AR@100 (medium) = 0.21598063, DetectionBoxes_Recall/AR@100 (small) = -1.0, Loss/classification_loss = 0.8041124, Loss/localization_loss = 0.35313264, Loss/regularization_loss = 0.15211834, Loss/total_loss = 1.30936, global_step = 1109, learning_rate = 0.28119853, loss = 1.30936
</code></pre>
<p>I use to grab the total loss for example:</p>
<pre><code>INFO.*Loss\/total_loss = ([0-9\.]+) 
</code></pre>
<p>That's it, cloudwatch automatically plot the total_loss in time.</p>
<p>You can define metrics either in the console or in the notebook, like this (just an example from my code):</p>
<pre><code>metrics = [{'Name': 'Loss', 'Regex': 'loss: ([0-9\.]+)'},
           {'Name': 'Accuracy', 'Regex': 'acc: ([0-9\.]+)'},
           {'Name': 'Epoch', 'Regex': 'Epoch ([0-9\.]+)'},
           {'Name': 'Validation_Acc', 'Regex': 'val_acc: ([0-9\.]+)'},
           {'Name': 'Validation_Loss', 'Regex': 'val_loss: ([0-9\.]+)'}]

tf_estimator = TensorFlow(entry_point='training.py', 
                          role=get_execution_role(),
                          train_instance_count=1, 
                          train_instance_type='ml.p2.xlarge',
                          train_max_run=172800,
                          output_path=s3_output_location,
                          framework_version='1.12',
                          py_version='py3',
                          metric_definitions = metrics,
                          hyperparameters = hyperparameters)
</code></pre>
<p>In order to test your regex, you can use a tool like <a href=""https://regex101.com/"" rel=""nofollow noreferrer"">this</a></p>
","4267439",0
1295,66802258,2,66142193,2021-03-25 15:04:37,1,"<p>For future readers I can confirm that is something that can happen very rarely (I' haven't experienced it anymore since then), but it's AWS fault. Same data, same algorithm.</p>
","4267439",0
1296,66802131,2,66793845,2021-03-25 14:57:52,1,"<p>Sagemaker copies the input data you specify in <code>s3train</code> into the instance in <code>/opt/ml/input/data/train/</code> and that's why you have an error, because as you can see from the error message is trying to concatenate the filename in the <code>lst</code> file with the path where it expect the image to be. So just put only the filenames in your <code>lst</code>and should be fine (remove the s3 path).</p>
","4267439",5
1297,67491187,2,67488064,2021-05-11 16:55:59,2,"<p>So I tried the same in Python and had a similar error and came across this:</p>
<p><a href=""https://github.com/Azure/azure-sdk-for-python/issues/16035"" rel=""nofollow noreferrer"">https://github.com/Azure/azure-sdk-for-python/issues/16035</a></p>
<p>Downgrading:</p>
<pre><code> PyJWT 
</code></pre>
<p>helped. The bizarre world of open source and its web of interdependencies!</p>
","283538",1
1298,67499381,2,67499339,2021-05-12 07:51:54,1,"<p>Run <code>mlflow.end_run()</code> to repair the active run. Then you can create new ones.</p>
<hr />
<p>Note: the problematic run should have <code>Lifecycle Stage: active</code> (rather than deleted) displayed in MLflow web UI, so if you deleted it when trying to solve the problem, it needs to be first restored / undeleted (it can be found in the &quot;trash&quot; using MLFlow web UI thus: Filter &gt; State &gt; Deleted).</p>
","15775425",0
1299,67503849,2,67498965,2021-05-12 12:37:22,1,"<p>Yep you're right -- create a <code>ComputeTarget</code> with a minimum of zero nodes. The <a href=""https://azure.microsoft.com/en-us/pricing/details/container-registry/#pricing"" rel=""nofollow noreferrer"">container registry costs</a> are ~$0.16 USD/day and, IIRC, that cost is bundled in with Azure Machine learning.</p>
<p>This is what our team does for our published pipelines in production.</p>
","3842610",0
1300,67535822,2,67533091,2021-05-14 14:13:53,1,"<p>On ml.azure.com, there is a &quot;Models&quot; option on the left-hand blade.</p>
<p><a href=""https://i.stack.imgur.com/Y7cZe.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Y7cZe.png"" alt=""UI Sidebar"" /></a></p>
","3842610",3
1301,67537139,2,67536581,2021-05-14 15:45:47,1,"<p>conda is actually not just a Python thing, you might be thinking of pip?</p>
<p>Conda is a package &amp; environment manager for nearly any kind of package, provided that it has been uploaded to anaconda. So you <em>can</em> use anaconda (and conda environment files) for R projects.</p>
<p>The trouble is that the <code>azuremlsdk</code> CRAN package is not hosted as an anaconda package, but is probably needed for the scoring service. Worth using a file like below to see what it works.</p>
<p>If it doesn't work, then I agree that this UI needs to generalized to better support R model deployment scenarios.</p>
<p>It is also possible to add the <code>azuremlsdk</code> CRAN package to anaconda, but that requires <a href=""https://stackoverflow.com/a/36653411/3842610"">some extra work</a>, but ideally you shouldn't have to require this much manual effort.</p>
<code>environment.yml</code>
<p>Here's an example conda dependencies file for R.</p>
<pre class=""lang-yaml prettyprint-override""><code>name: scoring_environment
channels:
  - defaults
dependencies:
  - r-base=3.6.1
  - r-essentials=3.6.0
  # whatever other dependencies you have
  - r-tidyverse=1.2.1
  - r-caret=6.0_83
</code></pre>
","3842610",3
1302,67537231,2,67535014,2021-05-14 15:53:45,1,"<p>Great to see people putting the R SDK through it's paces!</p>
<p>The vignette you're using is obviously a great way to get started. It seems you're almost all the way through without a hitch.</p>
<p>Deployment is always tricky, and I'm not expert myself. I'd point you to this <a href=""https://learn.microsoft.com/en-us/azure/machine-learning/how-to-troubleshoot-deployment-local?WT.mc_id=AI-MVP-5003930"" rel=""nofollow noreferrer"">guide on troubleshooting deployment locally</a>. Similar functionality exists for the R SDK, namely: <a href=""https://azure.github.io/azureml-sdk-for-r/reference/local_webservice_deployment_config.html"" rel=""nofollow noreferrer""><code>local_webservice_deployment_config()</code></a>.</p>
<p>So I think you change your example to this:</p>
<pre class=""lang-r prettyprint-override""><code>deployment_config &lt;- local_webservice_deployment_config(port = 8890)
</code></pre>
<p>Once you know the service is working locally, the issue you're having with the ACI webservice becomes a lot easier to narrow down.</p>
","3842610",8
1303,67577979,2,67486160,2021-05-17 23:00:23,2,"<p>The short answer is it depends, but a more nuance answer is depends what you want to do with the component.</p>
<p>As base knowledge, when a KFP pipeline is compiled, it's actually a series of different YAMLs that are launched by Argo Workflows. All of these needs to be container based to run on Kubernetes, even if the container itself has all python.</p>
<p>A function to Python Container Op is a quick way to get started with Kubeflow Pipelines. It was designed to model after Airflow's python-native DSL. It will take your python function and run it within a defined Python container. You're right it's easier to encapsulate all your work within the same Git folder. This set up is great for teams just getting started with KFP and don't mind some boilerplate to get going quickly.</p>
<p>Components really become powerful when your team needs to share work, or you have an enterprise ML platform that is creating template logic of how to run specific jobs in a pipeline. The components can be separately versioned and built to use on any of your clusters in the same way (underlying container should be stored in docker hub or ECR, if you're on AWS). There are inputs/outputs to prescribe how the run will execute using the component. You can imagine a team in Uber might use a KFP to pull data for number of drivers in a certain zone. The inputs to the component could be Geo coordinate box and also time of day of when to load the data. The component saves the data to S3, which then is loaded to your model for training. Without the component, there would be quite a bit of boiler plate that would need to copy the code across multiple pipelines and users.</p>
<p>I'm a former PM at AWS for SageMaker and open source ML integrations, and this is sharing from my experience looking at enterprise set ups.</p>
","2044299",0
1304,67616896,2,63473716,2021-05-20 08:41:29,2,"<p>You can use <code>{{workflow.annotations.pipelines.kubeflow.org/run_name}}</code> argo variable to get the run_name</p>
<p>For example,</p>
<pre class=""lang-py prettyprint-override""><code>@func_to_container_op
def dummy(run_id, run_name) -&gt; str:
    return run_id, run_name

@dsl.pipeline(
    name='test_pipeline',
)
def test_pipeline():
  dummy('{{workflow.labels.pipeline/runid}}', '{{workflow.annotations.pipelines.kubeflow.org/run_name}}')

</code></pre>
<p>You will find that the placeholders will be replaced with the correct run_id and run_name.</p>
","3523881",1
1305,67619153,2,66325857,2021-05-20 11:05:59,1,"<p>You can pass the source_dir to Hyperparameters like this:</p>
<pre><code>    response = sm_boto3.create_training_job(
        TrainingJobName=f&quot;{your job name}&quot;),
        HyperParameters={
            'model-name': 'my-model-name',
            'epochs': 1000,
            'batch-size': 64,
            'learning-rate': 0.01,
            'training-split': 0.80,
            'patience': 50,
            &quot;sagemaker_program&quot;: &quot;script.py&quot;, # this is where you specify your train script
            &quot;sagemaker_submit_directory&quot;: &quot;s3://&quot; + bucket + &quot;/&quot; + project + &quot;/&quot; + source, # your s3 URI like s3://sm/tensorflow/source/sourcedir.tar.gz
        },
        AlgorithmSpecification={
            &quot;TrainingImage&quot;: training_image,
            ...
        }, 
</code></pre>
<p>Note: make sure it's xxx.tar.gz otherwise. Otherwise Sagemaker will throw errors.</p>
<p>Refer to <a href=""https://github.com/aws/amazon-sagemaker-examples/blob/master/sagemaker-python-sdk/scikit_learn_randomforest/Sklearn_on_SageMaker_end2end.ipynb"" rel=""nofollow noreferrer"">https://github.com/aws/amazon-sagemaker-examples/blob/master/sagemaker-python-sdk/scikit_learn_randomforest/Sklearn_on_SageMaker_end2end.ipynb</a></p>
","15967118",0
1306,67621869,2,67570694,2021-05-20 13:58:49,1,"<p>SOLUTION :</p>
<p>i edit the /etc/docker/daemon.json with content:</p>
<pre><code>{
&quot;runtimes&quot;: {
    &quot;nvidia&quot;: {
        &quot;path&quot;: &quot;/usr/bin/nvidia-container-runtime&quot;,
        &quot;runtimeArgs&quot;: []
     } 
},
&quot;default-runtime&quot;: &quot;nvidia&quot; 
}
</code></pre>
<p>Then i Restart docker daemon:</p>
<pre><code>sudo system restart docker
</code></pre>
<p>it solved my problem.</p>
","13334199",0
1307,67625049,2,67622586,2021-05-20 17:12:08,1,"<p>Try this :</p>
<pre><code>from azureml.core.authentication import ServicePrincipalAuthentication
import requests,json

tenantId = '&lt;tenant id&gt;'

query_SP_object_id = '&lt;object ID of SP you want to query&gt;'

x=ServicePrincipalAuthentication(tenant_id= tenantId , service_principal_id='&lt;sp id&gt;', service_principal_password='&lt;sp secret&gt;')

reqURL = 'https://graph.windows.net/'+tenantId +'/applications/'+ query_SP_object_id +'/passwordCredentials?api-version=1.6'
result = requests.get(reqURL,headers={&quot;Authorization&quot;:'Bearer ' + x._get_graph_token()}).text

print(json.loads(result)['value'])
</code></pre>
<p>Result:
<a href=""https://i.stack.imgur.com/JxGFv.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/JxGFv.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.stack.imgur.com/zC8Ik.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/zC8Ik.png"" alt=""enter image description here"" /></a></p>
<p>Pls note in this case, we use sp object ID:</p>
<p><a href=""https://i.stack.imgur.com/22lrK.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/22lrK.png"" alt=""enter image description here"" /></a></p>
","10189459",1
1308,67636633,2,67635688,2021-05-21 12:14:45,9,"<p><strong>Install</strong></p>
<p>I usually use it as a Python package, in this case you need to install:</p>
<pre class=""lang-bash prettyprint-override""><code>pip install &quot;dvc[s3]&quot;
</code></pre>
<p><strong>Setup remote</strong></p>
<p>By default DVC supports AWS S3 storage and it works fine.<br />
It also supports &quot;S3-compatible storage&quot;, and MinIO in particular. In this case you have a <strong>bucket</strong> - a directory on a MinIO server where actual data is stored (it is similar to an AWS bucket). DVC uses AWS CLI to authenticate with AWS and in case of MinIO you need to pass credentials to <code>dvc</code> (not to the <code>minio</code> package).</p>
<p>The commands to setup MinIO as your DVC remote:</p>
<pre class=""lang-bash prettyprint-override""><code># setup default remote (change &quot;bucket-name&quot; to your minio backet name)
dvc remote add -d minio s3://bucket-name -f

# add information about storage url (where &quot;https://minio.mysite.com&quot; is your MinIO url)
dvc remote modify minio endpointurl https://minio.mysite.com

#  add MinIO credentials (e.g. from env. variables)
dvc remote modify minio access_key_id my_login
dvc remote modify minio secret_access_key my_password
</code></pre>
<p><strong>If you move from old remote</strong>, use the following commands to move your data:</p>
<p>Before setup (download old remote cache to the local machine - note it may take a long time):</p>
<pre class=""lang-bash prettyprint-override""><code>dvc pull -r &lt;old_remote_name&gt; --all-commits --all-tags --all-branches
</code></pre>
<p>After setup (upload all local cache data to a new remote):</p>
<pre class=""lang-bash prettyprint-override""><code>dvc push -r &lt;new_remote_name&gt; --all-commits --all-tags --all-branches
</code></pre>
","5256923",0
1309,67640789,2,67639665,2021-05-21 16:37:34,3,"<h2>short answer</h2>
<p>Totally been in your shoes before. This code sample seems a smidge out of date. Using <a href=""https://github.com/Azure/MachineLearningNotebooks/blob/master/how-to-use-azureml/work-with-data/datasets-tutorial/train-with-datasets/train-with-datasets.ipynb"" rel=""nofollow noreferrer"">this notebook</a> as a reference, can you try the following?</p>
<pre class=""lang-py prettyprint-override""><code>packages = CondaDependencies.create(
    pip_packages=['azureml-defaults','scikit-learn']
)
</code></pre>
<h2>longer  answer</h2>
<p><a href=""https://www.anaconda.com/blog/using-pip-in-a-conda-environment"" rel=""nofollow noreferrer"">Using pip with Conda</a> is not always smooth sailing. In this instance, conda isn't reporting up the issue that pip is having. The solution is to create and test this environment locally where we can get more information, which will at least will give you a more informative error message.</p>
<ol>
<li>Install anaconda  or miniconda (or use an Azure ML Compute Instance which has conda pre-installed)</li>
<li>Make a  file called environment.yml that looks like this</li>
</ol>
<pre class=""lang-yaml prettyprint-override""><code>name: aml_env
dependencies:
 - python=3.8
 - pip=21.0.1
 - pip:
    - azureml-defaults
    - azureml-dataprep[pandas]
    - scikit-learn==0.24.1
</code></pre>
<ol start=""3"">
<li>Create this environment with the command <code>conda env create -f environment.yml</code>.</li>
<li>respond to any discovered error message</li>
<li>If there' no error, use this new <code>environment.yml</code> with Azure ML like so</li>
</ol>
<pre class=""lang-py prettyprint-override""><code>sklearn_env = Environment.from_conda_specification(name = 'sklearn-env', file_path = './environment.yml')
</code></pre>
<h2>more context</h2>
<p>the error I'm guessing that's happening is when you reference a pip requirements file from a conda environment file. In this scenario, conda calls <code>pip install -r  requirements.txt</code> and if that command errors out, conda can't report the error.</p>
<h3><code>requirements.txt</code></h3>
<pre><code>scikit-learn==0.24.1
azureml-dataprep[pandas]
</code></pre>
<h3><code>environment.yml</code></h3>
<pre><code>name: aml_env
dependencies:
 - python=3.8
 - pip=21.0.1
 - pip:
    - -rrequirements.txt
</code></pre>
","3842610",6
1310,67683192,2,67505781,2021-05-25 07:03:45,0,"<p>For the benefit of community providing solution in answer section</p>
<blockquote>
<p>The problem was with the <code>TensorflowModel</code> framework version argument. After
changing the <code>framework_version</code> to <code>1.12</code> and installed version <code>TF 1.12</code> in
the <code>Sagemaker Jupyter</code> instance and retrained model locally using <code>TF 1.12</code> got same results. (paraphrased from Peter Van Katwyk)</p>
</blockquote>
","user11530462",0
1311,67684496,2,67677780,2021-05-25 08:37:55,1,"<p>It's not supported functionality according to <a href=""https://mlflow.org/docs/latest/cli.html#mlflow-run"" rel=""nofollow noreferrer"">documentation</a>, and <a href=""https://github.com/mlflow/mlflow/blob/master/mlflow/cli.py#L124"" rel=""nofollow noreferrer"">source code</a>, so you'll need to add your own wrapper to read parameters from file &amp; pass them explicitly.</p>
","18627",0
1312,67698871,2,67689868,2021-05-26 05:14:24,2,"<p>we don’t have a good way to extend the handling for generic Python class objects. However, we are planning to add support for that, basically by providing more information on the necessary hooks, and allowing users to extend a base class to implement the hook to match the desired class structure.
These types are currently supported:</p>
<p>pandas
numpy
pyspark
Standard Python object</p>
<p><a href=""https://learn.microsoft.com/en-us/azure/machine-learning/how-to-deploy-advanced-entry-script#automatically-generate-a-swagger-schema"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/machine-learning/how-to-deploy-advanced-entry-script#automatically-generate-a-swagger-schema</a></p>
","11297406",0
1313,67720172,2,67707288,2021-05-27 10:21:11,0,"<p>You will be charged as long as the model is running on an instance regardless of whether the traffic is going through it or not as still the model is running on an instance.</p>
","1508542",0
1314,67747382,2,67744934,2021-05-29 03:09:19,5,"<blockquote>
<p>It seems like dvc status is similar to what I'm asking for</p>
</blockquote>
<p><code>dvc status --cloud</code> will give you a list of &quot;new&quot; files if they that haven't been pushed to the (default) remote. It won't error out though, so your CI script should fail depending on the stdout message.</p>
<p>More info: <a href=""https://dvc.org/doc/command-reference/status#options"" rel=""nofollow noreferrer"">https://dvc.org/doc/command-reference/status#options</a></p>
<p>I'd also ask everyone to run <code>dvc install</code>, which will setup some Git hooks, including automatic <code>dvc push</code> with <code>git push</code>.</p>
<p>See <a href=""https://dvc.org/doc/command-reference/install"" rel=""nofollow noreferrer"">https://dvc.org/doc/command-reference/install</a></p>
","761963",1
1315,67751849,2,67745141,2021-05-29 13:19:38,1,"<p>Fixed this error by changing the entry point to</p>
<p><strong>ENTRYPOINT [ &quot;/usr/bin/python3.8&quot;]</strong></p>
","14879635",0
1316,67786986,2,67786052,2021-06-01 10:16:03,2,"<p>There are two functions for there:</p>
<ol>
<li><a href=""https://mlflow.org/docs/latest/python_api/mlflow.html#mlflow.log_artifact"" rel=""nofollow noreferrer"">log_artifact</a> - to log a local file or directory as an artifact</li>
<li><a href=""https://mlflow.org/docs/latest/python_api/mlflow.html#mlflow.log_artifacts"" rel=""nofollow noreferrer"">log_artifacts</a> - to log a contents of a local directory</li>
</ol>
<p>so it would be as simple as:</p>
<pre class=""lang-py prettyprint-override""><code>with mlflow.start_run():
    mlflow.log_artifact(&quot;encoder.pickle&quot;)
</code></pre>
<p>And you will need to use the <a href=""https://mlflow.org/docs/latest/models.html#model-customization"" rel=""nofollow noreferrer"">custom MLflow model</a> to use that pickled file, something like this:</p>
<pre class=""lang-py prettyprint-override""><code>import mlflow.pyfunc

class my_model(mlflow.pyfunc.PythonModel):
    def __init__(self, encoders):
        self.encoders = encoders

    def predict(self, context, model_input):
        _X = ...# do encoding using self.encoders.
        return str(self.ctx.predict([_X])[0])
</code></pre>
","18627",0
1317,67801013,2,64427396,2021-06-02 07:52:35,1,"<p>This just can be done in current version of kubeflow pipelines. It is a limitation, but you cannot change resources from the pipeline itself.</p>
","4814094",0
1318,67820981,2,67819912,2021-06-03 11:51:31,1,"<p>Some files that are tracked in a branch could be not tracked in another. So when you switch back to the &quot;non tracking&quot; branch, that files remain in the file system. Git does not clean stuff that does not track directly. Do not exchange the term not tracked by ignored. Files are not tracked until we &quot;add&quot; them in stage and commit.
You could cleanup the working git by running <code>git clean -f -d</code></p>
","566608",0
1319,67824532,2,67818831,2021-06-03 15:37:41,1,"<p><code>&lt;mycredential&gt;</code> should not be your bare auth key string. You need to create a shared auth key object.</p>
<pre><code>credentials = batchauth.SharedKeyCredentials(BATCH_ACCOUNT_NAME, BATCH_ACCOUNT_KEY)
batch_client = batch.BatchServiceClient(credentials, base_url=BATCH_ACCOUNT_URL)
</code></pre>
<p>Please see the <a href=""https://learn.microsoft.com/azure/batch/tutorial-parallel-python"" rel=""nofollow noreferrer"">Azure Batch Python tutorial</a>.</p>
","4896852",0
1320,67857242,2,67835498,2021-06-06 08:39:43,1,"<p>You need to run <code>mlflow ui</code> in the project directory itself, not inside the <code>mlruns</code> - if you look into the <a href=""https://mlflow.org/docs/latest/cli.html#mlflow-ui"" rel=""nofollow noreferrer"">documentation for <code>mlflow ui</code> command</a>, it says:</p>
<blockquote>
<p><code>--default-artifact-root &lt;URI&gt;</code></p>
<p>Path to local directory to store artifacts, for new experiments. Note that this flag does not impact already-created experiments. <strong>Default: ./mlruns</strong></p>
</blockquote>
","18627",0
1321,67865335,2,67863816,2021-06-07 02:39:17,0,"<p>It looks like they've left some of the code out, or changed the terminology and left in predictions by accident. predictions is an object that is defined on this page <a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/ex1-test-model.html"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/sagemaker/latest/dg/ex1-test-model.html</a></p>
<p>You'll have to work out what predictions is in your case.</p>
","12133434",0
1322,67871766,2,67870241,2021-06-07 12:38:23,3,"<p>This seems like a bug.  Incredibly, it's mentioned in the <a href=""https://www.youtube.com/watch?v=M_Ama8ZY8OQ&amp;t=400s"" rel=""nofollow noreferrer"">video</a> (at 6:40), but not in the <a href=""https://charmed-kubeflow.io/docs/install"" rel=""nofollow noreferrer"">docs</a> (on the same page).  It's also not actually written anywhere on the video description.  Wonderful.</p>
<p>You need to open a terminal on the machine with <code>kubectl</code> installed, then run:</p>
<pre class=""lang-sh prettyprint-override""><code>kubectl patch role -n kubeflow istio-ingressgateway-operator -p '{&quot;apiVersion&quot;:&quot;rbac.authorization.k8s.io/v1&quot;,&quot;kind&quot;:&quot;Role&quot;,&quot;metadata&quot;:{&quot;name&quot;:&quot;istio-ingressgateway-operator&quot;},&quot;rules&quot;:[{&quot;apiGroups&quot;:[&quot;*&quot;],&quot;resources&quot;:[&quot;*&quot;],&quot;verbs&quot;:[&quot;*&quot;]}]}'
</code></pre>
<p>The <code>istio-ingressgateway</code> service will come up a couple of mins later.</p>
","4709298",0
1323,67880161,2,67879851,2021-06-08 00:50:19,2,"<p>You can handle both by doing the login at the lower level of dockerd or containerd on the host itself. Otherwise not really, other than mounting the image pull secret into the container if it will respect a dockerconfig.</p>
","78722",4
1324,67898164,2,67456428,2021-06-09 05:36:41,2,"<p>pvolume is a bit of a weird concept which is a bit alien in KFP. The idea was that a volume is being &quot;passed&quot; between components similarly to normal outputs (when actually it's the same volume).</p>
<p>We advice our users to <em>avoid using the pvolume</em> feature and <em>avoid using volumes</em> in the components. Otherwise, the components and pipelines are not portable and have limited usability.</p>
<p>Please check out the samples, tutorials and components. Almost no pipelines use volumes.</p>
<p>Please check the following two tutorials for <a href=""https://github.com/kubeflow/pipelines/blob/master/samples/tutorials/Data%20passing%20in%20python%20components.ipynb"" rel=""nofollow noreferrer"">Python</a> and <a href=""https://github.com/Ark-kun/kfp_samples/blob/master/2019-10%20Kubeflow%20summit/106%20-%20Creating%20components%20from%20command-line%20programs/106%20-%20Creating%20components%20from%20command-line%20programs.ipynb"" rel=""nofollow noreferrer"">shell</a> components. Check how the pipelines usually look like. <a href=""https://github.com/kubeflow/pipelines/blob/master/components/XGBoost/_samples/sample_pipeline.py"" rel=""nofollow noreferrer"">example XGBoost training pipeline</a>.</p>
","1497385",3
1325,67919224,2,67899421,2021-06-10 10:19:58,0,"<p>Keras is now part of tensorflow, so you can just reformat your code to use <code>tf.keras</code> instead of <code>keras</code>. Since version 2.3.0 of tensorflow they are in sync, so it should not be that difficult.
You container is <a href=""https://aws.amazon.com/releasenotes/aws-deep-learning-containers-for-tensorflow-2-3-1-with-cuda-11-0/"" rel=""nofollow noreferrer"">this</a>, as you can see from the list of the packages, there is no <code>Keras</code>.
If you instead want to extend a pre-built container you can take a look <a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/prebuilt-containers-extend.html"" rel=""nofollow noreferrer"">here</a> but I don't recommend in this specific use-case, because also for future code maintainability you should go for <code>tf.keras</code></p>
","4267439",0
1326,67956022,2,67953241,2021-06-13 07:52:23,1,"<p>For non-local URIs, MLflow uses the Python's <code>tempfile.mkdtemp</code> function (<a href=""https://github.com/mlflow/mlflow/blob/1c43176cefb5531fbb243975b9c8c5bfb9775e66/mlflow/projects/utils.py#L140"" rel=""nofollow noreferrer"">source code</a>), that creates the temporary directory.  You may have some control over it by setting the <code>TMPDIR</code> environment variable as described in <a href=""https://docs.python.org/3/library/tempfile.html#tempfile.mkstemp"" rel=""nofollow noreferrer"">Python docs</a> (it lists <code>TMP</code> &amp; <code>TEMP</code> as well, but they didn't work for me on MacOS) - but it will set only &quot;base path&quot; for temporary directories and files, the directory/file names are still will be random.</p>
","18627",3
1327,67969250,2,67939827,2021-06-14 11:17:02,1,"<p>It turned out that my local docker registry wasn't visible from kubernetes. <code>kubectl get events</code> shows InternalError &quot;Unable to fetch image ... &quot;</p>
","9056337",0
1328,67999715,2,66656120,2021-06-16 09:13:29,1,"<p>smdistributed is only available on the SageMaker containers. It is supported for specific TensorFlow versions and you must add:</p>
<pre><code>distribution={'smdistributed': {
            'dataparallel': {
                'enabled': True
            }
        }}
</code></pre>
<p>On the estimator code in order to enable it</p>
","1115237",0
1329,68000517,2,67993641,2021-06-16 10:02:09,2,"<p>You have to install the lost package into your python, the default python does not have that. Please use the below yml:</p>
<pre><code>trigger:
- master

pool:
  vmImage: 'ubuntu-latest'

variables:
  solution: '**/*.sln'
  buildPlatform: 'Any CPU'
  buildConfiguration: 'Release'

steps:
- task: UsePythonVersion@0
  displayName: 'Use Python 3.8'
  inputs:
    versionSpec: 3.8

- script: python3 -m pip install --upgrade pip
  displayName: 'upgrade pip'

- script: python3 -m pip install azureml.core
  displayName: 'Install azureml.core'



- task: PythonScript@0
  inputs: 
    scriptSource: 'filepath'
    scriptPath: test.py
</code></pre>
","12450747",1
1330,68006239,2,68004538,2021-06-16 16:00:05,4,"<p>I'd recommend manual editing as the main route! (I believe that's officially recommended since <a href=""https://dvc.org/blog/dvc-2-0-release"" rel=""nofollow noreferrer"">DVC 2.0</a>)</p>
<p><code>dvc stage add</code> can still be very helpful for programmatic generation of pipelines files, but it doesn't support all the features of <code>dvc.yaml</code>, for example setting <code>vars</code> values or defining <a href=""https://dvc.org/doc/user-guide/project-structure/pipelines-files#foreach-stages"" rel=""nofollow noreferrer""><code>foreach</code> stages</a>.</p>
","761963",7
1331,68006783,2,67988138,2021-06-16 16:35:31,1,"<p>I think the error indicates that your environment is using pyarrow package which is of version 4.0.0 whereas azureml-dataset-runtime requires the package to be &gt;=0.17.0 but &lt;4.0.0</p>
<p>It would be easier for you to uninstall the package and install a specific version. The list of releases of pyarrow are available here.</p>
<p>Since you are using a notebook create new cells and run these commands.</p>
<pre><code> !pip uninstall pyarrow
 !pip install -y pyarrow==3.0.0
</code></pre>
","10965429",0
1332,68010088,2,68009703,2021-06-16 21:13:35,0,"<p>When calling <code>invoke_endpoint</code>, the underlying model invocation must take <a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/your-algorithms-inference-code.html#your-algorithms-inference-code-how-containe-serves-requests"" rel=""nofollow noreferrer"">less than 1 minute</a>. If a single model execution needs more time to execute, consider running the model in Lambda itself, in SageMaker Training API (if its coldstart is acceptable) or in a custom service. If the invocation is made of several shorter calls you can also chain multiple services together with Step Functions.</p>
","5331834",0
1333,68032770,2,67966905,2021-06-18 09:34:22,0,"<p>Consider that a Dataset was added as a reference to a Model with the name <code>'training_dataset'</code></p>
<p>In order to get a reference to this Dataset we use:</p>
<pre><code>model = Model(workspace, name)
dataset_id = next(dictionary['id'] for dictionary in model.serialize()['datasets'] if dictionary[&quot;name&quot;] == 'training_dataset')
dataset_reference = Dataset.get_by_id(workspace, dataset_id )
</code></pre>
<p>After this step we can use <code>dataset_reference</code> as any other AzureML Dataset Class object.</p>
","14384792",0
1334,68033718,2,67734831,2021-06-18 10:40:50,0,"<p>Since the Dataset Asset is a simple reference to a location in a Datastore. Assuming the model version and service name does not change, the Dataset reference also will not change. If however, with every Service Update - The model version changes then adding a Dataset with Relative Path:</p>
<pre><code>&lt;Subscription-ID&gt;/&lt;Resource-Group&gt;/&lt;Workspace&gt;/&lt;Webservice-Name&gt;/&lt;model-name&gt;/*/inputs/**/inputs*.csv
</code></pre>
<p>Will solve the problem. Since Data Drift is another service referencing this Dataset asset, it will keep working as expected.</p>
","14384792",0
1335,68035863,2,68034523,2021-06-18 13:16:26,5,"<p>There is a <a href=""https://mlflow.org/docs/latest/python_api/mlflow.tracking.html#mlflow.tracking.MlflowClient.download_artifacts"" rel=""noreferrer"">download_artifacts function</a> that allows you to get access to the logged artifact:</p>
<pre class=""lang-py prettyprint-override""><code>local_path = client.download_artifacts(run_id, &quot;train.csv&quot;, local_dir)
</code></pre>
<p>The model artifact could either downloaded using the same function (there should be the object called <code>model/model.pkl</code> (for scikit-learn, or something else), or you can load model by run:</p>
<pre class=""lang-py prettyprint-override""><code>loaded_model = mlflow.pyfunc.load_model(f&quot;runs:/{run_id}/model&quot;)
</code></pre>
","18627",0
1336,68062840,2,67866286,2021-06-21 05:42:31,1,"<p>You can use Sagemaker inference Pipeline. You need to create preprocessing script comprising of your preprocessing steps and create a Pipeline including Preprocess and model. Deploy pipeline to an endpoint for real time inference.</p>
<p>Reference:
<a href=""https://aws.amazon.com/blogs/machine-learning/preprocess-input-data-before-making-predictions-using-amazon-sagemaker-inference-pipelines-and-scikit-learn/"" rel=""nofollow noreferrer"">https://aws.amazon.com/blogs/machine-learning/preprocess-input-data-before-making-predictions-using-amazon-sagemaker-inference-pipelines-and-scikit-learn/</a></p>
<p><a href=""https://github.com/aws/amazon-sagemaker-examples/blob/master/sagemaker-python-sdk/scikit_learn_inference_pipeline/Inference%20Pipeline%20with%20Scikit-learn%20and%20Linear%20Learner.ipynb"" rel=""nofollow noreferrer"">https://github.com/aws/amazon-sagemaker-examples/blob/master/sagemaker-python-sdk/scikit_learn_inference_pipeline/Inference%20Pipeline%20with%20Scikit-learn%20and%20Linear%20Learner.ipynb</a></p>
","14711391",0
1337,68064612,2,68024476,2021-06-21 08:17:03,0,"<p>Based on the comments it seems to me that you are trying to open the wrong URL. If I understand you question, you are not running in a local environment, so you can not open <code>localhost</code>. The right URL for <code>sagemaker</code> from the docs is <code>https://&lt;notebook instance hostname&gt;/proxy/6006/</code></p>
","4267439",0
1338,68090316,2,68082912,2021-06-22 20:24:47,4,"<p>This is definitely possible, as DVC features are loosely coupled to one another. You can do pipelining by writing your dvc.yaml file(s), but avoid data management/versioning by using <code>cache: false</code> in the stage outputs (<a href=""https://dvc.org/doc/user-guide/project-structure/pipelines-files#output-subfields"" rel=""nofollow noreferrer""><code>outs</code> field</a>). See also helper <code>dvc stage add -O</code> (<a href=""https://dvc.org/doc/command-reference/stage/add#options"" rel=""nofollow noreferrer"">big O</a>, alias of <code>--outs-no-cache</code>).</p>
<p>And the same for initial data dependencies, you can <code>dvc add --no-commit</code> them (<a href=""https://dvc.org/doc/command-reference/add#options"" rel=""nofollow noreferrer"">ref</a>).</p>
<p>You do want to track <a href=""https://dvc.org/doc/user-guide/project-structure/pipelines-files#dvclock-file"" rel=""nofollow noreferrer"">dvc.lock</a> in Git though, so that DVC can determine the latest stage of the pipeline associated with the Git commit in every repo copy or branch.</p>
<p>You'll be responsible for placing the right data files/dirs (matching .dvc files and dvc.lock) in the workspace for <code>dvc repro</code> or <code>dvc exp run</code> to behave as expected. <code>dvc checkout</code> won't be able to help you.</p>
","761963",7
1339,68092878,2,68083831,2021-06-23 02:40:05,7,"<p>UPDATE</p>
<ul>
<li><p>Set three environment variables</p>
<ol>
<li>ENABLE_MULTI_MODEL as &quot;true&quot; (make sure it is string and not bool) and set <a href=""https://github.com/aws/sagemaker-pytorch-inference-toolkit/blob/master/src/sagemaker_pytorch_serving_container/torchserve.py#L74"" rel=""nofollow noreferrer"">SAGEMAKER_HANDLER</a> as custom model handler python module path if custom service else dont define it. Also make sure model name <a href=""https://github.com/aws/sagemaker-pytorch-inference-toolkit/blob/master/src/sagemaker_pytorch_serving_container/torchserve.py#L94"" rel=""nofollow noreferrer"">model.mar</a>, before compressing it as tar ball and storing in s3</li>
<li>TS_DEFAULT_WORKERS_PER_MODEL as number of vcpus</li>
<li>First environment variable makes sure torch serve env_vars are enabled and second one uses first setting and loads requested number of workers</li>
<li>Setting can be done by passing env dictionary argument to <a href=""https://sagemaker.readthedocs.io/en/stable/frameworks/pytorch/using_pytorch.html#create-an-estimator"" rel=""nofollow noreferrer"">PyTorch function</a>. Below is explanation as to why it works</li>
</ol>
</li>
<li><p>From the looks of it, sagemaker deployment for pytorch model as given in <a href=""https://sagemaker.readthedocs.io/en/stable/frameworks/pytorch/using_pytorch.html#create-an-estimator"" rel=""nofollow noreferrer"">Sagemaker SDK guide</a>, uses <a href=""https://github.com/aws/deep-learning-containers/blob/master/pytorch/inference/docker/1.8/py3/Dockerfile.cpu"" rel=""nofollow noreferrer"">this dockerfile</a>. In this docker, entrypoint is <a href=""https://github.com/aws/deep-learning-containers/blob/master/pytorch/inference/docker/build_artifacts/torchserve-entrypoint.py"" rel=""nofollow noreferrer"">torchserve-entrypoint.py</a> as in <a href=""https://github.com/aws/deep-learning-containers/blob/master/pytorch/inference/docker/1.8/py3/Dockerfile.cpu#L124"" rel=""nofollow noreferrer"">Dockerfile line#124</a>.</p>
</li>
<li><p>This <a href=""https://github.com/aws/deep-learning-containers/blob/master/pytorch/inference/docker/build_artifacts/torchserve-entrypoint.py"" rel=""nofollow noreferrer"">torchserve-entrypoint.py</a> calls <a href=""https://github.com/aws/deep-learning-containers/blob/master/pytorch/inference/docker/build_artifacts/torchserve-entrypoint.py"" rel=""nofollow noreferrer"">serving.main()</a> from <a href=""https://github.com/aws/sagemaker-pytorch-inference-toolkit/blob/master/src/sagemaker_pytorch_serving_container/serving.py"" rel=""nofollow noreferrer"">serving.py</a>. Which ends up calling <a href=""https://github.com/aws/sagemaker-pytorch-inference-toolkit/blob/master/src/sagemaker_pytorch_serving_container/serving.py#L34"" rel=""nofollow noreferrer"">torchserve.start_torchserve(handler_service=HANDLER_SERVICE)</a> from <a href=""https://github.com/aws/sagemaker-pytorch-inference-toolkit/blob/master/src/sagemaker_pytorch_serving_container/torchserve.py"" rel=""nofollow noreferrer"">torchserve.py</a>.</p>
</li>
<li><p><a href=""https://github.com/aws/sagemaker-pytorch-inference-toolkit/blob/master/src/sagemaker_pytorch_serving_container/torchserve.py#L34"" rel=""nofollow noreferrer"">At line 34 in torchserve.py</a> it defines &quot;/etc/default-ts.properties&quot; as DEFAULT_TS_CONFIG_FILE. This file is located <a href=""https://github.com/aws/sagemaker-pytorch-inference-toolkit/blob/master/src/sagemaker_pytorch_serving_container/etc/default-ts.properties"" rel=""nofollow noreferrer"">here</a>. In this file <a href=""https://github.com/aws/sagemaker-pytorch-inference-toolkit/blob/master/src/sagemaker_pytorch_serving_container/etc/default-ts.properties#L2"" rel=""nofollow noreferrer"">enable_envvars_config=true</a> is set. It will use this file setting IFF Environment variable &quot;ENABLE_MULTI_MODEL&quot; is set to &quot;false&quot; as refered <a href=""https://github.com/aws/sagemaker-pytorch-inference-toolkit/blob/master/src/sagemaker_pytorch_serving_container/torchserve.py#L167"" rel=""nofollow noreferrer"">here</a>. If it is set to &quot;true&quot; then it will use /etc/mme-ts.properties</p>
</li>
</ul>
<hr />
<p>As for the question <code>Are there any settings in AWS Sagemaker deployment to use all vCPUs to increase concurrency?</code>
There are various settings you can use
For models you can set <code>default_workers_per_model</code> in config.properties <code>TS_DEFAULT_WORKERS_PER_MODEL=$(nproc --all)</code> in environment variables. Environment variables take top priority.</p>
<p>Other than that for each model, you can set the number of workers by using management API, but sadly it is not possible to curl to management API in sagemaker. SO TS_DEFAULT_WORKERS_PER_MODEL is the best bet.
Setting this should make sure all cores are used.</p>
<p>But if you are using docker file then in entrypoint you can setup scripts which wait for model loading and curl to it to set number of workers</p>
<pre class=""lang-sh prettyprint-override""><code># load the model
curl -X POST localhost:8081/models?url=model_1.mar&amp;batch_size=8&amp;max_batch_delay=50
# after loading the model it is possible to set min_worker, etc
curl -v -X PUT http://localhost:8081/models/model_1?min_worker=1
</code></pre>
<p>About the other issue that logs confirm that not all cores are used, I face the same issue and believe that is a problem in the logging system. Please look at this issue <a href=""https://github.com/pytorch/serve/issues/782"" rel=""nofollow noreferrer"">https://github.com/pytorch/serve/issues/782</a>. The community itself agrees that if threads are not set, then by default then it prints 0, even if by default it uses 2*num_cores.</p>
<p><strong>For an exhaustive set of all configs possible</strong></p>
<pre class=""lang-py prettyprint-override""><code># Reference: https://github.com/pytorch/serve/blob/master/docs/configuration.md
# Variables that can be configured through config.properties and Environment Variables
# NOTE: Variables which can be configured through environment variables **SHOULD** have a
# &quot;TS_&quot; prefix
# debug
inference_address=http://0.0.0.0:8080
management_address=http://0.0.0.0:8081
metrics_address=http://0.0.0.0:8082
model_store=/opt/ml/model
load_models=model_1.mar
# blacklist_env_vars
# default_workers_per_model
# default_response_timeout
# unregister_model_timeout
# number_of_netty_threads
# netty_client_threads
# job_queue_size
# number_of_gpu
# async_logging
# cors_allowed_origin
# cors_allowed_methods
# cors_allowed_headers
# decode_input_request
# keystore
# keystore_pass
# keystore_type
# certificate_file
# private_key_file
# max_request_size
# max_response_size
# default_service_handler
# service_envelope
# model_server_home
# snapshot_store
# prefer_direct_buffer
# allowed_urls
# install_py_dep_per_model
# metrics_format
# enable_metrics_api
# initial_worker_port

# Configuration which are not documented or enabled through environment variables

# When below variable is set true, then the variables set in environment have higher precedence.
# For example, the value of an environment variable overrides both command line arguments and a property in the configuration file. The value of a command line argument overrides a value in the configuration file.
# When set to false, environment variables are not used at all
# use_native_io=
# io_ratio=
# metric_time_interval=
enable_envvars_config=true
# model_snapshot=
# version=
</code></pre>
","7035448",6
1340,68117484,2,68115476,2021-06-24 14:15:51,4,"<p><code>azureml.pipeline.core.StepSequence</code> lets you do exactly that.</p>
<blockquote>
<p>A StepSequence can be used to easily run steps in a specific order, without needing to specify data dependencies through the use of PipelineData.</p>
</blockquote>
<p>See <a href=""https://learn.microsoft.com/en-us/python/api/azureml-pipeline-core/azureml.pipeline.core.stepsequence?view=azure-ml-py"" rel=""nofollow noreferrer"">the docs</a> to read more.</p>
<p>However, the preferable way to have steps run in order is stitching them together via <code>PipelineData</code> or <code>OutputFileDatasetConfig</code>. In your example, does the <code>train_step</code> depend on outputs from the <code>etl step</code>? If so, consider having that be the way that steps are run in sequence. For more info see <a href=""https://learn.microsoft.com/en-us/azure/machine-learning/how-to-move-data-in-out-of-pipelines"" rel=""nofollow noreferrer"">this tutorial</a> for more info</p>
","3842610",5
1341,68164990,2,68075940,2021-06-28 14:17:27,1,"<p>My original <code>CustomPythonPackageTrainingJobRunOp</code> wasn't defining <code>worker_pool_spec</code> which was the reason for the error. After I specified <code>replica_count</code> and <code>machine_type</code> the error resolved. Final training op is:</p>
<pre><code>training_job_run_op = gcc_aip.CustomPythonPackageTrainingJobRunOp(
            project=project_id,
            display_name=training_job_name,
            model_display_name=model_display_name,
            python_package_gcs_uri=python_package_gcs_uri,
            python_module=python_module,
            container_uri=container_uri,
            staging_bucket=staging_bucket,
            base_output_dir=output_dir,
        model_serving_container_image_uri=model_serving_container_image_uri,
            replica_count=1,
            machine_type=&quot;n1-standard-4&quot;)

</code></pre>
","13758442",2
1342,68195967,2,68161725,2021-06-30 13:42:45,1,"<p>I found the error: instead of 'validation' you need to name the channel 'test', then it works:
rcf.fit({'train': train_data, 'test': test_data}, wait=True)</p>
","16332811",0
1343,68196994,2,68193944,2021-06-30 14:45:28,1,"<p>AWS System Manager (SSM) is designed to store keys and tokens securely.</p>
<p>Depending on how your notebook is defined, you could <a href=""https://sagemaker.readthedocs.io/en/stable/api/training/processing.html"" rel=""nofollow noreferrer"">use the 'env' property</a> directly or in <a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/your-algorithms-training-algo-running-container.html#your-algorithms-training-algo-running-container-environment-variables"" rel=""nofollow noreferrer"">training data</a>, or you could access SSM directly from sagemaker. For example this Snowflake KB article explains how to fetch auth info from ssm: <a href=""https://community.snowflake.com/s/article/Connecting-a-Jupyter-Notebook-Part-3"" rel=""nofollow noreferrer"">https://community.snowflake.com/s/article/Connecting-a-Jupyter-Notebook-Part-3</a></p>
","310175",0
1344,68303286,2,68303285,2021-07-08 14:10:50,2,"<p><strong>There are 2 solutions I've found:</strong></p>
<p>1.- Use the kernel &quot;Python 3.6 - AzureML&quot;</p>
<p>2.- <code>pip install azureml-core --upgrade</code></p>
<p>This will <strong>upgrade</strong></p>
<blockquote>
<p>azureml-core to 1.32.0</p>
</blockquote>
<p>But will <strong>downgrade</strong>:</p>
<blockquote>
<p>azure-mgmt-resource to 13.0.0 (was 18.0.0)</p>
</blockquote>
<blockquote>
<p>azure-mgmt-storage down to 11.2.0 (was 18.0.0)</p>
</blockquote>
<blockquote>
<p>urllib3 to 1.26.5 (was 1.26.6)</p>
</blockquote>
<p>This upgrade / downgrade allows the same package versions as in the python 3.6 anaconda install</p>
","13526512",1
1345,68374318,2,68374280,2021-07-14 08:09:21,2,"<p>When you call <code>fit()</code> you can pass this parameter <code>job_name=yourJobName</code></p>
","4267439",0
1346,68402407,2,68402406,2021-07-16 01:21:39,1,"<p>The problem caused by <code>mlflow.utils.autologging_utils</code> WARNING.</p>
<p>When the model is created, data input signature is saved on the <code>MLmodel</code> file with some.
You should change <code>response_length</code> signature input type from <code>string</code> to <code>double</code> by replacing</p>
<pre><code>{&quot;name&quot;: &quot;response_length&quot;, &quot;type&quot;: &quot;double&quot;}
</code></pre>
<p>instead of</p>
<pre><code>{&quot;name&quot;: &quot;response_length&quot;, &quot;type&quot;: &quot;string&quot;}
</code></pre>
<p>so it doesn't need to be converted. After serving the model with edited <code>MLmodel</code> file, the web server worked as expected.</p>
","11232272",0
1347,68414998,2,68258003,2021-07-16 20:39:54,3,"<p>First of all, you can generate &quot;disposable&quot; (called pre-signed) URLs which can be used for accessing SageMaker Studio User Profiles without any AWS credentials. These URls can be valid for max 5 minutes and can be generated with a single AWS API call.</p>
<p>One approach to provide Studio access to your users is to set up a service which can authenticate your Studio users and then calls the <a href=""https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_CreatePresignedDomainUrl.html"" rel=""nofollow noreferrer"">CreatePresignedDomainUrl SageMaker API</a> method to sends back the generated pre-signed URL to the user.</p>
<p>Alternatively, you can use AWS SSO as well, which can do most of the heavy lifting for you, especially if you'd like to integrate with a single sign-on service. AWS SSO integrates with SageMaker Studio and you can assign Studio user profiles to your onboarded users. Your users then can go through your single sign-on service and can launch the Studio without logging into the AWS Console.</p>
<p>An another approach is to use IAM Federation where you basically provide access to the AWS API and/or to the Console to your users which authenticated by an (external) identity provider. Federated users can assume specific roles to operate with the AWS API or the Management Console. For accessing SageMaker Studio, users just need to have the CreatePresignedDomainUrl access policy which allows them to create the pre-signed URL by themselves. If you want to isolate your SageMaker user profiles and ensure each federated user can access just those user profiles which are assigned to them, please see the following <a href=""https://aws.amazon.com/blogs/machine-learning/configuring-amazon-sagemaker-studio-for-teams-and-groups-with-complete-resource-isolation/"" rel=""nofollow noreferrer"">blog post</a> for more information.</p>
<p>And finally, please note that, once the user has logged in to Studio, the Execution Role configured for the specific user profile will determine what the Studio user can access and is able to do (e.g. spinning up SageMaker training jobs, deploying models, accessing S3, etc). Thus, you don't need to set up these policies for your IAM users or roles used by the federated users.</p>
","3891961",0
1348,68415208,2,68186468,2021-07-16 21:03:20,1,"<p>Amazon API Gateway currently does not provide first-class integration for SageMaker. But you can use these services via AWS SDK. If you wish, you can embed the AWS SDK calls into a service, host on AWS (e.g. running on EC2 or as lambda functions) and use API gateway to expose your REST API.</p>
<p>Actually, SageMaker is not fundamentally different from any other AWS service from this aspect.</p>
","3891961",2
1349,68434994,2,68422680,2021-07-19 04:05:19,2,"<p>Here is an <a href=""https://github.com/james-tn/highperformance_python_in_azure/blob/master/parallel_python_processing/pipeline_definition.ipynb"" rel=""nofollow noreferrer"">example</a> for PRS.
<a href=""https://learn.microsoft.com/en-us/python/api/azureml-pipeline-core/azureml.pipeline.core.pipelinedata?view=azure-ml-py"" rel=""nofollow noreferrer"">PipelineData</a> was intended to represent &quot;transient&quot; data from one step to the next one, while OutputDatasetConfig was intended for capturing the final state of a dataset (and hence why you see features like lineage, ADLS support, etc). PipelineData always outputs data in a folder structure like {run_id}{output_name}. OutputDatasetConfig allows to decouple the data from the run and hence it allows you to control where to land the data (although by default it will produce similar folder structure). The OutputDatasetConfig allows even to register the output as a Dataset, where getting rid of such folder structure makes sense. From the docs itself: &quot;Represent how to copy the output of a run and be promoted as a FileDataset. The OutputFileDatasetConfig allows you to specify how you want a particular local path on the compute target to be uploaded to the specified destination&quot;.</p>
<p><a href=""https://learn.microsoft.com/en-us/azure/machine-learning/tutorial-pipeline-batch-scoring-classification#create-dataset-objects"" rel=""nofollow noreferrer"">OutFileDatasetConfig</a> is a control plane concept to pass data between pipeline steps.</p>
","11297406",1
1350,68444248,2,68442999,2021-07-19 16:49:31,3,"<p>Kedro determines the execution order based on the interdependencies between the inputs/outputs of different nodes. In your case, node D doesn't depend on any of the other nodes, so execution order cannot be guaranteed. Similarly, it cannot be ensured that node D will <em>not</em> run in parallel to A, B and C if using a parallel runner.</p>
<p>That said, there are a couple of workarounds one could use achieve a particular execution order.</p>
<h5 id=""preferred-run-the-nodes-separately-62tl"">1 [Preferred] Run the nodes separately</h5>
<p>Instead of doing <code>kedro run --parallel</code>, you could do:</p>
<pre><code>kedro run --pipeline foo --node A --node B --node C --parallel; kedro run --pipeline foo --node D
</code></pre>
<p>This is arguably the preferred solution because it requires no code changes (which is good in case you ever run the same pipeline on a different machine). You could do <code>&amp;&amp;</code> instead of <code>;</code> if you want node D to run only if A, B and C succeded. If the running logic gets more complex, you could store it in a Makefile/bash script.</p>
<h5 id=""using-dummy-inputsoutputs-j7un"">2 Using dummy inputs/outputs</h5>
<p>You could also force the execution order by introducing dummy datasets. Something like:</p>
<pre class=""lang-py prettyprint-override""><code>def foo():
    return Pipeline([
        node(a, inputs=[&quot;train_x&quot;, &quot;test_x&quot;], outputs=[dict(bar_a=&quot;bar_a&quot;), &quot;a_done&quot;], name=&quot;A&quot;),
        node(b, inputs=[&quot;train_x&quot;, &quot;test_x&quot;], outputs=[dict(bar_b=&quot;bar_b&quot;), &quot;b_done&quot;], name=&quot;B&quot;),
        node(c, inputs=[&quot;train_x&quot;, &quot;test_x&quot;], outputs=[dict(bar_c=&quot;bar_c&quot;), &quot;c_done&quot;], name=&quot;C&quot;),
        node(d, inputs=[&quot;train_x&quot;, &quot;test_x&quot;, &quot;a_done&quot;, &quot;b_done&quot;, &quot;c_done&quot;], outputs=dict(bar_d=&quot;bar_d&quot;), name=&quot;D&quot;),     
    ])
</code></pre>
<p>Empty lists could do for the dummy datasets. The underlying functions would also have to return/take the additional arguments.</p>
<p>The advantage of this approach is that <code>kedro run --parallel</code> will immediately result in the desired execution logic. The disadvantage is that it pollutes the definition of nodes and underlying functions.</p>
<p>If you go down this road, you'll also have to decide whether you want to store the dummy datasets in the data catalog (pollutes even more, but allows to run node D on its own) or not (node D cannot run on its own).</p>
<hr />
<p>Related discussions [<a href=""https://github.com/quantumblacklabs/kedro/issues/132"" rel=""nofollow noreferrer"">1</a>, <a href=""https://stackoverflow.com/questions/58686533/how-to-run-the-nodes-in-sequence-as-declared-in-kedro-pipeline"">2</a>]</p>
","8929855",0
1351,68448565,2,67807756,2021-07-20 01:18:57,0,"<p>You have to add pip 20.1.1</p>
<p>Conda ruamel needs higher version of pip</p>
<pre><code>conda install pip=20.1.1
</code></pre>
","10330386",0
1352,68486018,2,67689671,2021-07-22 13:49:52,2,"<p>I found a workaround to reference the workspace in the scoring script. Below is a code snippet of how one can do that -</p>
<p>My deploy script looks like this :</p>
<pre><code>from azureml.core import Environment
from azureml.core.model import InferenceConfig

#Add python dependencies for the models
scoringenv = Environment.from_conda_specification(
                                   name = &quot;scoringenv&quot;,
                                   file_path=&quot;config_files/scoring_env.yml&quot;
                                    )
#Create a dictionary to set-up the env variables   
env_variables={'tenant_id':tenant_id,
                        'subscription_id':subscription_id,
                        'resource_group':resource_group,
                        'client_id':client_id,
                        'client_secret':client_secret
                        }
    
scoringenv.environment_variables=env_variables
            
# Configure the scoring environment
inference_config = InferenceConfig(
                                   entry_script='score.py',
                                   source_directory='scripts/',
                                   environment=scoringenv
                                        )
</code></pre>
<p>What I am doing here is creating an image with the python dependencies(in the scoring_env.yml) and passing a dictionary of the secrets as environment variables. I have the secrets stored in the key-vault.
You may define and pass native python datatype variables.</p>
<p>Now, In my score.py, I reference these environment variables in the init() like this -</p>
<pre><code>tenant_id = os.environ.get('tenant_id')
client_id = os.environ.get('client_id')
client_secret = os.environ.get('client_secret')
subscription_id = os.environ.get('subscription_id')
resource_group = os.environ.get('resource_group')
</code></pre>
<p>Once you have these variables, you may create a workspace object using Service Principal authentication like @Anders Swanson mentioned in his reply.</p>
<p>Another way to resolve this may be by using managed identities for AKS. I did not explore that option.</p>
<p>Hope this helps! Please let me know if you found a better way of solving this.</p>
<p>Thanks!</p>
","16409088",1
1353,68487405,2,68479297,2021-07-22 15:17:02,0,"<p>Looking <a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/adapt-inference-container.html"" rel=""nofollow noreferrer"">here</a> you can see that only some specific content types are supported by default, and images are not in this list. I think you have to either implement your <code>input_fn</code> function or adapt your data to one of the supported content types.</p>
","4267439",2
1354,68489321,2,66570138,2021-07-22 17:40:05,0,"<p>You can have maximum 1 studio domain per region, by the default limits. Though, it seems like you have two domains already provisioned. Try to delete all the domains through the AWS cli and recreate with the AWS Management Console.</p>
<p>Unfortunately, AWS Management Console cannot visualize more than one Studio domain.</p>
","3891961",1
1355,68489381,2,68396088,2021-07-22 17:45:23,0,"<p>The issue is when you are trying to parse your payload with data['body']. The data is not being passed in the format that the endpoint is expecting. Use the following code snippet to properly format/serialize your data for the endpoint. Also to make all this clearer make sure to check for your payload type to make sure you have not serialized again by accident.</p>
<pre><code>    data = json.loads(json.dumps(event))
    payload = json.dumps(data)
    response = runtime.invoke_endpoint(EndpointName=ENDPOINT_NAME,
                                       ContentType='application/json',
                                       Body=payload)
    result = json.loads(response['Body'].read().decode())
</code></pre>
<p>I work for AWS &amp; my opinions are my own</p>
","16504640",0
1356,68489535,2,67020040,2021-07-22 17:58:53,0,"<p>There may be a few issues here we can explore the paths and ways to resolve.</p>
<ol>
<li>Inference Code Error
Sometimes these errors occur when your payload or what you're feeding your endpoint is not in the appropriate format. When invoking the endpoint you want to make sure your data is in the correct format/encoded properly. For this you can use the serializer SageMaker provides when creating the endpoint. The serializer takes care of encoding for you and sends data in the appropriate format. Look at the following code snippet.</li>
</ol>
<pre><code>from sagemaker.predictor import csv_serializer
rf_pred = rf.deploy(1, &quot;ml.m4.xlarge&quot;, serializer=csv_serializer)
print(rf_pred.predict(payload).decode('utf-8'))
</code></pre>
<p>For more information about the different serializers based off the type of data you are feeding in check the following link.
<a href=""https://sagemaker.readthedocs.io/en/stable/api/inference/serializers.html"" rel=""nofollow noreferrer"">https://sagemaker.readthedocs.io/en/stable/api/inference/serializers.html</a></p>
<ol start=""2"">
<li>Throttling Limits Reached
Sometimes the payload you are feeding in may be too large or the API request rate may have been exceeded for the endpoint so experiment with a more compute heavy instance or increase retries in your boto3 configuration. Here is a link for an example of what retries are and configuring them for your endpoint.</li>
</ol>
<p><a href=""https://aws.amazon.com/premiumsupport/knowledge-center/sagemaker-python-throttlingexception/"" rel=""nofollow noreferrer"">https://aws.amazon.com/premiumsupport/knowledge-center/sagemaker-python-throttlingexception/</a></p>
<p>I work for AWS &amp; my opinions are my own</p>
","16504640",0
1357,68489678,2,68143997,2021-07-22 18:10:03,1,"<p>For this you can create an inline Bash shell in your SageMaker notebook as follows. This will take your Docker container, create the image, ECR repo if it does not exist and push the image.</p>
<pre><code>%%sh

# Name of algo -&gt; ECR
algorithm_name=your-algo-name

cd container #your directory with dockerfile and other sm components

chmod +x randomForest-Petrol/train #train file for container
chmod +x randomForest-Petrol/serve #serve file for container

account=$(aws sts get-caller-identity --query Account --output text)

# Region, defaults to us-west-2
region=$(aws configure get region)
region=${region:-us-west-2}

fullname=&quot;${account}.dkr.ecr.${region}.amazonaws.com/${algorithm_name}:latest&quot;

# If the repository doesn't exist in ECR, create it.
aws ecr describe-repositories --repository-names &quot;${algorithm_name}&quot; &gt; /dev/null 2&gt;&amp;1

if [ $? -ne 0 ]
then
    aws ecr create-repository --repository-name &quot;${algorithm_name}&quot; &gt; /dev/null
fi

# Get the login command from ECR and execute it directly
aws ecr get-login-password --region ${region}|docker login --username AWS --password-stdin ${fullname}

# Build the docker image locally with the image name and then push it to ECR
# with the full name.

docker build  -t ${algorithm_name} .
docker tag ${algorithm_name} ${fullname}

docker push ${fullname}
</code></pre>
<p>I am contributing this on behalf of my employer, AWS. My contribution is licensed under the MIT license. See here for a more detailed explanation
<a href=""https://aws-preview.aka.amazon.com/tools/stackoverflow-samples-license/"" rel=""nofollow noreferrer"">https://aws-preview.aka.amazon.com/tools/stackoverflow-samples-license/</a></p>
","16504640",0
1358,68512071,2,68367348,2021-07-24 16:53:55,1,"<p>It looks like <code>add_dataset_references()</code> needs to be called to have datasets displayed under models:</p>
<pre><code>model_registration.add_dataset_references([(&quot;input dataset&quot;, dataset)])
</code></pre>
","3783926",0
1359,68524750,2,68463080,2021-07-26 05:09:30,1,"<p>Register model:
Register a file or folder as a model by calling <a href=""https://learn.microsoft.com/en-us/python/api/azureml-core/azureml.core.model.model?view=azure-ml-py#register-workspace--model-path--model-name--tags-none--properties-none--description-none--datasets-none--model-framework-none--model-framework-version-none--child-paths-none-"" rel=""nofollow noreferrer"">Model.register()</a>.</p>
<p>In addition to the content of the model file itself, your registered model will also store model metadata -- model description, tags, and framework information -- that will be useful when managing and deploying models in your workspace. Using tags, for instance, you can categorize your models and apply filters when listing models in your workspace.</p>
<pre><code>model = Model.register(workspace=ws,
                       model_name='',                # Name of the registered model in your workspace.
                       model_path='',  # Local file to upload and register as a model.
                       model_framework=Model.Framework.SCIKITLEARN,  # Framework used to create the model.
                       model_framework_version=sklearn.__version__,  # Version of scikit-learn used to create the model.
                       sample_input_dataset=input_dataset,
                       sample_output_dataset=output_dataset,
                       resource_configuration=ResourceConfiguration(cpu=1, memory_in_gb=0.5),
                       description='Ridge regression model to predict diabetes progression.',
                       tags={'area': 'diabetes', 'type': 'regression'})

print('Name:', model.name)
print('Version:', model.version)
</code></pre>
<p>Deploy machine learning models to Azure: <a href=""https://learn.microsoft.com/en-us/azure/machine-learning/how-to-deploy-and-where?tabs=python"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/machine-learning/how-to-deploy-and-where?tabs=python</a></p>
<p>To Troubleshooting remote model deployment Please follow the <a href=""https://learn.microsoft.com/en-us/azure/machine-learning/how-to-troubleshoot-deployment?tabs=azcli#function-fails-get_model_path"" rel=""nofollow noreferrer"">document</a>.</p>
<p><a href=""https://i.stack.imgur.com/BL0Nm.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/BL0Nm.png"" alt=""enter image description here"" /></a></p>
","11297406",0
1360,68530277,2,68360738,2021-07-26 12:55:44,0,"<p>It will be supported in the near future, Running Python scripts on Azure with Azure Container Instances to connect the blob.
<a href=""https://kohera.be/tutorials-2/running-python-scripts-on-azure-with-azure-container-instances/"" rel=""nofollow noreferrer"">https://kohera.be/tutorials-2/running-python-scripts-on-azure-with-azure-container-instances/</a></p>
","11297406",0
1361,68536410,2,68533916,2021-07-26 21:16:04,0,"<p>So your label is y. You parse x and s in rformula.</p>
<p>x stays the same:</p>
<pre><code>+-----------+---+
|      x    | x |
+-----------+---+
|     1.0   |1.0|
|     2.0   |2.0|
|     0.0   |0.0|
+-----------+---+
</code></pre>
<p>s:</p>
<pre><code>+-----------+---+
|       s   | s |
+-----------+---+
|       a   |1.0|
|       b   |0.0|
|       a   |1.0|
+-----------+---+
</code></pre>
<p>I hope I could answer you question.
Rformula just converts the strings, standarize them and parse them into a vector.</p>
","16236118",6
1362,68568256,2,68559059,2021-07-28 23:03:26,2,"<p>It looks like you are looking for a combination of things.</p>
<p>First, Jorge mentioned you can set <code>endpointurl</code> to access Minio the same way as you would access regular S3:</p>
<pre><code>dvc remote add -d minio-remote s3://mybucket/path
dvc remote modify minio-remote endpointurl https://minio.example.com                          
</code></pre>
<p>Second, it seems you can create <em>two</em> remotes - one for S3, one for Minio and use <code>-r</code> option that is available for many data management related commands:</p>
<pre><code>dvc pull -r minio-remote
dvc pull -r s3-remote
dvc push -r minio-remote
...
</code></pre>
<p>This way you could <code>push</code>/<code>pull</code> data to/from a specific storage.</p>
<blockquote>
<p>But I need to setup my DVC to work with min.io as a hub between AWS.S3 and DVC</p>
</blockquote>
<p>There are other possible ways, I think to organize this. It indeed depends on what semantics you expect from <code>DVC mc-S3 pull</code>. Please let us know if <code>-r</code> is not enough and clarify the question- that would help us here.</p>
","298182",7
1363,68579418,2,68349739,2021-07-29 16:08:00,1,"<p>Here is sample for Multi-model Register and deploy. <a href=""https://github.com/Azure/MachineLearningNotebooks/blob/master/how-to-use-azureml/deployment/deploy-multi-model/multi-model-register-and-deploy.ipynb"" rel=""nofollow noreferrer"">https://github.com/Azure/MachineLearningNotebooks/blob/master/how-to-use-azureml/deployment/deploy-multi-model/multi-model-register-and-deploy.ipynb</a></p>
","11297406",2
1364,68580115,2,66554893,2021-07-29 17:00:25,0,"<p>When using the <code>ProcessingStep</code>, you can use an <code>S3 URI</code> as the code location, take a look on <a href=""https://github.com/aws/sagemaker-python-sdk/blob/9fc57555bba4fc1d33064478dc209a84a6726c57/src/sagemaker/workflow/steps.py#L374"" rel=""nofollow noreferrer"">this</a> for reference.</p>
","16247336",0
1365,68580170,2,63777462,2021-07-29 17:05:08,0,"<p>RDS is just a managed database running on an EC2 instance. You can connect to that database in a very same way as you would connect from an application. For example, you can use a python based DB client library (depending on what DB flavor you're using, e.g. Postgres) and configure with the connection string, as you would connect any other application to your RDS instance.</p>
<p>I would not recommend to connect to the RDS instance through the public interface. You can place your Notebook instance to the same VPC where your RDS instance is, thus you can talk to RDS directly through the VPC.</p>
","3891961",0
1366,68644024,2,68643893,2021-08-04 00:09:05,1,"<p>The MLflow model server accepts as input either JSON (pandas split-orient format) or CSV.
<a href=""https://mlflow.org/docs/latest/models.html#deploy-mlflow-models"" rel=""nofollow noreferrer"">https://mlflow.org/docs/latest/models.html#deploy-mlflow-models</a></p>
<p>You will need to convert your image into one of those two formats. Example:
<a href=""https://github.com/amesar/mlflow-examples/tree/master/python/keras_tf_mnist#score-mnist-png-file"" rel=""nofollow noreferrer"">https://github.com/amesar/mlflow-examples/tree/master/python/keras_tf_mnist#score-mnist-png-file</a></p>
","13980177",1
1367,68644185,2,68505595,2021-08-04 00:45:49,0,"<p>As the error message said, this issue is an internal issue, please raise a support ticket to assign a support engineer to investigate it.</p>
","9598801",0
1368,68644221,2,68644137,2021-08-04 00:53:58,0,"<p>when you're inside a &quot;Execute Python Script&quot; module or <code>PythonScriptStep</code>, the authentication for fetching the workspace is already done for you (unless you're trying to authenticate to different Azure ML workspace.</p>
<pre class=""lang-py prettyprint-override""><code>from azureml.core import Run
run = Run.get_context()

ws = run.experiment.workspace
</code></pre>
<p>You should be able to use that <code>ws</code> object to register a Dataset.</p>
","3842610",2
1369,68710729,2,68709594,2021-08-09 10:43:55,1,"<p>The <code>values</code> attribute of classification-type SkLearn tree models is essentially a data matrix <code>(n_leaves, n_classes)</code> (probability distributions associated with each leaf node).</p>
<p>In the first experiment this matrix has 4 columns, in the second experiment it has 10 columns - a 2.5X increase in size.</p>
","1808924",4
1370,68725996,2,68675606,2021-08-10 11:28:31,3,"<p>To answer your questions,</p>
<p><strong>1. Can these limits be set for any type of components?</strong></p>
<blockquote>
<p>Yes. Because, these limits are applicable to all Kubeflow components and are not specific to any particular type of component.
These components could be implemented to perform tasks with a set amount of resources.</p>
</blockquote>
<br />
<p><strong>2. Do these limits mean that the component resources will autoscale till they hit the limits?</strong></p>
<blockquote>
<p>No, there is no autoscaling performed by Vertex AI. Based on the limits set, Vertex AI chooses one suitable VM to perform the task.
Having a pool of workers is supported in Google Cloud Pipeline Components such as “<a href=""https://google-cloud-pipeline-components.readthedocs.io/en/google-cloud-pipeline-components-0.1.4/google_cloud_pipeline_components.aiplatform.html#google_cloud_pipeline_components.aiplatform.CustomContainerTrainingJobRunOp"" rel=""nofollow noreferrer"">CustomContainerTrainingJobRunOp</a>” and “<a href=""https://google-cloud-pipeline-components.readthedocs.io/en/google-cloud-pipeline-components-0.1.4/google_cloud_pipeline_components.aiplatform.html#google_cloud_pipeline_components.aiplatform.CustomPythonPackageTrainingJobRunOp"" rel=""nofollow noreferrer"">CustomPythonPackageTrainingJobRunOp</a>” as part of Distributed Training in Vertex AI. Otherwise, only 1 machine is used per step.</p>
</blockquote>
<br />
<p><strong>3. What happens if these limits are not specified? Does Vertex AI scale the resources as it sees fit?</strong></p>
<blockquote>
<p>If the limits are not specified, an “<a href=""https://cloud.google.com/compute/docs/general-purpose-machines#e2_machine_types"" rel=""nofollow noreferrer"">e2-standard-4</a>” VM is used for task execution as the default option.</p>
</blockquote>
<br />
<p><strong>EDIT:</strong> I have updated the links with the latest version of the documentation.</p>
","15745884",2
1371,68738294,2,68738148,2021-08-11 08:12:57,0,"<p>It's clear from the IAM policy that you've posted that you're only allowed to do an <code>iam:PassRole</code> on <code>arn:aws:iam::############:role/query_training_status-role</code> while Glue is trying to use the <code>arn:aws:iam::############:role/AWS-Glue-S3-Bucket-Access</code>. So you'll just need to update your IAM policy to allow <code>iam:PassRole</code> role as well for the other role.</p>
","907480",1
1372,68753776,2,68501612,2021-08-12 08:08:26,1,"<p>This issue was solved by the following:</p>
<p>I have mixed the <em>tracking_uri</em> with the <em>backend_store_uri</em>.</p>
<p>The <em>tracking_uri</em> is where the MLflow related data (e.g. tags, parameters, metrics, etc.) are saved, which can be a database. On the other hand, the <em>artifact_location</em> is where the artifacts (other, not MLflow related data belonging to the preprocessing/training/evaluation/etc. scripts).
What led me to mistakes is that by running mlflow server from command line one should set up for the <em>--backend-store-uri</em> the <em>tracking_uri</em> (also in the script by setting the <em>mlflow.set_tracking_uri()</em>) and for <em>--default-artifact-location</em> the location of the artifacts. Somehow I didn't get that the <em>tracking_uri</em> = <em>backend_store_uri</em>.</p>
","9062454",0
1373,68753986,2,68753045,2021-08-12 08:25:32,0,"<p>Strangely, my issue is solved after I installed sagemaker again but by using this code snippet</p>
<pre><code>!pip install -U sagemaker
</code></pre>
","16172873",0
1374,68780187,2,68775733,2021-08-14 03:51:49,1,"<p>Most AWS services are regional-based, meaning they run in a given region and do not spread <em>beyond</em> one region.</p>
<p>If you wish to run SageMaker in multiple regions, you would need to launch it <em>separately</em> in each region. So, you would only be 'choosing' <em>one</em> region when requesting SageMaker to perform some work.</p>
","174777",1
1375,68780495,2,68741326,2021-08-14 05:07:36,2,"<p>I had to force GPU use with the help of</p>
<pre><code>with tf.device('/device:GPU:0')
</code></pre>
","12819433",0
1376,68790810,2,68790568,2021-08-15 10:47:14,2,"<p>The issue is that your <code>payload</code> has invalid format. It should be one of:</p>
<pre><code>&lt;class 'bytes'&gt;, &lt;class 'bytearray'&gt;, file-like object
</code></pre>
<p>The following should address the error (note: you may have many other issues in your code):</p>
<pre><code>    payload = json.dumps(event)
    print(payload)
    
    response = runtime.invoke_endpoint(EndpointName=ENDPOINT_NAME, ContentType='application/json', Body=payload.encode())
</code></pre>
","248823",1
1377,68798264,2,68622242,2021-08-16 06:27:55,0,"<p>I managed to migrate experiments and runs from the older server to the new server by following these steps:</p>
<ul>
<li>I copied mlruns directory to the new server's location.</li>
<li>I created a different PostgreSQL database with the exact same content of the older server.</li>
<li>I changed artifact_uri field of RUNS table and artifact_location field of EXPERIMENTS table in the database to reflect new location of the experiments and runs.</li>
<li>Started server as this: <code>mlflow server --backend-store-uri &lt;postgresql-db&gt; &lt;db-pass&gt;@&lt;db&gt; --default-artifact-root &lt;new-artifact-location&gt; -h 0.0.0.0 -p 8000 </code></li>
</ul>
","10170533",0
1378,68804208,2,68798737,2021-08-16 14:06:01,0,"<p>You can use <code>argparse</code> - ClearML will auto-magically log all parameters in the task's configuration section (under hyper-parameters section) - see <a href=""https://github.com/allegroai/clearml/blob/master/examples/frameworks/keras/keras_tensorboard.py#L56"" rel=""nofollow noreferrer"">this</a> example. You can also just connect any dictionary (see <a href=""https://github.com/allegroai/clearml/blob/master/examples/frameworks/ignite/cifar_ignite.py#L23"" rel=""nofollow noreferrer"">this</a> example)</p>
","11682840",2
1379,68827803,2,68823606,2021-08-18 06:58:43,1,"<p><code>tracking_uri</code> is the URL of the MLflow server (remote, or built-in in Databricks) that will be used to log metadata &amp; model (see <a href=""https://mlflow.org/docs/latest/quickstart.html#launch-a-tracking-server-on-a-remote-machine"" rel=""nofollow noreferrer"">doc</a>).  In your case, this will be the URL pointing to your EC2 instance that should be configured in programs that will log parameters into your server.</p>
<p><code>backend_store_uri</code> - is used by MLflow server to configure where to store this data - on filesystem, in SQL-compatible database, etc. (see <a href=""https://mlflow.org/docs/latest/cli.html#cmdoption-mlflow-server-backend-store-uri"" rel=""nofollow noreferrer"">doc</a>). If you use SQL database, then you also need to provide the <code>--default-artifact-root</code> option to point where to store generated artifacts (images, model files, etc.)</p>
","18627",2
1380,68830938,2,68802388,2021-08-18 10:47:03,0,"<p>The answer (see <a href=""https://github.com/aws/amazon-sagemaker-examples/issues/1026"" rel=""nofollow noreferrer"">GitHub</a> discussion) is that this error message is simply false.</p>
<p>To avoid this error, the model's local filename (usually for the form <code>model_filename.tar.gz</code>) must be used, not the model name.</p>
<p>The <a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/invoke-multi-model-endpoint.html"" rel=""nofollow noreferrer"">documentation</a> does say this, though it lacks essential detail.</p>
<p>I found <a href=""https://github.com/aws/amazon-sagemaker-examples/blob/master/advanced_functionality/multi_model_xgboost_home_value/xgboost_multi_model_endpoint_home_value.ipynb"" rel=""nofollow noreferrer"">this to be the best example</a>.  See the last part  of that Notebook, in which <code>invoke_endpoint</code> is used (rather than a predictor as used earlier in the Notebook).</p>
<p>As to the location of that model file: This <a href=""https://github.com/aws/amazon-sagemaker-examples/blob/master/advanced_functionality/multi_model_bring_your_own/multi_model_endpoint_bring_your_own.ipynb"" rel=""nofollow noreferrer"">Notebook</a> says:</p>
<blockquote>
<p>When creating the Model entity for multi-model endpoints, the container's ModelDataUrl is the S3 prefix where the model
artifacts that are invokable by the endpoint are located. The rest of the S3 path will be specified when invoking the model.</p>
</blockquote>
","39242",0
1381,68839777,2,68814941,2021-08-18 21:55:49,1,"<p>Had to change the <a href=""https://gist.github.com/romeokienzler/9eb9e422d1ec2c1d1c5f7ad73bb7f6f9"" rel=""nofollow noreferrer"">component.yaml</a> now it works</p>
<p>Changed syntax for command definition and therefore also needed to find out how to pass parameters:</p>
<pre><code>implementation:
  container:
    image: continuumio/anaconda3:2020.07
    command:
    - sh
    - -ec
    - |
      host=$0
      database=$1
      user=$2
      password=$3
      port=$4
      sql=$5
      data_dir=$6
      output_data_csv=$7
      mkdir -p $output_data_csv
      wget https://raw.githubusercontent.com/IBM/claimed/master/component-library/input/input-postgresql.ipynb
      ipython ./input-postgresql.ipynb host=$host database=$database user=$user password=$password port=$port sql=$sql data_dir=$data_dir output_data_csv=$output_data_csv
    - {inputValue: host}
    - {inputValue: database}
    - {inputValue: user}
    - {inputValue: password}
    - {inputValue: port}
    - {inputValue: sql}
    - {inputValue: data_dir}
    - {outputPath: output_data_csv}
</code></pre>
","3656912",0
1382,68847252,2,68728504,2021-08-19 11:45:35,0,"<p>I was able to fix this by setting the Beam SDK address to localhost instead of using a load balancer. So the config I use now is:</p>
<pre><code>        &quot;--runner=FlinkRunner&quot;,
        &quot;--parallelism=4&quot;,
        f&quot;--flink_master={flink_url}:8081&quot;,
        &quot;--environment_type=EXTERNAL&quot;,
        &quot;--environment_config=localhost:50000&quot;, # &lt;--- Changed the address to localhost
        &quot;--flink_submit_uber_jar&quot;,
        &quot;--worker_harness_container_image=none&quot;,
</code></pre>
","15971266",0
1383,68848477,2,68527422,2021-08-19 13:07:40,3,"<p>Have you tried using the <code>mlflow.spark.load_model</code>?</p>
<p>I'm having a very similar issue over here, but but using the spark method. I tried using the <code>mlflow.spark.load_model('runs:/run-id/my-model')</code> method and I got this weird error:</p>
<pre><code>FileNotFoundError: [Errno 2] No such file or directory: '/dbfs/tmp/mlflow/weird-id-folder'
</code></pre>
<p>Searching for the docs, I see the problem that we are facing (which seems to be different), seems to be a signature problem.</p>
<p>According with other part of the <a href=""https://www.mlflow.org/docs/latest/models.html#model-signature"" rel=""nofollow noreferrer"">docs</a> we have that the signature logged with the model will help to define what type of input the model has. The problem for me here is that my input is a Spark Sparse Vector -- which is not supported... Right now I'm trying to convert that into a column-based signature.</p>
<p>Have you tried something like this?</p>
<hr />
<p>UPDATE:</p>
<p>I would like to add that in my case adding the signature did solve the problem. All I did was ignore the vectors and consider only the input data and output data.</p>
<p>I took a look into the notebook, but haven't seen any mlflow logs, anyway, I do suppose you are logging your experiment according to <a href=""https://docs.databricks.com/applications/mlflow/tracking.html#log-runs-to-a-notebook-or-workspace-experiment"" rel=""nofollow noreferrer"">this</a> and using the <code>mlflow.spark</code> flavor.</p>
<p>If so, consider using all your data transformation and model fit in the same pipeline, using <code>from pyspark.ml import Pipeline</code>. Before logging the model, consider going under signature and registering the model schema.</p>
<pre><code>import mlflow.spark
from mlflow.models.signature import infer_signature

with mlflow.start_run():
    [...]
    # executing train &amp; test pipelines:
    model = pipeline.fit(train_features) # training model
    predictions = model.transform(test_features) # testing model
    train_signature = train_features.select('input_data') # ignores all other features created on the pipeline
    prediction_signature = predictions.select('input_data', 'prediction') # ignores all other features created on the training pipeline 
    signature = infer_signature(train_signature, prediction_signature) # register model schema
    mlflow.spark.log_model(model, 'transactions-classification', signature=signature) # logging model to mlflow
    [...]
</code></pre>
<p>After logging the model to the experiment, in a different notebook, you can use the load_model function as:</p>
<pre><code># importing model
import mlflow.spark
model_path = 'runs:/run-id'
model = mlflow.spark.load_model(model_path)
</code></pre>
<p>And it will work! :D</p>
","4423770",2
1384,68876751,2,68825648,2021-08-21 21:03:27,1,"<p>tl;dr: two options:</p>
<ul>
<li>Copy the <code>hyperparameters.json</code> file to <code>/opt/ml/model</code> in the training logic and it will be packed with the model artifacts;</li>
<li>Pass whatever parameters you want through the <code>PrimaryContainer</code> parameter's <code>Environment</code> property.</li>
</ul>
<p>Long version:</p>
<p>That file, <code>opt/ml/input/config/hyperparameters.json</code>, (in fact the whole <code>/opt/ml/input</code> folder) is mounted on the <strong>training</strong> container when it is created. It is provided by SageMaker, based on information you provide, <em>only</em> for training purposes. SageMaker does not change your container in any way, and it doesn't preserve this or any configuration file it passes to the training job once training is done. If you want to pass parameters to the inference endpoint, that is not the way.</p>
<p>You <em>could</em> copy the <code>hyperparameters.json</code> file to the <code>/opt/ml/model</code> folder, and it'd be packed with the model in the <code>model.tar.gz</code> tarball. Your infrence code could then use that - but that's not the prescribed way to pass parameters to an endpoint, and it cause problems with your framework.</p>
<p>The generally prescribed way to pass parameters to SageMaker endpoints is through the <strong>environment</strong>. If you check the <a href=""https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/sagemaker.html#SageMaker.Client.create_model"" rel=""nofollow noreferrer"">boto3 docs for create_model</a>, you'll see that there's an <code>Environment</code> key within the <code>PrimaryContainer</code> parameter (also for each of the <code>Containers</code> parameter). In fact, your code above already uses that to pass a <code>version</code> parameter. You should use that to pass any parameters to your model and, from there, to the endpoint based on it.</p>
","3521917",0
1385,68950607,2,68913914,2021-08-27 08:51:39,1,"<p>If you want to serve multiple models from the same framework using the same endpoint then you can use multi-model endpoints. Due to using the same framework (e.g. only sklearn models), multi-model endpoints make it to the endpoint when they are called. You can have thousands of those models under one endpoint. Multi-container endpoints on the other hand allow serving models from multiple frameworks, e.g. one TensorFlow, one XGBoost and so on, with direct invocation again. However in this case there's <a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/multi-container-direct.html"" rel=""nofollow noreferrer"">limit of 5 different models</a> on a single endpoint.</p>
<p>So depending on the problem you are working, if you need to use multiple frameworks on a single endpoint then you will need to use multi-container endpoint with direct invocation. Otherwise you can use the multi-model endpoint.</p>
<p><a href=""https://towardsdatascience.com/deploy-thousands-of-models-on-sagemaker-real-time-endpoints-with-automatic-retraining-pipelines-4eef7521d5a3"" rel=""nofollow noreferrer"">Reference</a></p>
","4467226",1
1386,68979053,2,68944750,2021-08-30 04:49:53,1,"<p>MOUNTING ADLS2 to AML so you can save files into your mountPoint directly. <a href=""https://learn.microsoft.com/en-us/azure/machine-learning/how-to-access-data#azure-data-lake-storage-generation-2"" rel=""nofollow noreferrer"">Here</a> is the example of registering the storage and <a href=""https://learn.microsoft.com/en-us/python/api/azureml-core/azureml.data.file_dataset.filedataset?view=azure-ml-py#mount-mount-point-none----kwargs-"" rel=""nofollow noreferrer"">here</a> shows how to mount your registered datastore.</p>
","11297406",0
1387,68979487,2,68959934,2021-08-30 05:57:32,1,"<p>The issue is because <code>gradio</code> package using existing Flask package (version 1.0.3). But as your application required Flask&gt;=1.1.1, therefore it is showing error. You need to uninstall the existing Flask package and then install the latest required version.</p>
<p>To uninstall the existing package:
<code>!pip uninstall Flask -y</code></p>
<p>To install latest package:
<code>!pip install Flask&gt;=1.1.1</code></p>
<p><strong>Then, make sure to restart your runtime to pick up the new Flask using the Runtime -&gt; Restart runtime menu.</strong></p>
<p>Finally, import gradio.</p>
","15969041",0
1388,68982510,2,68917725,2021-08-30 10:23:45,0,"<p>Posted community wiki for better visibility. Feel free to expand it.</p>
<hr />
<p>Kubernetes supports accessing node's GPU (AMD or NVIDIA) via <a href=""https://kubernetes.io/docs/tasks/manage-gpus/scheduling-gpus/#using-device-plugins"" rel=""nofollow noreferrer"">Device Plugins</a>.</p>
<p>Worth noting that this support is experimental for now.</p>
<blockquote>
<p>Kubernetes includes experimental support for managing AMD and NVIDIA GPUs (graphical processing units) across several nodes.</p>
</blockquote>
<p>For more information please see <a href=""https://kubernetes.io/docs/tasks/manage-gpus/scheduling-gpus/"" rel=""nofollow noreferrer"">official documentation</a>.</p>
","16391991",0
1389,69007441,2,68193708,2021-09-01 03:25:29,2,"<p>Your <code>sagemaker</code> service-namespace does not have any registered scaling targets. You need to first run <code>register-scalable-target</code> before running <code>put-scaling-policy</code>.</p>
<pre><code>aws application-autoscaling register-scalable-target \
    --service-namespace sagemaker \
    --scalable-dimension sagemaker:variant:DesiredInstanceCount \
    --resource-id endpoint/EmbeddingEndpoint/variant/SimpleVariant
</code></pre>
","10059738",0
1390,69018769,2,68407740,2021-09-01 18:13:05,1,"<p>In the end we have opted to version control a python file from which the yaml specification is auto-generated.</p>
<p>The notebook can also be auto-generated using jupytext.</p>
<p>The python file is thus the master.</p>
<p>I can't see a satisfactory way to generate the spec from a notebook, but I stand to be corrected.</p>
","1021819",0
1391,69024006,2,69024005,2021-09-02 04:01:47,100,"<h1>Answer</h1>
<p>There is no one such resource from AWS that provides the comprehensive view of how to use SageMaker SDK Estimator to train and save models.</p>
<h2>Alternative Overview Diagram</h2>
<p>I put a diagram and brief explanation to get the overview on how SageMaker Estimator runs a training.</p>
<ol>
<li><p>SageMaker sets up a docker container for a training job where:</p>
<ul>
<li>Environment variables are set as in <a href=""https://github.com/aws/sagemaker-containers#important-environment-variables"" rel=""noreferrer"">SageMaker Docker Container. Environment Variables</a>.</li>
<li>Training data is setup under <code>/opt/ml/input/data</code>.</li>
<li>Training script codes are setup under <code>/opt/ml/code</code>.</li>
<li><code>/opt/ml/model</code> and <code>/opt/ml/output</code> directories are setup to store training outputs.</li>
</ul>
</li>
</ol>
<pre><code>/opt/ml
├── input
│   ├── config
│   │   ├── hyperparameters.json  &lt;--- From Estimator hyperparameter arg
│   │   └── resourceConfig.json
│   └── data
│       └── &lt;channel_name&gt;        &lt;--- From Estimator fit method inputs arg
│           └── &lt;input data&gt;
├── code
│   └── &lt;code files&gt;              &lt;--- From Estimator src_dir arg
├── model
│   └── &lt;model files&gt;             &lt;--- Location to save the trained model artifacts
└── output
    └── failure                   &lt;--- Training job failure logs
</code></pre>
<ol start=""2"">
<li><p>SageMaker Estimator <code>fit(inputs)</code> method executes the training script. Estimator <code>hyperparameters</code> and <code>fit</code> method <code>inputs</code> are provided as its command line arguments.</p>
</li>
<li><p>The training script saves the model artifacts in the <code>/opt/ml/model</code> once the training is completed.</p>
</li>
<li><p>SageMaker archives the artifacts under <code>/opt/ml/model</code> into <code>model.tar.gz</code> and save it to the S3 location specified to <code>output_path</code> Estimator parameter.</p>
</li>
<li><p>You can set Estimator <code>metric_definitions</code> parameter to extract model metrics from the training logs. Then you can monitor the training progress in the SageMaker console metrics.</p>
</li>
</ol>
<p><a href=""https://i.stack.imgur.com/gi8bU.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/gi8bU.png"" alt=""enter image description here"" /></a></p>
<p>I believe AWS needs to stop mass-producing verbose, redundant, wordy, scattered, and obsolete documents. AWS needs to understand <strong>A picture is worth thousand words</strong>.</p>
<p>Have diagrams and piece document parts together in a <strong>context</strong> with a clear objective to achieve.</p>
<hr />
<h1>Problem</h1>
<p>AWS documentations need serious re-design and re-structuring. Just to understand <strong>how to train and save a model</strong> forces us going through dozens of scattered,  fragmented, verbose, redundant documentations, which are often obsolete, incomplete, and sometime incorrect.</p>
<p>It is well-summarized in <a href=""https://nandovillalba.medium.com/why-i-think-gcp-is-better-than-aws-ea78f9975bda"" rel=""noreferrer"">Why I think GCP is better than AWS</a>:</p>
<blockquote>
<p>It’s not that AWS is harder to use than GCP, it’s that <strong>it is needlessly hard</strong>; a disjointed, sprawl of infrastructure primitives with poor cohesion between them.  <br><br>
A challenge is nice, a confusing mess is not, and <strong>the problem with AWS is that a large part of your working hours will be spent untangling their documentation and weeding through features and products to find what you want</strong>, rather than focusing on cool interesting challenges.</p>
</blockquote>
<p>Especially the SageMaker team keeps changing implementations without updating documents. Its roll-out was also inconsistent, e.g. SDK version 2 was rolled out in the SageMaker Studio making the AWS examples in Github incompatible without announcing it. Whereas SageMaker instance still had SDK 1, hence code worked in Instance but not in Studio.</p>
<p>It is mind-boggling that we have to go through these many disorganized redundant pages, which we cannot expect accuracy, to figure out how to use the SageMaker SDK Estimator for training.</p>
<h2>Documents for Model Training</h2>
<ul>
<li><a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/how-it-works-training.html"" rel=""noreferrer"">Train a Model with Amazon SageMaker</a></li>
</ul>
<p>This document gives 20,000 feet overview of how SageMaker training but does not give any clue what to do.</p>
<ul>
<li><a href=""https://sagemaker-workshop.com/custom/containers.html"" rel=""noreferrer"">Running a container for Amazon SageMaker training</a></li>
</ul>
<p>This document gives an overview of how SageMaker training looks like. However, this is not up-to-date as it is based on <a href=""https://github.com/aws/sagemaker-containers"" rel=""noreferrer"">SageMaker Containers</a> which is obsolete.</p>
<blockquote>
<p>WARNING: This package has been deprecated. Please use the SageMaker Training Toolkit for model training and the SageMaker Inference Toolkit for model serving.</p>
</blockquote>
<ul>
<li><a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/ex1-train-model.html"" rel=""noreferrer"">Step 4: Train a Model</a></li>
</ul>
<p>This document layouts the steps for training.</p>
<blockquote>
<p>The Amazon SageMaker Python SDK provides framework estimators and generic estimators to train your model while orchestrating the machine learning (ML) lifecycle accessing the SageMaker features for training and the AWS infrastructures</p>
</blockquote>
<ul>
<li><a href=""https://sagemaker.readthedocs.io/en/stable/overview.html#train-a-model-with-the-sagemaker-python-sdk"" rel=""noreferrer"">Train a Model with the SageMaker Python SDK</a></li>
</ul>
<blockquote>
<p>To train a model by using the SageMaker Python SDK, you:</p>
<ul>
<li>Prepare a training script</li>
<li>Create an estimator</li>
<li>Call the fit method of the estimator</li>
</ul>
</blockquote>
<p>Finally this document gives concrete steps and ideas. However still missing comprehensiv details about Environment Variables, Directory structure in the SageMaker docker container**, S3 for uploading code, placing data, S3 where the trained model is saved, etc.</p>
<ul>
<li><a href=""https://sagemaker.readthedocs.io/en/stable/frameworks/tensorflow/using_tf.html"" rel=""noreferrer"">Use TensorFlow with the SageMaker Python SDK</a></li>
</ul>
<p>This documents is focused on TensorFlow Estimator implementation steps. Use <a href=""https://github.com/aws/amazon-sagemaker-examples/blob/master/frameworks/tensorflow/get_started_mnist_train.ipynb"" rel=""noreferrer"">Training a Tensorflow Model on MNIST</a> Github example to accompany with to follow the actual implementation.</p>
<h2>Documents for passing parameters and data locations</h2>
<ul>
<li><a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/your-algorithms-training-algo-running-container.html#your-algorithms-training-algo-running-container-inputdataconfig"" rel=""noreferrer"">How Amazon SageMaker Provides Training Information</a></li>
</ul>
<blockquote>
<p>This section explains how SageMaker makes training information, such as training data, hyperparameters, and other configuration information, available to your Docker container.</p>
</blockquote>
<p>This document finally gives the idea of how parameters and data are passed around but again, not comprehensive.</p>
<ul>
<li><a href=""https://github.com/aws/sagemaker-containers#important-environment-variables"" rel=""noreferrer"">SageMaker Docker Container Environment Variables</a></li>
</ul>
<p>This documentation is marked as <strong>deprecated</strong> but the only document which explains the SageMaker Environment Variables.</p>
<blockquote>
<h3>IMPORTANT ENVIRONMENT VARIABLES</h3>
<ul>
<li>SM_MODEL_DIR</li>
<li>SM_CHANNELS</li>
<li>SM_CHANNEL_{channel_name}</li>
<li>SM_HPS</li>
<li>SM_HP_{hyperparameter_name}</li>
<li>SM_CURRENT_HOST</li>
<li>SM_HOSTS</li>
<li>SM_NUM_GPUS</li>
</ul>
<h3>List of provided environment variables by SageMaker Containers</h3>
<ul>
<li>SM_NUM_CPUS</li>
<li>SM_LOG_LEVEL</li>
<li>SM_NETWORK_INTERFACE_NAME</li>
<li>SM_USER_ARGS</li>
<li>SM_INPUT_DIR</li>
<li>SM_INPUT_CONFIG_DIR</li>
<li>SM_OUTPUT_DATA_DIR</li>
<li>SM_RESOURCE_CONFIG</li>
<li>SM_INPUT_DATA_CONFIG</li>
<li>SM_TRAINING_ENV</li>
</ul>
</blockquote>
<h2>Documents for SageMaker Docker Container Directory Structure</h2>
<ul>
<li><a href=""https://sagemaker-workshop.com/custom/containers.html"" rel=""noreferrer"">Running a container for Amazon SageMaker training</a></li>
</ul>
<pre><code>/opt/ml
├── input
│   ├── config
│   │   ├── hyperparameters.json
│   │   └── resourceConfig.json
│   └── data
│       └── &lt;channel_name&gt;
│           └── &lt;input data&gt;
├── model
│   └── &lt;model files&gt;
└── output
    └── failure
</code></pre>
<p>This document explains the directory structure and purpose of each directory.</p>
<blockquote>
<h3>The input</h3>
<ul>
<li>/opt/ml/input/config contains information to control how your program runs. hyperparameters.json is a JSON-formatted dictionary of hyperparameter names to values. These values will always be strings, so you may need to convert them. resourceConfig.json is a JSON-formatted file that describes the network layout used for distributed training. Since scikit-learn doesn’t support distributed training, we’ll ignore it here.</li>
<li>/opt/ml/input/data/&lt;channel_name&gt;/ (for File mode) contains the input data for that channel. The channels are created based on the call to CreateTrainingJob but it’s generally important that channels match what the algorithm expects. The files for each channel will be copied from S3 to this directory, preserving the tree structure indicated by the S3 key structure.</li>
<li>/opt/ml/input/data/&lt;channel_name&gt;_&lt;epoch_number&gt; (for Pipe mode) is the pipe for a given epoch. Epochs start at zero and go up by one each time you read them. There is no limit to the number of epochs that you can run, but you must close each pipe before reading the next epoch.</li>
</ul>
<h3>The output</h3>
<ul>
<li>/opt/ml/model/ is the directory where you write the model that your algorithm generates. Your model can be in any format that you want. It can be a single file or a whole directory tree. SageMaker will package any files in this directory into a compressed tar archive file. This file will be available at the S3 location returned in the DescribeTrainingJob result.</li>
<li>/opt/ml/output is a directory where the algorithm can write a file failure that describes why the job failed. The contents of this file will be returned in the FailureReason field of the DescribeTrainingJob result. For jobs that succeed, there is no reason to write this file as it will be ignored.</li>
</ul>
</blockquote>
<p>However, this is not up-to-date as it is based on <a href=""https://github.com/aws/sagemaker-containers"" rel=""noreferrer"">SageMaker Containers</a> which is obsolete.</p>
<h2>Documents for Model Saving</h2>
<p>The information on where the trained model is saved and in what format are fundamentally missing. The training script needs to save the model under <code>/opt/ml/model</code> and the format and sub-directory structure depend on the frameworks e,g TensorFlow, Pytorch. This is because SageMaker deployment uses the Framework dependent model-serving, e,g. TensorFlow Serving for TensorFlow framework.</p>
<p>This is not clearly documented and causing confusions. The developer needs to specify which format to use and under which sub-directory to save.</p>
<p>To use TensorFlow Estimator training and deployment:</p>
<ul>
<li><a href=""https://sagemaker-examples.readthedocs.io/en/latest/aws_sagemaker_studio/frameworks/keras_pipe_mode_horovod/keras_pipe_mode_horovod_cifar10.html#Deploy-the-trained-model"" rel=""noreferrer"">Deploy the trained model</a></li>
</ul>
<blockquote>
<p>Because <strong>we’re using TensorFlow Serving for deployment</strong>, our training script <strong>saves the model in TensorFlow’s SavedModel format</strong>.</p>
</blockquote>
<ul>
<li><a href=""https://github.com/aws/amazon-sagemaker-examples/blob/master/frameworks/tensorflow/code/train.py#L159-L166"" rel=""noreferrer"">amazon-sagemaker-examples/frameworks/tensorflow/code/train.py </a></li>
</ul>
<pre><code>    # Save the model
    # A version number is needed for the serving container
    # to load the model
    version = &quot;00000000&quot;
    ckpt_dir = os.path.join(args.model_dir, version)
    if not os.path.exists(ckpt_dir):
        os.makedirs(ckpt_dir)
    model.save(ckpt_dir)
</code></pre>
<p>The code is saving the model in <code>/opt/ml/model/00000000</code> because this is for TensorFlow serving.</p>
<ul>
<li><a href=""https://www.tensorflow.org/guide/saved_model"" rel=""noreferrer"">Using the SavedModel format</a></li>
</ul>
<blockquote>
<p>The save-path follows a convention used by TensorFlow Serving where the last path component (1/ here) is a version number for your model - it allows tools like Tensorflow Serving to reason about the relative freshness.</p>
</blockquote>
<ul>
<li><a href=""https://www.tensorflow.org/tfx/tutorials/serving/rest_simple#save_your_model"" rel=""noreferrer"">Train and serve a TensorFlow model with TensorFlow Serving</a></li>
</ul>
<blockquote>
<p>To load our trained model into TensorFlow Serving we first need to save it in SavedModel format. This will create a protobuf file in a well-defined directory hierarchy, and will include a version number. TensorFlow Serving allows us to select which version of a model, or &quot;servable&quot; we want to use when we make inference requests. Each version will be exported to a different sub-directory under the given path.</p>
</blockquote>
<h2>Documents for API</h2>
<p>Basically the SageMaker SDK Estimator implements the <a href=""https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_CreateTrainingJob.html"" rel=""noreferrer"">CreateTrainingJob</a> API for training part. Hence, better to understand how it is designed and what parameters need to be defined. Otherwise working on Estimators are like walking in the dark.</p>
<hr />
<h1>Example</h1>
<h2>Jupyter Notebook</h2>
<pre><code>import sagemaker
from sagemaker import get_execution_role

sagemaker_session = sagemaker.Session()
role = get_execution_role()
bucket = sagemaker_session.default_bucket()

metric_definitions = [
    {&quot;Name&quot;: &quot;train:loss&quot;, &quot;Regex&quot;: &quot;.*loss: ([0-9\\.]+) - accuracy: [0-9\\.]+.*&quot;},
    {&quot;Name&quot;: &quot;train:accuracy&quot;, &quot;Regex&quot;: &quot;.*loss: [0-9\\.]+ - accuracy: ([0-9\\.]+).*&quot;},
    {
        &quot;Name&quot;: &quot;validation:accuracy&quot;,
        &quot;Regex&quot;: &quot;.*step - loss: [0-9\\.]+ - accuracy: [0-9\\.]+ - val_loss: [0-9\\.]+ - val_accuracy: ([0-9\\.]+).*&quot;,
    },
    {
        &quot;Name&quot;: &quot;validation:loss&quot;,
        &quot;Regex&quot;: &quot;.*step - loss: [0-9\\.]+ - accuracy: [0-9\\.]+ - val_loss: ([0-9\\.]+) - val_accuracy: [0-9\\.]+.*&quot;,
    },
    {
        &quot;Name&quot;: &quot;sec/sample&quot;,
        &quot;Regex&quot;: &quot;.* - \d+s (\d+)[mu]s/sample - loss: [0-9\\.]+ - accuracy: [0-9\\.]+ - val_loss: [0-9\\.]+ - val_accuracy: [0-9\\.]+&quot;,
    },
]

import uuid

checkpoint_s3_prefix = &quot;checkpoints/{}&quot;.format(str(uuid.uuid4()))
checkpoint_s3_uri = &quot;s3://{}/{}/&quot;.format(bucket, checkpoint_s3_prefix)

from sagemaker.tensorflow import TensorFlow

# --------------------------------------------------------------------------------
# 'trainingJobName' msut satisfy regular expression pattern: ^[a-zA-Z0-9](-*[a-zA-Z0-9]){0,62}
# --------------------------------------------------------------------------------
base_job_name = &quot;fashion-mnist&quot;
hyperparameters = {
    &quot;epochs&quot;: 2, 
    &quot;batch-size&quot;: 64
}
estimator = TensorFlow(
    entry_point=&quot;fashion_mnist.py&quot;,
    source_dir=&quot;src&quot;,
    metric_definitions=metric_definitions,
    hyperparameters=hyperparameters,
    role=role,
    input_mode='File',
    framework_version=&quot;2.3.1&quot;,
    py_version=&quot;py37&quot;,
    instance_count=1,
    instance_type=&quot;ml.m5.xlarge&quot;,
    base_job_name=base_job_name,
    checkpoint_s3_uri=checkpoint_s3_uri,
    model_dir=False
)
estimator.fit()
</code></pre>
<h2>fashion_mnist.py</h2>
<pre><code>import os
import argparse
import json
import multiprocessing

import numpy as np
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, Flatten, BatchNormalization
from tensorflow.keras.layers import Conv2D, MaxPooling2D
from tensorflow.keras.layers.experimental.preprocessing import Normalization
from tensorflow.keras import backend as K

print(&quot;TensorFlow version: {}&quot;.format(tf.__version__))
print(&quot;Eager execution is: {}&quot;.format(tf.executing_eagerly()))
print(&quot;Keras version: {}&quot;.format(tf.keras.__version__))


image_width = 28
image_height = 28


def load_data():
    fashion_mnist = tf.keras.datasets.fashion_mnist
    (x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()

    number_of_classes = len(set(y_train))
    print(&quot;number_of_classes&quot;, number_of_classes)

    x_train = x_train / 255.0
    x_test = x_test / 255.0
    x_full = np.concatenate((x_train, x_test), axis=0)
    print(x_full.shape)

    print(type(x_train))
    print(x_train.shape)
    print(x_train.dtype)
    print(y_train.shape)
    print(y_train.dtype)

    # ## Train
    # * C: Convolution layer
    # * P: Pooling layer
    # * B: Batch normalization layer
    # * F: Fully connected layer
    # * O: Output fully connected softmax layer

    # Reshape data based on channels first / channels last strategy.
    # This is dependent on whether you use TF, Theano or CNTK as backend.
    # Source: https://github.com/keras-team/keras/blob/master/examples/mnist_cnn.py
    if K.image_data_format() == 'channels_first':
        x = x_train.reshape(x_train.shape[0], 1, image_width, image_height)
        x_test = x_test.reshape(x_test.shape[0], 1, image_width, image_height)
        input_shape = (1, image_width, image_height)
    else:
        x_train = x_train.reshape(x_train.shape[0], image_width, image_height, 1)
        x_test = x_test.reshape(x_test.shape[0], image_width, image_height, 1)
        input_shape = (image_width, image_height, 1)

    return x_train, y_train, x_test, y_test, input_shape, number_of_classes

# tensorboard --logdir=/full_path_to_your_logs

validation_split = 0.2
verbosity = 1
use_multiprocessing = True
workers = multiprocessing.cpu_count()


def train(model, x, y, args):
    # SavedModel Output
    tensorflow_saved_model_path = os.path.join(args.model_dir, &quot;tensorflow/saved_model/0&quot;)
    os.makedirs(tensorflow_saved_model_path, exist_ok=True)

    # Tensorboard Logs
    tensorboard_logs_path = os.path.join(args.model_dir, &quot;tensorboard/&quot;)
    os.makedirs(tensorboard_logs_path, exist_ok=True)

    tensorboard_callback = tf.keras.callbacks.TensorBoard(
        log_dir=tensorboard_logs_path,
        write_graph=True,
        write_images=True,
        histogram_freq=1,  # How often to log histogram visualizations
        embeddings_freq=1,  # How often to log embedding visualizations
        update_freq=&quot;epoch&quot;,
    )  # How often to write logs (default: once per epoch)

    model.compile(
        optimizer='adam',
        loss=tf.keras.losses.sparse_categorical_crossentropy,
        metrics=['accuracy']
    )
    history = model.fit(
        x,
        y,
        shuffle=True,
        batch_size=args.batch_size,
        epochs=args.epochs,
        validation_split=validation_split,
        use_multiprocessing=use_multiprocessing,
        workers=workers,
        verbose=verbosity,
        callbacks=[
            tensorboard_callback
        ]
    )
    return history


def create_model(input_shape, number_of_classes):
    model = Sequential([
        Conv2D(
            name=&quot;conv01&quot;,
            filters=32,
            kernel_size=(3, 3),
            strides=(1, 1),
            padding=&quot;same&quot;,
            activation='relu',
            input_shape=input_shape
        ),
        MaxPooling2D(
            name=&quot;pool01&quot;,
            pool_size=(2, 2)
        ),
        Flatten(),  # 3D shape to 1D.
        BatchNormalization(
            name=&quot;batch_before_full01&quot;
        ),
        Dense(
            name=&quot;full01&quot;,
            units=300,
            activation=&quot;relu&quot;
        ),  # Fully connected layer
        Dense(
            name=&quot;output_softmax&quot;,
            units=number_of_classes,
            activation=&quot;softmax&quot;
        )
    ])
    return model


def save_model(model, args):
    # Save the model
    # A version number is needed for the serving container
    # to load the model
    version = &quot;00000000&quot;
    model_save_dir = os.path.join(args.model_dir, version)
    if not os.path.exists(model_save_dir):
        os.makedirs(model_save_dir)
    print(f&quot;saving model at {model_save_dir}&quot;)
    model.save(model_save_dir)


def parse_args():
    # --------------------------------------------------------------------------------
    # https://docs.python.org/dev/library/argparse.html#dest
    # --------------------------------------------------------------------------------
    parser = argparse.ArgumentParser()

    # --------------------------------------------------------------------------------
    # hyperparameters Estimator argument are passed as command-line arguments to the script.
    # --------------------------------------------------------------------------------
    parser.add_argument('--epochs', type=int, default=10)
    parser.add_argument('--batch-size', type=int, default=64)

    # /opt/ml/model
    # sagemaker.tensorflow.estimator.TensorFlow override 'model_dir'.
    # See https://sagemaker.readthedocs.io/en/stable/frameworks/tensorflow/\
    # sagemaker.tensorflow.html#sagemaker.tensorflow.estimator.TensorFlow
    parser.add_argument('--model_dir', type=str, default=os.environ['SM_MODEL_DIR'])

    # /opt/ml/output
    parser.add_argument(&quot;--output_dir&quot;, type=str, default=os.environ[&quot;SM_OUTPUT_DIR&quot;])

    args = parser.parse_args()
    return args


if __name__ == &quot;__main__&quot;:
    args = parse_args()
    print(&quot;---------- key/value args&quot;)
    for key, value in vars(args).items():
        print(f&quot;{key}:{value}&quot;)

    x_train, y_train, x_test, y_test, input_shape, number_of_classes = load_data()
    model = create_model(input_shape, number_of_classes)

    history = train(model=model, x=x_train, y=y_train, args=args)
    print(history)
    
    save_model(model, args)
    results = model.evaluate(x_test, y_test, batch_size=100)
    print(&quot;test loss, test accuracy:&quot;, results)
</code></pre>
<h2>SageMaker Console</h2>
<p><a href=""https://i.stack.imgur.com/ctcLy.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/ctcLy.png"" alt=""enter image description here"" /></a></p>
<h2>Notebook output</h2>
<pre><code>2021-09-03 03:02:04 Starting - Starting the training job...
2021-09-03 03:02:16 Starting - Launching requested ML instancesProfilerReport-1630638122: InProgress
......
2021-09-03 03:03:17 Starting - Preparing the instances for training.........
2021-09-03 03:04:59 Downloading - Downloading input data
2021-09-03 03:04:59 Training - Downloading the training image...
2021-09-03 03:05:23 Training - Training image download completed. Training in progress.2021-09-03 03:05:23.966037: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.
2021-09-03 03:05:23.969704: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:105] SageMaker Profiler is not enabled. The timeline writer thread will not be started, future recorded events will be dropped.
2021-09-03 03:05:24.118054: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.
2021-09-03 03:05:26,842 sagemaker-training-toolkit INFO     Imported framework sagemaker_tensorflow_container.training
2021-09-03 03:05:26,852 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)
2021-09-03 03:05:27,734 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:
/usr/local/bin/python3.7 -m pip install -r requirements.txt
WARNING: You are using pip version 21.0.1; however, version 21.2.4 is available.
You should consider upgrading via the '/usr/local/bin/python3.7 -m pip install --upgrade pip' command.

2021-09-03 03:05:29,028 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)
2021-09-03 03:05:29,045 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)
2021-09-03 03:05:29,062 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)
2021-09-03 03:05:29,072 sagemaker-training-toolkit INFO     Invoking user script

Training Env:

{
    &quot;additional_framework_parameters&quot;: {},
    &quot;channel_input_dirs&quot;: {},
    &quot;current_host&quot;: &quot;algo-1&quot;,
    &quot;framework_module&quot;: &quot;sagemaker_tensorflow_container.training:main&quot;,
    &quot;hosts&quot;: [
        &quot;algo-1&quot;
    ],
    &quot;hyperparameters&quot;: {
        &quot;batch-size&quot;: 64,
        &quot;epochs&quot;: 2
    },
    &quot;input_config_dir&quot;: &quot;/opt/ml/input/config&quot;,
    &quot;input_data_config&quot;: {},
    &quot;input_dir&quot;: &quot;/opt/ml/input&quot;,
    &quot;is_master&quot;: true,
    &quot;job_name&quot;: &quot;fashion-mnist-2021-09-03-03-02-02-305&quot;,
    &quot;log_level&quot;: 20,
    &quot;master_hostname&quot;: &quot;algo-1&quot;,
    &quot;model_dir&quot;: &quot;/opt/ml/model&quot;,
    &quot;module_dir&quot;: &quot;s3://sagemaker-us-east-1-316725000538/fashion-mnist-2021-09-03-03-02-02-305/source/sourcedir.tar.gz&quot;,
    &quot;module_name&quot;: &quot;fashion_mnist&quot;,
    &quot;network_interface_name&quot;: &quot;eth0&quot;,
    &quot;num_cpus&quot;: 4,
    &quot;num_gpus&quot;: 0,
    &quot;output_data_dir&quot;: &quot;/opt/ml/output/data&quot;,
    &quot;output_dir&quot;: &quot;/opt/ml/output&quot;,
    &quot;output_intermediate_dir&quot;: &quot;/opt/ml/output/intermediate&quot;,
    &quot;resource_config&quot;: {
        &quot;current_host&quot;: &quot;algo-1&quot;,
        &quot;hosts&quot;: [
            &quot;algo-1&quot;
        ],
        &quot;network_interface_name&quot;: &quot;eth0&quot;
    },
    &quot;user_entry_point&quot;: &quot;fashion_mnist.py&quot;
}

Environment variables:

SM_HOSTS=[&quot;algo-1&quot;]
SM_NETWORK_INTERFACE_NAME=eth0
SM_HPS={&quot;batch-size&quot;:64,&quot;epochs&quot;:2}
SM_USER_ENTRY_POINT=fashion_mnist.py
SM_FRAMEWORK_PARAMS={}
SM_RESOURCE_CONFIG={&quot;current_host&quot;:&quot;algo-1&quot;,&quot;hosts&quot;:[&quot;algo-1&quot;],&quot;network_interface_name&quot;:&quot;eth0&quot;}
SM_INPUT_DATA_CONFIG={}
SM_OUTPUT_DATA_DIR=/opt/ml/output/data
SM_CHANNELS=[]
SM_CURRENT_HOST=algo-1
SM_MODULE_NAME=fashion_mnist
SM_LOG_LEVEL=20
SM_FRAMEWORK_MODULE=sagemaker_tensorflow_container.training:main
SM_INPUT_DIR=/opt/ml/input
SM_INPUT_CONFIG_DIR=/opt/ml/input/config
SM_OUTPUT_DIR=/opt/ml/output
SM_NUM_CPUS=4
SM_NUM_GPUS=0
SM_MODEL_DIR=/opt/ml/model
SM_MODULE_DIR=s3://sagemaker-us-east-1-316725000538/fashion-mnist-2021-09-03-03-02-02-305/source/sourcedir.tar.gz
SM_TRAINING_ENV={&quot;additional_framework_parameters&quot;:{},&quot;channel_input_dirs&quot;:{},&quot;current_host&quot;:&quot;algo-1&quot;,&quot;framework_module&quot;:&quot;sagemaker_tensorflow_container.training:main&quot;,&quot;hosts&quot;:[&quot;algo-1&quot;],&quot;hyperparameters&quot;:{&quot;batch-size&quot;:64,&quot;epochs&quot;:2},&quot;input_config_dir&quot;:&quot;/opt/ml/input/config&quot;,&quot;input_data_config&quot;:{},&quot;input_dir&quot;:&quot;/opt/ml/input&quot;,&quot;is_master&quot;:true,&quot;job_name&quot;:&quot;fashion-mnist-2021-09-03-03-02-02-305&quot;,&quot;log_level&quot;:20,&quot;master_hostname&quot;:&quot;algo-1&quot;,&quot;model_dir&quot;:&quot;/opt/ml/model&quot;,&quot;module_dir&quot;:&quot;s3://sagemaker-us-east-1-316725000538/fashion-mnist-2021-09-03-03-02-02-305/source/sourcedir.tar.gz&quot;,&quot;module_name&quot;:&quot;fashion_mnist&quot;,&quot;network_interface_name&quot;:&quot;eth0&quot;,&quot;num_cpus&quot;:4,&quot;num_gpus&quot;:0,&quot;output_data_dir&quot;:&quot;/opt/ml/output/data&quot;,&quot;output_dir&quot;:&quot;/opt/ml/output&quot;,&quot;output_intermediate_dir&quot;:&quot;/opt/ml/output/intermediate&quot;,&quot;resource_config&quot;:{&quot;current_host&quot;:&quot;algo-1&quot;,&quot;hosts&quot;:[&quot;algo-1&quot;],&quot;network_interface_name&quot;:&quot;eth0&quot;},&quot;user_entry_point&quot;:&quot;fashion_mnist.py&quot;}
SM_USER_ARGS=[&quot;--batch-size&quot;,&quot;64&quot;,&quot;--epochs&quot;,&quot;2&quot;]
SM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate
SM_HP_BATCH-SIZE=64
SM_HP_EPOCHS=2
PYTHONPATH=/opt/ml/code:/usr/local/bin:/usr/local/lib/python37.zip:/usr/local/lib/python3.7:/usr/local/lib/python3.7/lib-dynload:/usr/local/lib/python3.7/site-packages

Invoking script with the following command:

/usr/local/bin/python3.7 fashion_mnist.py --batch-size 64 --epochs 2


TensorFlow version: 2.3.1
Eager execution is: True
Keras version: 2.4.0
---------- key/value args
epochs:2
batch_size:64
model_dir:/opt/ml/model
output_dir:/opt/ml/output
</code></pre>
<hr />
<h1>Model deployment to SageMaker Endpoint</h1>
<p>See <a href=""https://stackoverflow.com/questions/65168915/aws-sagemaker-how-to-load-trained-sklearn-model-to-serve-for-inference/65173663#65173663"">AWS SageMaker - How to load trained sklearn model to serve for inference?</a></p>
<hr />
<h1>Updates</h1>
<h2>Environment Variables</h2>
<ul>
<li><a href=""https://github.com/aws/sagemaker-training-toolkit/blob/master/ENVIRONMENT_VARIABLES.md"" rel=""noreferrer"">sagemaker-training-toolkit/ENVIRONMENT_VARIABLES.md</a></li>
</ul>
","4281353",6
1392,69035694,2,69020388,2021-09-02 18:51:45,2,"<p>The open-source version of FiftyOne is designed primarily for individual users. The best experience for multi-user collaboration is FiftyOne Teams. You can sign up here: <a href=""https://voxel51.com/#teams-form"" rel=""nofollow noreferrer"">https://voxel51.com/#teams-form</a></p>
<p><strong>About this error specifically</strong>:</p>
<p>On the backend, calling <code>fiftyone app launch --remote</code> in effect runs the following Python commands:</p>
<pre class=""lang-py prettyprint-override""><code>session = fo.launch_app(remote=True)
session.wait()
</code></pre>
<p>For remote sessions, the <a href=""https://voxel51.com/docs/fiftyone/api/fiftyone.core.session.html#fiftyone.core.session.Session.wait"" rel=""nofollow noreferrer""><code>session.wait()</code></a> call will block until something connects to it, and then will continue blocking until all connected tabs are closed.</p>
<p>There is a timeout built in to handle the case when the tab is refreshed so that the session is not immediately closed. In some cases, we have noticed that the refresh takes longer than the timeout, and sessions are closed prematurely. This is being looked into.</p>
<p><a href=""https://github.com/voxel51/fiftyone/pull/1245"" rel=""nofollow noreferrer"">The next release</a> provides an argument that will cause <code>wait</code> to block indefinitely. You will be able to call <code>fiftyone app launch --remote --wait 0</code>.</p>
<p>In the meantime, I would recommend writing and calling a small script (<code>launch_app.py</code>) to permanently block until it is exited.</p>
<pre class=""lang-py prettyprint-override""><code>import fiftyone as fo

session = fo.launch_app(remote=True)

# Indefinite blocking
while True:
    pass
</code></pre>
<pre class=""lang-sh prettyprint-override""><code>python launch_app.py
</code></pre>
","14277626",2
1393,69068380,2,68172002,2021-09-06 01:43:49,1,"<p>So I figured out how to do it by following the directions contained within this piece of Microsoft Documentation:
<a href=""https://learn.microsoft.com/en-us/azure/machine-learning/how-to-manage-rest"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/machine-learning/how-to-manage-rest</a></p>
<p>Specifically, it required performing two of the calls in the documentation;</p>
<ul>
<li>The first to get an AAD token using an Azure Service Principle that is authorised to access the Machine Learning Instance.</li>
</ul>
<blockquote>
<p>curl -X POST <a href=""https://login.microsoftonline.com/"" rel=""nofollow noreferrer"">https://login.microsoftonline.com/</a>/oauth2/token -d &quot;grant_type=client_credentials&amp;resource=https%3A%2F%2Fmanagement.azure.com%2F&amp;client_id=&amp;client_secret=&quot;</p>
</blockquote>
<ul>
<li>The second to use this token to trigger your pipeline from its rest endpoint. This one I had to figure out myself a little, but below is the basic structure I used.</li>
</ul>
<blockquote>
<p>curl -X POST {PIPELINE_REST_ENDPOINT} -H &quot;Authorisation:Bearer {AAD_TOKEN}&quot; -H &quot;Content-Type: application/json&quot; -d &quot;{&quot;ExperimentName&quot;: &quot;{EXPERIMENT_NAME}&quot;,&quot;ParameterAssignments&quot;: {}}&quot;</p>
</blockquote>
","15048097",0
1394,69068540,2,67713876,2021-09-06 02:23:51,0,"<p>This issue has been resolved by an update to Azure Machine Learning; You can now run pipelines with a flag set to &quot;Continue on Failure Step&quot;, which means that steps following the failed data export will continue to run.</p>
<p>This does mean you will need to design your pipeline to be able to handles upstream failures in its downstream modules; this must be done very carefully.</p>
","15048097",0
1395,69084257,2,69076270,2021-09-07 07:47:20,1,"<p>Problem Found:</p>
<p>So no fault of my own, I keep ensuring these fields are on their own lines in <code>classes.txt</code> and <code>Ctrl+S</code>. Then when I reopen the file, <strong>after runtime</strong>, it'll have fields be on the same line again.</p>
<p><a href=""https://i.stack.imgur.com/UbRTm.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/UbRTm.png"" alt=""enter image description here"" /></a></p>
<p>To fix this, on line <code>classes.write(uni_label)</code>.</p>
<p>I replaced it with <code>classes.write('\n'+uni_label)</code>.</p>
","16105404",1
1396,69101156,2,69000752,2021-09-08 10:14:20,1,"<p>You need to add support for more content types.</p>
<p>Since you would like to pass a string or a parameter, I suggest you add support for &quot;application/json&quot; MIME media type (<a href=""https://stackoverflow.com/questions/477816/what-is-the-correct-json-content-type"">What is the correct JSON content type?</a>). Then your users will call the API with a Json that you can parse and extract parameters from in the backend.</p>
<p>For example, if you have two parameters <code>age</code> and <code>gender</code> you want to pass to your model. You can put them in the following Json datastructure:</p>
<pre class=""lang-json prettyprint-override""><code>{
 &quot;age&quot;: ...,
 &quot;gender&quot;: ...
}
</code></pre>
<p>Then add support for loading the Json and extracting the parameters in the backend as follows:</p>
<pre class=""lang-py prettyprint-override""><code>if flask.request.content_type == &quot;application/json&quot;:
    data = flask.request.data.decode(&quot;utf-8&quot;)
    data = json.loads(data)
    parameter1 = data['age']
    parameter2 = data['gender']
    ...
</code></pre>
","1762211",0
1397,69117607,2,69104302,2021-09-09 11:45:21,0,"<p>The solution to the problem was to check the CloudWatch Log events under <code>CloudWatch -&gt; Log groups -&gt; /aws/sagemaker/NotebookInstances -&gt; aws-sm-notebook-instance/LifecycleConfigOnCreate</code> to find the following error-message:</p>
<pre><code>/bin/bash: /tmp/OnCreate_2021-09-08-12-24rw5al34g: /bin/bash^M: bad interpreter: No such file or directory
</code></pre>
<p>A bit of internet research brought me to <a href=""https://askubuntu.com/questions/304999/not-able-to-execute-a-sh-file-bin-bashm-bad-interpreter/305001#305001"">this solution related to newline characters in shell-scripts</a>, which depend on whether you are on <code>Windows</code> or a <code>UNIX</code>-system.
As I'm working on Windows, the shell-scripts created in VS-Code comprised dos-specific <code>CRLF</code> newline-handling, which could be resolved via the button on the bottom-right in <code>VS-Code</code> to switch the <em>carriage return</em> (CRLF) character to the <em>line feed</em> (LF) character used by UNIX.</p>
<p>As the compute instance employed by AWS Sagemaker is a Linux-system, it cannot handle the dos-style CRLF newline-characters in the shell-scripts and this &quot;adds&quot; a <code>^M</code> after <code>/bin/bash</code> which obviously leads to an error as such an interpreter does not exist.</p>
<p>So, finally <code>terraform apply</code> worked out well:</p>
<pre><code>$ terraform apply
...
...
aws_sagemaker_notebook_instance.notebook_instance: Still creating... [7m30s elapsed]
aws_sagemaker_notebook_instance.notebook_instance: Still creating... [7m40s elapsed]
aws_sagemaker_notebook_instance.notebook_instance: Creation complete after 7m43s [id=aws-sm-notebook-instance]

Apply complete! Resources: 1 added, 1 changed, 1 destroyed.
</code></pre>
","12298276",0
1398,69131245,2,69115832,2021-09-10 11:03:17,4,"<p>So I managed to find an answer to this and wanted to leave what I found here in case anyone else stumbles onto a similar problem.</p>
<p>It turns out my feelings around the error were correct and the solution did indeed lie in how the tf.Dataset object was presented.</p>
<p>This can be demonstrated when I ran some code which simulated the incoming data using randomly generated tensors.</p>
<pre><code>tensors = [tf.random.uniform(shape = (1, 82)) for i in range(739)]
# This gives us a list of 739 tensors which hold 1 value for 82 'features' simulating the dataset I had

dataset = tf.data.Dataset.from_tensor_slices(tensors)
dataset = dataset.map(lambda x : (x, x))
# This returns a dataset which marks the training set and target as the same
# which is what the Autoecnoder model is looking for

model.fit(dataset ...) 
</code></pre>
<p>Following this I proceeded to do the same thing with the dataset returned by the _input_fn. Given that the tfx DataAccessor object returns a features_dict however I needed to combine the tensors in that dict together to create a single tensor.</p>
<p>This is how my _input_fn looks now:</p>
<pre><code>def create_target_values(features_dict: Dict[str, tf.Tensor]) -&gt; tuple:
    value_tensor = tf.concat(list(features_dict.values()), axis = 1)
    return (features_dict, value_tensor)

def _input_fn(
    file_pattern,
    data_accessor: tfx.components.DataAccessor,
    tf_transform_output: tft.TFTransformOutput,
    batch_size: int) -&gt; tf.data.Dataset:
    &quot;&quot;&quot;
    Generates features and label for tuning/training.
      Args:
        file_pattern: List of paths or patterns of input tfrecord files.
        data_accessor: DataAccessor for converting input to RecordBatch.
        tf_transform_output: A TFTransformOutput.
        batch_size: representing the number of consecutive elements of returned
          dataset to combine in a single batch
      Returns:
        A dataset that contains (features, target_tensor) tuple where features is a
          dictionary of Tensors, and target_tensor is a single Tensor that is a concatenated tensor of all the
          feature values.
    &quot;&quot;&quot;
    dataset = data_accessor.tf_dataset_factory(
        file_pattern,
        tfxio.TensorFlowDatasetOptions(batch_size = batch_size),
        tf_transform_output.transformed_metadata.schema
    )
    
    dataset = dataset.map(lambda x: create_target_values(features_dict = x))
    return dataset.repeat()
</code></pre>
","16861356",3
1399,69170605,2,69068520,2021-09-14 00:52:58,3,"<p>After speaking with Microsoft Support, it turns out this error was a platform error introduced in a recent update of Azure ML. Their product team are currently investigating a solution.</p>
<p>As a temporary fix, if you see this issue, you can try switching between using your personal endpoint and the generic regional endpoint; In this case, the error was only introduced for using personal endpoints. The endpoints in question have the following formats:</p>
<ul>
<li>Personal: <code>https://&lt;COGNITIVE-SERVICES-INSTANCE&gt;.cognitiveservices.azure.com/</code></li>
<li>Regional: <code>https://&lt;REGION&gt;.api.cognitive.microsoft.com/</code></li>
</ul>
","15048097",1
1400,69171762,2,69126555,2021-09-14 04:21:08,1,"<p>I think I find sort of a workaround/solution for this for now, but I think this issue needs to be addressed in MLFloow anyways.</p>
<p>What I did is not the best way probably.
I used a python package called <a href=""https://scikeras.readthedocs.io/en/latest/"" rel=""nofollow noreferrer"">scikeras</a> that does this wrapping and then could log the model</p>
<p>The code:</p>
<pre><code>import scikeras 
import tensorflow as tf 
from tensorflow.keras.models import Sequential 
from tensorflow.keras.layers import Input, Dense, Dropout, LSTM, Flatten, Activation 
 
from scikeras.wrappers import KerasClassifier 
  
 
class ModelWrapper(mlflow.pyfunc.PythonModel): 
    def __init__(self, model): 
        self.model = model 
 
    def predict(self, context, model_input): 
        return self.model.predict(model_input) 
 
conda_env =  _mlflow_conda_env( 
      additional_conda_deps=None, 
      additional_pip_deps=[ 
        &quot;cloudpickle=={}&quot;.format(cloudpickle.__version__),  
        &quot;scikit-learn=={}&quot;.format(sklearn.__version__), 
        &quot;numpy=={}&quot;.format(np.__version__), 
        &quot;tensorflow=={}&quot;.format(tf.__version__), 
        &quot;scikeras=={}&quot;.format(scikeras.__version__), 
      ], 
      additional_conda_channels=None, 
  ) 
 
param = { 
   &quot;dense_l1&quot;: 20, 
   &quot;dense_l2&quot;: 20, 
   &quot;optimizer__learning_rate&quot;: 0.1, 
   &quot;optimizer&quot;: &quot;Adam&quot;, 
   &quot;loss&quot;:&quot;binary_crossentropy&quot;, 
} 
 
  
def create_model(dense_l1, dense_l2, meta): 
  
  n_features_in_ = meta[&quot;n_features_in_&quot;] 
  X_shape_ = meta[&quot;X_shape_&quot;] 
  n_classes_ = meta[&quot;n_classes_&quot;] 
 
  model = Sequential() 
  model.add(Dense(n_features_in_, input_shape=X_shape_[1:], activation=&quot;relu&quot;)) 
  model.add(Dense(dense_l1, activation=&quot;relu&quot;)) 
  model.add(Dense(dense_l2, activation=&quot;relu&quot;)) 
  model.add(Dense(1, activation=&quot;sigmoid&quot;)) 
 
  return model   
 
mlflow.sklearn.autolog() 
with mlflow.start_run(run_name=&quot;sample_run&quot;): 
 
  classfier = KerasClassifier( 
    create_model, 
    loss=param[&quot;loss&quot;], 
    dense_l1=param[&quot;dense_l1&quot;], 
    dense_l2=param[&quot;dense_l2&quot;], 
    optimizer__learning_rate = param[&quot;optimizer__learning_rate&quot;], 
    optimizer= param[&quot;optimizer&quot;], 
) 
 
  # fit the pipeline 
  clf = Pipeline(steps=[('preprocessor', preprocessor), 
                      ('estimator', classfier)])   
 
  h = clf.fit(X_train, y_train.values) 
  # log scores 
  acc_score = clf.score(X=X_test, y=y_test) 
  mlflow.log_metric(&quot;accuracy&quot;, acc_score) 
  signature = infer_signature(X_test, clf.predict(X_test)) 
  model_nn = ModelWrapper(clf,)  
 
  mlflow.pyfunc.log_model( 
      python_model= model_nn, 
      artifact_path = &quot;model&quot;,  
      signature = signature,  
      conda_env = conda_env 
  ) 
</code></pre>
","5937757",0
1401,69184844,2,69158205,2021-09-14 21:46:38,4,"<p>The question isn't clear but let me explain this point.</p>
<p>When you launch a Glue Development endpoint you can attach either a SageMaker notebook or Zeppelin notebook. Both will be created and configured by Glue and your script will be executed on the Glue Dev endpoint.</p>
<p><strong>If your question is &quot;what is the difference between a SageMaker notebook created from Glue console and a SageMaker notebook created from SageMaker console?</strong></p>
<p>When you create a notebook instance from Glue console, the created notebook will always have public internet access enabled. <a href=""https://aws.amazon.com/blogs/machine-learning/understanding-amazon-sagemaker-notebook-instance-networking-configurations-and-advanced-routing-options/"" rel=""nofollow noreferrer"">This blog</a> explains the difference between the networking configurations with SM notebooks. You cannot also create the notebook with a specific disk size but you can stop the notebook once it's created and increase disk size.</p>
<p><strong>If your question is &quot;what is the difference between SageMaker notebook and Zeppelin notebooks?&quot;</strong></p>
<p>The answer is the first one used Jupter (very popular) while the second one uses Zeppelin.</p>
<p><strong>If your question is &quot;what is the difference between using only a SageMaker notebook versus using SM notebook + Glue dev Endpoint?&quot;</strong></p>
<p>The answer is: if you are running normal pandas + numpy without using Spark, SM notebook is much cheaper (if you use small instance type and if your data is relatively small). However, if you are trying to process a large dataset and you are planning to use spark, then SM notebook + Glue Dev endpoint will be the best option to develop the job which will be executed later as a Glue Job (transformation job) (server less).</p>
<p>SM notebook is like running python code on an EC2 instance versus SM notebook + Glue which is used to develop ETL jobs which you can launch to process deltas.</p>
","4391011",1
1402,69224163,2,67062145,2021-09-17 13:21:05,2,"<p>You can pass the run_id directly to <code>start_run</code>:</p>
<pre class=""lang-py prettyprint-override""><code>mlflow.start_run(experiment_id=1,
                 run_name=x,
                 run_id=&lt;run_id_of_interrupted_run&gt; # pass None to start a new run
                 ) 
</code></pre>
<p>Of course, you have to store the run_id for this. You can get it with <a href=""https://mlflow.org/docs/latest/python_api/mlflow.entities.html#mlflow.entities.RunInfo.run_id"" rel=""nofollow noreferrer""><code>run.info.run_id</code></a></p>
","727341",0
1403,69248456,2,69203143,2021-09-20 02:13:08,2,"<p>Currently, GCP don't support A2 Machine type for normal KF Components. A potential workaround right now is to use <a href=""https://cloud.google.com/vertex-ai/docs/training/create-custom-job"" rel=""nofollow noreferrer""><strong>GCP custom job component</strong></a> that you can explicitly specify the machine type.</p>
","5638018",0
1404,69256523,2,69254978,2021-09-20 14:52:13,1,"<p>You can use Amazon Forecast, which has ARIMA <a href=""https://docs.aws.amazon.com/forecast/latest/dg/aws-forecast-recipe-arima.html"" rel=""nofollow noreferrer"">built in</a></p>
<p>Or, if you prefer SageMaker, you need to build your own Docker container, publish it to ECR, and then use that</p>
<p><a href=""https://sagemaker-examples.readthedocs.io/en/latest/advanced_functionality/scikit_bring_your_own/scikit_bring_your_own.html#The-example"" rel=""nofollow noreferrer"">https://sagemaker-examples.readthedocs.io/en/latest/advanced_functionality/scikit_bring_your_own/scikit_bring_your_own.html#The-example</a></p>
","223478",0
1405,69265212,2,69265000,2021-09-21 07:49:25,3,"<p>Could you please run <code>dvc doctor</code> and rerun <code>dvc push</code> and add <code>-vv</code> flag. And give the two results?</p>
<pre><code>PermissionError: The AWS Access Key Id you provided does not exist in our records.
</code></pre>
<p>Does the <code>aws cli</code> works correctly for you? First setup <code>AWS_ACCESS_KEY_ID</code> and <code>AWS_SECRET_ACCESS_KEY</code> in envs then</p>
<pre><code>aws s3 ls s3://mybucket/dvcstore
</code></pre>
","5357932",4
1406,69313548,2,69099627,2021-09-24 10:32:40,3,"<p>The issue rooted in the model version used for the xgboost framework. from 1.3.0 on the default output changed from pickle to json and the sagemaker documentation does not seem to have been updated accordingly. So if you want to read the model via</p>
<pre><code>    tar.extractall(path=&quot;.&quot;)

model = pkl.load(open(&quot;xgboost-model&quot;, &quot;rb&quot;))
</code></pre>
<p>as described in the sagemaker docs, you need to import the XGBOOST framework with with a former version, e.g. 1.2.1.</p>
","11074378",2
1407,69366802,2,69338516,2021-09-28 18:26:47,0,"<p>This official AWS <a href=""https://aws.amazon.com/blogs/machine-learning/speed-up-yolov4-inference-to-twice-as-fast-on-amazon-sagemaker/"" rel=""nofollow noreferrer"">blog post</a> has information on how to deploy YOLOv4. I wonder if you can use it as a guide and change the model to v5.</p>
<p>If not, there is a 3rd party implementation of YOLOv5 <a href=""https://github.com/HKT-SSA/yolov5-on-sagemaker"" rel=""nofollow noreferrer"">here</a>.</p>
","6509765",0
1408,69399066,2,69245175,2021-09-30 21:36:16,1,"<p>So a few learnings on this interacting with the stellar Azure Machine Learning engineering team.</p>
<ol>
<li>When calling the <code>read_delimited_files()</code> method, ensure that the output folder does not have many inputs or files. For example, if all intermediate outputs are saved to a common folder, it may read all the prior inputs into this folder, and depending upon the shape of the data, borrow the schema from the first file, or confuse all of them together. This can lead to inconsistencies and errors. In my case, I was dumping many files to the same location, hence this was causing confusion for this method. The fix is either to distinctly mark the output folder (e.g. with a UUID) or give different paths.</li>
<li>The dataframe from <code>read_delimiter_files()</code> may treat all columns as object type which can lead to a data type check failure (i.e. label_column needs to be numeric). To mitigate, explictly state the type. For example:</li>
</ol>
<pre class=""lang-py prettyprint-override""><code>from azureml.data import DataType
prepped_data = prepped_data.read_delimited_files(set_column_types={&quot;Place&quot;:DataType.to_float()})
</code></pre>
","3728562",0
1409,69400605,2,69179914,2021-10-01 04:56:18,4,"<p>This happens, during the ground-truth-merge job, when the spark can't find any data either in '/opt/ml/processing/groundtruth/' or '/opt/ml/processing/input_data/' directories. And that can happen when either you haven't sent any requests to the sagemaker endpoint or there are no ground truths.</p>
<p>I got this error because, the folder <code>/opt/ml/processing/input_data/</code> of the docker volume mapped to the monitoring container had no data to process. And that happened because, the thing that facilitates entire process, including fetching data couldn't find any in S3. and that happened because, there was an extra slash(<code>/</code>) in the directory to which endpoint's captured-data will be saved. to elaborate, while creating the endpoint, I had mentioned the directory as <code>s3://&lt;bucket-name&gt;/&lt;folder-1&gt;/</code>, while it should have just been <code>s3://&lt;bucket-name&gt;/&lt;folder-1&gt;</code>. so, while the thing that copies data from S3 to docker volume tried to fetch data of that hour, the directory it tried to extract the data from was <code>s3://&lt;bucket-name&gt;/&lt;folder-1&gt;//&lt;endpoint-name&gt;/&lt;variant-name&gt;/&lt;year&gt;/&lt;month&gt;/&lt;date&gt;/&lt;hour&gt;</code>(notice the two slashes). So, when I created the endpoint-configuration again with the slash removed in S3 directory, this error wasn't present and ground-truth-merge operation was successful as part of model-quality-monitoring.</p>
<p><em>I am answering this question because, someone read the question and upvoted it. meaning, someone else has faced this problem too. so, I have mentioned what worked for me. And I wrote this, so that StackExchange doesn't think I am spamming the forum with questions.</em></p>
","11814996",0
1410,69488921,2,69444523,2021-10-07 23:40:33,0,"<p>You can download a numpy array from_files method of dataset similar to shown below</p>
<pre><code> def file_download(path):
            dataset = Dataset.File.from_files(path=(self.datastore,self.train_data_outputs_folder+&quot;/&quot;+ path))
            datafolder = os.path.join(os.getcwd(), 'numpy_file')
            dataset.download(datafolder, overwrite=True)
</code></pre>
<p>the essence is that you got to download the files to your local repo. you can use that using the download method.</p>
","5663073",1
1411,69489370,2,64810620,2021-10-08 01:07:52,2,"<p>With Keras models you have to load the model first using the base model path, then you can continue training from there instead of building a new model.</p>
<p>Your Trainer component looks correct, but in <code>run_fn</code> do the following instead:</p>
<pre class=""lang-py prettyprint-override""><code>def run_fn(fn_args: FnArgs):
  model = tf.keras.models.load_model(fn_args.base_model)
  model.fit(train_dataset, steps_per_epoch=fn_args.train_steps, validation_data=eval_dataset,
              validation_steps=fn_args.eval_steps, callbacks=[tensorboard_callback],
              epochs=5)
</code></pre>
","983464",0
1412,69492489,2,69466354,2021-10-08 08:04:08,0,"<p>I found the solution of this issue. It is a tricky problem due to spanish characters, my system's user profile in &quot;C:/&quot; is &quot;fcañizares&quot; (Cañizares is my first last name). I have created another user named &quot;fcanizares&quot; and all is working fine. Hope you find this solution helpfull.</p>
<p>PS: Moral of the issue, get rid of the extrange characters!</p>
","12841052",0
1413,69498893,2,69498670,2021-10-08 16:16:25,1,"<p><strong>AWS IAM is a free service</strong> - you do not get charged for roles, policies or any other aspect of IAM.</p>
<p>From <a href=""https://aws.amazon.com/iam/#:%7E:text=IAM%20is%20a%20feature%20of,AWS%20services%20by%20your%20users."" rel=""nofollow noreferrer"">the documentation</a>:</p>
<blockquote>
<p>IAM is a feature of your AWS account offered at no additional charge. You will be charged only for use of other AWS services by your users.</p>
</blockquote>
","4800344",2
1414,69500037,2,69499960,2021-10-08 18:04:12,0,"<p>Ended up fixing the issue by adding <code>--content-type text/csv</code> and using base64 and it worked like a charm.</p>
","12576581",1
1415,69520695,2,67352949,2021-10-11 03:23:01,2,"<p>here's how to combine hyperparameters with an AML pipeline: <a href=""https://learn.microsoft.com/en-us/python/api/azureml-pipeline-steps/azureml.pipeline.steps.hyperdrivestep?view=azure-ml-py"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/python/api/azureml-pipeline-steps/azureml.pipeline.steps.hyperdrivestep?view=azure-ml-py</a></p>
<p>Alternatively, here's a sample notebook: <a href=""https://github.com/Azure/MachineLearningNotebooks/blob/master/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-parameter-tuning-with-hyperdrive.ipynb"" rel=""nofollow noreferrer"">https://github.com/Azure/MachineLearningNotebooks/blob/master/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-parameter-tuning-with-hyperdrive.ipynb</a></p>
","534681",0
1416,69524054,2,69489759,2021-10-11 09:36:30,0,"<p>Kubeflow job is just a Kubernetes job, thus you are limited with Kubernetes job entrypoint being a single command.
However you can still chain multiple commands into a single <code>sh</code> command:</p>
<pre class=""lang-sh prettyprint-override""><code>sh -c &quot;echo 'my first job' &amp;&amp; echo 'my second job'&quot;
</code></pre>
<p>So that you kubeflow command can be:</p>
<pre class=""lang-py prettyprint-override""><code>command = [
'/bin/sh', '-c', '/home/jovyan/deploy.sh &amp;&amp; python /home/jovyan/test.py'
]
</code></pre>
","4986644",0
1417,69524376,2,69479488,2021-10-11 10:01:04,0,"<p>Fortunately, I have been resolved my problem. I list some solutions for the same error which can help you in the future if you face the same problem.</p>
<ol>
<li>File names. The file names should be the same suggested in MLFlow docs <code>https://mlflow.org/ </code>. For example not <code>conda.yamp</code>, but <code>conda.yaml</code>, as there was such problem in <code>https://github.com/mlflow/mlflow/issues/3856</code></li>
<li>The <code>conda.yaml</code> file does not support Tab, please consider using spaces instead</li>
<li>In the MLProject file name 'P' should be the upper case before MLFlow 1.4. But the later versions it does not matter as explained there <code>https://github.com/mlflow/mlflow/issues/1094</code></li>
<li>(In my case) MLProject file is space sensitive. Let the <code> https://github.com/mlflow/mlflow/tree/master/examples</code> GitHub examples guide you.</li>
</ol>
","13018302",0
1418,69531588,2,69526483,2021-10-11 19:35:41,1,"<p>The best solution that comes to my mind is <a href=""https://python-poetry.org/"" rel=""nofollow noreferrer"">Poetry</a>.
It automatically creates the folder structure like a python package.</p>
<p>Folder structure <a href=""https://i.stack.imgur.com/losG5.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/losG5.png"" alt=""this"" /></a></p>
<p>Poerty creates a project.toml file when project is initialized. This can serve as requirement.txt file for production.You can add production and development package separately in this file using command line or editing the file.</p>
<p>Project.toml
<a href=""https://i.stack.imgur.com/JbuSI.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/JbuSI.png"" alt=""this"" /></a></p>
<p>It also creates separate environment for the project which helps in minimizing the conflict with other project.</p>
","17116349",2
1419,69533935,2,69522401,2021-10-12 00:57:13,3,"<p>Assuming the <code>az ml</code> command returns a json array string and you want to
check if the array includes the value of variable <code>SERVNAME</code>, would you
please try:</p>
<pre><code>SERVNAME=&quot;ner&quot;
SERVICE='[ &quot;ner&quot;, &quot;aks-gpu-ner-0306210907&quot;, &quot;aks-gpu-ner-30012231&quot;, &quot;aks-gpu-ner-1305211336&quot;]'

if [[ $SERVICE =~ &quot;\&quot;$SERVNAME\&quot;&quot; ]]; then
    echo &quot;Service Found&quot;
    # put your command here to update the service
else
    echo &quot;Service Not Found&quot;
    # put your command here to deploy new service
fi
</code></pre>
<p>The regex operator <code>$SERVICE =~ &quot;\&quot;$SERVNAME\&quot;&quot;</code> matches if the string <code>$SERVICE</code>
contains the substring <code>$SERVNAME</code> enclosed with double quotes.</p>
<p>If <code>jq</code> is available, you could also say:</p>
<pre><code>result=$(echo &quot;$SERVICE&quot; | jq --arg var &quot;$SERVNAME&quot; '. | index($var)')
if [[ $result != &quot;null&quot; ]]; then
    echo &quot;Service Found&quot;
else
    echo &quot;Service Not Found&quot;
fi
</code></pre>
","8572380",6
1420,69562452,2,69561386,2021-10-13 21:20:31,1,"<p>Ok, through a lot of trial-and-error I was able to come up with two ways of acquiring a token that allows me to hit my Azure Machine Learning Pipeline Endpoint through the REST api.  One uses Microsoft.Identity.Client &amp; one uses Azure.Identity.</p>
<pre><code>using Microsoft.Identity.Client;

...

public static async Task&lt;string&gt; GetAccessToken()
{
      var clientId = Environment.GetEnvironmentVariable(&quot;AZURE_CLIENT_ID&quot;);
      var clientSecret = Environment.GetEnvironmentVariable(&quot;AZURE_CLIENT_SECRET&quot;);
      var tenantId = Environment.GetEnvironmentVariable(&quot;AZURE_TENANT_ID&quot;);

   
      var app = ConfidentialClientApplicationBuilder.Create(clientId)
                                                .WithClientSecret(clientSecret)                                                
                                                .WithAuthority(AzureCloudInstance.AzurePublic, tenantId)
                                                .Build();
      var result = await app.AcquireTokenForClient(new string[] { &quot;https://ml.azure.com/.default&quot; }).ExecuteAsync();
      return result.AccessToken;
}
</code></pre>
<p>Or:</p>
<pre><code>using Azure.Identity;
...

public static async Task&lt;string&gt; GetAccessToken()
{
      var clientId = Environment.GetEnvironmentVariable(&quot;AZURE_CLIENT_ID&quot;);
      var clientSecret = Environment.GetEnvironmentVariable(&quot;AZURE_CLIENT_SECRET&quot;);
      var tenantId = Environment.GetEnvironmentVariable(&quot;AZURE_TENANT_ID&quot;);


      var cred = new ClientSecretCredential(tenantId, clientId, clientSecret);
      var token =  await cred.GetTokenAsync(new Azure.Core.TokenRequestContext(new string[] { &quot;https://ml.azure.com/.default&quot; }));
      return token.Token;
}
</code></pre>
","1486998",0
1421,69615647,2,69613815,2021-10-18 11:57:00,0,"<p>The tokenizer needs to be installed and imported in any case:</p>
<pre><code>pip install transformers
pip install sentencepiece
</code></pre>
<p>Then the tokenizer needs to be passed the following way:</p>
<pre class=""lang-py prettyprint-override""><code>tokenizer = M2M100Tokenizer.from_pretrained(&quot;facebook/m2m100_1.2B&quot;)
predictor.predict({
    'inputs': &quot;The answer to the universe is&quot;,
    'parameters': {
        'forced_bos_token_id': tokenizer.get_lang_id(&quot;it&quot;)
    }
})
</code></pre>
","2153636",0
1422,69632538,2,60017893,2021-10-19 14:16:58,0,"<p>Tested again 1.5 years later, and a <code>pip install vowpalwabbit</code> works fine on notebook instance. In training job, adding vowpalwabbit in a <code>requirements.txt</code> send to an AWS-managed Scikit learn container (<code>141502667606.dkr.ecr.eu-west-1.amazonaws.com/sagemaker-scikit-learn:0.23-1-cpu-py3</code>) also installs successfully. Both tested with vowpalwabbit-8.11.0</p>
","5331834",0
1423,69639579,2,69587252,2021-10-20 02:20:12,0,"<p>Ok I found the answer. Because the model is expecting the <code>input_1</code> name, then in <code>_get_serve_image_fn</code>, I need to create the dictionary key, such as:</p>
<pre><code>def _get_serve_image_fn(model, tf_transform_output):
  &quot;&quot;&quot;Returns a function that feeds the input tensor into the model.&quot;&quot;&quot;
  model.tft_layer = tf_transform_output.transform_features_layer()
  @tf.function
  def serve_image_fn(serialized_tf_examples):
    
    feature_spec = tf_transform_output.raw_feature_spec()
    feature_spec.pop(_LABEL_KEY)
    parsed_features = tf.io.parse_example(serialized_tf_examples, feature_spec)

    transformed_features = model.tft_layer(parsed_features)
    transformed_features[model.get_layer('mobilenet_1.00_224').layers[0].name] = transformed_features[_transformed_name(_IMAGE_KEY)]
    del transformed_features[_transformed_name(_IMAGE_KEY)]
    
    return model(transformed_features)

  return serve_image_fn
</code></pre>
","983464",0
1424,69648900,2,69227917,2021-10-20 15:29:11,1,"<p>Just add the below environment variables:</p>
<pre><code>export AWS_ACCESS_KEY_ID=&lt;your-aws-access-key-id&gt;
export AWS_SECRET_ACCESS_KEY = &lt;your-aws-secret-access-key&gt;
</code></pre>
","848510",0
1425,69667732,2,69666500,2021-10-21 19:24:58,0,"<p>The problem was most likely with templates for sagemaker that was runned before creating the instance.</p>
","3079439",0
1426,69675195,2,66850856,2021-10-22 10:20:21,1,"<p>The “score.py” exposed in trained_model_outputs is for customized deployment, only have model init and scoring logic, user can add their own pre-process and post-process code on top of that.
The scoring logic that having pre-process logic is available through Designer deployed web service, which can be on both AKS and ACI. You can follow this doc: Tutorial: <a href=""https://review.learn.microsoft.com/en-us/azure/machine-learning/tutorial-designer-automobile-price-deploy?branch=release-build-2021-azureml"" rel=""nofollow noreferrer"">Deploy ML models with the designer - Azure Machine Learning | Microsoft Docs</a>.</p>
","11297406",0
1427,69705201,2,69703225,2021-10-25 08:54:16,2,"<p>Yes, you can do that by passing the <code>-p port_number</code> command-line switch when starting MLflow server (see <a href=""https://mlflow.org/docs/latest/cli.html#cmdoption-mlflow-server-p"" rel=""nofollow noreferrer"">docs</a>). Please note, that to be able to use ports below 1024, the server needs to be run as root.</p>
","18627",0
1428,69729133,2,69685819,2021-10-26 19:48:29,0,"<p>I was able to get this to work by using AWS:Include and Fn:Transform and storing my environment variables as json in passed s3 file.</p>
<p>My cfn template looks like:</p>
<pre><code>Resources:
  SageMakerModel:
    Type: 'AWS::SageMaker::Model'
    Properties:
      ExecutionRoleArn: 
        Ref: ExecutionRoleArn
      EnableNetworkIsolation: false
      PrimaryContainer:
        Environment:
          Fn::Transform:
            Name: AWS::Include
            Parameters:
              Location: &lt;your S3 file&gt;
        Image: 
          Ref: ImageURI
</code></pre>
<p>My s3 file looks like:</p>
<pre><code>{
  &quot;REQUEST_KEEP_ALIVE_TIME_SEC&quot;: &quot;90&quot;
}
</code></pre>
","2408399",0
1429,69737444,2,69725612,2021-10-27 11:01:01,1,"<p>DVC has few &quot;levels&quot; of config, that can be controlled with proper flag:</p>
<ul>
<li><code>--local</code> - repository level, ignored by git by default - designated for project-scope, sensitive data</li>
<li>project - same as above, not ignored - designated to specify non-sensitive data (it is the default)</li>
<li><code>--global</code> / <code>--system</code> - for common config for more repositories.</li>
</ul>
<p>More information can be found in the <a href=""https://dvc.org/doc/command-reference/config#description"" rel=""nofollow noreferrer"">docs</a>.</p>
","3406563",1
1430,69757368,2,69737649,2021-10-28 15:59:25,0,"<p>you can see daily (or even hourly if you opt-in) cost and usage by instance type with:<br />
<code>aws ce get-cost-and-usage --time-period Start=2021-10-26,End=2021-10-27 --granularity DAILY --metrics &quot;UsageQuantity&quot; &quot;BlendedCost&quot; --group-by Type=DIMENSION,Key=INSTANCE_TYPE</code><br />
Note that SageMaker instance types names starts with: <code>ml.*</code></p>
<p>To view things in the finest resolution you'll need to produce detailed billing reports (DBR): <a href=""https://docs.aws.amazon.com/cur/latest/userguide/detailed-billing.html"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/cur/latest/userguide/detailed-billing.html</a><br />
It will generates CSV reports in S3, which you could query with Athena using SQL: <a href=""https://docs.aws.amazon.com/cur/latest/userguide/cur-query-athena.html"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/cur/latest/userguide/cur-query-athena.html</a></p>
","121956",0
1431,69757927,2,69678393,2021-10-28 16:38:12,0,"<p>I adjusted your Dockerfile and it builds successfully for me.</p>
<pre><code>FROM jupyter/scipy-notebook
ARG defaultuser=jovyan
USER root
ENV DEBIAN_FRONTEND noninteractive
RUN apt-get update &amp;&amp; \
    apt-get -y install gcc mono-mcs &amp;&amp; \
    rm -rf /var/lib/apt/lists/*
USER $defaultuser

RUN pip3 install sagemaker-training

COPY train.py /opt/ml/code/train.py

ENV SAGEMAKER_PROGRAM train.py
</code></pre>
<p>(I had to adjust for the fact that the default user from the base container isn't root, when installing GCC)</p>
","121956",0
1432,69763741,2,69757539,2021-10-29 04:44:14,0,"<p>The schema of request body for a zero-shot classification model is defined in this <a href=""https://github.com/huggingface/notebooks/blob/772ddf7140bccc443da265c90c95eda99e69c564/sagemaker/10_deploy_model_from_s3/deploy_transformer_model_from_s3.ipynb"" rel=""nofollow noreferrer"">link</a>.</p>
<pre><code>{
    &quot;inputs&quot;: &quot;Hi, I recently bought a device from your company but it is not working as advertised and I would like to get reimbursed!&quot;,
    &quot;parameters&quot;: {
        &quot;candidate_labels&quot;: [
            &quot;refund&quot;,
            &quot;legal&quot;,
            &quot;faq&quot;
        ]
    }
}
</code></pre>
","7001697",0
1433,69767807,2,69764100,2021-10-29 11:05:01,4,"<p>The price depends on the number of <strong>vCPU</strong> and <strong>GBs</strong> of memory requested for the container group. You are charged based on the <strong>vCPU request</strong> for your container group rounded up to the nearest whole number for the duration (measured in seconds) <strong>your instance is running</strong>. You are also charged for the <strong>GB request</strong> for your container group rounded up to the nearest tenths place for the duration (measured in seconds) your <strong>container group is running</strong>. There is an additional charge of $0.000012 per vCPU second for Windows software duration on Windows container groups. Check here <a href=""https://azure.microsoft.com/en-us/pricing/details/container-instances/"" rel=""nofollow noreferrer"">Pricing - Container Instances | Microsoft Azure</a> for details</p>
<ul>
<li>After Deployed the Azure Machine Learning managed online endpoint (preview).</li>
<li>Have at least <a href=""https://github.com/MicrosoftDocs/azure-docs/blob/master/articles/role-based-access-control/role-assignments-portal.md"" rel=""nofollow noreferrer"">Billing Reader</a> access on the subscription where the endpoint is deployed</li>
</ul>
<p>To know the costs estimation</p>
<ol>
<li><p>In the <a href=""https://portal.azure.com/"" rel=""nofollow noreferrer"">Azure portal</a>, Go to your subscription</p>
</li>
<li><p>Select <strong>Cost Analysis</strong> for your subscription.</p>
</li>
</ol>
<p><img src=""https://i.imgur.com/W2eaRIO.png"" alt=""enter image description here"" /></p>
<p>Create a filter to scope data to your Azure Machine learning workspace resource:</p>
<ol>
<li><p>At the top navigation bar, select <strong>Add filter</strong>.</p>
</li>
<li><p>In the first filter dropdown, select <strong>Resource</strong> for the filter type.</p>
</li>
<li><p>In the second filter dropdown, select your Azure Machine Learning workspace.</p>
</li>
</ol>
<p><img src=""https://i.imgur.com/HEvprph.png"" alt=""enter image description here"" /></p>
<p>Create a tag filter to show your managed online endpoint and/or managed online deployment:</p>
<ol>
<li><p>Select <strong>Add filter</strong> &gt; <strong>Tag</strong> &gt; <strong>azuremlendpoint</strong>: &quot;&lt; your endpoint name&gt;&quot;</p>
</li>
<li><p>Select <strong>Add filter</strong> &gt; <strong>Tag</strong> &gt; <strong>azuremldeployment</strong>: &quot;&lt; your deployment name&gt;&quot;.</p>
</li>
</ol>
<p><img src=""https://i.imgur.com/1aapYGB.png"" alt=""enter image description here"" /></p>
<p>Refer  <a href=""https://github.com/MicrosoftDocs/azure-docs/blob/master/articles/machine-learning/how-to-view-online-endpoints-costs.md"" rel=""nofollow noreferrer"">here </a> for more detailed steps</p>
","15997690",0
1434,69768603,2,69768602,2021-10-29 12:06:32,2,"<p>This is the way I found to create a clean output in csv format using python, from a batch endpoint invoke in AzureML:</p>
<pre><code>def run(mini_batch):
    batch = []
    for file_path in mini_batch:
        df = pd.read_csv(file_path)
        
        # Do any data quality verification here:
        if 'id' not in df.columns:
            logger.error(&quot;ERROR: CSV file uploaded without id column&quot;)
            return None
        else:
            df['id'] = df['id'].astype(str)

        # Now we need to create the predictions, with previously loaded model in init():
        df['prediction'] = model.predict(df)
        # or alternative, df[MULTILABEL_LIST] = model.predict(df)

        batch.append(df)

    batch_df = pd.concat(batch)

    # After joining all data, we create the columns headers as a string,
    # here we remove the square brackets and apostrophes:
    azureml_columns = str(batch_df.columns.tolist())[1:-1].replace('\'','')
    result = []
    result.append(azureml_columns)

    # Now we have to parse all values as strings, row by row, 
    # adding a comma between each value
    for row in batch_df.iterrows():
        azureml_row = str(row[1].values).replace(' ', ',')[1:-1].replace('\'','').replace('\n','')
        result.append(azureml_row)

    logger.info(&quot;Finished Run&quot;)
    return result
</code></pre>
","13526512",0
1435,69769215,2,69751254,2021-10-29 12:53:19,1,"<p>Use Run.create_children method which will start child runs that are “local” to the parent run, and don’t need authentication.</p>
<p>For AMLcompute max_concurrent_runs map to maximum number of nodes that will be used to run  a hyperparameter tuning run.
So there would be 1 execution per node.</p>
<p>single service deployed but you can load multiple model versions in the init then the score function, depending on the request’s param, uses particular model version to score.
or with the new ML Endpoints (Preview).
<a href=""https://learn.microsoft.com/en-us/azure/machine-learning/concept-endpoints"" rel=""nofollow noreferrer"">What are endpoints (preview) - Azure Machine Learning | Microsoft Docs</a></p>
","11297406",0
1436,69782666,2,69782294,2021-10-30 22:40:56,1,"<p>Hmm,  as far as I know there's unfortunately no way to load a <a href=""https://jsonlines.org/"" rel=""nofollow noreferrer"">JSONL</a> format data using <code>json.loads</code>. One option though, is to come up with a helper function that can convert it to a valid JSON string, as below:</p>
<pre class=""lang-py prettyprint-override""><code>import json

string = &quot;&quot;&quot;
{
    &quot;predictions&quot;: [[0.875780046, 0.124219939], [0.892282844, 0.107717164], [0.887681246, 0.112318777]
    ]
}
{
    &quot;predictions&quot;: [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0]
    ]
}
{
    &quot;predictions&quot;: [[0.391415, 0.608585], [0.992118478, 0.00788147748], [0.0, 1.0]
    ]
}
&quot;&quot;&quot;


def json_lines_to_json(s: str) -&gt; str:
    # replace the first occurrence of '{'
    s = s.replace('{', '[{', 1)

    # replace the last occurrence of '}
    s = s.rsplit('}', 1)[0] + '}]'

    # now go in and replace all occurrences of '}' immediately followed
    # by newline with a '},'
    s = s.replace('}\n', '},\n')

    return s


print(json.loads(json_lines_to_json(string)))
</code></pre>
<p>Prints:</p>
<pre><code>[{'predictions': [[0.875780046, 0.124219939], [0.892282844, 0.107717164], [0.887681246, 0.112318777]]}, {'predictions': [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]}, {'predictions': [[0.391415, 0.608585], [0.992118478, 0.00788147748], [0.0, 1.0]]}]
</code></pre>
<p><strong>Note:</strong> your first example actually doesn't seem like valid JSON (or at least JSON lines from my understanding). In particular, this part appears to be invalid due to a trailing comma after the last array element:</p>
<pre><code>{&quot;vector&quot;: [0.017906909808516502, 0.052080217748880386, -0.1460590809583664, ], ...}
</code></pre>
<p>To ensure it's valid after calling the helper function, you'd also need to remove the trailing commas, so each line is in the below format:</p>
<pre><code>{&quot;vector&quot;: [0.017906909808516502, 0.052080217748880386, -0.1460590809583664 ], ...},
</code></pre>
<hr />
<p>There also appears to be a <a href=""https://stackoverflow.com/questions/50475635/loading-jsonl-file-as-json-objects/50475669"">similar question</a> where they suggest splitting on newlines and calling <code>json.loads</code> on each line; actually it should be (slightly) less performant to call <code>json.loads</code> multiple times on each object, rather than once on the list, as I show below.</p>
<pre class=""lang-py prettyprint-override""><code>from timeit import timeit
import json


string = &quot;&quot;&quot;\
{&quot;vector&quot;: [0.017906909808516502, 0.052080217748880386, -0.1460590809583664 ], &quot;word&quot;: &quot;blah blah blah&quot;}
{&quot;vector&quot;: [0.01027186680585146, 0.04181386157870293, -0.07363887131214142 ], &quot;word&quot;: &quot;blah blah blah&quot;}
{&quot;vector&quot;: [0.011699287220835686, 0.04741542786359787, -0.07899319380521774 ], &quot;word&quot;: &quot;blah blah blah&quot;}\
&quot;&quot;&quot;


def json_lines_to_json(s: str) -&gt; str:

    # Strip newlines from end, then replace all occurrences of '}' followed
    # by a newline, by a '},' followed by a newline.
    s = s.rstrip('\n').replace('}\n', '},\n')

    # return string value wrapped in brackets (list)
    return f'[{s}]'


n = 10_000

print('string replace:        ', timeit(r'json.loads(json_lines_to_json(string))', number=n, globals=globals()))
print('json.loads each line:  ', timeit(r'[json.loads(line) for line in string.split(&quot;\n&quot;)]', number=n, globals=globals()))
</code></pre>
<p>Result:</p>
<pre><code>string replace:         0.07599360000000001
json.loads each line:   0.1078384
</code></pre>
","10237506",2
1437,69800440,2,69763003,2021-11-01 17:21:14,1,"<p>Comprehend now natively support to detect custom-defined entities for pdf documents. To do so, you can try the following steps:</p>
<ol>
<li>Follow this <a href=""https://github.com/aws-samples/amazon-comprehend-semi-structured-documents-annotation-tools"" rel=""nofollow noreferrer"">github readme</a> to start the annotation process for PDF documents.</li>
<li>Once the annotations are produced. You can use Comprehend CreateEntityRecognizer API to train a custom entity model for Semi-structured document”</li>
<li>Once entity recognizer is trained, you can use StartEntitiesDetectionJob API to run inference for PDF documents</li>
</ol>
","17302131",2
1438,69803211,2,69799179,2021-11-01 21:58:44,1,"<p><a href=""https://www.tensorflow.org/tfx/guide/bulkinferrer"" rel=""nofollow noreferrer"">BulkInferrer</a> is the newest component added in the TFX library to support the batch inference of unlabelled data.</p>
<p><a href=""https://i.stack.imgur.com/cOA5t.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/cOA5t.jpg"" alt=""enter image description here"" /></a>
<sup><I><a href=""https://stackoverflow.com/users/10231415/robert-crowe"">Robert Crowe</a> nicely positioned BulkInferrer in the inference pipeline, during the recent <a href=""https://2021.beamsummit.org/sessions/ml-inference-at-scale/"" rel=""nofollow noreferrer"">BEAM summit</a></I></sup></p>
<p>Here is a list of use cases of why would someone use the BulkInferrer, trying to approach it in the case of ml-pipelines rather than data-pipelines:</p>
<ol>
<li>Periodically in an <strong>batch job</strong>, such as a nightly run to infer scores for the data received last day. For example when the inference latency is high and needs to run offline to populate a cache with scores, or when large amount of instanced need to be inferred.</li>
<li>When the inference results are not expected to be served directly back to the inference request (ie real-time) but can be executed <strong>asynchronously</strong>. For example, to shadow test a new model prior to its release.</li>
<li>In an <strong>event-driven</strong> fashion. For example, trigger a model retraining when inferring batches of data to and model drift is detected.</li>
<li>For <strong>cost optimisation</strong>. For example on low throughput models/services there might be long idle times of cpu/gpu instances.</li>
</ol>
<p>To do this in your ML pipeline without retraining your model, you can include BulkInferrer indeed at the end of the pipeline and <a href=""https://www.usenix.org/conference/opml19/presentation/baylor"" rel=""nofollow noreferrer"">reuse the results from previous runs</a> if the inputs and configuration has not changed. This is achieved by both Argo and Tekton workflow managers on Kubeflow pipelines, as they implement TFX, see <a href=""https://www.kubeflow.org/docs/components/pipelines/caching/"" rel=""nofollow noreferrer"">step caching</a>.</p>
","3181539",8
1439,69811187,2,69735547,2021-11-02 13:19:35,2,"<p>You're using the <code>v1</code> <code>kfp.compiler</code> which indeed produces a yaml output and your API client is <code>v2</code>, which indeed expects a json input. Use the <code>kfp.v2.compiler</code> to be consistent with the versions, the later saves the pipeline in json, as expected by your <code>api_client</code>.</p>
<ul>
<li><a href=""https://github.com/kubeflow/pipelines/blob/master/samples/core/helloworld/hello_world.py"" rel=""nofollow noreferrer"">Sample v1 hello world produces a yaml</a></li>
<li><a href=""https://github.com/kubeflow/pipelines/blob/master/samples/v2/hello_world.py"" rel=""nofollow noreferrer"">Sample v2 hello world produces a json</a></li>
</ul>
<p>You can find an example of creating a v2 pipeline on gcp <a href=""https://cloud.google.com/vertex-ai/docs/pipelines/build-pipeline"" rel=""nofollow noreferrer"">here</a>.</p>
","3181539",0
1440,69830014,2,69827748,2021-11-03 18:28:47,3,"<p>Note that the TEST DATASET SUPPORT is a feature still in PRIVATE PREVIEW. It'll probably be released as PUBLIC PREVIEW later in NOVEMBER, but until then, you need to be enrolled in the PRIVATE PREVIEW in order to see the &quot;Test runs and metrics&quot; in the UI. You can send me an email to cesardl at microsoft dot com and send me your AZURE SUBSCRIPTION ID to be enabled so you see it in the UI.</p>
<p>You can see further info on how to get started here:
<a href=""https://github.com/Azure/automl-testdataset-preview"" rel=""nofollow noreferrer"">https://github.com/Azure/automl-testdataset-preview</a></p>
<p>About how to use it, you need to either provide the test_Data (specific Test AML Tabular Dataset that for instance you loaded from a file os split manually previously)
or you can provide a test_size which is the % (i.e. 0.2 is 20%) to be split from the single/original dataset.</p>
<p>About the TEST metrics, since you can make multiple TEST runs against a single model, you need to go to the specific TEST run available under the link &quot;Test results&quot;</p>
<p><a href=""https://i.stack.imgur.com/3pPPS.png"" rel=""nofollow noreferrer"">enter image description here</a></p>
","6362976",0
1441,69847695,2,69693666,2021-11-05 01:24:36,0,"<p>I came to the conclusion that I didn't need to deploy this through SageMaker.  Since the final linear_kernel output was a Dictionary I could do quick ID lookups to find correlations.</p>
<p>I have it working on AWS with API Gateway/Lambda, DynamoDB and an EC2 server to collect, process and plug the data into DynamoDB for fast lookups.  No expensive SageMaker endpoint needed.</p>
","17231573",0
1442,69853178,2,69853177,2021-11-05 12:22:23,0,"<p>For those of you who (might) have encountered the same issue, here's the fix:</p>
<p>1). Install the newest version of docker-compose:</p>
<pre><code>sh-4.2$ sudo curl -L &quot;https://github.com/docker/compose/releases/download/1.29.2/docker-compose-$(uname -s)-$(uname -m)&quot; -o /usr/local/bin/docker-compose
sh-4.2$ sudo chmod +x /usr/local/bin/docker-compose
</code></pre>
<p>2). Change your <code>PATH</code> accordingly (since docker-compose is installed using <code>conda</code> and is picked up first) or use <code>/usr/local/bin/docker-compose</code> onwards:</p>
<pre><code>sh-4.2$ PATH=/usr/local/bin:$PATH
sh-4.2$ docker-compose version
docker-compose version 1.29.2, build 5becea4c
docker-py version: 5.0.0
CPython version: 3.7.10
OpenSSL version: OpenSSL 1.1.0l  10 Sep 2019
</code></pre>
<p>Perhaps, the issue is related to this:</p>
<blockquote>
<p>On August 9, 2021 the Jupyter Notebook and Jupyter Lab open source software projects announced 2 security concerns that could impact Amazon Sagemaker Notebook Instance customers.</p>
<p>Sagemaker has deployed updates to address these concerns, and we recommend customers with existing notebook sessions to stop and restart their notebook instance(s) to benefit from these updates. Notebook instances launched after August 10, 2021, when updates were deployed, are not impacted by this issue and do not need to be restarted.</p>
</blockquote>
","2996867",0
1443,69886126,2,68491585,2021-11-08 15:39:42,2,"<p>I received feedback from the developers (see the <a href=""https://github.com/kubeflow/pipelines/issues/6120"" rel=""nofollow noreferrer"">closed issue</a>). This is one of the current caveats of multi-user mode (see <a href=""https://www.kubeflow.org/docs/components/pipelines/multi-user/#in-cluster-api-request-authentication"" rel=""nofollow noreferrer"">documentation</a>). This usage is now being supported through <a href=""https://github.com/kubeflow/pipelines/issues/5138"" rel=""nofollow noreferrer"">#5138</a>.</p>
","16505992",0
1444,69893622,2,69891041,2021-11-09 06:22:28,1,"<p>You don’t have to rewrite the components, there is no mapping of components of tfx in kfp, as they are not competitive tools.</p>
<p>With tfx you create the components and then you use an <a href=""https://www.tensorflow.org/tfx/guide/custom_orchestrator"" rel=""nofollow noreferrer"">orchestrator</a> to run them. Kubeflow pipelines is one of the orchestrators.</p>
<p>The <code>tfx.orchestration.pipeline</code> will wrap your tfx components and create your pipeline.</p>
<p>We have two schedulers behind kubeflow pipelines: Argo (used by gcp) and Tekton (used by openshift). There are examples for <a href=""https://github.com/kubeflow/kfp-tekton/blob/master/samples/kfp-tfx/tfx-taxi-on-prem/TFX-taxi-sample-pvc.py"" rel=""nofollow noreferrer"">tfx with kubeflow pipelines using tekton</a> and <a href=""https://github.com/kubeflow/pipelines/blob/74c7773ca40decfd0d4ed40dc93a6af591bbc190/samples/core/parameterized_tfx_oss/parameterized_tfx_oss.py"" rel=""nofollow noreferrer"">tfx with kubeflow pipelines using argo</a> in the respective repositories.</p>
","3181539",0
1445,69893636,2,69857932,2021-11-09 06:24:48,3,"<p>Vertex AI is a newer platform with limitations that will be improved over time. “signature_name” can be added to HTTP JSON Payload in <a href=""https://cloud.google.com/vertex-ai/docs/reference/rest/v1/projects.locations.endpoints/rawPredict"" rel=""nofollow noreferrer"">RawPredictRequest</a> or from <a href=""https://cloud.google.com/sdk/gcloud/reference/ai/endpoints/raw-predict"" rel=""nofollow noreferrer"">gcloud</a> as you have done but right now this is not available in regular predict requests.</p>
<p><strong>Using HTTP JSON payload :</strong></p>
<p>Example:</p>
<p>input.json :</p>
<pre><code>{
   &quot;instances&quot;: [
     [&quot;male&quot;, 29.8811345124283, 26.0, 1, &quot;S&quot;, &quot;New York, NY&quot;, 0, 0],
     [&quot;female&quot;, 48.0, 39.6, 1, &quot;C&quot;, &quot;London / Paris&quot;, 0, 1]],
 
     &quot;signature_name&quot;: &lt;string&gt;
}

</code></pre>
<pre><code>curl \
-X POST \
-H &quot;Authorization: Bearer $(gcloud auth print-access-token)&quot; \
-H &quot;Content-Type: application/json&quot; \
https://us-central1-aiplatform.googleapis.com/v1/projects/${PROJECT_ID}/locations/us-central1/endpoints/${ENDPOINT_ID}:rawPredict \
-d &quot;@input.json&quot;

</code></pre>
","15835642",0
1446,69907205,2,57857195,2021-11-10 02:07:56,3,"<p>RStudio is now available as a managed service in SageMaker. <a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/rstudio.html"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/sagemaker/latest/dg/rstudio.html</a></p>
","3980239",0
1447,69927107,2,69828187,2021-11-11 10:55:50,2,"<p>This is because of the path mismatch. The entrypoint is trying to look for &quot;model_handler.py&quot; in <code>WORKDIR</code> directory of the container.
To avoid this, always specify absolute path when working with containers.</p>
<p>Moreover your code looks confusing. Please use this sample code as the reference:</p>
<pre><code>import subprocess
from subprocess import CalledProcessError
import model_handler
from retrying import retry
from sagemaker_inference import model_server
import os


def _retry_if_error(exception):
    return isinstance(exception, CalledProcessError or OSError)


@retry(stop_max_delay=1000 * 50, retry_on_exception=_retry_if_error)
def _start_mms():
    # by default the number of workers per model is 1, but we can configure it through the
    # environment variable below if desired.
    # os.environ['SAGEMAKER_MODEL_SERVER_WORKERS'] = '2'
    print(&quot;Starting MMS -&gt; running &quot;, model_handler.__file__)
    model_server.start_model_server(handler_service=model_handler.__file__ + &quot;:handle&quot;)


def main():
    _start_mms()
    # prevent docker exit
    subprocess.call([&quot;tail&quot;, &quot;-f&quot;, &quot;/dev/null&quot;])

main()
</code></pre>
<p>Further, notice this line - <code>model_server.start_model_server(handler_service=model_handler.__file__ + &quot;:handle&quot;) </code>
Here we are starting the server, and telling it to call <code>handle()</code> function in model_handler.py to invoke your custom logic for all incoming requests.</p>
<p>Also remember that Sagemaker BYOC requires model_handler.py to implement another function <code>ping()</code></p>
<p>So your &quot;model_handler.py&quot; should look like this -</p>
<pre><code>custom_handler = CustomHandler()

# define your own health check for the model over here
def ping():
    return &quot;healthy&quot;


def handle(request, context): # context is necessary input otherwise Sagemaker will throw exception
    if request is None:
        return &quot;SOME DEFAULT OUTPUT&quot;
    try:
        response = custom_handler.predict_fn(request)
        return [response] # Response must be a list otherwise Sagemaker will throw exception

    except Exception as e:
        logger.error('Prediction failed for request: {}. \n'
                     .format(request) + 'Error trace :: {} \n'.format(str(e)))
</code></pre>
","7492545",0
1448,69932676,2,69931411,2021-11-11 17:44:00,6,"<p>I'm the PM that released AzureML Notebooks, you can't activate a Conda env from a cell, you have to create a new kernel will the Conda Env. Here are the instructions: <a href=""https://learn.microsoft.com/en-us/azure/machine-learning/how-to-access-terminal#add-new-kernels"" rel=""noreferrer"">https://learn.microsoft.com/en-us/azure/machine-learning/how-to-access-terminal#add-new-kernels</a></p>
","15331818",8
1449,69941391,2,69940562,2021-11-12 10:27:09,3,"<p>Yes this works with Kedro. You're actually pointing a really old version of the docs, nowadays all filesystem based datasets in Kedro use <a href=""https://github.com/fsspec/filesystem_spec"" rel=""nofollow noreferrer"">fsspec</a> under the hood which means they work with S3, HDFS, local and many more filesystems seamlessly.</p>
<p>The ADLS Gen2 is supported by <code>ffspec</code> via the underlying <code>adlfs</code> library which is <a href=""https://github.com/fsspec/adlfs"" rel=""nofollow noreferrer"">documented here</a>.</p>
<p>From a Kedro point of view all you need to do is declare your catalog entry like so:</p>
<pre class=""lang-yaml prettyprint-override""><code> motorbikes:
     type: pandas.CSVDataSet
     filepath: abfs://your_bucket/data/02_intermediate/company/motorbikes.csv
     credentials: dev_az
</code></pre>
<p>We also have more examples <a href=""https://kedro.readthedocs.io/en/stable/05_data/01_data_catalog.html"" rel=""nofollow noreferrer"">here</a>, particularly example 15.</p>
","2010808",0
1450,69945595,2,69944447,2021-11-12 15:50:35,1,"<p>The solution is:</p>
<pre class=""lang-py prettyprint-override""><code>mlflow.set_tracking_uri(uri=f'file://{hydra.utils.to_absolute_path(&quot;../output/mlruns&quot;)}')
exp = mlflow.get_experiment_by_name(name='Emegency_landing')
if not exp:
    experiment_id = mlflow.create_experiment(name='Emegency_landing',
                                                 artifact_location=f'file://{hydra.utils.to_absolute_path(&quot;../output/mlruns&quot;)}')
else:
    experiment_id = exp.experiment_id
</code></pre>
<p>And then you should pass the experiment Id to:</p>
<pre class=""lang-py prettyprint-override""><code>with mlflow.start_run(experiment_id=experiment_id):
     pass 
</code></pre>
<p>If you don't mention the <code>/path/mlruns</code>, when you run the command of <code>mlflow ui</code>, it will create another folder automatically named <code>mlruns</code>. so, pay attention to this point to have the same name as <code>mlruns</code>.</p>
","4025749",0
1451,69954054,2,69772073,2021-11-13 12:01:56,0,"<p>I ended up :</p>
<ul>
<li>adding a json variable to model the parameter list</li>
<li>adding a string variable and defaulting it to <a href=""https://argoproj.github.io/argo-workflows/variables/"" rel=""nofollow noreferrer"">{{workflow.name}}</a> so it can be used to deduce which &quot;run number&quot; is currently being executed (e.g. myjob-xyz-1, myjob-abc-2, etc) within the pipeline.</li>
</ul>
","1019720",0
1452,69974546,2,69962965,2021-11-15 12:39:44,0,"<p>It turns out the status of a feature group after its creation is <code>Created</code> but before you can ingest any rows you need to simply wait until it's <code>Active</code>:</p>
<pre class=""lang-py prettyprint-override""><code>while status != 'Created':
        try:
            status = feature_group.describe()['OfflineStoreStatus']['Status']
        except:
            pass
        print('Offline store status: {}'.format(status))    
        sleep(15)
</code></pre>
","2153636",0
1453,70002922,2,62489539,2021-11-17 10:46:20,4,"<p>For anyone else landing here, I had a similar issue when creating sklearn models from a ModelPackage.</p>
<p>Error message in endpoint logs when trying to create endpoint:</p>
<blockquote>
<p>AttributeError: 'NoneType' object has no attribute 'startswith'</p>
</blockquote>
<p>Solved with the following when defining the model package:</p>
<ul>
<li>Env variable <code>SAGEMAKER_SUBMIT_DIRECTORY</code> should be set to the directory on the container, normally <code>'/opt/ml/model/'</code></li>
<li><code>SAGEMAKER_PROGRAM</code> should be set to the name of the serving script, e.g. 'sagemaker_serve.py'</li>
</ul>
<p>These are specified under the 'Environment' section part of each entry in the 'Containers' section.</p>
","6459497",0
1454,70006162,2,70005957,2021-11-17 14:26:33,1,"<p>Whilst it's heavily encouraged to use git with Kedro it's not required and as such no part of Kedro (except <a href=""https://github.com/quantumblacklabs/kedro-starters"" rel=""nofollow noreferrer"">kedro-starters</a> if we're being pedantic) is 'aware' of git.</p>
<p>In your <code>before_pipeline_hook</code> there it is pretty easy for you to retrieve the info <a href=""https://stackoverflow.com/questions/14989858/get-the-current-git-hash-in-a-python-script"">via the techniques documented here</a>. It seems trivial for the whole codebase, a bit more involved if you want to say provide pipeline specific hashes.</p>
","2010808",0
1455,70025605,2,69681031,2021-11-18 19:26:06,1,"<p>I didn't realized in the first place that <code>ConcatPlaceholder</code> accept both Artifact and string. This is exactly what I wanted to achieve:</p>
<pre><code>ConcatPlaceholder([OutputPathPlaceholder('drt_model'), '/model'])
</code></pre>
","6430839",0
1456,70037740,2,70034212,2021-11-19 16:08:42,1,"<p>The .tar.gz file is a model artifact</p>
<p>To create a model, you combine the algorithm container and the model artifact</p>
<p>You can do so in the Console, under Inference &gt; Models &gt; Create Model</p>
<p>What do you mean by &quot;How to use that model to the training data set offline?&quot;</p>
<p>If you mean, &quot;run a batch transformation&quot;, then once you create a model, you can select the model in the console, then click 'Create batch transform job'</p>
<p>If you want to do it locally, then you can use the SageMaker Python SDK in <a href=""https://sagemaker.readthedocs.io/en/stable/overview.html#local-mode"" rel=""nofollow noreferrer"">Local Mode</a> and run the transform on your local computer (requires Docker)</p>
","223478",2
1457,70038364,2,70036318,2021-11-19 17:00:39,2,"<p>As of <em>right now</em> (it may change), it's impossible to pass results between jobs if you use multi-task job.</p>
<p>But you can call another notebook as a child job if you use <a href=""https://docs.databricks.com/notebooks/notebook-workflows.html"" rel=""nofollow noreferrer"">notebook workflows</a>  and function <code>dbutils.notebooks.run</code>:</p>
<pre class=""lang-py prettyprint-override""><code># notebook 1
... training code ...
dbutils.notebooks.run(&quot;notebook2&quot;, 300, {&quot;run_id&quot;: run_id})
</code></pre>
","18627",0
1458,70051492,2,70051420,2021-11-21 03:02:34,1,"<p>Change <code>python</code> to <code>python3</code> in the <code>MLproject</code> file to the resolve error.</p>
<pre><code>command: &quot;python3 tracking.py {alpha} {l1_ratio}&quot;
</code></pre>
","14337775",0
1459,69355794,2,69355385,2021-09-28 04:24:54,1,"<p>When we tried to pass a quite lengthy content as argument value to a Pipeline. You can try to upload file to blob, optionally create a dataset, then pass on dataset name or file path to AML pipeline as parameter. The pipeline step will read content of the file from the blob.</p>
","11297406",0
1460,69351997,2,69036277,2021-09-27 19:02:52,1,"<p>It ended up being because of vCPU quota.</p>
<p>After increasing the quota, parallel tasks can run at the same time as expected.</p>
","1171446",0
1461,69349393,2,69248339,2021-09-27 15:36:43,1,"<p>It seems that that the obscure setting I was missing was <code>--experiments=pre_optimize=all</code> in the Beam pipeline options. This results in the following code being run and a <code>RESHUFFLE</code> being included in the Splittable DoFn expansion: <a href=""https://github.com/apache/beam/blob/v2.32.0/sdks/python/apache_beam/runners/portability/fn_api_runner/translations.py#L1433"" rel=""nofollow noreferrer"">https://github.com/apache/beam/blob/v2.32.0/sdks/python/apache_beam/runners/portability/fn_api_runner/translations.py#L1433</a></p>
<p>For those reading this in the future, this worked with Beam 2.32.0 and Flink 1.13.2 -- this is no doubt going to change at some point so this answer may no longer be relevant.</p>
","12090735",0
1462,69335771,2,69335737,2021-09-26 14:15:52,0,"<p>You can create a lambda function and expose it using API gateway to be called by your angular app. this lambda in return will call the  Sagemaker function you have</p>
<p><a href=""https://aws.amazon.com/blogs/machine-learning/call-an-amazon-sagemaker-model-endpoint-using-amazon-api-gateway-and-aws-lambda/"" rel=""nofollow noreferrer"">https://aws.amazon.com/blogs/machine-learning/call-an-amazon-sagemaker-model-endpoint-using-amazon-api-gateway-and-aws-lambda/</a></p>
","12615044",0
1463,68913355,2,68909644,2021-08-24 19:50:06,0,"<p>Your first approach sounds fine:</p>
<ul>
<li>The original data is left as-is in case you need to repeat the process later (perhaps with changes / improvements).</li>
<li>The system will work for any data source once you plumb it in; i.e. you can re-use the transformation and load parts.</li>
</ul>
<p>I don't know anything about SageMaker, etc, or the whole CVS thing, but once you have the data in your ML DB you can obviously export it to any format you like later, like CVS.</p>
","39094",0
1464,68902887,2,68812238,2021-08-24 06:44:14,2,"<p>There is no official way to export a Databricks MLflow run from one workspace to another. However, there is an &quot;unofficial&quot; tool that does most of the job with the main limitation being that notebook revisions linked to a run cannot be exported due to lack of a REST API endpoint for this.</p>
<p><a href=""https://github.com/amesar/mlflow-export-import"" rel=""nofollow noreferrer"">https://github.com/amesar/mlflow-export-import</a></p>
","13980177",0
1465,68898313,2,68718719,2021-08-23 19:53:41,2,"<p>I recently found the solution which can be done by the following two approaches:</p>
<ol>
<li>Use the customized predict function at the moment of saving the model (check <a href=""https://www.mlflow.org/docs/latest/models.html#model-customization"" rel=""nofollow noreferrer"">databricks</a> documentation for more details).</li>
</ol>
<p>example give by Databricks</p>
<pre><code>class AddN(mlflow.pyfunc.PythonModel):

    def __init__(self, n):
        self.n = n

    def predict(self, context, model_input):
        return model_input.apply(lambda column: column + self.n)
# Construct and save the model
model_path = &quot;add_n_model&quot;
add5_model = AddN(n=5)
mlflow.pyfunc.save_model(path=model_path, python_model=add5_model)

# Load the model in `python_function` format
loaded_model = mlflow.pyfunc.load_model(model_path)
</code></pre>
<ol start=""2"">
<li>Load the model artefacts as we are downloading the artefact:</li>
</ol>
<pre><code>from mlflow.tracking import MlflowClient

client = MlflowClient()

tmp_path = client.download_artifacts(run_id=&quot;0c7946c81fb64952bc8ccb3c7c66bca3&quot;, path='model/model.pkl')

f = open(tmp_path,'rb')

model = pickle.load(f)

f.close()

 

client.list_artifacts(run_id=&quot;0c7946c81fb64952bc8ccb3c7c66bca3&quot;, path=&quot;&quot;)

client.list_artifacts(run_id=&quot;0c7946c81fb64952bc8ccb3c7c66bca3&quot;, path=&quot;model&quot;)
</code></pre>
","9553352",4
1466,68873476,2,68569742,2021-08-21 13:32:41,5,"<p>Your ml.c5.xlarge instance comes with 4 vCPU. However, Python only uses a single CPU by default. (Source: <a href=""https://stackoverflow.com/questions/64121703/can-i-apply-multithreading-for-computationally-intensive-task-in-python"">Can I apply multithreading for computationally intensive task in python?</a>)</p>
<p>As a result, the overall CPU utilization of your ml.c5.xlarge instance is low. To utilize all the vCPUs, you can try multiprocessing.</p>
<p>The examples below are performed using a 2 vCPU + 4 GiB instance.</p>
<p>In the first picture, multiprocessing is not set up. The instance CPU utilization peaks at around 50%.</p>
<p>single processing:<br />
<img src=""https://i.stack.imgur.com/lcn8K.png"" alt=""single processing"" /></p>
<p>In the second picture, I created 50 processes to be run simultaneously. The instance CPU utilization rises to 100% immediately.</p>
<p>multiprocessing:<br />
<img src=""https://i.stack.imgur.com/tk65n.png"" alt=""multiprocessing"" /></p>
","14229798",1
1467,68864690,2,68820617,2021-08-20 15:33:31,3,"<p>If you want to have a look at the containers for the different Frameworks as they are supported by Amazon SageMaker, you can check the GitHub:</p>
<ul>
<li>Deep Learning Containers: <a href=""https://github.com/aws/deep-learning-containers"" rel=""nofollow noreferrer"">Github Link</a></li>
<li>XGBoost Container: <a href=""https://github.com/aws/sagemaker-xgboost-container"" rel=""nofollow noreferrer"">Github Link</a></li>
<li>SKLearn Container: <a href=""https://github.com/aws/sagemaker-scikit-learn-container"" rel=""nofollow noreferrer"">Github Link</a></li>
<li>Spark Container: <a href=""https://github.com/aws/sagemaker-spark-container"" rel=""nofollow noreferrer"">Github Link</a></li>
</ul>
<p>This should answer your question regarding dockerfiles, training and deployment.</p>
<p>The <a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/algos.html"" rel=""nofollow noreferrer"">built-in algorithms</a> in Amazon SageMaker (also called 1P algorithms) are part of AWS PI, so they are not publicly available.</p>
","5323559",1
1468,68658973,2,68658385,2021-08-04 23:22:42,2,"<h2>update</h2>
<p>sharing OP's solution here for easier discovery</p>
<pre class=""lang-py prettyprint-override""><code>import pandas as pd
from azureml.core import Workspace, Run, Dataset

def azureml_main(dataframe1 = None, dataframe2 = None):
    run = Run.get_context()
    ws = run. experiment.workspace
    datastore = ws.get_default_datastore()
    ds = Dataset.Tabular.register_pandas_dataframe(
        dataframe1, datastore, 'data_set_name',
        description = 'data set description.')
    return dataframe1,
</code></pre>
<h2>original answer</h2>
<p>Sorry you're struggling. You're very close!</p>
<p>A few things may be the culprit here.</p>
<ol>
<li>It looks like you're using the <code>Dataset</code> class, which has been deprecated. I recommend trying <code>Dataset.Tabular.register_pandas_dataframe()</code> (<a href=""https://learn.microsoft.com/en-us/python/api/azureml-core/azureml.data.dataset_factory.tabulardatasetfactory?view=azure-ml-py#register-pandas-dataframe-dataframe--target--name--description-none--tags-none--show-progress-true-"" rel=""nofollow noreferrer"">docs link</a>) instead of <code>Dataset.from_pandas_dataframe()</code>. (<a href=""https://github.com/Azure/MachineLearningNotebooks/blob/master/how-to-use-azureml/work-with-data/dataset-api-change-notice.md"" rel=""nofollow noreferrer"">more about the Dataset API deprecation</a>)</li>
<li>More conjectire here, but another thing is there might be some limitations to using dataset registration within an &quot;Execute Python Script&quot; (EPS) module due to:
<ol>
<li>the workspace object might not have the right permissions</li>
<li>you might not be able to use the <code>register_pandas_dataframe</code> method inside the EPS module, but might have better luck with save the dataframe first to parquet, then calling <code>Dataset.Tabular.from_parquet_files</code></li>
</ol>
</li>
</ol>
<p>Hopefully something works here!</p>
","3842610",1
1469,68658916,2,68616817,2021-08-04 23:11:09,0,"<h1>Cause</h1>
<p>Did not use VPC Only sagemaker deployment as having used the Quick Start onboard.</p>
<h1>Fix</h1>
<ol>
<li>Deleted the SageMaker Studio. R</li>
<li>Recreated by using the Standard Setup + VPC only
<a href=""https://i.stack.imgur.com/zZ7be.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/zZ7be.png"" alt=""enter image description here"" /></a></li>
<li>Added the NAT and configured the routing tables.</li>
</ol>
<h1>References</h1>
<ul>
<li><a href=""https://aws.amazon.com/blogs/machine-learning/securing-amazon-sagemaker-studio-connectivity-using-a-private-vpc/"" rel=""nofollow noreferrer"">Securing Amazon SageMaker Studio connectivity using a private VPC</a></li>
<li><a href=""https://github.com/aws-samples/amazon-sagemaker-studio-vpc-networkfirewall"" rel=""nofollow noreferrer"">Amazon SageMaker Studio in a private VPC with NAT Gateway and Network Firewall</a></li>
</ul>
","4281353",1
1470,68369813,2,68356746,2021-07-13 22:05:27,3,"<p>As I commented above, yes, <code>mlflow.create_experiment()</code> does allow you set the artifact location using the <code>artifact_location</code> parameter.</p>
<p>However, sort of related, the problem with setting the <code>artifact_location</code> using the <code>create_experiment()</code> function is that once you create a experiment, MLflow will throw an error if you run the <code>create_experiment()</code> function again.</p>
<p>I didn't see this in the docs but it's confirmed that if an experiment already exists in the backend-store, MlFlow will not allow you to run the same <code>create_experiment()</code> function again. And as of this post, MLfLow does not have <code>check_if_exists</code> flag or a <code>create_experiments_if_not_exists()</code> function.</p>
<p>To make things more frustrating, you cannot set the <code>artifcact_location</code> in the <code>set_experiment()</code> function either.</p>
<p>So here is a pretty easy work around, it also avoids the &quot;ERROR mlflow.utils.rest_utils...&quot; stdout logging as well.
:</p>
<pre><code>import os
from random import random, randint

from mlflow import mlflow,log_metric, log_param, log_artifacts
from mlflow.exceptions import MlflowException

try:
    experiment = mlflow.get_experiment_by_name('oof')
    experiment_id = experiment.experiment_id
except AttributeError:
    experiment_id = mlflow.create_experiment('oof', artifact_location='s3://mlflow-minio/sample/')

with mlflow.start_run(experiment_id=experiment_id) as run:
    mlflow.set_tracking_uri('http://localhost:5000')
    print(&quot;Running mlflow_tracking.py&quot;)

    log_param(&quot;param1&quot;, randint(0, 100))
    
    log_metric(&quot;foo&quot;, random())
    log_metric(&quot;foo&quot;, random() + 1)
    log_metric(&quot;foo&quot;, random() + 2)

    if not os.path.exists(&quot;outputs&quot;):
        os.makedirs(&quot;outputs&quot;)
    with open(&quot;outputs/test.txt&quot;, &quot;w&quot;) as f:
        f.write(&quot;hello world!&quot;)

    log_artifacts(&quot;outputs&quot;)
</code></pre>
<p>If it is the user's first time creating the experiment, the code will run into an AttributeError since <code>experiment_id</code> does not exist and the <code>except</code> code block gets executed creating the experiment.</p>
<p>If it is the second, third, etc the code is run, it will only execute the code under the <code>try</code> statement since the experiment now exists. Mlflow will now create a 'sample' key in your s3 bucket. Not fully tested but it works for me at least.</p>
","3386440",0
1471,68290120,2,68241601,2021-07-07 16:50:07,0,"<p>Your code looks ready to be executed with MLRun. Depending on how MLRun is installed, you have to configure your environment to find the API. Take a look here <a href=""https://docs.mlrun.org/en/latest/install.html"" rel=""nofollow noreferrer"">https://docs.mlrun.org/en/latest/install.html</a></p>
<p>Here is a short script you can use to test.</p>
<pre><code>from mlrun import code_to_function

fn = code_to_function('train_iris', handler='train_iris', kind='job', filename=&quot;&lt;YOUR PYTHON FILE PATH&gt;&quot;)
# RUN LOCAL
fn.run(project='iris',local=True)

# RUN IN KUBERNETES (if you are running MLRUN in Kubernetes)
fn.run(project='iris',local=False)

</code></pre>
","16316319",0
1472,68260374,2,66574569,2021-07-05 18:00:26,3,"<p>In your <code>train.py</code> file, changing the environment variable from</p>
<p><code>parser.add_argument('--data-dir', type=str, default=os.environ['SM_CHANNEL_TRAINING'])</code></p>
<p>to</p>
<p><code>parser.add_argument('--data-dir', type=str, default=os.environ['SM_CHANNEL_TRAIN'])</code> should address the issue.</p>
<p>This is the case with Torch's framework_version 1.3.1 but other versions might also be affected. Here is the <a href=""https://github.com/aws/sagemaker-python-sdk/issues/1292"" rel=""nofollow noreferrer"">link</a> for your reference.</p>
","8367307",0
1473,68256105,2,68251533,2021-07-05 12:24:52,2,"<p>I faced a similar issue on other AWS services. Usually for managed services AWS uses read-only containers approach and leave just one folder of the filesystem for read/write that persist across the stop/restart cycle.
Reguarding the packages installation the seems to be to install your custom environment on the notebook instance's Amazon EBS volume, as described <a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/nbi-add-external.html"" rel=""nofollow noreferrer"">here</a></p>
","4267439",0
1474,68253425,2,65992776,2021-07-05 09:05:11,7,"<p>You should call you <code>experiment_id</code> in the <code>start_run()</code>:</p>
<pre><code>mlflow.set_experiment(&quot;experiment name&quot;)
experiment = mlflow.get_experiment_by_name(&quot;experiment name&quot;)

with mlflow.start_run(experiment_id=experiment.experiment_id):
     # train model
</code></pre>
<p><strong>Note</strong>: If you use <code>set_tracking_uri()</code>, you should <code>set_experiment()</code> after that.</p>
","5962113",1
1475,68237344,2,68237132,2021-07-03 14:57:20,1,"<p>Solution was to add the following to the Dockerfile:</p>
<pre><code># Install Cython
RUN pip3 install Cython

# Install pip dependencies
RUN HOROVOD_WITH_PYTORCH=1 \
    pip install 'matplotlib&gt;=3.3,&lt;3.4' \
                ...
                'git+https://github.com/philferriere/cocoapi.git#egg=pycocotools&amp;subdirectory=PythonAPI'
</code></pre>
","985012",0
1476,68229088,2,68026549,2021-07-02 17:17:12,3,"<p>Turns out you can't at the moment but according to this <a href=""https://github.com/tensorflow/tfx/issues/3194#issuecomment-802598448"" rel=""nofollow noreferrer"">issue</a>, this feature is coming.</p>
<p>An alternative solution is to convert your TFX pipeline to a Kubeflow pipeline. Vertex AI pipelines support kubeflow and with these you can set memory and cpu constraints at the component level.</p>
<pre class=""lang-py prettyprint-override""><code>@component // imported from kfp.dsl
def MyComponent(Input[Dataset] input_data):
  // ...

@pipeline // imported from kfp.dsl
def MyPipeline(...):
  component = MyComponent(...)
  component.set_memory_limit('64G') // alternative to set_memory_request(...)
</code></pre>
","2005440",2
1477,69691655,2,69567330,2021-10-23 20:05:35,0,"<p>Using dockerfile for creating a new environment and installing chrome with requirements solved the problem.</p>
","14292976",0
1478,69690759,2,69689266,2021-10-23 18:00:53,3,"<p>If you look into the Python API, the very <a href=""https://mlflow.org/docs/latest/python_api/mlflow.client.html"" rel=""nofollow noreferrer"">first example</a> in <code>mlflow.tracking package</code> that shows how to create the <code>MLflowClient</code> is really showing how to tag experiment using the <code>client.set_experiment_tag</code> function (<a href=""https://mlflow.org/docs/latest/python_api/mlflow.client.html#mlflow.tracking.MlflowClient.set_experiment_tag"" rel=""nofollow noreferrer"">doc</a>):</p>
<pre class=""lang-py prettyprint-override""><code>from mlflow.tracking import MlflowClient

# Create an experiment with a name that is unique and case sensitive.
client = MlflowClient()
experiment_id = client.create_experiment(&quot;Social NLP Experiments&quot;)
client.set_experiment_tag(experiment_id, &quot;nlp.framework&quot;, &quot;Spark NLP&quot;)
</code></pre>
<p>you can also set it for model version with <a href=""https://mlflow.org/docs/latest/python_api/mlflow.client.html#mlflow.client.MlflowClient.set_model_version_tag"" rel=""nofollow noreferrer"">set_model_version_tag</a> function, and for registered model with <a href=""https://mlflow.org/docs/latest/python_api/mlflow.client.html#mlflow.tracking.MlflowClient.set_registered_model_tag"" rel=""nofollow noreferrer"">set_registered_model_tag</a>.</p>
","18627",2
1479,69365219,2,69356567,2021-09-28 16:19:04,1,"<p>I work on the Notebooks team in AzureML, I just tried this. Did this just start happening today?</p>
<p>It seems like things are working as expected: <a href=""https://i.stack.imgur.com/xeDIT.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/xeDIT.png"" alt=""enter image description here"" /></a></p>
","15331818",2
1480,69354762,2,69099123,2021-09-28 01:22:53,0,"<p>You need to install libsndfile and it's development files. Unfortunately it's not packaged up in a repository, but you can get it installed by downloading the RPMs directly</p>
<pre><code>wget http://ftp.altlinux.org/pub/distributions/ALTLinux/Sisyphus/x86_64/RPMS.classic//libsndfile-1.0.31-alt1.x86_64.rpm
wget http://ftp.altlinux.org/pub/distributions/ALTLinux/Sisyphus/x86_64/RPMS.classic//libsndfile-devel-1.0.28-alt1.x86_64.rpm

sudo yum localinstall libsndfile-devel-1.0.31-alt1.x86_64.rpm
</code></pre>
<p>You can also install <a href=""https://github.com/libsndfile/libsndfile"" rel=""nofollow noreferrer"">libsndfile</a> from source.</p>
<p>Another option is to switch to Ubuntu, which has packages for <code>libsndfile</code> and <code>libsndfile-dev</code>. If you're looking to deploy tensorflow specifically then you should start with the <a href=""https://aws.amazon.com/machine-learning/amis/"" rel=""nofollow noreferrer"">AWS Deep Learning AMI</a>. These AMIs are built by AWS and come with the drivers and a bunch of frameworks already installed. These AMis have Ubuntu flavored versions.</p>
","212774",4
1481,70092932,2,70092793,2021-11-24 08:37:13,0,"<p>I was finally able to resolve the issue by using the pip method instead of the conda method:
<code>add_pip_package('numpy')</code> instead of <code>add_conda_package('numpy')</code>
I can imagine this being the reason for other packages as well.</p>
<hr />
<h2>Full solution</h2>
<pre><code>from azureml.core import Environment
from azureml.core.conda_dependencies import CondaDependencies


def get_env() -&gt; Environment:
    conda = CondaDependencies()

    # add channels
    conda.add_channel('defaults')
    conda.add_channel('conda-forge')
    conda.add_channel('pytorch')

    # Python
    conda.add_conda_package('python=3.8')

    # Other conda packages
    conda.add_conda_package('cudatoolkit=11.3')
    conda.add_conda_package('pip')
    conda.add_conda_package('python-dateutil')
    conda.add_conda_package('python-dotenv')
    conda.add_conda_package('pytorch=1.10')
    conda.add_conda_package('torchaudio')
    conda.add_conda_package('torchvision')
    conda.add_conda_package('wheel')
    #conda.add_conda_package('numpy=1.21.2') # &lt;--- Error with this import 

    # Add pip packages
    conda.add_pip_package('numpy') # &lt;--- Fixes import error

    # create environment
    env = Environment('test_env')
    env.python.conda_dependencies = conda

    return env
</code></pre>
","15354710",0
1482,70099642,2,70009118,2021-11-24 16:29:46,0,"<p>The bug was fixed in the last version of the manifests, then I have finally installed kubeflow directly from the <a href=""https://github.com/kubeflow/manifests"" rel=""nofollow noreferrer"">manifests</a>.</p>
<p>But still I am in touch with one Kubeflow developer, I will post here the right way to do modify/deploy if interested.</p>
","10680282",0
1483,70100109,2,70098779,2021-11-24 17:01:13,3,"<p><a href=""https://mlflow.org/docs/latest/tracking.html#logging-to-a-tracking-server"" rel=""nofollow noreferrer"">MLflow documentation</a> says:</p>
<blockquote>
<p><code>MLFLOW_TRACKING_USERNAME</code> and <code>MLFLOW_TRACKING_PASSWORD</code> - username and password to use with HTTP Basic authentication. To use Basic authentication, you must set both environment variables.</p>
</blockquote>
<p>So you just need to set these variables in your code using <code>os.environ</code>:</p>
<pre class=""lang-py prettyprint-override""><code>os.environ['MLFLOW_TRACKING_USERNAME'] = 'name'
os.environ['MLFLOW_TRACKING_PASSWORD'] = 'pass'
</code></pre>
","18627",3
1484,70102015,2,69046990,2021-11-24 19:39:22,22,"<p>There are a couple of options for you to accomplish that.</p>
<p>One that is really simple is adding all additional files to a folder, example:</p>
<pre><code>.
├── my_package
│   ├── file1.py
│   ├── file2.py
│   └── requirements.txt
└── preprocessing.py
</code></pre>
<p>Then send this entire folder as another input under the same <code>/opt/ml/processing/input/code/</code>, example:</p>
<pre class=""lang-py prettyprint-override""><code>from sagemaker.sklearn.processing import SKLearnProcessor
from sagemaker.processing import ProcessingInput, ProcessingOutput

sklearn_processor = SKLearnProcessor(
    framework_version=&quot;0.20.0&quot;,
    role=role,
    instance_type=&quot;ml.m5.xlarge&quot;,
    instance_count=1,
)

sklearn_processor.run(
    code=&quot;preprocessing.py&quot;,  # &lt;- this gets uploaded as /opt/ml/processing/input/code/preprocessing.py
    inputs=[
        ProcessingInput(source=input_data, destination='/opt/ml/processing/input'),
        # Send my_package as /opt/ml/processing/input/code/my_package/
        ProcessingInput(source='my_package/', destination=&quot;/opt/ml/processing/input/code/my_package/&quot;)
    ],
    outputs=[
        ProcessingOutput(output_name=&quot;train_data&quot;, source=&quot;/opt/ml/processing/train&quot;),
        ProcessingOutput(output_name=&quot;test_data&quot;, source=&quot;/opt/ml/processing/test&quot;),
    ],
    arguments=[&quot;--train-test-split-ratio&quot;, &quot;0.2&quot;],
)
</code></pre>
<p>What happens is that <code>sagemaker-python-sdk</code> is going to put your argument <code>code=&quot;preprocessing.py&quot;</code> under <code>/opt/ml/processing/input/code/</code> and you will have <code>my_package/</code> under the same directory.</p>
<p><strong>Edit:</strong></p>
<p>For the <code>requirements.txt</code>, you can add to your <code>preprocessing.py</code>:</p>
<pre class=""lang-py prettyprint-override""><code>import sys
import subprocess

subprocess.check_call([
    sys.executable, &quot;-m&quot;, &quot;pip&quot;, &quot;install&quot;, &quot;-r&quot;,
    &quot;/opt/ml/processing/input/code/my_package/requirements.txt&quot;,
])
</code></pre>
","2373882",5
1485,70113499,2,70111193,2021-11-25 15:13:20,3,"<p>There is no such thing, like load <code>latest</code>, but:</p>
<ul>
<li>You can specify the stage (<code>staging</code>, <code>production</code>) - see <a href=""https://www.mlflow.org/docs/latest/concepts.html#referencing-artifacts"" rel=""nofollow noreferrer"">docs</a></li>
<li>You can find latest version using the <a href=""https://www.mlflow.org/docs/latest/python_api/mlflow.client.html#mlflow.client.MlflowClient.get_latest_versions"" rel=""nofollow noreferrer"">get_latest_versions</a> function - but it will also return latest per stage</li>
</ul>
<p>So you need to define what <code>latest</code> means for you.</p>
","18627",1
1486,70159585,2,70156610,2021-11-29 18:28:09,2,"<p>As described, I think the best solution is to use one Workspace, not two.  It sounds like you have Team 1 and Team 2 sharing contributions on a single project.  What may work better is to define user roles in the Azure ML workspace, such that Team 2 has permissions to deploy models, and Team 1 has permission to create models.</p>
<p>Otherwise you can always write Python code using the ML SDK to connect to any workspace given you know the subscription, resource group, workspace name etc.</p>
<pre><code>from azure.core import Workspace, Model

# connect to an existing workspace
name = 'WorkspaceName'
sub = 'subscriptionName'
resource_group = 'resourceGroupName'
ws = Workspace.get(name=name, subscription_id=sub, resource_group=resource_group) 

# retrieve existing model
model = Model(ws, name='your model name')
</code></pre>
","16510343",3
1487,70184259,2,70169519,2021-12-01 12:31:28,0,"<p>Finally, I made a class that contains every metadata and saved it as an model argument:</p>
<pre><code>model = LogisticRegression()
model.fit(X, y)
model.metadata = ModelMetadata(**metadata_dic)
mlflow.sklearn.log_model(model, &quot;model&quot;)
</code></pre>
<p>Here I lost the customizable <code>predict</code> process, but after reading the <code>MLFlow</code> documentation is not very clear how to proceed.</p>
<p>If anyone finds a good approach It would be very appreciated.</p>
","11067209",0
1488,70188476,2,70163094,2021-12-01 17:13:23,2,"<p>It depends how you define Dev and Prod.</p>
<ul>
<li><p>If by Dev and Prod you mean different AWS account (which is a good practice - see <a href=""https://docs.aws.amazon.com/whitepapers/latest/organizing-your-aws-environment/benefits-of-using-multiple-aws-accounts.html"" rel=""nofollow noreferrer"">doc</a> and <a href=""https://aws.amazon.com/blogs/devops/aws-building-a-secure-cross-account-continuous-delivery-pipeline/"" rel=""nofollow noreferrer"">blog</a>), you cannot share fractions of a model registry from a given account to another account, but you can create triggers to export models from one model registry to another, as documented in this blog post <a href=""https://aws.amazon.com/blogs/machine-learning/patterns-for-multi-account-hub-and-spoke-amazon-sagemaker-model-registry/"" rel=""nofollow noreferrer"">https://aws.amazon.com/blogs/machine-learning/patterns-for-multi-account-hub-and-spoke-amazon-sagemaker-model-registry/</a></p>
</li>
<li><p>If your Dev and Prod live in the same AWS account and you are just looking for ways to differentiate them, you can use:</p>
<ul>
<li>Model Registry Status information</li>
<li>Tags</li>
</ul>
</li>
</ul>
","5331834",7
1489,70191757,2,70191668,2021-12-01 22:01:10,3,"<p>SageMaker has two things called Pipelines: <a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/pipelines.html"" rel=""nofollow noreferrer"">Model Building Pipelines</a> and <a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/inference-pipelines.html"" rel=""nofollow noreferrer"">Serial Inference Pipelines</a>. I believe you're referring to the former</p>
<p>A model building pipeline defines steps in a machine learning workflow, such as pre-processing, hyperparameter tuning, batch transformations, and setting up endpoints</p>
<p>A serial inference pipeline is two or more SageMaker models run one after the other</p>
<p>A model building pipeline is defined in JSON, and is hosted/run in some sort of proprietary, serverless fashion by SageMaker</p>
<blockquote>
<p>Is sagemaker pipelines a stand-alone service/feature? Because I don't see any option to create them through the console, though I do see CloudFormation and CDK resources.</p>
</blockquote>
<p>You can create/modify them using the <a href=""https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_CreatePipeline.html"" rel=""nofollow noreferrer"">API</a>, which can also be called via the <a href=""https://docs.aws.amazon.com/cli/latest/reference/sagemaker/create-pipeline.html"" rel=""nofollow noreferrer"">CLI</a>, <a href=""https://sagemaker.readthedocs.io/en/stable/workflows/pipelines/sagemaker.workflow.pipelines.html#sagemaker.workflow.pipeline.Pipeline.create"" rel=""nofollow noreferrer"">Python SDK</a>, or <a href=""https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-sagemaker-pipeline.html"" rel=""nofollow noreferrer"">CloudFormation</a>. These all use the AWS API under the hood</p>
<p>You can start/stop/view them in SageMaker Studio:</p>
<pre><code>Left-side Navigation bar &gt; SageMaker resources &gt; Drop-down menu &gt; Pipelines
</code></pre>
<blockquote>
<p>Is a sagemaker pipeline essentially codepipeline? How do these integrate, how do these differ?</p>
</blockquote>
<p>Unlikely. CodePipeline is more for building and deploying code, not specific to SageMaker. There is no direct integration as far as I can tell, other than that you can start a SM pipeline with CP</p>
<blockquote>
<p>There's also a Python SDK, how does this differ from the CDK and CloudFormation?</p>
</blockquote>
<p>The Python SDK is a stand-alone library to interact with SageMaker in a developer-friendly fashion. It's more dynamic than CloudFormation. Let's you build pipelines using code. Whereas CloudFormation takes a static JSON string</p>
<p>A very simple example of Python SageMaker SDK usage:</p>

<pre class=""lang-python prettyprint-override""><code>processor = SKLearnProcessor(
    framework_version=&quot;0.23-1&quot;,
    instance_count=1,
    instance_type=&quot;ml.m5.large&quot;,
    role=&quot;role-arn&quot;,
)

processing_step = ProcessingStep(
    name=&quot;processing&quot;,
    processor=processor,
    code=&quot;preprocessor.py&quot;
)

pipeline = Pipeline(name=&quot;foo&quot;, steps=[processing_step])
pipeline.upsert(role_arn = ...)
pipeline.start()
</code></pre>
<p><code>pipeline.definition()</code> produces rather verbose JSON like this:</p>

<pre class=""lang-json prettyprint-override""><code>{
&quot;Version&quot;: &quot;2020-12-01&quot;,
&quot;Metadata&quot;: {},
&quot;Parameters&quot;: [],
&quot;PipelineExperimentConfig&quot;: {
    &quot;ExperimentName&quot;: {
        &quot;Get&quot;: &quot;Execution.PipelineName&quot;
    },
    &quot;TrialName&quot;: {
        &quot;Get&quot;: &quot;Execution.PipelineExecutionId&quot;
    }
},
&quot;Steps&quot;: [
    {
        &quot;Name&quot;: &quot;processing&quot;,
        &quot;Type&quot;: &quot;Processing&quot;,
        &quot;Arguments&quot;: {
            &quot;ProcessingResources&quot;: {
                &quot;ClusterConfig&quot;: {
                    &quot;InstanceType&quot;: &quot;ml.m5.large&quot;,
                    &quot;InstanceCount&quot;: 1,
                    &quot;VolumeSizeInGB&quot;: 30
                }
            },
            &quot;AppSpecification&quot;: {
                &quot;ImageUri&quot;: &quot;246618743249.dkr.ecr.us-west-2.amazonaws.com/sagemaker-scikit-learn:0.23-1-cpu-py3&quot;,
                &quot;ContainerEntrypoint&quot;: [
                    &quot;python3&quot;,
                    &quot;/opt/ml/processing/input/code/preprocessor.py&quot;
                ]
            },
            &quot;RoleArn&quot;: &quot;arn:aws:iam::123456789012:role/foo&quot;,
            &quot;ProcessingInputs&quot;: [
                {
                    &quot;InputName&quot;: &quot;code&quot;,
                    &quot;AppManaged&quot;: false,
                    &quot;S3Input&quot;: {
                        &quot;S3Uri&quot;: &quot;s3://bucket/preprocessor.py&quot;,
                        &quot;LocalPath&quot;: &quot;/opt/ml/processing/input/code&quot;,
                        &quot;S3DataType&quot;: &quot;S3Prefix&quot;,
                        &quot;S3InputMode&quot;: &quot;File&quot;,
                        &quot;S3DataDistributionType&quot;: &quot;FullyReplicated&quot;,
                        &quot;S3CompressionType&quot;: &quot;None&quot;
                    }
                }
            ]
        }
    }
  ]
}
</code></pre>
<p>You could <em>use</em> the above JSON with CloudFormation/CDK, but you <em>build</em> the JSON with the SageMaker SDK</p>
<p>You can also define model building workflows using Step Function State Machines, using the <a href=""https://aws-step-functions-data-science-sdk.readthedocs.io/en/stable/"" rel=""nofollow noreferrer"">Data Science SDK</a>, or <a href=""https://sagemaker.readthedocs.io/en/stable/workflows/airflow/index.html"" rel=""nofollow noreferrer"">Airflow</a></p>
","223478",5
1490,70204408,2,70202848,2021-12-02 18:04:58,1,"<p>To date Studio Lab doesn't support package installs that require root access. It does support packages installable via pip and conda. You can do that either in your notebook with the %, rather than the !, or you can do that via opening a terminal.</p>
<ul>
<li><a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/studio-lab-use-manage.html"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/sagemaker/latest/dg/studio-lab-use-manage.html</a></li>
</ul>
<p>If you'd like to open an issue you're welcome to do that on our repository right here:</p>
<ul>
<li><a href=""https://github.com/aws/studio-lab-examples/blob/main/.github/ISSUE_TEMPLATE/bug-report-for-sagemaker-studio-lab.md"" rel=""nofollow noreferrer"">https://github.com/aws/studio-lab-examples/blob/main/.github/ISSUE_TEMPLATE/bug-report-for-sagemaker-studio-lab.md</a></li>
</ul>
<p>Thanks for trying out Studio Lab!</p>
","4348604",3
1491,70241297,2,70240640,2021-12-06 05:39:34,0,"<pre><code>import sagemaker
from sagemaker.inputs import TrainingInput
from sagemaker.serializers import CSVSerializer
from sagemaker.session import TrainingInput
from sagemaker import image_uris
from sagemaker.session import Session

# initialize hyperparameters
hyperparameters = {
    &quot;max_depth&quot;:&quot;5&quot;,
    &quot;eta&quot;:&quot;0.1&quot;,
    &quot;gamma&quot;:&quot;4&quot;,
    &quot;min_child_weight&quot;:&quot;6&quot;,
    &quot;subsample&quot;:&quot;0.7&quot;,
    &quot;objective&quot;:&quot;binary:logistic&quot;,
    &quot;num_round&quot;:&quot;25&quot;}

# set an output path where the trained model will be saved
bucket = sagemaker.Session().default_bucket()
output_path = 's3://{}/{}/output'.format(bucket, 'rain-xgb-built-in-algo')


# this line automatically looks for the XGBoost image URI and builds an 
XGBoost container.
# specify the repo_version depending on your preference.
xgboost_container = sagemaker.image_uris.retrieve(&quot;xgboost&quot;, 'ap-southeast- 
2', &quot;1.3-1&quot;)


# construct a SageMaker estimator that calls the xgboost-container
estimator = sagemaker.estimator.Estimator(image_uri=xgboost_container, 
                                      hyperparameters=hyperparameters,
                                      role=sagemaker.get_execution_role(),
                                      instance_count=1, 
                                      instance_type='ml.m5.large', 
                                      volume_size=5, # 5 GB 
                                      output_path=output_path)



 # define the data type and paths to the training and validation datasets

 train_input = TrainingInput(&quot;s3://{}/{}/&quot;.format(bucket,'train'), 
 content_type='csv')
 validation_input = TrainingInput(&quot;s3://{}/{}&quot;.format(bucket,'validation'), 
 content_type='csv')


 # execute the XGBoost training job
 estimator.fit({'train': train_input, 'validation': validation_input})
</code></pre>
<p>I have rewritten as above and could run training.
thank you !</p>
","17257284",1
1492,70247085,2,70225564,2021-12-06 14:29:38,0,"<p>Your <a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/studio-tasks-manage-storage.html"" rel=""nofollow noreferrer"">home directory in Amazon SageMaker Studio is stored in Amazon EFS file system</a>. The file system grows and shrink as you add and remove files.<br />
According to <a href=""https://docs.aws.amazon.com/efs/latest/ug/limits.html#limits-fs-specific"" rel=""nofollow noreferrer"">Amazon EFS limits</a> the <strong>maximum size for a single file is 47.9TiB.</strong></p>
","121956",0
1493,70249692,2,70248817,2021-12-06 17:46:46,2,"<p>Sounds like you need an inference DAG. Amazon SageMaker Inference pipelines currently supports only a chain of handlers, where the output of handler N is the input for handler N+1.</p>
<p>You could change model1's predict_fn() to return both (input_object, outputs), and output_fn(). output_fn() will receive these two objects as the predictions, and will handle serializing both as json. model2's input_fn() will need to know how to parse this pair input.</p>
<p>Consider implementing this as a generic pipeline handling mechanism that adds the input to the model's output. This way you could reuse it for all models and pipelines.</p>
<p>You could allow the model to be deployed as a standalone model, and as a part of a pipeline, and apply the relevant input/output handling behavior that will be triggered by the presence of an environment variable (<code>Environment</code> dict), which you can specify when <a href=""https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/sagemaker.html#SageMaker.Client.create_model"" rel=""nofollow noreferrer"">creating</a> the inference pipelines model.</p>
","121956",5
1494,70255055,2,70252478,2021-12-07 04:44:33,2,"<p>This appears to be a bug introduced by Microsoft's rollout of their new Compute Common Runtime.</p>
<p>If I go into any nodes failing with the <code>JobConfigurationMaxSizeExceeded</code> exception and manually set <code>AZUREML_COMPUTE_USE_COMMON_RUNTIME:false</code> in their  <code>Environment JSON</code> field, then they work correctly.</p>
","1486998",0
1495,70266662,2,70258080,2021-12-07 20:38:32,6,"<p>One way would be to <a href=""https://stackoverflow.com/questions/12332975/installing-python-module-within-code"">call pip from Python</a>:</p>

<pre class=""lang-python prettyprint-override""><code>subprocess.check_call([sys.executable, &quot;-m&quot;, &quot;pip&quot;, &quot;install&quot;, package])
</code></pre>
<p>Another way would be to use an <a href=""https://sagemaker.readthedocs.io/en/stable/frameworks/sklearn/sagemaker.sklearn.html"" rel=""nofollow noreferrer"">SKLearn Estimator</a> (training job) instead, to do the same thing. You can provide the <code>source_dir</code>, which can include a <code>requirements.txt</code> file, and these requirements will be installed for you</p>
<pre class=""lang-python prettyprint-override""><code>estimator = SKLearn(
    entry_point=&quot;foo.py&quot;,
    source_dir=&quot;./foo&quot;, # no trailing slash! put requirements.txt here
    framework_version=&quot;0.23-1&quot;,
    role = ...,
    instance_count = 1,
    instance_type = &quot;ml.m5.large&quot;
)
</code></pre>
","223478",0
1496,70271902,2,68682085,2021-12-08 08:24:37,0,"<p>If we look at the networks that were created on a clean Sagemaker Notebook instance, we can notice a user-defined bridge network named <code>sagemaker-local</code>:</p>
<pre class=""lang-bash prettyprint-override""><code>$ docker network ls
NETWORK ID          NAME                DRIVER              SCOPE
f1d5a59a8c9e        bridge              bridge              local
6142e6764495        host                host                local
194adfb00f0a        none                null                local
99de6c086aa8        sagemaker-local     bridge              local
</code></pre>
<p>If we then attach to this custom bridge, we will be able to assume the correct role (the one attached to the Sagemaker Notebook instance itself):</p>
<pre class=""lang-bash prettyprint-override""><code>$ docker run --network sagemaker-local amazon/aws-cli sts get-caller-identity
{
    &quot;UserId&quot;: &quot;AROAU2P5VGYMMVxxxxxJ:SageMaker&quot;,
    &quot;Account&quot;: &quot;1234567890&quot;,
    &quot;Arn&quot;: &quot;arn:aws:sts::1234567890:assumed-role/DataScientist/SageMaker&quot;
}
</code></pre>
<hr />
<p><strong>UPDATE</strong></p>
<p>As of this writing (10 Dec 2021) you don't need to attach to <code>sagemaker-local</code> bridge network anymore, the default <code>bridge</code> will work as well (note <code>--network bridge</code> is implicit in this call):</p>
<pre class=""lang-bash prettyprint-override""><code>$ docker run amazon/aws-cli sts get-caller-identity
{
    &quot;UserId&quot;: &quot;AROAU2P5VGYMMVxxxxxJ:SageMaker&quot;,
    &quot;Account&quot;: &quot;1234567890&quot;,
    &quot;Arn&quot;: &quot;arn:aws:sts::1234567890:assumed-role/DataScientist/SageMaker&quot;
}
</code></pre>
<p>Make sure you restart your SageMaker Notebook instance.</p>
<p>Also, <a href=""https://github.com/aws/amazon-sagemaker-examples/blob/master/sagemaker-python-sdk/mxnet_gluon_mnist/setup.sh"" rel=""nofollow noreferrer"">here</a> I found some manual patching (iptables etc.), but with the update it's already patched.</p>
<p>Thanks to AWS who fixed this :)</p>
<hr />
<p><strong>UPDATE (October 2022)</strong>
It looks this had been fixed only in <code>notebook-al2-v1</code> and <code>notebook-al2-v2</code>. The problem with the <code>notebook-al1-v1</code> is that the network patch for the <code>sagemaker-local</code> user-defined bridge network is applied only once the instance is started, which means that after a Notebook restart one would need to apply the <a href=""https://github.com/aws/amazon-sagemaker-examples/blob/main/sagemaker-python-sdk/mxnet_gluon_mnist/setup.sh#L80"" rel=""nofollow noreferrer"">network patch</a> manually.</p>
","2996867",0
1497,70294676,2,70287087,2021-12-09 18:06:18,0,"<blockquote>
<p>As the Pipeline object doesn't support .deploy method, how to deploy this pipeline?</p>
</blockquote>
<p>Pipeline does not have a <code>.deploy()</code> method, no</p>
<p>Use <code>pipeline.upsert(role_arn='...')</code> to create/update the pipeline definition to SageMaker, then call <code>pipeline.start()</code> . Docs <a href=""https://sagemaker.readthedocs.io/en/stable/workflows/pipelines/sagemaker.workflow.pipelines.html#pipeline"" rel=""nofollow noreferrer"">here</a></p>
<blockquote>
<p>While inference/scoring, When we receive a raw data(single row for each source), how to trigger the pipeline?</p>
</blockquote>
<p>There are actually two types of pipelines in SageMaker. Model Building Pipelines (which you have in your question), and <a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/inference-pipelines.html"" rel=""nofollow noreferrer"">Serial Inference Pipelines</a>, which are used for Inference. AWS definitely should have called the former &quot;workflows&quot;</p>
<p>You can use a model building pipeline to setup a serial inference pipeline</p>
<p>To do pre-processing in a serial inference pipeline, you want to train an encoder/estimator (such as SKLearn) and save its model. Then train a learning algorithm, and save its model, then create a <a href=""https://sagemaker.readthedocs.io/en/stable/api/inference/pipeline.html"" rel=""nofollow noreferrer"">PipelineModel</a> using both models</p>
","223478",4
1498,70336438,2,70335470,2021-12-13 14:43:44,0,"<p><code>download</code> has a forward-slash <code>/</code> as first character of save directory (second parameter). I removed this:</p>
<pre><code>download = od.download(url, 'data/gri/')
</code></pre>
<p>Output:</p>
<pre><code>...
Downloading http://youxin.37.com/uploads/file/1556248045.pdf to data/gri/1556248045.pdf
450560it [00:02, 207848.59it/s]
...
</code></pre>
","16105404",1
1499,70352941,2,70306493,2021-12-14 17:13:31,0,"<p>This can be solved by increasing the number of epochs in training to a more realistic value.</p>
<p>Currently, the model trains in fewer than 300 seconds (which is when the following timestamp would be recorded) and presumably the loss function.</p>
<p>Changes to make:</p>
<pre class=""lang-py prettyprint-override""><code>hyperparameters={
    'epochs': 100, # increase the number of epochs to realistic value!
    'train_batch_size': batch_size,
    'model_name': model_checkpoint,
    'task': task,
}
</code></pre>
","5125264",0
1500,70366989,2,70279636,2021-12-15 16:19:23,0,"<p>It looks like the bug was fixed. I just ran it on a cluster without changing any of the parameters. Thank you Yutong for the help!</p>
","13764121",0
1501,70382233,2,70335049,2021-12-16 16:11:09,0,"<p>So the issue really was related to hosting the model using the sagemaker inference toolkit and MMS which always uses the multi-model scenario which is not supported by serverless inference.</p>
<p>I ended up writing my own Flask API which actually is nearly as easy and more customizable. Ping me for details if you're interested.</p>
","3414626",3
1502,70412229,2,70411715,2021-12-19 14:19:40,4,"<p>This is Python code mixed with shell script magic calls (the <code>!commands</code>).</p>
<p>Magic commands aren't unique to this platform, you can use them in <a href=""https://ipython.readthedocs.io/en/stable/interactive/magics.html"" rel=""nofollow noreferrer"">Jupyter</a>, but this particular code is meant to be run on their platform. In what seems like a fairly convoluted way of running R scripts as processing jobs.</p>
<p>However, the only thing you really need to focus on is the R script, and the final two cell blocks. The instruction at the top (don't change this line) creates a file (preprocessing.R) which gets executed later, and then you can see the results.</p>
<p>Just run all the code cells in that order, with your own custom R code in the first cell. Note the line <code>plot_key = &quot;census_plot.png&quot;</code> in the last cell. This refers to the image being created in the R code. As for other output types (eg text) you'll have to look up the necessary Python package (PIL is an image manipulation package) and adapt accordingly.</p>
<p>Try this to get the CSV file that the R script is also generating (this code is not validated, so you might need to fix any problems that arise):</p>
<pre><code>import csv

csv_key = &quot;plot_data.csv&quot;
csv_in_s3 = &quot;{}/{}&quot;.format(preprocessed_csv_data, csv_key)
!aws s3 cp {csv_in_s3} .

file = open(csv_key)
dat = csv.reader(file)

display(dat)
</code></pre>
<p>So now you should have an idea of how two different output types the R script example generates are being handled, and from there you can try and adapt your own R code based on what it outputs.</p>
","user17714926",0
1503,70437141,2,70356856,2021-12-21 14:35:00,0,"<p>I'm glad you solved most of your main issues and found a workaround for model declaration.</p>
<p>For your <code>input.output</code> observation on <code>gcs_source_uris</code>, the reason behind it is because the way the function/class returns the value. If you dig inside the class/methods of <code>google_cloud_pipeline_components</code>  you will find that it implements a structure that will allow you to use <code>.outputs</code> from the returned value of the function called.</p>
<p>If you go to the implementation of one of the components of the pipeline you will find that it returns an output array from <code>convert_method_to_component</code> function. So, in order to have that implemented in your custom class/function your function should return a value which can be called as an attribute. Below is a basic implementation of it.</p>
<pre><code>class CustomClass():
     def __init__(self):
       self.return_val = {'path':'custompath','desc':'a desc'}
      
     @property
     def output(self):
       return self.return_val 

hello = CustomClass()
print(hello.output['path'])
</code></pre>
<p>If you want to dig more about it you can go to the following pages:</p>
<ul>
<li><p><a href=""https://github.com/bharathdsce/kubeflow/blob/fcd627714664956b2c280b0109b64633bc99fa05/components/google-cloud/google_cloud_pipeline_components/aiplatform/utils.py#L383"" rel=""nofollow noreferrer"">convert_method_to_component</a>, which is the implementation of <code>convert_method_to_component</code></p>
</li>
<li><p><a href=""https://www.programiz.com/python-programming/property"" rel=""nofollow noreferrer"">Properties</a>, basics of property in python.</p>
</li>
</ul>
","14551567",0
1504,70450847,2,70420241,2021-12-22 14:46:22,1,"<blockquote>
<p>It turns out the kubeflow pipeline 1.7.0 does not work with kubernetes version higher than 1.22. I used kubernetes 1.21.8 with minikube and there is no problem installing kubeflow pipeline 1.7.0.</p>
</blockquote>
<p>Yes, this is correct behaviour.</p>
<p>You have mentioned:</p>
<blockquote>
<p>In particular, what does it mean:</p>
</blockquote>
<pre><code>unable to recognize &quot;github.com/kubeflow/pipelines/manifests/kustomize/cluster-scoped-resources?ref=1.7.0&quot;: no matches for kind &quot;CustomResourceDefinition&quot; in version &quot;apiextensions.k8s.io/v1beta1&quot;
</code></pre>
<p>This is connected directly to <a href=""https://kubernetes.io/blog/2021/07/14/upcoming-changes-in-kubernetes-1-22/#api-changes"" rel=""nofollow noreferrer"">Kubernetes version 1.22 api changes</a>:</p>
<blockquote>
<p>The  <strong>v1.22</strong>  release will stop serving the API versions we've listed immediately below. These are all beta APIs that were previously deprecated in favor of newer and more stable API versions.</p>
<ul>
<li>Beta versions of the  <code>ValidatingWebhookConfiguration</code>  and  <code>MutatingWebhookConfiguration</code>  API (the  <strong>admissionregistration.k8s.io/v1beta1</strong>  API versions)</li>
<li>The beta  <code>CustomResourceDefinition</code>  API (<strong>apiextensions.k8s.io/v1beta1</strong>)</li>
<li>The beta  <code>APIService</code>  API (<strong>apiregistration.k8s.io/v1beta1</strong>)</li>
<li>The beta  <code>TokenReview</code>  API (<strong>authentication.k8s.io/v1beta1</strong>)</li>
<li>Beta API versions of  <code>SubjectAccessReview</code>,  <code>LocalSubjectAccessReview</code>,  <code>SelfSubjectAccessReview</code>  (API versions from  <strong>authorization.k8s.io/v1beta1</strong>)</li>
<li>The beta  <code>CertificateSigningRequest</code>  API (<strong>certificates.k8s.io/v1beta1</strong>)</li>
<li>The beta  <code>Lease</code>  API (<strong>coordination.k8s.io/v1beta1</strong>)</li>
<li>All beta  <code>Ingress</code>  APIs (the  <strong>extensions/v1beta1</strong>  and  <strong>networking.k8s.io/v1beta1</strong>  API versions)</li>
</ul>
</blockquote>
<p>As of version 1.22 it is not possible to use <code>apiextensions.k8s.io/v1beta1</code>(this API is no longer available) and if you want to install a pipeline using this type of API, you can only use Kubernetes version 1.21.</p>
","15407542",0
1505,70476937,2,70445997,2021-12-24 22:41:29,2,"<p>I managed to create my pipeline using docker and docker_env in MLflow. It is not necessary to run d-in-d, the &quot;sibling approach&quot; is sufficient. This approach is described here:</p>
<p><a href=""https://jpetazzo.github.io/2015/09/03/do-not-use-docker-in-docker-for-ci/"" rel=""nofollow noreferrer"">https://jpetazzo.github.io/2015/09/03/do-not-use-docker-in-docker-for-ci/</a></p>
<p>and it is the preferred method to avoid d-in-d.</p>
<p>One needs to be very careful when mounting volumes within the primary and secondary docker environments: all volume mounts happen in the host machine.</p>
","10857741",0
1506,70513415,2,70513398,2021-12-28 22:59:15,8,"<p>use the following commands to install git-lfs</p>
<pre><code>!curl -s https://packagecloud.io/install/repositories/github/git-lfs/script.rpm.sh | sudo bash

!sudo yum install git-lfs -y

!git lfs install
</code></pre>
<p>that should make it work</p>
","17784343",0
1507,70542175,2,70538271,2021-12-31 12:07:40,2,"<p>These are static, unique URLs that can be associated with multiple published pipeline versions (you can make one pipeline the default).</p>
<p>Pipeline Endpoints:<a href=""https://learn.microsoft.com/en-us/python/api/azureml-pipeline-core/azureml.pipeline.core.pipelineendpoint?view=azure-ml-py"" rel=""nofollow noreferrer"">azureml.pipeline.core.PipelineEndpoint class - Azure Machine Learning Python | Microsoft Docs</a></p>
","11297406",0
1508,70548551,2,70047920,2022-01-01 11:24:33,2,"<p>I managed to update sklearn to version 0.24.2 via the following command:</p>
<pre><code>!conda update scikit-learn --yes
</code></pre>
<p>To further update it, you probably have to also update Python, which is version 3.6 in the current conda_python3 kernel on Sagemaker.</p>
<p>It also looks promising to create your custom conda environment, as explained here: <a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/nbi-add-external.html"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/sagemaker/latest/dg/nbi-add-external.html</a></p>
","8971938",0
1509,70561340,2,70560288,2022-01-03 03:08:55,4,"<p>If you are using a local remote this way, you won't be able to have to the same <code>url</code> on both platforms since the mount points are different (as you already realized).</p>
<p>The simplest way to configure this would be to pick one (Linux or Windows) <code>url</code> to use as your default case that gets git-committed into <code>.dvc/config</code>. On the other platform you (or your users) can override that <code>url</code> in the local configuration file: <code>.dvc/config.local</code>.</p>
<p>(Note that <code>.dvc/config.local</code> is a git-ignored file and will not be included in any commits)</p>
<p>So if you wanted Windows to be the default case, in <code>.dvc/config</code> you would have:</p>
<pre><code> [core]
    analytics = false
    remote = remote_storage
['remote &quot;remote_storage&quot;']
    url = \\my_shared_storage\project_dir
</code></pre>
<p>and on your Linux machine you would add the file <code>.dvc/config.local</code> containing:</p>
<pre><code>['remote &quot;remote_storage&quot;']
    url = /mnt/mount_point/project_dir
</code></pre>
<p>See the DVC docs for <code>dvc config --local</code> and <code>dvc remote modify --local</code> for more details:</p>
<ul>
<li><a href=""https://dvc.org/doc/command-reference/config#description"" rel=""nofollow noreferrer"">https://dvc.org/doc/command-reference/config#description</a></li>
<li><a href=""https://dvc.org/doc/command-reference/remote/modify#command-options-flags"" rel=""nofollow noreferrer"">https://dvc.org/doc/command-reference/remote/modify#command-options-flags</a></li>
</ul>
","1538451",0
1510,70565424,2,70268372,2022-01-03 11:55:36,1,"<p>Here is <a href=""https://learn.microsoft.com/en-us/azure/machine-learning/concept-automated-ml#customize-featurization"" rel=""nofollow noreferrer"">link</a> to the document for feature customization.</p>
<p>Using the SDK you can specify &quot;feauturization&quot;: 'auto' / 'off' / 'FeaturizationConfig' in your <a href=""https://learn.microsoft.com/en-us/python/api/azureml-train-automl-client/azureml.train.automl.automlconfig.automlconfig?view=azure-ml-py"" rel=""nofollow noreferrer"">AutoMLConfig</a> object. Learn more about <a href=""https://learn.microsoft.com/en-us/azure/machine-learning/how-to-configure-auto-features"" rel=""nofollow noreferrer"">enabling featurization</a>.</p>
<p>Automated ML tries out different ML models that have different settings which control for overfitting.  Automated ML will pick which overfitting parameter configuration is best based on the best score (e.g. accuracy) it gets from hold-out data.  The kind of overfitting settings these models has includes:</p>
<ul>
<li>Explicitly penalizing overly-complex models in the loss function that the ML model is optimizing</li>
<li>Limiting model complexity before training, for example by limiting the size of trees in an ensemble tree learning model (e.g. gradient boosting trees or random forest)</li>
</ul>
<p><a href=""https://learn.microsoft.com/en-us/azure/machine-learning/concept-manage-ml-pitfalls"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/machine-learning/concept-manage-ml-pitfalls</a></p>
","11297406",0
1511,70565710,2,70380861,2022-01-03 12:20:37,1,"<p>As rightly suggested by @Anand Sowmithiran in comment section, This looks more like a bug with the SDK.</p>
<p>You can raise <a href=""https://azure.microsoft.com/en-us/support/create-ticket/"" rel=""nofollow noreferrer"">Azure support ticket</a></p>
","11104805",1
1512,70566482,2,70565147,2022-01-03 13:27:57,2,"<p>After going over <a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/data-wrangler-shut-down.html"" rel=""nofollow noreferrer"">the docs</a>, I found that I needed to shut down the wrangler instance under Running Instances and Kernels button.</p>
","10280578",0
1513,70568872,2,70544873,2022-01-03 16:41:11,2,"<p>it turns out the issue is a typo in curl script. it's Sepal.Width not Sepal.With.</p>
<pre><code>curl --location --request POST 'localhost:5000/invocations' \
--header 'Content-Type: application/json' \
--data-raw '{&quot;columns&quot;: [&quot;Sepal.Length&quot;, &quot;Sepal.Width&quot;, &quot;Petal.Length&quot;, &quot;Petal.Width&quot;], &quot;data&quot;: [[5.2, 4.1, 1.5, 0.1], [6.4, 2.8, 5.6, 2.1], [7.9, 3.8, 6.4, 2.0], [6.7, 3.1, 5.6, 2.4], [6.3, 3.4, 5.6, 2.4]]}'
</code></pre>
","2951327",0
1514,70574515,2,70539698,2022-01-04 05:46:56,0,"<p>I found the solution.</p>
<p>I have added <code>[&quot;AWS_DEFAULT_REGION&quot;]</code> to the environment variables and it worked.</p>
","848510",0
1515,70590195,2,70560467,2022-01-05 09:03:45,3,"<p>You can clone the repo and build the image yourself and push it to your container registry.
This is one workaround to fix this until the official image is back.</p>
<pre><code>git clone https://github.com/jlewi/cloud-endpoints-controller.git
cd cloud-endpoints-controller
git checkout 0.2.1
docker build . -t &lt;YOUR DOCKER REGISTRY&gt;/cloud-endpoints-controller:0.2.1
docker push &lt;YOUR DOCKER REGISTRY&gt;/cloud-endpoints-controller:0.2.1
</code></pre>
<p>And this use the new image in your pod spec.</p>
","11355682",0
1516,70608819,2,70599052,2022-01-06 14:38:47,0,"<p>This bucket policy made it work :</p>
<pre class=""lang-json prettyprint-override""><code>{
    &quot;Version&quot;: &quot;2012-10-17&quot;,
    &quot;Statement&quot;: [
        {
            &quot;Sid&quot;: &quot;Statement1&quot;,
            &quot;Effect&quot;: &quot;Allow&quot;,
            &quot;Principal&quot;: {
                &quot;AWS&quot;: &quot;arn:aws:iam::&lt;account-id&gt;:role/&lt;iot-role&gt;&quot;
            },
            &quot;Action&quot;: &quot;*&quot;,
            &quot;Resource&quot;: [
                &quot;arn:aws:s3:::&lt;bucket-name&gt;&quot;,
                &quot;arn:aws:s3:::&lt;bucket-name&gt;/*&quot;
            ]
        }
    ]
}
</code></pre>
<p>I still don't fully get it, because the role had full access on s3 buckets so i don't know why editing the bucket's policy changed something, but it works.</p>
","13071947",0
1517,70620323,2,70571948,2022-01-07 11:09:28,1,"<p>First open a terminal, using the same compute target as you want to use with your Notebook afterwards, and to use and <strong>existing environment</strong> you can do:</p>
<pre><code>conda activate existing_env
conda install ipykernel
python -m ipykernel install --user --name existing_env --display-name &quot;Python 3.8 - Existing Environment&quot;   
</code></pre>
<p>However, to create a <strong>new environment</strong> and use it in you AzureML Notebook, you have to do the following commands:</p>
<pre><code>conda create --name new_env python=3.8
conda activate new_env
conda install pip
conda install ipykernel
python -m ipykernel install --user --name new_env --display-name &quot;Python 3.8 - New Environment&quot;
</code></pre>
<p>And then last, but not least, you have to edit the Jupyter Kernel display names:</p>
<p><strong>IMPORTANT</strong> Please ensure you are comfortable running all these steps:</p>
<pre><code>jupyter kernelspec list
cd &lt;folder-that-matches-the-kernel-of-your-environment&gt;
sudo nano kernel.json
</code></pre>
<p>Then edit the name to match what you want and save the file.</p>
","13526512",0
1518,70627512,2,70620074,2022-01-07 21:45:41,2,"<p>I believe that <strong>mlflow models serve</strong> will only accept POST input to the /invocations path.<br />
If you want something custom I would suggest:</p>
<ol>
<li><a href=""https://mlserver.readthedocs.io/en/latest/"" rel=""nofollow noreferrer"">Seldon</a></li>
<li>Create a simple Flask app to do it as illustrated in this blog <a href=""https://docs.faculty.ai/user-guide/experiments/index.html"" rel=""nofollow noreferrer"">post</a>.</li>
</ol>
","9388056",0
1519,70641881,2,70632239,2022-01-09 13:45:20,1,"<p>You could use whatever IDE you choose (including your laptop).<br />
SaegMaker tuning job (<a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/automatic-model-tuning-ex.html"" rel=""nofollow noreferrer"">example</a>) is <strong>asynchronous</strong>, so you can safely close your IDE after launching it. You can <a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/automatic-model-tuning-monitor.html"" rel=""nofollow noreferrer"">monitor the job the AWS web console,</a> or with a <a href=""https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_DescribeHyperParameterTuningJob.html"" rel=""nofollow noreferrer"">DescribeHyperParameterTuningJob API call</a>.</p>
<p>You can launch TensorFlow, PyTorch, XGBoost, Scikit-learn, and other <a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/frameworks.html"" rel=""nofollow noreferrer"">popular ML frameworks</a>, using one of the built-in framework containers, avoiding the extra work of bringing your own container.</p>
","121956",2
1520,70657401,2,70603137,2022-01-10 18:39:43,1,"<p>Yes, though it depends on what part of SageMaker you mean (Training, Notebook, Inference, etc).</p>
<p>Last week, an integration between SageMaker and Delta Lake was documented here (custom docker in the SageMaker Processing API)</p>
<p><a href=""https://github.com/eitansela/sagemaker-delta-sharing-demo/tree/main/delta_lake_bring_your_own_container_processing"" rel=""nofollow noreferrer"">https://github.com/eitansela/sagemaker-delta-sharing-demo/tree/main/delta_lake_bring_your_own_container_processing</a></p>
","5331834",0
1521,70675984,2,66592313,2022-01-12 03:28:12,0,"<p>This had no answers for 10 months, and now they are coming in :).  I figuerd this out quite a while ago but haven't gotten around to posting the answer.  Here it is.</p>
<p>From the training script, you can get the workspace from the run context as follows:</p>
<pre><code>from azureml.core import Run
Run.get_context()
ws = run.experiment.workspace
</code></pre>
","9059160",0
1522,70699106,2,70335823,2022-01-13 15:30:39,1,"<p>I had a similar situation where python couldn't find the files that were supposed to exist in the root of the Azure ML project folder after deploying. After investigation, I realized that Azure ML invokes your scripting code from a different root folder.</p>
<p>Here is an example of an operation that reads from the relative path where your code exists:</p>
<pre><code>    SCRIPT_DIRECTORY = os.path.dirname(os.path.realpath(__file__))
    with open(SCRIPT_DIRECTORY+'filename.json', 'w') as outfile:
        json.dump(dict_object, outfile)
</code></pre>
<p>You can then join to <code>SCRIPT_DIRECTORY </code> the relative path of your git folder, before your output.</p>
<p>Alternatively, as per your comment &quot;./output is not getting created&quot;, you can force it with:</p>
<p><code>os.makedirs(&quot;./outputs&quot;, exist_ok=True)</code></p>
<p><code>exist_ok</code> (optional) : A default value <code>False</code> is used for this parameter. If the target directory already exists an <code>OSError</code> is raised if its value is <code>False</code> otherwise not. For value <code>True</code> leaves directory unaltered.</p>
","13526512",5
1523,70736460,2,70692270,2022-01-17 04:15:13,0,"<p>Finally after lot of head banging, I have been able to consistently repro this bug in another Azure ML Workspace.</p>
<p>I tried deploying the same sample in a brand new Azure ML workspace created and it went smoothly.</p>
<p>At this point I remembered that I had upgraded the Storage Account of my previous AML Workspace to DataLake Gen2.</p>
<p>So I did the same upgrade in this new workspace’s storage account. After the upgrade, when I try to deploy the same endpoint, I get the same <code>DriverFileNotFoundError</code>!</p>
<p>It seems Azure ML does not support Storage Account with DataLake Gen2 capabilities although the support page says otherwise. <a href=""https://learn.microsoft.com/en-us/azure/machine-learning/how-to-access-data#supported-data-storage-service-types"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/machine-learning/how-to-access-data#supported-data-storage-service-types</a>.</p>
<p>At this point my only option is to recreate a new workspace and deploy my code there. Hope Azure team fixes this soon.</p>
","348805",0
1524,70739534,2,70730897,2022-01-17 10:03:49,1,"<p>You can. Use <a href=""https://aws.amazon.com/blogs/machine-learning/use-the-amazon-sagemaker-local-mode-to-train-on-your-notebook-instance/"" rel=""nofollow noreferrer"">SageMaker Local</a> to execute training and inference locally in docker containers on your laptop.<br />
In the Estimator, you'll specify: <code>instance_type='local'</code> or <code>instance_type='local_gpu'</code></p>
","121956",2
1525,70746339,2,70745798,2022-01-17 19:05:26,1,"<p>Solved it!!!!!!!!!!!!</p>
<pre><code>prefix = &quot;checking-with-new-bucket&quot;
training_input_path = sagemaker_session.upload_data('train.csv', bucket = 'checking-with-new-bucket',key_prefix = prefix + &quot;/training&quot;)
training_input_path
</code></pre>
<p>Which gave output as</p>
<pre><code>'s3://checking-with-new-bucket/checking-with-new-bucket/training/train.csv'
</code></pre>
","13844933",0
1526,70776637,2,70455676,2022-01-19 19:26:46,1,"<p>I was able to work through the issue! Here is the code for the register step that ended up working with my Tensorflow model:</p>
<pre><code># Package the model
pipeline_model = PipelineModel(models=[model], role=params[&quot;role&quot;].default_value, sagemaker_session=session)

# Create a RegisterModel step, which registers the model with Sagemaker Model Registry.
register_step = RegisterModel(
    name=&quot;Bar&quot;,
    model=pipeline_model,
    model_data=train_step.properties.ModelArtifacts.S3ModelArtifacts,
    content_types=[&quot;application/json&quot;],
    response_types=[&quot;application/json&quot;],
    inference_instances=[config[&quot;instance&quot;][&quot;inference&quot;]],
    transform_instances=[config[&quot;instance&quot;][&quot;transform&quot;]],
    model_package_group_name=&quot;Foo&quot;,
    approval_status=&quot;Approved&quot;,
)
</code></pre>
","15764456",0
1527,70783199,2,70771911,2022-01-20 08:51:36,2,"<p>I've found the solution of the problem. I hope it can be of help to someone.
It works if you set all the parameter name and widget name in lower case.</p>
<pre class=""lang-py prettyprint-override""><code>start_date_param = PipelineParameter(name=&quot;start_date&quot;, default_value='2022-01-19')

# Define data ingestion step
data_loading_step = DatabricksStep(
        name=&quot;Data Loading&quot;,
        existing_cluster_id=db_cluster_id,
        notebook_path=data_loading_path,
        run_name=&quot;Loading raw data&quot;,
        notebook_params={
            'start_date': start_date_param,
        },
        compute_target=dbricks_compute,
        instance_pool_id=instance_pool_id,
        num_workers=num_workers,
        allow_reuse=False
    )
</code></pre>
<pre><code>dbutils.widgets.text(&quot;start_date&quot;, &quot;&quot;, &quot;start_date(YYYY-MM-DD)&quot;)
</code></pre>
","10901182",1
1528,70802505,2,70779880,2022-01-21 13:58:19,0,"<p>I have resolved this issue. My problem was that I was splitting the data into test and train BEFORE converting the data into doc-term matrices, which resulted in test and train datasets of different dimensionality, which threw off SageMaker's algorithm. Once I convereted all of the input data into a doc-term matrix, and THEN split it into test and train, the hyperparameter optimization operation completed.</p>
","17935130",0
1529,70825062,2,70820661,2022-01-23 18:03:46,0,"<p>So apparently <code>--default-artifact-root</code> argument has to be used when launching server/ui. The only downside is that that default artifact root is relative to development environment, so if you are running mlflow server in docker and specify default-artifact-root to e.g. <code>some/path</code> then the artifacts are going to be saved to your <strong>local machine</strong> to that path (<strong>not inside docker container</strong>). Probably the best solution is to use remote storage such as S3/Blob.</p>
","6156353",2
1530,70860992,2,70833499,2022-01-26 09:14:08,1,"<p>OK, I got it working : I started over from scratch and it worked.</p>
<p>I have no idea what was wrong in all my preceding tries, and that is terrible.</p>
<p>Multiple problems and how I (think I) solved them :</p>
<ul>
<li><code>joblib</code> : I actually didn't need it to load my Keras model. But the problem was not with this specific library, rather that I couldn't add dependencies to the inference environment.</li>
<li><code>Environment</code> : finally, I was only able to make things work with a custom env : <code>Environment.from_conda_specification(name=version, file_path=&quot;conda_dependencies.yml&quot;)</code> . I haven't been able to add my libraries (or specify a specific package version) to a &quot;currated environment&quot;. I don't know why though...</li>
<li><code>TensorFlow</code> : last problem I had was that I trained and registered my model in AzureML Notebook's <code>azureml_py38_PT_TF</code> kernel (<code>tensorflow==2.7.0</code>), and tried to load it in the inference Docker image (<code>tensorflow==2.4.0</code>). So I had to specify the version of TensorFlow I wanted to use in the inference image (which required the previous point to be solved).</li>
</ul>
<p>What finally worked :</p>
<ul>
<li>notebook.ipynb</li>
</ul>
<pre class=""lang-py prettyprint-override""><code>import uuid
from azureml.core import Workspace, Environment, Model
from azureml.core.webservice import AciWebservice
from azureml.core.model import InferenceConfig


version = &quot;test-&quot;+str(uuid.uuid4())[:8]

env = Environment.from_conda_specification(name=version, file_path=&quot;conda_dependencies.yml&quot;)
inference_config = InferenceConfig(entry_script=&quot;score.py&quot;, environment=env)

ws = Workspace.from_config()
model = Model(ws, model_name)

aci_config = AciWebservice.deploy_configuration(
    cpu_cores=1,
    memory_gb=1,
)

service = Model.deploy(
    workspace=ws,
    name=version,
    models=[model],
    inference_config=inference_config,
    deployment_config=aci_config,
    overwrite=True,
)

service.wait_for_deployment(show_output=True)
</code></pre>
<ul>
<li>conda_dependencies.yml</li>
</ul>
<pre class=""lang-yaml prettyprint-override""><code>channels:
- conda-forge
dependencies:
- python=3.8
- pip:
  - azureml-defaults
  - azureml-sdk
  - numpy
  - tensorflow==2.7.0

</code></pre>
<ul>
<li>score.py</li>
</ul>
<pre class=""lang-py prettyprint-override""><code>import os
import json
import numpy as np
import tensorflow as tf


def init():
    global model

    model_path = os.path.join(os.getenv(&quot;AZUREML_MODEL_DIR&quot;), &quot;model/data/model&quot;)
    model = tf.keras.models.load_model(model_path)



def run(raw_data):
    data = np.array(json.loads(raw_data)[&quot;data&quot;])
    y_hat = model.predict(data)

    return y_hat.tolist()

</code></pre>
","1415232",0
1531,70878131,2,70877982,2022-01-27 11:53:22,3,"<p>Hi when you see json format think more dict.
So write it like this:</p>
<pre><code>Containers:
- ModelPackageName: arn:aws:sagemaker:us-east-1:123456789123:model-package/name/25
  Environment:
     SAGEMAKER_CONTAINER_LOG_LEVEL: 20
</code></pre>
<p>For IAM Policies the PolicyDocument is json type and this is how AWS do it in their exempel:</p>
<pre><code>Type: 'AWS::IAM::Policy'
Properties:
  PolicyName: CFNUsers
  PolicyDocument:
    Version: &quot;2012-10-17&quot;
    Statement:
      - Effect: Allow
        Action:
          - 'cloudformation:Describe*'
          - 'cloudformation:List*'
          - 'cloudformation:Get*'
        Resource: '*'
  Groups:
    - !Ref CFNUserGroup
</code></pre>
","14711735",3
1532,70928762,2,70928761,2022-01-31 15:43:57,1,"<p>The only way I found to fix the issue was to use the <code>Model</code> Class instead:</p>
<pre><code>        model = Model.register(
            workspace=ws,
            model_name=model_name,
            model_path=model_path,
            model_framework=Model.Framework.SCIKITLEARN,
            model_framework_version=sklearn.__version__,
            description='Model Deescription',
            tags={'Name' : 'ModelName', 'Type' : 'Production'},
            model_framework=Model.Framework.SCIKITLEARN,
            model_framework_version='1.0'
            )
</code></pre>
","13526512",0
1533,70929124,2,70929123,2022-01-31 16:08:50,0,"<p>The only way I've found so far to make this work, was to run it on a terminal of the compute-target itself. That's how the docker error goes away. Trying to run the experiment from a terminal of a different compute instance raises the exception.</p>
","13526512",0
1534,70930507,2,70928144,2022-01-31 17:45:43,1,"<p>You need to make the &quot;username&quot; part of the config personalized based on who is running the command. There are a few options to do this (based on <a href=""https://dvc.org/doc/command-reference/remote/modify#available-parameters-per-storage-type"" rel=""nofollow noreferrer"">this document</a>, see the SSH part):</p>
<h2>Basic options are:</h2>
<ul>
<li>User defined in the SSH config file (e.g. <code>~/.ssh/config</code>) for this host (URL);</li>
<li>Current system user;</li>
</ul>
<p>So, the simplest even options could be just remove it from the URL and rely on the current system user?</p>
<h2>Local (git-ignored or per-project DVC config) config</h2>
<p>You could do is to remove the <code>username</code> part from the <code>url</code> and run something like this:</p>
<pre><code>dvc remote modify --local storage_server user username
</code></pre>
<p><code>--local</code> here means that DVC will create a separate additional config that will be ignored by Git. This way if every user runs this command in every project they use they will customize the username.</p>
<hr />
<p>Let me know if that helps or something doesn't work. I'll try to help.</p>
","298182",2
1535,70956659,2,70955450,2022-02-02 13:40:35,1,"<p>With your current code sample, it is not quite clear what specific task you are performing, but for the sake of this answer, I'll assume you're doing text classification.</p>
<p>Most importantly, though, we can read the following in <a href=""https://huggingface.co/docs/sagemaker/reference#inference-toolkit-api"" rel=""nofollow noreferrer"">Huggingface's Sagemaker reference document</a> (bold highlight by me):</p>
<blockquote>
<p>The Inference Toolkit accepts inputs in the inputs key, and <strong>supports additional <code>pipelines</code> parameters in the <code>parameters</code> key</strong>. You can provide any of the supported <code>kwargs</code> from <code>pipelines</code> as parameters.</p>
</blockquote>
<p>If we check out the <a href=""https://huggingface.co/docs/transformers/v4.16.2/en/main_classes/pipelines#transformers.TextClassificationPipeline.__call__"" rel=""nofollow noreferrer"">accepted arguments by the <code>TextClassificationPipeline</code></a>, we can see that there is indeed one that returns all samples:</p>
<blockquote>
<p><code>return_all_scores</code> (bool, optional, defaults to False) — Whether to return scores for all labels.</p>
</blockquote>
<p>While I unfortunately don't have access to Sagemaker inference, I can run a sample to illustrate the output with a local pipeline:</p>
<pre class=""lang-py prettyprint-override""><code>from transformers import pipeline
# uses 2-way sentiment classification model per default
pipe = pipeline(&quot;text-classification&quot;) 

pipe(&quot;I am really angry right now &gt;:(&quot;, return_all_scores=True)
# Output: [[{'label': 'NEGATIVE', 'score': 0.9989138841629028},
#           {'label': 'POSITIVE', 'score': 0.0010860705515369773}]]
</code></pre>
<p>Based on the slightly different input format expected by Sagemaker, coupled with the example given in <a href=""https://github.com/huggingface/notebooks/blob/master/sagemaker/10_deploy_model_from_s3/deploy_transformer_model_from_s3.ipynb"" rel=""nofollow noreferrer"">this notebook</a>, I would assume that a corrected input in your own example code should look like this:</p>
<pre class=""lang-json prettyprint-override""><code>{
    &quot;inputs&quot;: text,
    &quot;parameters&quot;: {&quot;return_all_scores&quot;: True}
}
</code></pre>
","3607203",4
1536,70976650,2,70975320,2022-02-03 18:43:01,4,"<p>You can use the following EventBridge rule pattern:</p>
<pre class=""lang-json prettyprint-override""><code>{
  &quot;source&quot;: [&quot;aws.sagemaker&quot;],
  &quot;detail-type&quot;: [&quot;SageMaker Processing Job State Change&quot;],
  &quot;detail&quot;: {
    &quot;ProcessingJobStatus&quot;: [&quot;Failed&quot;, &quot;Completed&quot;, &quot;Stopped&quot;]
  }
}
</code></pre>
<p>The ProcessingJobStatus list can be modified based on which statuses you want to handle.</p>
<p>You can set a Lambda function as the target of your EventBridge rule.</p>
<p>Here is a sample event which will be passed to your Lambda, taken from AWS console:</p>
<pre class=""lang-json prettyprint-override""><code>{
  &quot;version&quot;: &quot;0&quot;,
  &quot;id&quot;: &quot;0a15f67d-aa23-0123-0123-01a23w89r01t&quot;,
  &quot;detail-type&quot;: &quot;SageMaker Processing Job State Change&quot;,
  &quot;source&quot;: &quot;aws.sagemaker&quot;,
  &quot;account&quot;: &quot;123456789012&quot;,
  &quot;time&quot;: &quot;2019-05-31T21:49:54Z&quot;,
  &quot;region&quot;: &quot;us-east-1&quot;,
  &quot;resources&quot;: [&quot;arn:aws:sagemaker:us-west-2:012345678987:processing-job/integ-test-analytics-algo-54ee3282-5899-4aa3-afc2-7ce1d02&quot;],
  &quot;detail&quot;: {
    &quot;ProcessingInputs&quot;: [{
      &quot;InputName&quot;: &quot;InputName&quot;,
      &quot;S3Input&quot;: {
        &quot;S3Uri&quot;: &quot;s3://input/s3/uri&quot;,
        &quot;LocalPath&quot;: &quot;/opt/ml/processing/input/local/path&quot;,
        &quot;S3DataType&quot;: &quot;MANIFEST_FILE&quot;,
        &quot;S3InputMode&quot;: &quot;PIPE&quot;,
        &quot;S3DataDistributionType&quot;: &quot;FULLYREPLICATED&quot;
      }
    }],
    &quot;ProcessingOutputConfig&quot;: {
      &quot;Outputs&quot;: [{
        &quot;OutputName&quot;: &quot;OutputName&quot;,
        &quot;S3Output&quot;: {
          &quot;S3Uri&quot;: &quot;s3://output/s3/uri&quot;,
          &quot;LocalPath&quot;: &quot;/opt/ml/processing/output/local/path&quot;,
          &quot;S3UploadMode&quot;: &quot;CONTINUOUS&quot;
        }
      }],
      &quot;KmsKeyId&quot;: &quot;KmsKeyId&quot;
    },
    &quot;ProcessingJobName&quot;: &quot;integ-test-analytics-algo-54ee3282-5899-4aa3-afc2-7ce1d02&quot;,
    &quot;ProcessingResources&quot;: {
      &quot;ClusterConfig&quot;: {
        &quot;InstanceCount&quot;: 3,
        &quot;InstanceType&quot;: &quot;ml.c5.xlarge&quot;,
        &quot;VolumeSizeInGB&quot;: 5,
        &quot;VolumeKmsKeyId&quot;: &quot;VolumeKmsKeyId&quot;
      }
    },
    &quot;StoppingCondition&quot;: {
      &quot;MaxRuntimeInSeconds&quot;: 2000
    },
    &quot;AppSpecification&quot;: {
      &quot;ImageUri&quot;: &quot;012345678901.dkr.ecr.us-west-2.amazonaws.com/processing-uri:latest&quot;
    },
    &quot;NetworkConfig&quot;: {
      &quot;EnableInterContainerTrafficEncryption&quot;: true,
      &quot;EnableNetworkIsolation&quot;: false,
      &quot;VpcConfig&quot;: {
        &quot;SecurityGroupIds&quot;: [&quot;SecurityGroupId1&quot;, &quot;SecurityGroupId2&quot;, &quot;SecurityGroupId3&quot;],
        &quot;Subnets&quot;: [&quot;Subnet1&quot;, &quot;Subnet2&quot;]
      }
    },
    &quot;RoleArn&quot;: &quot;arn:aws:iam::012345678987:role/SageMakerPowerUser&quot;,
    &quot;ExperimentConfig&quot;: {},
    &quot;ProcessingJobArn&quot;: &quot;arn:aws:sagemaker:us-west-2:012345678987:processing-job/integ-test-analytics-algo-54ee3282-5899-4aa3-afc2-7ce1d02&quot;,
    &quot;ProcessingJobStatus&quot;: &quot;Completed&quot;,
    &quot;LastModifiedTime&quot;: 1589879735000,
    &quot;CreationTime&quot;: 1589879735000
  }
}
</code></pre>
<p><strong>Edit:</strong></p>
<p>If you want to match a ProcessingJobName with specific prefix:</p>
<pre class=""lang-json prettyprint-override""><code>{
  &quot;source&quot;: [&quot;aws.sagemaker&quot;],
  &quot;detail-type&quot;: [&quot;SageMaker Processing Job State Change&quot;],
  &quot;detail&quot;: {
    &quot;ProcessingJobStatus&quot;: [&quot;Failed&quot;, &quot;Completed&quot;, &quot;Stopped&quot;],
    &quot;ProcessingJobName&quot;: [{
      &quot;prefix&quot;: &quot;standarize-data&quot;
    }]
  }
}
</code></pre>
","17896613",5
1537,70979983,2,70978331,2022-02-04 00:04:39,1,"<p><strong>UPDATE:</strong><br />
I tested the <code>Output[HTML]</code> from kfp sdk v2 and it works but I came across other issues.<br />
First of all, Kubeflow html viewer creates an iframe with blank src and srcdoc=&quot;your static html&quot;. This made it impossible to use an iframe in your html as you'd have a nested iframe (the parent from the html viewer and the nested one from your actual html).</p>
<p><strong>Solution :</strong></p>
<p>I found a solution that works on KFP SDK v1 and v2 for all use cases, I used markdown visualization instead of HTML visualization. Since markdown supports inline HTML, I was able to directly paste my html to the markdown output. Compared to using HTML visualization, this supports iframe.</p>
<p>Here is some code to illustrate the solution :</p>
<pre class=""lang-py prettyprint-override""><code>from kfp.components import create_component_from_func


def markdown_vis(mlpipeline_ui_metadata_path: kfp.components.OutputPath()):
    import json

    metadata = {
    'outputs' : [
    {
      'storage': 'inline',
      'source': f&quot;&lt;iframe src=\&quot;https://www.google.ca/\&quot; frameborder=\&quot;0\&quot; allowFullScreen=\&quot;true\&quot; width=\&quot;950\&quot; height=\&quot;600\&quot;/&gt;&quot;,
      'type': 'markdown',
    }]
    }

    with open(mlpipeline_ui_metadata_path, 'w') as metadata_file:
        json.dump(metadata, metadata_file)

markdown_op = create_component_from_func(markdown_vis)
</code></pre>
<p>I also tested the doc and it works :</p>
<ul>
<li>KFP V1 Doc : <a href=""https://www.kubeflow.org/docs/components/pipelines/sdk/output-viewer/#markdown-1"" rel=""nofollow noreferrer"">https://www.kubeflow.org/docs/components/pipelines/sdk/output-viewer/#markdown-1</a></li>
<li>KFP V2 Doc : <a href=""https://www.kubeflow.org/docs/components/pipelines/sdk/output-viewer/#markdown"" rel=""nofollow noreferrer"">https://www.kubeflow.org/docs/components/pipelines/sdk/output-viewer/#markdown</a></li>
</ul>
","7270039",0
1538,70980502,2,70888883,2022-02-04 01:35:29,2,"<p>When your input data is in JSON line format and you choose a SingleRecord BatchStrategy, your container will receive a single JSON payload body like below</p>
<pre><code>{ &lt;some JSON data&gt; }
</code></pre>
<p>However, if you use MultiRecord, Batch transform will split your JSON line input (which might contain 100 lines for example) into multiple records (say 10 records) all sent at once to your container as shown below:</p>
<pre><code>{ &lt;some JSON data&gt; }
{ &lt;some JSON data&gt; }
{ &lt;some JSON data&gt; }
{ &lt;some JSON data&gt; }
.
.
.
{ &lt;some JSON data&gt; }
</code></pre>
<p>Therefore your container should be able to handle such input for it to work. However, from the error message, I can see it is complaining about invalid JSON format as it reads the second row of the request.</p>
<p>I also noticed that you have supplied <code>ContentType</code> and <code>AcceptType</code> as <code>application/json</code> but instead should be <code>application/jsonlines</code></p>
<p>Could you please test your container to see if it can handle multiple JSON line records per single invocation.</p>
","3348943",0
1539,70983857,2,70968460,2022-02-04 09:10:35,1,"<p>After some time researching the problem I've stumbled upon <a href=""https://github.com/kubeflow/pipelines/issues/6848"" rel=""nofollow noreferrer"">this</a> Github issue. The problem was originated by a mismatch between <a href=""https://google-cloud-pipeline-components.readthedocs.io/en/google-cloud-pipeline-components-0.2.2/index.html"" rel=""nofollow noreferrer""><code>google_cloud_pipeline_components</code></a> and <a href=""https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.19/#envvar-v1-core"" rel=""nofollow noreferrer""><code>kubernetes_api</code></a> docs. In this case, <code>serving_container_environment_variables</code> is typed as an <code>Optional[dict[str, str]]</code> whereas it should have been typed as a <code>Optional[list[dict[str, str]]]</code>. A similar mismatch can be found for <code>serving_container_ports</code> argument as well. Passing arguments following kubernetes documentation did the trick:</p>
<pre class=""lang-py prettyprint-override""><code>model_upload_op = gcc_aip.ModelUploadOp(
    project=&quot;my-project&quot;,
    location=&quot;us-west1&quot;,
    display_name=&quot;session_model&quot;,
    serving_container_image_uri=&quot;gcr.io/my-project/pred:latest&quot;,
    serving_container_environment_variables=[
        {&quot;name&quot;: &quot;MODEL_BUCKET&quot;, &quot;value&quot;: &quot;ml_session_model&quot;}
    ],
    serving_container_ports=[{&quot;containerPort&quot;: 5000}],
    serving_container_predict_route=&quot;/predict&quot;,
    serving_container_health_route=&quot;/health&quot;,
)
</code></pre>
","9046275",0
1540,71004266,2,70976353,2022-02-06 04:46:54,2,"<p>Taking the path from your error message:</p>
<pre><code>en_core_web_sm-2.3.1/config.cfg
</code></pre>
<p>You have a model for v2.3, but it's looking for a <code>config.cfg</code>, which is only a thing in v3 of spaCy. It looks like you upgraded spaCy without realizing it.</p>
<p>There are two ways to fix this. One is to reinstall the model with <code>spacy download</code>, which will get a version that matches your current spaCy version. If you are just starting something that is probably the best idea. Based on the release date of scrubadub, it seems to be intended for use with spaCy v3.</p>
<p>However, note that v2 and v3 are pretty different - if you have a project with v2 of spaCy you might want to downgrade instead.</p>
","355715",0
1541,71012333,2,65102618,2022-02-07 00:29:46,0,"<p>After talking back-in-forth with our IT department I figured out that custom libraries installation was blocked for security reasons.</p>
","6886838",0
1542,71053334,2,70873792,2022-02-09 16:34:59,0,"<p>You've got the right idea: the fewer datapoints are in each file, the less likely a given file is to fail. The issue is that while you can pass a prefix with many files to <a href=""https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_CreateTransformJob.html#SageMaker-CreateTransformJob-request-MaxPayloadInMB"" rel=""nofollow noreferrer"">CreateTransformJob</a>, partitioning one datapoint per file at least requires an S3 read per datapoint, plus a model invocation per datapoint, which is probably not great. Be aware also that <a href=""https://forums.aws.amazon.com/thread.jspa?messageID=1000415&amp;tstart=0"" rel=""nofollow noreferrer"">apparently there are hidden rate limits</a>.</p>
<p>Here are a couple options:</p>
<ol>
<li><p>Partition into small-ish files, and plan on failures being rare. Hopefully, not many of your datapoints would actually fail. If you partition your dataset into e.g. 100 files, then a single failure only requires reprocessing 1% of your data. Note that Sagemaker has built-in retries, too, so most of the time failures should be caused by your data/logic, not randomness on Sagemaker's side.</p>
</li>
<li><p>Deal with failures directly in your model. The same doc you quoted in your question also says:</p>
</li>
</ol>
<blockquote>
<p>If you are using your own algorithms, you can use placeholder text, such as ERROR, when the algorithm finds a bad record in an input file. For example, if the last record in a dataset is bad, the algorithm places the placeholder text for that record in the output file.</p>
</blockquote>
<p>Note that the reason Batch Transform does this whole-file failure is to maintain a 1-1 mapping between rows in the input and the output. If you can substitute the output for failed datapoints with an error message from inside your model, without actually causing the model itself to fail processing, Batch Transform will be happy.</p>
","7859515",2
1543,71085071,2,71023918,2022-02-11 18:41:46,0,"<p>I found out that the python script step copies everything inside the source_dir and therefore in my case it was copying the modules and not the root folder. So I had to put the dir Preprocess inside another dir and mention the new dir as source_dir.</p>
","5368122",0
1544,71128128,2,71127858,2022-02-15 14:23:56,2,"<p><a href=""https://pypi.org/project/scikit-learn/1.0.1/"" rel=""nofollow noreferrer""><code>scikit-learn</code> 1.0.1</a> and up require Python &gt;= 3.7; you use Python 3.6. You need to upgrade Python or downgrade <code>imbalanced-learn</code>. <a href=""https://pypi.org/project/imbalanced-learn/0.8.1/"" rel=""nofollow noreferrer""><code>imbalanced-learn</code> 0.8.1</a> allows Python 3.6 so</p>
<pre><code>!pip install -U &quot;imbalanced-learn &lt; 0.9&quot;
</code></pre>
","7976758",1
1545,71135315,2,71135260,2022-02-16 01:13:37,2,"<p>Here's an ugly way to do it: global state.</p>
<pre class=""lang-py prettyprint-override""><code>class Bar:
    active = 0
    def __init__(self, x):
        self.x = x
    def __enter__(self):
        Bar.active += 1
        return self
    def __exit__(self, *a, **k):
        Bar.active -= 1

from functools import wraps

def assert_bar(func):
    @wraps(func)
    def wrapped(*vargs, **kwargs):
        if Bar.active &lt;= 0:
            # raises even if asserts are disabled
            raise AssertionError()
        return func(*vargs, **kwargs)
    return wrapped
</code></pre>
<p>Unfortunately I don't think there is any non-ugly way to do it. If you aren't going to pass around a <code>Bar</code> instance yourself then you must rely on some state existing somewhere else to tell you that a <code>Bar</code> instance exists and is currently being used as a context manager.</p>
<p>The only way you can avoid that global state is to store the state in the instance, which means the decorator needs to be an instance method and the instance needs to exist before the function is declared:</p>
<pre class=""lang-py prettyprint-override""><code>from functools import wraps

class Bar:
    def __init__(self, x):
        self.x = x
        self.active = 0
    def __enter__(self):
        self.active += 1
        return self
    def __exit__(self, *a, **k):
        self.active -= 1
    def assert_this(self, func):
        @wraps(func)
        def wrapped(*vargs, **kwargs):
            if self.active &lt;= 0:
                raise AssertionError()
            return func(*vargs, **kwargs)
        return wrapped

bar = Bar(1)

@bar.assert_this
def foo(x):
    print(x + 1)

with bar:
    foo(1)
</code></pre>
<p>This is still &quot;global state&quot; in the sense that the function <code>foo</code> now holds a reference to the <code>Bar</code> instance that holds the state. But it may be more palatable if <code>foo</code> is only ever going to be a local function.</p>
","12299000",1
1546,71136332,2,71135228,2022-02-16 04:16:49,1,"<p>Go to your VM config and test your connection through the 'connect' tab.  Is your test successful?  If not, check if port 22 is blocked.  Watch for automated blocking rules applied to your VM.</p>
<p>we have DSVM attach in preview - might be interesting for you: <a href=""https://github.com/Azure/azureml-previews/tree/main/previews/dsvm-attach"" rel=""nofollow noreferrer"">https://github.com/Azure/azureml-previews/tree/main/previews/dsvm-attach</a>.</p>
","11297406",0
1547,71152263,2,71152047,2022-02-17 03:45:39,1,"<p>You are probably using <strong>old boto3</strong> version. <code>ServerlessConfig</code> is a very new configuration option. You need to upgrade to the latest version (1.21.1) if possible.</p>
","248823",4
1548,71161289,2,71155959,2022-02-17 15:51:31,2,"<p>I would try to use <a href=""https://dvc.org/doc/command-reference/get"" rel=""nofollow noreferrer""><code>dvc get</code></a> in this case:</p>
<pre class=""lang-sh prettyprint-override""><code>dvc get -o random_forest_pred.csv --rev random_forest . pred_test.csv
</code></pre>
<p>It should bring the <code>pred_test.csv</code> from the <code>random_forest</code> branch.</p>
<blockquote>
<p>Mind the <code>.</code> before the <code>pred_test.csv</code> please, it's needed and it means that &quot;use the current repo&quot;, since <code>dvc get</code> could also be used on other repos (e.g. GitHub URL)</p>
</blockquote>
<p>Then I think you could use some CLI or write a script to join the files:</p>
<p><a href=""https://unix.stackexchange.com/questions/293775/merging-contents-of-multiple-csv-files-into-single-csv-file"">https://unix.stackexchange.com/questions/293775/merging-contents-of-multiple-csv-files-into-single-csv-file</a></p>
","298182",2
1549,71165689,2,71136057,2022-02-17 21:39:04,0,"<p>Yes, if you <a href=""https://docs.aws.amazon.com/cli/latest/reference/sagemaker/list-user-profiles.html"" rel=""nofollow noreferrer"">list</a> and <a href=""https://docs.aws.amazon.com/cli/latest/reference/sagemaker/describe-user-profile.html"" rel=""nofollow noreferrer"">describe</a> the domain users, you'll get back the user's <code>HomeEfsFileSystemUid</code> value.<br />
Here's a CLI example:</p>
<pre><code>aws sagemaker describe-user-profile --domain-id d-lcn1vbt47yku --user-profile-name default-1588670743757
{
    ...
    &quot;UserProfileName&quot;: &quot;default-1588670743757&quot;,
    &quot;HomeEfsFileSystemUid&quot;: &quot;200005&quot;,
    ...
}
</code></pre>
","121956",0
1550,71169565,2,71169178,2022-02-18 07:07:54,0,"<p>As per the <a href=""https://pypi.org/project/azureml-contrib-dataset/"" rel=""nofollow noreferrer"">PyPi</a>, <code>azureml.contrib.dataset</code> has been deprecated and <code>azureml.data</code> should be used instead:</p>
<blockquote>
<p>The azureml-contrib-dataset package has been deprecated and might not
receive future updates and removed from the distribution altogether.
Please use azureml-core instead.</p>
</blockquote>
","406896",0
1551,71181773,2,68846704,2022-02-19 02:41:45,2,"<p>The access is dictated by the execution role that is attached to the SageMaker notebook. <a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-roles.html"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-roles.html</a> goes through how add additional s3 permissions to a SageMaker execution role.</p>
","16504654",0
1552,71192375,2,71172207,2022-02-20 07:55:27,1,"<p>Change the crontab syntax to <code>0 21 * * * shutdown.py</code></p>
<p>Then create a shutdown.py which is reduced version of the <a href=""https://github.com/aws-samples/amazon-sagemaker-notebook-instance-lifecycle-config-samples/blob/master/scripts/auto-stop-idle/autostop.py"" rel=""nofollow noreferrer"">autostop.py</a> and contains mainly:</p>
<pre><code>...
print('Closing notebook')
client = boto3.client('sagemaker')
client.stop_notebook_instance(NotebookInstanceName=get_notebook_name())
</code></pre>
<p>BTW: triggering <code>shutdown now</code> directly from the crontab command didn't work for me, therefore calling the SageMaker API instead.</p>
","121956",0
1553,71202374,2,71185505,2022-02-21 07:06:52,0,"<p>Here are some possible solutions :</p>
<ul>
<li>Save the model in some other way, e.g. the JSON specified here <a href=""https://xgboost.readthedocs.io/en/latest/tutorials/saving_model.html"" rel=""nofollow noreferrer"">https://xgboost.readthedocs.io/en/latest/tutorials/saving_model.html</a></li>
<li>Limit the allowed range of xgboost versions to those that are known to work with our model. This could lead to issues in the future, for example if the aging version of xgboost we require is no longer supported by newer versions of Python.</li>
<li>Using <code>save_model</code> to save in JSON is worth a shot to try.</li>
</ul>
","17623897",0
1554,71211617,2,70931309,2022-02-21 18:56:57,4,"<p>The way to access the model's signature without downloading the MLModel file is under the loaded model. And then you'll access the model's attributes, such as its signature or even other Pyfunc-defined methods.</p>
<pre class=""lang-py prettyprint-override""><code>import mlflow

model = mlflow.pyfunc.load_model(&quot;runs:/&lt;run_id&gt;/model&quot;)
print(model._model_meta._signature)
</code></pre>
","8830597",3
1555,71214538,2,70156631,2022-02-22 00:21:59,1,"<p>You can pass your environment dict in your Model as:</p>
<pre><code>Model(
.
.
env= {&quot;my_env&quot;: &quot;my_env_value&quot;}
.
.
)
</code></pre>
<p>SageMaker will pass the enviroments dict to your container and you can access it in your predict.py script for example with:</p>
<pre><code>my_env = os.environ.get('my_env',&quot;env key not set in Model&quot;)
print(my_env)
</code></pre>
<p>If your env dict was passed to your Model containing they <code>my_env</code> then you will receive the output : <code>my_env_value</code>. Else, then you will receive <code>env key not set in Model</code></p>
<p>I work for AWS and my opinions are my own.</p>
","9796588",0
1556,71218834,2,71068837,2022-02-22 09:32:16,0,"<p>The way to do that is either:</p>
<ol>
<li><p>Register the artifacts in model registry and get them in scoring.</p>
</li>
<li><p>Configure output of pipeline step as PipelineData or OutputFileDatasetConfig, write artifacts to configured output. While scoring, get run of the train pipeline, get its outputs, retrieve the artifacts. This involves experiment name to get run of the pipeline.</p>
</li>
</ol>
","5368122",0
1557,71228831,2,71126832,2022-02-22 21:59:21,1,"<p>I will answer your questions inline below:</p>
<ol>
<li><em>Which method is good for validating user input data within inference.py?</em></li>
</ol>
<p>Seeing that you have a <code>handler</code> function, <code>input_handler</code> and <code>output_handler</code> are ignored. Thus, inside your <code>handler</code> function (as you are correctly doing) you can have the validation logic.</p>
<ol start=""2"">
<li><em>If such validation tests fail (e.g. wrong data types or data not in allowed range, etc.), how is it possible to return appropriate error messages with status codes to the user?</em></li>
</ol>
<p>I like to think of my SageMaker endpoint as a web server. Thus, you can return any valid HTTP response code with a response message. Please see this example <a href=""https://github.com/aws/amazon-sagemaker-examples/blob/main/sagemaker_batch_transform/tensorflow_cifar-10_with_inference_script/code/inference.py#L47"" rel=""nofollow noreferrer"">inference.py</a> file that I found as a reference.</p>
<pre><code>_return_error(
            415, 'Unsupported content type &quot;{}&quot;'.format(context.request_content_type or &quot;Unknown&quot;)
        )

def _return_error(code, message):
    raise ValueError(&quot;Error: {}, {}&quot;.format(str(code), message))
</code></pre>
<ol start=""3"">
<li><em>How is this compatible with the API gateway placed above the endpoint?</em></li>
</ol>
<p>Please see this <a href=""https://aws.amazon.com/blogs/machine-learning/creating-a-machine-learning-powered-rest-api-with-amazon-api-gateway-mapping-templates-and-amazon-sagemaker/"" rel=""nofollow noreferrer"">link</a> for details on Creating a machine learning-powered REST API with Amazon API Gateway mapping templates and Amazon SageMaker.</p>
","9796588",0
1558,71229124,2,71120471,2022-02-22 22:30:51,1,"<p>I suspect you are trying to deploy a serverless endpoint provisioned with 1GB of memory. As discussed <a href=""https://repost.aws/questions/QU35dVp2D9SKKUnnVYGw9Z7A/how-to-check-determine-image-container-size-for-aws-managed-images"" rel=""nofollow noreferrer"">here</a> &quot;You can increase the memory size of your endpoint with the MemorySizeInMB parameter, more info in this documentation: <a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/serverless-endpoints-create.html#serverless-endpoints-create-config%22"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/sagemaker/latest/dg/serverless-endpoints-create.html#serverless-endpoints-create-config&quot;</a></p>
<p>In order to view the uncompressed size of an image you can use the following example command:</p>
<pre><code>$ docker pull 763104351884.dkr.ecr.us-east-1.amazonaws.com/tensorflow-training:1.15.2-cpu-py27-ubuntu18.04

$ docker inspect -f &quot;{{ .Size }}&quot; 763104351884.dkr.ecr.us-east-1.amazonaws.com/tensorflow-training:1.15.2-cpu-py27-ubuntu18.04
</code></pre>
<p>Kindly also note that you will need to provision enough memory to accommodate your model as well. Please see this <a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/serverless-endpoints.html#serverless-endpoints-how-it-works-memory"" rel=""nofollow noreferrer"">link</a> for more information.</p>
","9796588",0
1559,71242437,2,71221741,2022-02-23 18:40:27,1,"<p>I replace</p>
<pre><code>epoch_count = ParameterInteger(name=&quot;EpochCount&quot;, default_value=1)
</code></pre>
<p>with</p>
<pre><code>epoch_count = ParameterString(name=&quot;EpochCount&quot;, default_value=&quot;1&quot;)
</code></pre>
<p>And it works. Maybe we can only use an integer in pipeline parameters from the sagemaker notebook. But epoch_count is being used in the docker container, which is not directly something of Sagemaker, and that's my understanding.</p>
","2930793",0
1560,71249735,2,71246559,2022-02-24 09:25:18,1,"<p>It is not possible to get access or SSH into the machine that's running your deployment. So, one way is to assert the versions of your dependencies in the <code>model_fn</code> inside &quot;inference.py&quot; something like below.</p>
<p>if your requirements.txt looks like this:</p>
<pre><code>numpy==1.20.3
pandas==1.3.4
</code></pre>
<p>get the versions and assert them in `model_fn like below:</p>
<pre><code>import os

### your other code ###

def model_fn(model_dir):
    # assuming you have numpy and pandas
    assert os.popen(&quot;python3 -m pip freeze | grep -E 'numpy|pandas'&quot;).read() == 'numpy==1.20.3\npandas==1.3.4\n'
    ### your other code ###
    return xxxx

### your other code ###
</code></pre>
","11814996",0
1561,71255910,2,71024584,2022-02-24 17:26:19,1,"<p>You can use your model name with the <a href=""https://learn.microsoft.com/en-us/python/api/azureml-core/azureml.core.model.model?view=azure-ml-py#azureml-core-model-model-get-model-path"" rel=""nofollow noreferrer"">Model.get_model_path()</a> method to retrieve the path of the model file or files on the local file system. If you register a folder or a collection of files, this API returns the path of the directory that contains those files.</p>
<p>More info you may want to refer: <a href=""https://learn.microsoft.com/en-us/azure/machine-learning/how-to-deploy-advanced-entry-script#azureml_model_dir"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/machine-learning/how-to-deploy-advanced-entry-script#azureml_model_dir</a></p>
","9598801",0
1562,71256562,2,70933814,2022-02-24 18:24:01,4,"<p>The SageMaker Python SDK repackages your model to include your <code>entry_point</code> and <code>source_dir</code> files and uploads this &quot;new&quot; tar ball to the SageMaker default bucket.</p>
<p>You can change this behavior by setting the <code>default_bucket</code> in your <code>sagemaker_session</code> as follows:</p>
<pre><code>sagemaker_session = sagemaker.Session(default_bucket=&quot;&lt;mybucket&gt;&quot;)

clf_sm_model = serving.Model(model_data='s3://mybucket/mytrainedmodel/model.tar.gz',         
                    .
                    .
                    sagemaker_session=sagemaker_session)
                    .
                    )
</code></pre>
","9796588",0
1563,71262872,2,71262010,2022-02-25 08:12:59,3,"<p>To download a model from Databricks workspace you need to do two things:</p>
<ol>
<li><p>Set MLFlow tracking URI to databricks using python API</p>
</li>
<li><p>Setup databricks authentication. I prefer authenticating by setting the following environment variables, you can also use databricks CLI to authenticate:</p>
<pre><code>DATABRICKS_HOST

DATABRICKS_TOKEN
</code></pre>
</li>
<li><p>Here's a basic code snippet to download a model from Databricks workspace model registry:</p>
<pre><code>import os
import mlflow
from mlflow.store.artifact.models_artifact_repo import ModelsArtifactRepository

model_name = &quot;example-model-name&quot;
model_stage = &quot;Staging&quot;  # Should be either 'Staging' or 'Production'

mlflow.set_tracking_uri(&quot;databricks&quot;)

os.makedirs(&quot;model&quot;, exist_ok=True)
local_path = ModelsArtifactRepository(
    f'models:/{model_name}/{model_stage}').download_artifacts(&quot;&quot;, dst_path=&quot;model&quot;)

print(f'{model_stage} Model {model_name} is downloaded at {local_path}')
</code></pre>
<p>Running above python script will download an ML model in the model directory.</p>
<p><strong>Containerizing MLFlow model serving with Docker</strong></p>
<p>The next step is to package this downloaded model in a docker image and serve a model when you run the image.</p>
</li>
</ol>
<p>Here's a basic Dockerfile to do the same:</p>
<pre><code>FROM continuumio/miniconda3

ENV MLFLOW_HOME /opt/mlflow
ENV MLFLOW_VERSION 1.12.1
ENV PORT 5000

RUN conda install -c conda-forge mlflow=${MLFLOW_VERSION}

COPY model/ ${MLFLOW_HOME}/model

WORKDIR ${MLFLOW_HOME}

RUN mlflow models prepare-env -m ${MLFLOW_HOME}/model

RUN useradd -d ${MLFLOW_HOME} mlflow
RUN chown mlflow: ${MLFLOW_HOME}
USER mlflow

CMD mlflow models serve -m ${MLFLOW_HOME}/model --host 0.0.0.0 --port ${PORT}
</code></pre>
<p>For more information you can follow this <a href=""https://dev.to/itachiredhair/downloading-mlflow-model-from-databricks-and-serving-with-docker-38ip"" rel=""nofollow noreferrer"">article</a> from Akshay Milmile</p>
","11104805",1
1564,71263735,2,71255132,2022-02-25 09:30:17,2,"<p>Check in all your API methods that you haven't specified &quot;Use Action Name&quot; for any integration request, and then left the &quot;Action&quot; field blank. If you do the &quot;AWS ARN for integration contains invalid action&quot; error message will be shown.</p>
<p><a href=""https://i.stack.imgur.com/EXEnQ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/EXEnQ.png"" alt=""action type choice"" /></a></p>
","5107719",0
1565,71265781,2,71053554,2022-02-25 12:20:04,1,"<p>Thanks for opening the issue, I added a +1.
In the meantime, you can use alternative SDKs to train Random Cut Forest and set test channel distribution to FullyReplicated.</p>
<p>For example, those SDKs should give you this control:</p>
<ul>
<li>AWS CLI <a href=""https://docs.aws.amazon.com/cli/latest/reference/sagemaker/create-training-job.html"" rel=""nofollow noreferrer"">create_training_job</a></li>
<li><a href=""https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/sagemaker.html#SageMaker.Client.create_training_job"" rel=""nofollow noreferrer"">boto3 create_training_job</a></li>
<li>SageMaker Python SDK <a href=""https://sagemaker.readthedocs.io/en/stable/api/training/estimators.html#"" rel=""nofollow noreferrer"">Estimator</a> to which you pass the RCF docker image in <code>image_uri</code> parameter</li>
</ul>
","5331834",1
1566,71266206,2,70835006,2022-02-25 12:57:03,2,"<p>Thanks for your questions. Here are answers:</p>
<ul>
<li><p><strong>SageMaker PySpark SDK</strong> <a href=""https://sagemaker-pyspark.readthedocs.io/en/latest/"" rel=""nofollow noreferrer"">https://sagemaker-pyspark.readthedocs.io/en/latest/</a> does the opposite of what you want: being able to call a non-spark (or spark) SageMaker job from a Spark environment. Not sure that's what you need here.</p>
</li>
<li><p><strong>Running Spark in SageMaker jobs</strong>. While you can use SageMaker Notebooks to connect to a remote EMR cluster for interactive coding, you do not need EMR to run Spark in SageMaker jobs (Training and Processing). You have 2 options:</p>
<ul>
<li><p><a href=""https://sagemaker.readthedocs.io/en/stable/amazon_sagemaker_processing.html#pysparkprocessor"" rel=""nofollow noreferrer"">SageMaker Processing has a built-in Spark Container</a>, which is easy to use but unfortunately not connected to SageMaker Model Tuning (that works with Training only). If you use this, you will have to find and use a third-party, external parameter search library ; for example <a href=""https://aws.amazon.com/blogs/machine-learning/run-distributed-hyperparameter-and-neural-architecture-tuning-jobs-with-syne-tune/"" rel=""nofollow noreferrer"">Syne Tune</a> from AWS itself (that supports bayesian optimization)</p>
</li>
<li><p>SageMaker Training can run custom docker-based jobs, on one or multiple machines. If you can fit your Spark code within SageMaker Training spec, then you will be able to use SageMaker Model Tuning to tune your Spark code. However there is no framework container for Spark on SageMaker Training, so you would have to build your own, and I am not aware of any examples. Maybe you could get inspiration from the <a href=""https://github.com/aws/sagemaker-spark-container"" rel=""nofollow noreferrer"">Processing container code here</a> to build a custom Training container</p>
</li>
</ul>
</li>
</ul>
<p>Your idea of using the Training job as a client to launch an EMR cluster is good and should work (if SM has the right permissions), and will indeed allow you to use SM Model Tuning. I'd recommend:</p>
<ul>
<li>each SM job to create a new transient cluster (auto-terminate after step) to keep costs low and avoid tuning results to be polluted by inter-job contention that could arise if running everything on the same cluster.</li>
<li>use the cheapest possible instance type for the SM estimator, because it will need to stay up during all duration of your EMR experiment to collect and print your final metric (accuracy, duration, cost...)</li>
</ul>
<p>In the same spirit, I once used SageMaker Training myself to launch Batch Transform jobs for the sole purpose of leveraging the bayesian search API to find an inference configuration that minimizes cost.</p>
","5331834",1
1567,71266245,2,70486162,2022-02-25 13:00:46,1,"<p>are you still facing this issue?</p>
<p>I am in eu-west-2 using a SageMaker Studio notebook and the TensorFlow 2.6 Python 3.8 CPU Optimized image (running app is tensorflow-2.6-cpu-py38-ubuntu20.04-v1).</p>
<p>When I run the below commands, I get the right outputs.</p>
<pre><code>!python3 -V
</code></pre>
<p>returns Python 3.8.2</p>
<pre><code>import sys
sys.version 
</code></pre>
<p>returns
3.8.2 (default, Dec  9 2021, 06:26:16) \n[GCC 9.3.0]'</p>
<pre><code>import tensorflow as tf
print(tf.__version__)
</code></pre>
<p>returns 2.6.2</p>
<p>It seems this has now been fixed</p>
","18308751",0
1568,71266563,2,71260306,2022-02-25 13:27:22,3,"<p>It is indeed possible to invoke sagemaker endpoints from sagemaker without using any other AWS services and that is also manifested by the fact that they have invocation URLs.</p>
<p>Here's how you set it up:</p>
<ol>
<li>create an user with only programmatic access and attach a policy json that should look something like below:</li>
</ol>
<pre><code>{
    &quot;Version&quot;: &quot;2012-10-17&quot;,
    &quot;Statement&quot;: [
        {
            &quot;Sid&quot;: &quot;VisualEditor0&quot;,
            &quot;Effect&quot;: &quot;Allow&quot;,
            &quot;Action&quot;: &quot;sagemaker:InvokeEndpoint&quot;,
            &quot;Resource&quot;: &quot;arn:aws:sagemaker:&lt;region&gt;:&lt;account-id&gt;:endpoint/&lt;endpoint-name&gt;&quot;
        }
    ]
} 
</code></pre>
<p>you can replace <code>&lt;endpoint-name&gt;</code> with <code>*</code> to let this user invoke all endpoints.</p>
<ol start=""2"">
<li><p>use the ACCESS-KEY and SECRET-ACCESS-KEY to configure authorisation in postman like shown in this screenshot. also add the parameters in advanced tab like shown in the screenshot.
<a href=""https://i.stack.imgur.com/cYkTf.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/cYkTf.png"" alt=""enter image description here"" /></a></p>
</li>
<li><p>then fill up your body with the relevant content type.</p>
</li>
<li><p>then add or remove additional headers like variant-name or model-name, if you have them set up and the headers should look like shown in this screenshot: <a href=""https://i.stack.imgur.com/NLqkV.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/NLqkV.png"" alt=""enter image description here"" /></a></p>
</li>
<li><p>send the request to receive reponse like this
<a href=""https://i.stack.imgur.com/uA4kF.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/uA4kF.png"" alt=""enter image description here"" /></a></p>
</li>
</ol>
<p><em><strong>URL and credentials in the above screenshots doesn't work anymore, duh!</strong></em></p>
<p>and if you want code to invoke the endpoint directly using some back-end language, <a href=""https://stackoverflow.com/a/70803026/11814996"">here's code for python</a>.</p>
","11814996",5
1569,71315292,2,71161777,2022-03-01 22:22:16,0,"<p>The inference code and requirement.txt should be stored as part of model.gz while training.  They will be used in the batch transform!!</p>
","11314562",0
1570,71339352,2,71338160,2022-03-03 15:05:59,4,"<p>You did everything alright. In the end, after pulling, you can see that when using <code>dvc exp show</code> your experiments will be there. To restore the experiment available from your experiment list into your workspace, you simply need to run <code>dvc exp apply exp_66</code>. DVC will make sure that the changes corresponding to this experiment will be checked out.</p>
<p>Your workflow seems correct so far. One addition: once you make sure one of the experiments is what you want to &quot;keep&quot; in git history, you can use <code>dvc exp branch {exp_id} {branch_name}</code> to create a separate branch for this experiment. Then you can use <code>git</code> commands to save the changes.</p>
","3406563",5
1571,71341425,2,71340893,2022-03-03 17:38:04,2,"<p>The endpoint is essentially a Flask web server running in a Docker container</p>
<p>If it's a scikit-learn image, when you invoke the endpoint, it loads your script from S3, then...</p>
<p>It calls <code>input_fn(request_body: bytearray, content_type) -&gt; np.ndarray</code> to parse the <code>request_body</code> into a numpy array</p>
<p>Then it calls your <code>model_fn(model_dir: str) -&gt; object</code> function to load the model from <code>model_dir</code> and return the model</p>
<p>Then it calls <code>predict_fn(input_object: np.ndarray, model: object) -&gt; np.array</code>, which calls your <code>model.predict()</code> function and returns the prediction</p>
<p>Then it calls <code>output_fn(prediction: np.array, accept: str)</code> to take the result from <code>predict_fn</code> and encode it to the <code>accept</code> type</p>
<p>You don't need to implement all of these functions yourself, as there are defaults</p>
<p>You <strong>do</strong> need to implement <code>model_fn</code></p>
<p>You only need to implement <code>input_fn</code> if you have non numeric data</p>
<p>You only need to implement <code>predict_fn</code> if your model uses something other than <code>.predict()</code></p>
<p>You can see how the default function implementations work <a href=""https://github.com/aws/sagemaker-scikit-learn-container/blob/master/src/sagemaker_sklearn_container/serving.py"" rel=""nofollow noreferrer"">here</a></p>
","223478",0
1572,71341852,2,71338750,2022-03-03 18:08:26,0,"<p>You could run the script in the notebook itself but it would not deploy with SageMaker provided capabilities then. The estimator that you are seeing is what specifies to SageMaker what framework you are using and the training script that you are passing in. If you ran the script code in the notebook that would be like training in your local environment. By passing in the script to the Estimator you are running a SageMaker training job. The estimator is meant to encapsulate training on SageMaker.</p>
<p>SageMaker Estimator Overview: <a href=""https://sagemaker.readthedocs.io/en/stable/overview.html"" rel=""nofollow noreferrer"">https://sagemaker.readthedocs.io/en/stable/overview.html</a></p>
","16504640",0
1573,71342802,2,71273364,2022-03-03 19:34:08,1,"<p>When <code>get_run_args</code> or <code>run</code> is called on a SparkJarProcessor, the <code>submit_class</code> <a href=""https://github.com/aws/sagemaker-python-sdk/blob/dev/src/sagemaker/spark/processing.py#L1143"" rel=""nofollow noreferrer"">is used to set a property on the processor itself</a> which is why you don't see it in the <code>get_run_args</code> output.</p>
<p>That processor property will be used during pipeline definition generation to set the <a href=""https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_AppSpecification.html#sagemaker-Type-AppSpecification-ContainerEntrypoint"" rel=""nofollow noreferrer"">ContainerEntrypoint</a> argument to <code>CreateProcessingJob</code>.</p>
<p>Example:</p>
<pre><code>run_args = spark_processor.get_run_args(
    submit_app=&quot;my.jar&quot;,
    submit_class=&quot;program.to.run&quot;,
    arguments=[]
)

step_process = ProcessingStep(
    name=&quot;SparkJarProcessStep&quot;,
    processor=spark_processor,
    inputs=run_args.inputs,
    outputs=run_args.outputs,
    code=run_args.code
)

pipeline = Pipeline(
    name=&quot;myPipeline&quot;,
    parameters=[],
    steps=[step_process],
)

definition = json.loads(pipeline.definition())
definition
</code></pre>
<p>The output of <code>definition</code>:</p>
<pre><code>...
'Steps': [{'Name': 'SparkJarProcessStep',
   'Type': 'Processing',
   'Arguments': {'ProcessingResources': {'ClusterConfig': {'InstanceType': 'ml.m5.xlarge',
      'InstanceCount': 2,
      'VolumeSizeInGB': 30}},
    'AppSpecification': {'ImageUri': '153931337802.dkr.ecr.us-west-2.amazonaws.com/sagemaker-spark-processing:2.4-cpu',
     'ContainerEntrypoint': ['smspark-submit',
      '--class',
      'program.to.run',
      '--local-spark-event-logs-dir',
      '/opt/ml/processing/spark-events/',
      '/opt/ml/processing/input/code/my.jar']},
...
</code></pre>
","895615",1
1574,71350846,2,71197045,2022-03-04 11:35:04,0,"<p>The only way that i found right now is to use the CreateTrainigJob API (<a href=""https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_CreateTrainingJob.html#sagemaker-CreateTrainingJob-request-RoleArn"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_CreateTrainingJob.html#sagemaker-CreateTrainingJob-request-RoleArn</a>). The following steps are needed:</p>
<ul>
<li>I am not sure if this will work with Bring your own script method for E.g with a Tensorflow estimator</li>
<li>it works with a build your own container approach</li>
<li>Using the CreateTrainigJob API i created the configs which in turn includes all the needed configs like - training, experiment, algporthm etc and passed that to SagemakerTrainingOperator</li>
</ul>
","14554050",0
1575,71355714,2,71346372,2022-03-04 18:22:35,0,"<p>If both the models need to be jointly optimized, you could run a SageMaker HPO job in script mode and define both the models in the script. Or you could run two HPO jobs, optimize each model, and then create the Pipeline Model. There is no native support for doing an HPO job on a PipelineModel.</p>
<p>I work at AWS and my opinions are my own.</p>
","16247336",1
1576,71357806,2,71063820,2022-03-04 22:22:14,1,"<p>Thanks for reporting it.
This is a bug in handling of the parquet files with columns but empty row set. This has been fixed already and will be included in next release.</p>
","15643694",1
1577,71357828,2,71075255,2022-03-04 22:25:24,1,"<p>Thanks for reporting it.
This is a bug in handling of the parquet files with columns but empty row set. This has been fixed already and will be included in next release.</p>
<p>I could not repro the hang on multiple files, though, so if you could provide more info on that would be nice.</p>
","15643694",3
1578,71362310,2,71014584,2022-03-05 12:46:16,1,"<p><strong>Update on 5th March, 2022</strong></p>
<p>I posted this as a support ticket with Azure. Following is the answer I have received:</p>
<blockquote>
<p>As you can see from our documentation of <a href=""https://learn.microsoft.com/en-us/python/api/azureml-core/azureml.data.tabulardataset?view=azure-ml-py"" rel=""nofollow noreferrer"">TabularDataset Class</a>,
the “stream_column” parameter is required. So, that error is occurring
because you are not passing any parameters when you are calling the
download method.    The “stream_column” parameter should have the
stream column to download/mount. So, you need to pass the column name
that contains the paths from which the data will be streamed.<br />
Please find an example <a href=""https://learn.microsoft.com/en-us/azure/machine-learning/how-to-use-labeled-dataset#explore-labeled-datasets-via-pandas-dataframe"" rel=""nofollow noreferrer"">here</a>.</p>
</blockquote>
","406896",0
1579,71371918,2,71351821,2022-03-06 16:02:17,2,"<p>Try specifying bucket name w/o a gs://. This should fix the issue. One more stackoverflow post that says the same thing: <a href=""https://stackoverflow.com/questions/53436615/cloud-storage-python-client-fails-to-retrieve-bucket"">Cloud Storage python client fails to retrieve bucket</a></p>
<p>any storage bucket you try to access in GCP has a unique address to access it. That address starts with a gs:// always which specifies that it is a cloud storage url. Now, GCS apis are designed such that they need the bucket name only to work with it. Hence, you just pass the bucket name. If you were accessing the bucket via browser you will need the complete address to access and hence the gs:// prefix as well.</p>
","9890415",0
1580,71387046,2,71308112,2022-03-07 20:45:03,2,"<p>From the <a href=""https://github.com/aws/amazon-sagemaker-examples/blob/main/advanced_functionality/scikit_bring_your_own/container/decision_trees/predictor.py#L88"" rel=""nofollow noreferrer"">scikit_bring_your_own</a> example, I suggest testing by setting the Response as follows:</p>
<pre><code>return flask.Response(response= output_float, status=200, mimetype=&quot;text/csv&quot;)
</code></pre>
","9796588",1
1581,71387804,2,71385524,2022-03-07 22:06:30,1,"<p>I am not sure which <code>TrainingImage</code> you are using and all the files in your container.
That being said, I suspect you are using a custom container.</p>
<p>SageMaker Training Jobs look for a <code>train</code> file and run your container as <a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/your-algorithms-training-algo-dockerfile.html"" rel=""nofollow noreferrer"">follows</a>:</p>
<pre><code>docker run image train
</code></pre>
<p>You can change this behavior by setting the <code>ENTRYPOINT</code> in your Dockerfile. Please see this example <a href=""https://github.com/aws/amazon-sagemaker-examples/blob/main/r_examples/r_byo_r_algo_hpo/Dockerfile#L47"" rel=""nofollow noreferrer"">Dockerfile</a> from the <a href=""https://github.com/aws/amazon-sagemaker-examples/tree/main/r_examples/r_byo_r_algo_hpo"" rel=""nofollow noreferrer"">r_byo_r_algo_hpo</a> example.</p>
","9796588",1
1582,71395457,2,70538098,2022-03-08 12:56:27,2,"<p>I had the same issue with catboost model.
The way I solved it was by saving the artifacts in a local dir</p>
<pre><code>import os
from mlflow.tracking import MlflowClient
client = MlflowClient()
local_dir = &quot;/dbfs/FileStore/user/models&quot;
local_path = client.download_artifacts('run_id', &quot;model&quot;, local_dir)```

```model_path = '/dbfs/FileStore/user/models/model/model.cb'
model = CatBoostClassifier()
model = model.load_model(model_path)
model.predict_proba(test_set)```
</code></pre>
","8716240",0
1583,71399097,2,71398882,2022-03-08 17:22:08,3,"<p>A <code>CUDA out of memory</code> error indicates that your GPU RAM (Random access memory) is full. This is different from the storage on your device (which is the info you get following the <code>df -h</code> command).</p>
<p>This memory is occupied by the model that you load into GPU memory, which is independent of  your dataset size. The GPU memory required by the model is at least twice the actual size of the model, but most likely closer to 4 times (initial weights, checkpoint, gradients, optimizer states, etc).</p>
<p>Things you can try:</p>
<ul>
<li>Provision an instance with more GPU memory</li>
<li>Decrease batch size</li>
<li>Use a different (smaller) model</li>
</ul>
","18410773",2
1584,71409206,2,71245000,2022-03-09 12:14:43,0,"<p>I solved this issue by adding the location to the TabularDatasetCreateJob:</p>
<pre><code>    dataset_create_op = gcc_aip.TabularDatasetCreateOp(
    project=project,
    display_name=display_name, 
    bq_source=bq_source,
    location = gcp_region
)
</code></pre>
<p>I now have the same issue with the model training job but I have learnt that a lot of the functions in the above code take a location parameter, or default to us-central1. I will update if I get any further.</p>
","11761839",0
1585,71410748,2,71407308,2022-03-09 14:14:58,0,"<p>After much more digging I found out that the &quot;Consume&quot; scripts provided with the endpoint are wrong (Python and C#) .</p>
<p>When making a call to the endpoint the GlobalParameters expects an integer value, but the provided scripts have wrapped the values in double quotes hence making it a string:</p>
<pre><code> },
 &quot;GlobalParameters&quot;: {
     &quot;quantiles&quot;: &quot;0.025,0.975&quot;
 }
</code></pre>
<p>If you are using Python to consume the model, when making call to the endpoint your GlobalParameters should be define as this:</p>
<pre><code> },
 &quot;GlobalParameters&quot;: {
     &quot;quantiles&quot;: [0.025,0.975]
 }
</code></pre>
<p>wrapped in square brackets</p>
<blockquote>
<p>[0.025,0.975]</p>
</blockquote>
<p>and not in double quotes &quot;</p>
<blockquote>
<p><em>I have also opened a ticket with microsoft so hopefully they will fix the code provided in the &quot;consume&quot; section of every endpoint</em></p>
</blockquote>
","2041092",0
1586,71442914,2,71378280,2022-03-11 18:08:34,1,"<p>To summarize the discussion in the comments thread.</p>
<p>Most likely it's happening since DVC can't get access to a private repo on GitLab. (The error message is obscure and should be fixed.)</p>
<p>The same way you would not be able to run:</p>
<pre><code>!git clone https://gitlab.com/org/&lt;private-repo&gt;
</code></pre>
<p>It also returns a pretty obscure error:</p>
<pre><code>Cloning into '&lt;private-repo&gt;'...
fatal: could not read Username for 'https://gitlab.com': No such device or address
</code></pre>
<p>(I think it's something related to how tty is setup in Colab?)</p>
<p>The best approach to solve this is to use SSH like described <a href=""https://medium.com/@sadiaafrinpurba/how-to-clone-private-github-repo-in-google-colab-using-ssh-77384cfef18f"" rel=""nofollow noreferrer"">here</a> for example.</p>
","298182",2
1587,71465145,2,71292240,2022-03-14 08:58:23,0,"<p>I figured it out, by using the Notebooks (do not work in Firefox for me, only on Chrome).
There it is possible to handle the dataset in python, transform it to pandas, manipulate it and save it to the datastore.</p>
","14712963",0
1588,71509545,2,71492980,2022-03-17 09:01:30,0,"<p>Ok I figured it out.
The task was wrong, the source and target language need to be in the task (HF_TASK)</p>
<p>So for example:
<code>'HF_TASK': 'translation_en_to_fr'</code></p>
","13626048",0
1589,71527132,2,71467176,2022-03-18 12:38:20,3,"<p>From <a href=""https://github.com/aws/sagemaker-tensorflow-extensions#using-the-pipemodedataset"" rel=""nofollow noreferrer"">here</a>:</p>
<blockquote>
<p>A PipeModeDataset can read TFRecord, RecordIO, or text line records.</p>
</blockquote>
<p>While your'e trying to read binary (PNG) files. I don't see a relevant <a href=""https://github.com/aws/sagemaker-tensorflow-extensions/tree/tf-2/src/pipemode_op/RecordReader"" rel=""nofollow noreferrer"">record reader here</a> to help you do that.<br />
You could build your own format pipe implementation like shown <a href=""https://aws.amazon.com/blogs/machine-learning/accelerate-model-training-using-faster-pipe-mode-on-amazon-sagemaker/"" rel=""nofollow noreferrer"">here</a>, but it's considerably more effort.</p>
<p>Alternatively, you mentioned your files are scattered in different folders, but if your files common path contains less than 2M files, you could use <a href=""https://aws.amazon.com/about-aws/whats-new/2021/10/amazon-sagemaker-fast-file-mode/"" rel=""nofollow noreferrer"">FastFile mode</a> to <strong>stream</strong> data. Currently, FastFile only supports an S3 Prefix, so you won't be able to use a manifest.</p>
<p>Also see this <a href=""https://aws.amazon.com/blogs/machine-learning/choose-the-best-data-source-for-your-amazon-sagemaker-training-job/"" rel=""nofollow noreferrer"">general pros/cons discussion of the different available storage and input types available in SageMaker</a>.</p>
","121956",3
1590,71530292,2,71522857,2022-03-18 16:32:21,2,"<p>There are a few options:</p>
<ol>
<li><p>Write some data to a text file at <code>/opt/ml/output/message</code>, then call <a href=""https://docs.aws.amazon.com/cli/latest/reference/sagemaker/describe-processing-job.html"" rel=""nofollow noreferrer"">DescribeProcessingJob</a> (using Boto3 or the AWS CLI or API) and retrieve the <code>ExitMessage</code> value</p>
<pre><code>aws sagemaker describe-processing-job \
  --processing-job-name foo \
  --output text \
  --query ExitMessage
</code></pre>
</li>
<li><p>Add a new output to your processing job and send data there</p>
</li>
<li><p>If your <code>train_data</code> is in CSV, JSON, or Parquet then use an <a href=""https://docs.aws.amazon.com/cli/latest/reference/s3api/select-object-content.html"" rel=""nofollow noreferrer"">S3 Select query</a> on <code>train_data</code> for it's # of rows/columns</p>
<pre><code>aws s3api select-object-content \
  --bucket foo \
  --key 'path/to/train_data.csv' \
  --expression &quot;SELECT count(*) FROM s3object&quot; \
  --expression-type 'SQL' \
  --input-serialization '{&quot;CSV&quot;: {}}' \
  --output-serialization '{&quot;CSV&quot;: {}}' /dev/stdout
</code></pre>
</li>
</ol>
<p>Set <code>expression</code> to <code>select * from s3object limit 1</code> to get the columns</p>
","223478",1
1591,71547861,2,70671957,2022-03-20 14:56:55,2,"<ol>
<li>Yes, you can create a python function/ general containers with code baked in which executes whatever task you like.</li>
</ol>
<ul>
<li><p>pre-defined component -
<a href=""https://www.kubeflow.org/docs/components/pipelines/sdk-v2/component-development/"" rel=""nofollow noreferrer"">https://www.kubeflow.org/docs/components/pipelines/sdk-v2/component-development/</a></p>
</li>
<li><p>python component - <a href=""https://www.kubeflow.org/docs/components/pipelines/sdk-v2/python-function-components/"" rel=""nofollow noreferrer"">https://www.kubeflow.org/docs/components/pipelines/sdk-v2/python-function-components/</a></p>
</li>
</ul>
<ol start=""2"">
<li>KFP is an abstraction op top of Argo workflows.
it gives you the ability to create Workflows using python instead of writing YAML files. Check out this article : <a href=""https://towardsdatascience.com/build-your-data-pipeline-on-kubernetes-using-kubeflow-pipelines-sdk-and-argo-eef69a80237c"" rel=""nofollow noreferrer"">https://towardsdatascience.com/build-your-data-pipeline-on-kubernetes-using-kubeflow-pipelines-sdk-and-argo-eef69a80237c</a></li>
</ol>
<ul>
<li>since Argo Workflows development is advancing independently from KFP it's safe to assume there will be missing features in KFP (Which are the community will add according to demands).</li>
</ul>
<ol start=""3"">
<li>that's a big question.
in general, airflow has sensors, SLA feature/ huge store of operators/sensors/reports/plugins and a bigger community since it's not ML oriented.</li>
</ol>
","12226809",1
1592,71587161,2,71579883,2022-03-23 12:23:37,0,"<p>For some reason, AWS has decided to not make its built-in algorithms directly compatible with Neo... However, you can re-engineer the network parameters using the model.tar.gz output file and then compile.</p>
<p>Step 1: Extract model from tar file</p>
<pre><code>import tarfile
#path to local tar file
model = 'ss_model.tar.gz'

#extract tar file 
t = tarfile.open(model, 'r:gz')
t.extractall()
</code></pre>
<p>This should output two files:
model_algo-1, model_best.params</p>
<ol start=""2"">
<li>Load weights into network from gluon model zoo for the architecture that you chose</li>
</ol>
<p>In this case I used DeepLabv3 with resnet50</p>
<pre><code>import gluoncv
import mxnet as mx
from gluoncv import model_zoo
from gluoncv.data.transforms.presets.segmentation import test_transform

model = model_zoo.DeepLabV3(nclass=2, backbone='resnet50', pretrained_base=False, height=800, width=1280, crop_size=240)
model.load_parameters(&quot;model_algo-1&quot;)
</code></pre>
<ol start=""3"">
<li>Check the parameters have loaded correctly by making a prediction with new model</li>
</ol>
<p>Use an image that was used for training.</p>
<pre><code>#use cpu
ctx = mx.cpu(0)
#decode image bytes of loaded file
img = image.imdecode(imbytes)

#transform image
img = test_transform(img, ctx)
img = img.astype('float32')
print('tranformed image shape: ', img.shape)

#get prediction
output = model.predict(img)
</code></pre>
<ol start=""4"">
<li>Hybridise model into output required by Sagemaker Neo</li>
</ol>
<p>Additional check for image shape compatibility</p>
<pre><code>model.hybridize()
model(mx.nd.ones((1,3,800,1280)))
export_block('deeplabv3-res50', model, data_shape=(3,800,1280), preprocess=None, layout='CHW')
</code></pre>
<ol start=""5"">
<li>Recompile model into tar.gz format</li>
</ol>
<p>This contains the params and json file which Neo looks for.</p>
<pre><code>tar = tarfile.open(&quot;comp_model.tar.gz&quot;, &quot;w:gz&quot;)
for name in [&quot;deeplabv3-res50-0000.params&quot;, &quot;deeplabv3-res50-symbol.json&quot;]:
    tar.add(name)
tar.close()
</code></pre>
<ol start=""6"">
<li>Save tar.gz file to s3 and then compile using Neo GUI</li>
</ol>
","16569056",1
1593,71611763,2,71499094,2022-03-25 03:13:13,1,"<p>azureml python sdk does not support py3.10 yet, AutoML sdk supports py&lt;=3.8.</p>
","12126562",0
1594,71612326,2,71611419,2022-03-25 04:55:37,4,"<p><a href=""https://stackoverflow.com/questions/69194172/how-to-reference-both-a-python-and-environment-variable-in-jupyter-bash-magic"">How to reference both a python and environment variable in jupyter bash magic?</a></p>
<p>Try</p>
<pre><code>!echo /usr/local/lib/nodejs/node-{NODE_VER}-{NODE_DISTRO}/bin:$$PATH
</code></pre>
<p><code>$$PATH</code> forces it to use the system variable rather than try to find a Python/local one.</p>
<p>Various examples:</p>
<pre><code>In [130]: foo = 'foo*.txt'
In [131]: HOME = 'myvar'
In [132]: !echo $foo
foo1.txt foobar0.txt foobar2.txt foobar3.txt foo.txt
In [133]: !echo $foo $HOME
foo1.txt foobar0.txt foobar2.txt foobar3.txt foo.txt myvar
In [134]: !echo $foo $$HOME
foo1.txt foobar0.txt foobar2.txt foobar3.txt foo.txt /home/paul
In [135]: !echo $foo $PWD
/home/paul/mypy
In [136]: !echo $foo $$PWD
foo1.txt foobar0.txt foobar2.txt foobar3.txt foo.txt /home/paul/mypy
In [137]: !echo {foo} $PWD
{foo} /home/paul/mypy
In [138]: !echo {foo} $$PWD
foo1.txt foobar0.txt foobar2.txt foobar3.txt foo.txt /home/paul/mypy
</code></pre>
<p>Any variable not locally defined forces the behavior you see:</p>
<pre><code>In [139]: !echo $abc

In [140]: !echo {foo} $abc
{foo}
</code></pre>
<p>It may put the substitution in a <code>try/except</code> block, and &quot;give up&quot; if there's any <code>NameError</code>.</p>
<p>This substitution can occur in most of the magics, not just <code>!</code>.</p>
","901925",1
1595,71612583,2,71609028,2022-03-25 05:33:05,1,"<blockquote>
<p>Error - &quot;The model you attempted to retrieve requires 'xgboost' to be
installed at '==1.3.3'. You have 'xgboost==1.3.3', please reinstall
'xgboost==1.3.3' (e.g. <code>pip install xgboost==1.3.3</code>) and rerun the
previous command.&quot;</p>
</blockquote>
<p>As given in above error message, it should be <code>pip install xgboost==1.3.3</code> not <code>py-xgboost&lt;=1.3.3</code></p>
<p>If it does not work, try downgraded version of <code>xgboost</code></p>
<pre><code>pip install xgboost==0.90
</code></pre>
<p>Refer this github <a href=""https://github.com/Azure/MachineLearningNotebooks/issues/1421"" rel=""nofollow noreferrer"">link</a></p>
","11104805",1
1596,71624360,2,71619743,2022-03-26 00:16:56,0,"<p>You can use the <a href=""https://beam.apache.org/documentation/dsls/dataframes/overview/"" rel=""nofollow noreferrer"">Beam Dataframes API</a> to do the join and other preprocessing exactly as you would have in Pandas. You can then use <code>to_pcollection</code> to get a PCollection that you can pass directly to your Tensorflow Transform operations, or save it as a file to read in later.</p>
<p>For top-level functions (such as merge) one needs to do</p>
<pre><code>from apache_beam.dataframe.pandas_top_level_functions import pd_wrapper as beam_pd
</code></pre>
<p>and use operations <code>beam_pd.func(...)</code> in place of <code>pd.func(...)</code>.</p>
","582333",4
1597,71648904,2,71623200,2022-03-28 14:15:01,1,"<p>To activate them on notebooks, use the Kernel Switcher. If you have <code>ipykernel</code> installed, you should be able to see the environments in the Kernel switcher. For a sample implementation on a DataScience image, see <a href=""https://github.com/durgasury/efs_backed_conda"" rel=""nofollow noreferrer"">this github repository</a>.</p>
","2458691",0
1598,71657950,2,71655510,2022-03-29 07:21:07,0,"<p><a href=""https://docs.aws.amazon.com/general/latest/gr/sagemaker.html"" rel=""nofollow noreferrer"">Here</a> in the documentation you can see the default sagemaker service quotas. Unfortunately, it's not yet possible to see the current quotas according to this <a href=""https://repost.aws/questions/QUweO83CSlTu-3Zn2RxdESWg/how-do-i-check-my-current-sage-maker-service-quotas"" rel=""nofollow noreferrer"">post</a>.</p>
","4267439",0
1599,71660433,2,71623732,2022-03-29 10:29:18,1,"<p>SageMaker Studio currently runs JupyterLab v1.2 (as confirmed by <em>Help &gt; About JupyterLab</em>), and per the <a href=""https://github.com/bloomberg/ipydatagrid#Installation"" rel=""nofollow noreferrer"">ipydatagrid installation instructions</a>, current/recent versions of this widget require v3+... So I think this is most likely your problem - as there were breaking changes in the interfaces for extensions between these major versions.</p>
<p>I had a quick look at the past releases of <code>ipydatagrid</code> to see if using an older version would be possible, and it seems like the documented JLv3 requirement gets added between <a href=""https://github.com/bloomberg/ipydatagrid/tree/0.2.16"" rel=""nofollow noreferrer"">v0.2.16</a> and <a href=""https://github.com/bloomberg/ipydatagrid/tree/1.0.1"" rel=""nofollow noreferrer"">v1.0.1</a> (which are adjacent on GitHub).</p>
<p>However, the old install instructions documented on 0.2 don't seem to work anymore: I get <code>ValueError: &quot;jupyter-datagrid&quot; is not a valid npm package</code> and also note that versions &lt;1.0 don't seem to be present <a href=""https://libraries.io/pypi/ipydatagrid/versions"" rel=""nofollow noreferrer"">on PyPI</a>.</p>
<p>So unfortunately I think (unless/until SM Studio gets a JupyterLab version upgrade), this widget's not likely to work unless you dive in to building it from an old source code version.</p>
","13352657",0
1600,71662271,2,69277390,2022-03-29 12:45:08,0,"<p>For SKLearnProcessor, the ideal way to specify default bucket is by creating a sagemaker session with that bucket, and sending that as sagemaker_session parameter. Example:</p>
<pre><code>from sagemaker.session import Session    
sklearn_processor = SKLearnProcessor(framework_version='0.20.0',
                                     role='&lt;arn-role&gt;',
                                     instance_type='ml.m5.xlarge',
                                     instance_count=1,
                                     sagemaker_session=Session(default_bucket='&lt;s3-bucket-name&gt;'))
</code></pre>
<p>I know this is not your exact question but you have added an alternative to this in your question details. So I am adding it here as a cleaner approach.</p>
","3339201",1
1601,71679462,2,71509160,2022-03-30 14:47:46,1,"<p>Try using glob module and filter method instead of list comprehension.</p>
<pre><code>import glob
from os.path import isfile
mypath = &quot;./temp/*&quot;
docsOnDisk = glob.glob(mypath)
verified_docsOnDisk = list(filter(lambda x:isfile(x), docsOnDisk))
</code></pre>
<p>glob should give only existing files. Its not needed to verify them by using isfile(). But still if you need to try it out then you can use filter method instead of list comprehension. To skip verification, you can comment last line.</p>
","14450325",0
1602,71687975,2,71687131,2022-03-31 06:40:38,3,"<p>This was actually very simple, below is an example using FastAPI to import and mount the MLflow WSGI application.</p>
<pre><code>import os
import subprocess
from fastapi import FastAPI
from fastapi.middleware.wsgi import WSGIMiddleware

from mlflow.server import app as mlflow_app

app = FastAPI()
app.mount(&quot;/&quot;, WSGIMiddleware(mlflow_app))

BACKEND_STORE_URI_ENV_VAR = &quot;_MLFLOW_SERVER_FILE_STORE&quot;
ARTIFACT_ROOT_ENV_VAR = &quot;_MLFLOW_SERVER_ARTIFACT_ROOT&quot;
ARTIFACTS_DESTINATION_ENV_VAR = &quot;_MLFLOW_SERVER_ARTIFACT_DESTINATION&quot;
PROMETHEUS_EXPORTER_ENV_VAR = &quot;prometheus_multiproc_dir&quot;
SERVE_ARTIFACTS_ENV_VAR = &quot;_MLFLOW_SERVER_SERVE_ARTIFACTS&quot;
ARTIFACTS_ONLY_ENV_VAR = &quot;_MLFLOW_SERVER_ARTIFACTS_ONLY&quot;

def parse_args():
    a = argparse.ArgumentParser()
    a.add_argument(&quot;--host&quot;, type=str, default=&quot;0.0.0.0&quot;)
    a.add_argument(&quot;--port&quot;, type=str, default=&quot;5000&quot;)
    a.add_argument(&quot;--backend-store-uri&quot;, type=str, default=&quot;sqlite:///mlflow.db&quot;)
    a.add_argument(&quot;--serve-artifacts&quot;, action=&quot;store_true&quot;, default=False)
    a.add_argument(&quot;--artifacts-destination&quot;, type=str)
    a.add_argument(&quot;--default-artifact-root&quot;, type=str)
    a.add_argument(&quot;--gunicorn-opts&quot;, type=str, default=&quot;&quot;)
    a.add_argument(&quot;--n-workers&quot;, type=str, default=1)
    return a.parse_args()

def run_command(cmd, env, cwd=None):
    cmd_env = os.environ.copy()
    if cmd_env:
        cmd_env.update(env)
    child = subprocess.Popen(
        cmd, env=cmd_env, cwd=cwd, text=True, stdin=subprocess.PIPE
    )
    child.communicate()
    exit_code = child.wait()
    if exit_code != 0:
        raise Exception(&quot;Non-zero exitcode: %s&quot; % (exit_code))
    return exit_code

def run_server(args):
    env_map = dict()
    if args.backend_store_uri:
        env_map[BACKEND_STORE_URI_ENV_VAR] = args.backend_store_uri
    if args.serve_artifacts:
        env_map[SERVE_ARTIFACTS_ENV_VAR] = &quot;true&quot;
    if args.artifacts_destination:
        env_map[ARTIFACTS_DESTINATION_ENV_VAR] = args.artifacts_destination
    if args.default_artifact_root:
        env_map[ARTIFACT_ROOT_ENV_VAR] = args.default_artifact_root

    print(f&quot;Envmap: {env_map}&quot;)

    #opts = args.gunicorn_opts.split(&quot; &quot;) if args.gunicorn_opts else []
    opts = args.gunicorn_opts if args.gunicorn_opts else &quot;&quot;

    cmd = [
        &quot;gunicorn&quot;, &quot;-b&quot;, f&quot;{args.host}:{args.port}&quot;, &quot;-w&quot;, f&quot;{args.n_workers}&quot;, &quot;-k&quot;, &quot;uvicorn.workers.UvicornWorker&quot;, &quot;server:app&quot;
    ]
    run_command(cmd, env_map)

def main():
    args = parse_args()
    run_server(args)

if __name__ == &quot;__main__&quot;:
    main()
</code></pre>
<p>Run like</p>
<pre><code>python server.py --artifacts-destination s3://mlflow-mr --default-artifact-root s3://mlflow-mr --serve-artifacts
</code></pre>
<p>Then navigate to your browser and see the tracking server running! This allows you to insert custom FastAPI middleware in front of the tracking server</p>
","4652515",0
1603,71693135,2,71660619,2022-03-31 13:12:46,0,"<p>As pointed out by @rok (thank you!) it is not possible in this situation to pass arguments to <code>docker run</code>, although it would be if switching to ECS.</p>
<p>It is however possible to pass the <code>--shm-size</code> argument to <code>docker build</code> when building the image to push to ECR. This seems to have fixed the problem, albeit it does require a new Docker image to be built and pushed whenever wanting to change this parameter.</p>
","1821979",3
1604,71705309,2,71694816,2022-04-01 10:13:25,0,"<p>I found the problem was that when creating an environment from a YAML specification file, one of my <strong>conda dependencies</strong> was <code>cmake</code>, which I needed to allow installation of another python module. The docker image is exactly the same as a previously created environment.</p>
<p>Removing the <code>cmake</code> dependency from the YAML file, eliminated the issue. So the workaround is to install it using a Dockerfile.</p>
<p>The error message was very misleading to start with, but got there in the end after understanding that AzureML reuses a cached image, based on the hash value, from the environment definition accordingly to <a href=""https://learn.microsoft.com/en-us/azure/machine-learning/concept-environments#image-caching-and-reuse"" rel=""nofollow noreferrer"">this</a></p>
<p>So for that reason, the automatically created <code>Autosave</code> docker image  references to that same build, which only happens once when the first job is sent.</p>
","13526512",0
1605,71718636,2,71662401,2022-04-02 14:59:10,0,"<p>I solved it. In my case it works best like this:</p>
<p><a href=""https://i.stack.imgur.com/JKEmr.png"" rel=""nofollow noreferrer"">Imports</a></p>
<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"" data-babel=""false"">
<div class=""snippet-code"">
<pre class=""snippet-code-html lang-html prettyprint-override""><code>#Import libraries
from pyspark.sql.functions import col, pandas_udf,udf,lit
from notebookutils.mssparkutils import azureML
from azureml.core import Workspace, Model
from azureml.core.authentication import ServicePrincipalAuthentication
from azureml.core.model import Model
import joblib
import pandas as pd

ws = azureML.getWorkspace(""AzureMLService"")
spark.conf.set(""spark.synapse.ml.predict.enabled"",""true"")</code></pre>
</div>
</div>
</p>
<p><a href=""https://i.stack.imgur.com/ij760.png"" rel=""nofollow noreferrer"">Predict function</a></p>
<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"" data-babel=""false"">
<div class=""snippet-code"">
<pre class=""snippet-code-html lang-html prettyprint-override""><code>def forecastModel():
    model_path = Model.get_model_path(model_name=""modelName"", _workspace=ws)
    modeljob = joblib.load(model_path + ""/model.pkl"")

    validation_data = spark.read.format(""csv"") \
                            .option(""header"", True) \
                            .option(""inferSchema"",True) \
                            .option(""sep"", "";"") \
                            .load(""abfss://....csv"")

    validation_data_pd = validation_data.toPandas()


    predict = modeljob.forecast(validation_data_pd)

    return predict</code></pre>
</div>
</div>
</p>
","18235008",1
1606,71745786,2,71696407,2022-04-05 02:43:25,0,"<p>The solution is to augment the stock estimator image with the right components and then it can be run in the SageMaker script mode:</p>
<pre><code>FROM    763104351884.dkr.ecr.us-east-1.amazonaws.com/pytorch-training:1.10-gpu-py38

COPY requirements.txt /tmp/requirements.txt
RUN pip install -r /tmp/requirements.tx
</code></pre>
<p>The key is to make sure <code>nvidia</code> runtime is used at build time, so <code>daemon.json</code> needs to be configured accordingly:</p>
<pre class=""lang-json prettyprint-override""><code>{
    &quot;default-runtime&quot;: &quot;nvidia&quot;,
    &quot;runtimes&quot;: {
        &quot;nvidia&quot;: {
            &quot;path&quot;: &quot;nvidia-container-runtime&quot;,
            &quot;runtimeArgs&quot;: []
        }
    }
}
</code></pre>
<p>This is still not a complete solution, because viability of the build for SageMaker depends on the host where the build is performed.</p>
","1687102",0
1607,71747990,2,71738894,2022-04-05 07:47:03,1,"<p>By default AWS limits the number of instances you can use, here you have the default <a href=""https://docs.aws.amazon.com/general/latest/gr/sagemaker.html"" rel=""nofollow noreferrer"">limits</a>. As the error message says, you have to request for a limit increase, you can do it from <a href=""https://us-east-1.console.aws.amazon.com/support/home?region=us-east-1&amp;skipRegion=true#/case/create"" rel=""nofollow noreferrer"">here</a>, it will take couple of days from my experience.</p>
","4267439",0
1608,71750480,2,71496966,2022-04-05 10:50:32,1,"<p>Try Updating the pipeline component using the command:</p>
<p><code>pip3 install --force-reinstall google_cloud_pipeline_components==0.1.3</code></p>
","11028850",0
1609,71757468,2,71751105,2022-04-05 19:34:44,2,"<p>SageMaker Training Jobs will compress any files located in <code>/opt/ml/model</code> which is the value of <a href=""https://github.com/aws/sagemaker-training-toolkit/blob/master/ENVIRONMENT_VARIABLES.md#sm_model_dir"" rel=""nofollow noreferrer""><code>SM_MODEL_DIR</code></a> and upload it to S3 automatically. You could look at saving your file to <code>SM_MODEL_DIR</code> (Your classification report will thus be uploaded to S3 in the model tar ball).</p>
<p>The <a href=""https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/s3.html#S3.Client.upload_file"" rel=""nofollow noreferrer""><code>upload_file()</code></a> function requires you to pass an S3 bucket.
You could also look at manually specify an S3 bucket in your code to upload the file to.</p>
<pre><code>s3.meta.client.upload_file(classification_report_file_name, &lt;YourS3Bucket&gt;,
                            f&quot;{args.eval_model_name}_classification_report.csv&quot;)
</code></pre>
","9796588",0
1610,71761861,2,71752458,2022-04-06 06:17:24,0,"<p>I had the same problem while trying to load a model from model registry with mismatching versions (client 1.22.0).</p>
<p>I had to downgrade the client version to make it work.</p>
<p>Downgraded first the client to 1.21 and then server to 1.20</p>
<p>Refer - <a href=""https://docs.databricks.com/dev-tools/api/latest/mlflow.html#operation/transition-model-version-stage"" rel=""nofollow noreferrer"">https://docs.databricks.com/dev-tools/api/latest/mlflow.html#operation/transition-model-version-stage</a></p>
","11104805",0
1611,71763011,2,71747545,2022-04-06 07:55:01,0,"<p>| - This pipe symbol in YAML document is used for <em><strong>&quot;Multiple line statements&quot;</strong></em></p>
<pre><code>description: |
  # Azure Machine Learning &quot;hello world&quot; job

  This is a &quot;hello world&quot; job running in the cloud via Azure Machine Learning!

  ## Description

  Markdown is supported in the studio for job descriptions! You can edit the description there or via CLI.
</code></pre>
<p>in the above example, we need to write some multiple line description. So, we need to use &quot;|&quot; symbol</p>
<p>&quot;&gt;&quot; - This symbol is used to save some content directly to a specific location document.</p>
<pre><code>command: echo &quot;hello world&quot; &gt; ./outputs/helloworld.txt
</code></pre>
<p>In this above command, we need to post <strong>&quot;hello world&quot;</strong> to <em><strong>&quot;helloworld.txt&quot;</strong></em></p>
<p>Check the below link for complete documentation regarding YAML files.</p>
<p><a href=""https://learn.microsoft.com/en-us/azure/machine-learning/reference-yaml-job-command"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/machine-learning/reference-yaml-job-command</a></p>
<p>All these symbols are the YAML job commands which are used to accomplish a specific task through CLI.</p>
","18428148",0
1612,71774864,2,71665007,2022-04-07 00:02:35,1,"<p>Currently Data Wrangler does not support browsing Views in the UI but you can still query them with SELECT.</p>
","18730568",0
1613,71795325,2,69795917,2022-04-08 10:19:06,0,"<p>I found it does work if I run tensorboard from the terminal, not ideal but a decent workaround for the moment.</p>
","1904282",0
1614,71829758,2,71827884,2022-04-11 14:38:53,2,"<p>AWS services trying to call other AWS services and perform actions are not allowed to do so by default. For example, SageMaker Notebooks are basically EC2 instances. In order for SageMaker to create EC2 instances, it has to have a policy which allows e.g., injecting ENIs to a VPC. Since you probably do not want to do all that by yourself (it is a managed Notebook service after all), you have to give SageMaker permissions to perform actions on your behalf. Enter <strong>execution roles</strong>. For SageMaker, you can read more in [1]. Other services that you will commonly find using execution roles are Lambda, ECS and many others. An IAM role usually consists of two parts:</p>
<ol>
<li>Trust relationship (I like to call it trust policy)</li>
<li>Permissions policy</li>
</ol>
<p>The first one decides which principal (AWS identifier, Service etc. [2]) will be able to assume the role. In your example, that is:</p>
<pre><code>data &quot;aws_iam_policy_document&quot; &quot;sm_assume_role_policy&quot; {
  statement {
    actions = [&quot;sts:AssumeRole&quot;]
    
    principals {
      type = &quot;Service&quot;
      identifiers = [&quot;sagemaker.amazonaws.com&quot;]
    }
  }
}
</code></pre>
<p>What this policy says is &quot;I am going to allow SageMaker (which is of type <code>Service</code>) to assume any role to which this policy is attached and perform actions that are defined in the permissions policy&quot;. The permissions policy is:</p>
<pre><code># Attaching the AWS default policy, &quot;AmazonSageMakerFullAccess&quot;
resource &quot;aws_iam_policy_attachment&quot; &quot;sm_full_access_attach&quot; {
  name = &quot;sm-full-access-attachment&quot;
  roles = [aws_iam_role.notebook_iam_role.name]
  policy_arn = &quot;arn:aws:iam::aws:policy/AmazonSageMakerFullAccess&quot;
}
</code></pre>
<p>Without going into too much details about what the AWS managed policy for SageMaker does, it is enough to see the <code>FullAccess</code> part for it to be clear. What you could do if you want to be extra careful is to define a customer managed policy [3] for SageMaker notebooks. This permissions policy will be attached to the IAM role(s) defined in the <code>roles</code> argument. Note that it is a list, so multiple roles can have the same permissions policy attached.</p>
<p>Last, but not the least, the glue between the trust and permissions policy is the role itself:</p>
<pre><code>resource &quot;aws_iam_role&quot; &quot;notebook_iam_role&quot; {
  name = &quot;sm_notebook_role&quot;
  assume_role_policy = data.aws_iam_policy_document.sm_assume_role_policy.json
}
</code></pre>
<p>As you can see, the <code>assume_role_policy</code> is the policy which will allow SageMaker to perform actions in the AWS account based on the permissions defined in the permissions policy.</p>
<p>This topic is much more complex than in this answer, but it should give you a fair amount of information.</p>
<p>NOTE: In theory, the same role accessing information in AWS and running the AWS API actions when using Terraform could be used for SageMaker, but I would strongly advise against it. Always keep in mind separation of concerns and principle of least privilege.</p>
<hr />
<p>[1] <a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-roles.html#sagemaker-roles-create-execution-role"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-roles.html#sagemaker-roles-create-execution-role</a></p>
<p>[2] <a href=""https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_elements_principal.html"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_elements_principal.html</a></p>
<p>[3] <a href=""https://docs.aws.amazon.com/acm/latest/userguide/authen-custmanagedpolicies.html"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/acm/latest/userguide/authen-custmanagedpolicies.html</a></p>
","8343484",0
1615,71850503,2,71846804,2022-04-13 00:26:28,1,"<p>As you can read <a href=""https://www.mlflow.org/docs/latest/python_api/mlflow.keras.html#module-mlflow.keras"" rel=""nofollow noreferrer"">here</a>. You can use <code>mlflow.tensorflow.autolog()</code> and this, (from doc):</p>
<blockquote>
<p>Enables (or disables) and configures autologging from Keras to MLflow. Autologging captures the following information:</p>
<blockquote>
<p>fit() or fit_generator() parameters; optimizer name; learning rate; epsilon
...</p>
</blockquote>
</blockquote>
<p>For example:</p>
<pre><code># !pip install mlflow
import tensorflow as tf
import mlflow
import numpy as np


X_train = np.random.rand(100,100)
y_train = np.random.randint(0,10,100)
    

model = tf.keras.Sequential()
model.add(tf.keras.Input(100,))
model.add(tf.keras.layers.Dense(256, activation='relu'))
model.add(tf.keras.layers.Dropout(rate=.4))
model.add(tf.keras.layers.Dense(10, activation='sigmoid'))        
model.compile(loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),
              optimizer='Adam', 
              metrics=['accuracy'])
model.summary()


mlflow.tensorflow.autolog()
history = model.fit(X_train, y_train, epochs=100, batch_size=50)
</code></pre>
<p>Or as you mention in the comment you can use <code>mlflow.set_tracking_uri()</code> like below:</p>
<pre><code>mlflow.set_tracking_uri('http://127.0.0.1:5000')
tracking_uri = mlflow.get_tracking_uri()
with mlflow.start_run(run_name='PARENT_RUN') as parent_run:
    batch_size=50
    history = model.fit(X_train, y_train, epochs=2, batch_size=batch_size)
    mlflow.log_param(&quot;batch_size&quot;, batch_size)  
</code></pre>
<p>For getting results:</p>
<pre><code>!mlflow ui
</code></pre>
<p>Output:</p>
<pre><code>[....] [...] [INFO] Starting gunicorn 20.1.0
[....] [...] [INFO] Listening at: http://127.0.0.1:5000 (****)
[....] [...] [INFO] Using worker: sync
[....] [...] [INFO] Booting worker with pid: ****
</code></pre>
<p><a href=""https://i.stack.imgur.com/9XXoi.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/9XXoi.png"" alt=""enter image description here"" /></a>
<a href=""https://i.stack.imgur.com/V2tvM.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/V2tvM.png"" alt=""enter image description here"" /></a></p>
","1740577",3
1616,71852940,2,71847442,2022-04-13 06:38:31,1,"<p>You can find the relevant checkpoint save/load code in <a href=""https://github.com/huggingface/notebooks/blob/main/sagemaker/05_spot_instances/sagemaker-notebook.ipynb"" rel=""nofollow noreferrer"">Spot Instances - Amazon SageMaker x Hugging Face Transformers</a>.<br />
(The example enables Spot instances, but you can use on-demand).</p>
<ol>
<li>In hyperparameters you set: <code>'output_dir':'/opt/ml/checkpoints'</code>.</li>
<li>You define a <code>checkpoint_s3_uri</code> in the Estimator (which is unique to the series of jobs you'll run).</li>
<li>You add code for train.py to support checkpointing:</li>
</ol>
<blockquote>
<pre><code>from transformers.trainer_utils import get_last_checkpoint

# check if checkpoint existing if so continue training
if get_last_checkpoint(args.output_dir) is not None:
    logger.info(&quot;***** continue training *****&quot;)
    last_checkpoint = get_last_checkpoint(args.output_dir)
    trainer.train(resume_from_checkpoint=last_checkpoint)
else:
    trainer.train()
</code></pre>
</blockquote>
","121956",2
1617,71880474,2,71880336,2022-04-15 05:37:28,1,"<p>On a local machine,</p>
<ul>
<li>Make sure to install AWS CLI: <a href=""https://docs.aws.amazon.com/cli/latest/userguide/getting-started-install.html"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/cli/latest/userguide/getting-started-install.html</a></li>
<li>Create an access key id and secret access key to access Sagemaker services locally: <a href=""https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_access-keys.html"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_access-keys.html</a></li>
<li>Set these credentials using <code>aws configure</code> command locally.</li>
<li>The code should work fine except getting the execution role. You can either hard code the Sagemaker role in the code (not best practice) or store it in the Parameter store and access it from there.</li>
</ul>
","1926852",1
1618,71881279,2,71860839,2022-04-15 07:34:10,0,"<p>Looks like you've completed 8 steps and it just takes very long. What's your step time?<br />
It might be due to data loading. Where ia data stored? Try to take data loading out of the picture by caching and feeding a single image to the DNN repeatedly and see if that helps.</p>
","121956",1
1619,71907680,2,71872506,2022-04-18 04:47:54,0,"<p>To resolve <code>ERROR: Could not build wheels for pynacl which use PEP 517 and cannot be installed directly</code> this error, try either of the following ways:</p>
<ol>
<li><p>Install missing dependencies:</p>
<pre><code>sudo apt install libpython3-dev build-essential
</code></pre>
</li>
<li><p>Upgrade pip:</p>
<pre><code>pip3 install --upgrade pip
</code></pre>
</li>
<li><p>Upgrade pip with setuptools wheel:</p>
<pre><code>pip3 install --upgrade pip setuptools wheel
</code></pre>
</li>
<li><p>Reinstall PEP517:</p>
<pre><code>pip3 install p5py
pip3 install PEP517
</code></pre>
</li>
</ol>
<p>You can refer to  <a href=""https://stackoverflow.com/questions/61365790/error-could-not-build-wheels-for-scipy-which-use-pep-517-and-cannot-be-installe"">ERROR: Could not build wheels for scipy which use PEP 517 and cannot be installed directly</a>, <a href=""https://stackoverflow.com/questions/64038673/could-not-build-wheels-for-which-use-pep-517-and-cannot-be-installed-directly"">Could not build wheels for _ which use PEP 517 and cannot be installed directly - Easy Solution</a> and <a href=""https://github.com/martomi/chiadog/issues/44"" rel=""nofollow noreferrer"">failed building wheel for pynacl</a></p>
","9303470",0
1620,71949770,2,71783228,2022-04-21 06:21:15,1,"<blockquote>
<p>How do I specify a nodeSelector while deploying an Azure ML model to an AKS Cluster? Or in general, is there a way to merge my custom pod spec with the one generated by Azure ML library?</p>
</blockquote>
<p>As per <a href=""https://learn.microsoft.com/en-us/azure/machine-learning/how-to-attach-arc-kubernetes?tabs=studio"" rel=""nofollow noreferrer"">Configure Kubernetes clusters for machine learning</a>:</p>
<p><code>nodeSelector</code> : Set the node selector so the extension components and the training/inference workloads will only be deployed to the nodes with all specified selectors.</p>
<p>For example:</p>
<p><code>nodeSelector.key=value</code> , <code>nodeSelector.node-purpose=worker</code> and <code>nodeSelector.node-region=eastus</code></p>
<p>You can refer to <a href=""https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#built-in-node-labels"" rel=""nofollow noreferrer"">Assigning Pods to Nodes</a> and <a href=""https://github.com/Azure/AKS/issues/2866"" rel=""nofollow noreferrer"">Cannot create nodepool with node-restriction.kubernetes.io/ prefix label</a></p>
","9303470",0
1621,71965351,2,71948090,2022-04-22 08:01:42,1,"<p>Your question is very broad and the best way forward would depend on other details of your use-case, so we will have to make some assumptions.</p>
<p>[Queue manager]
SageMaker does <em>not</em> have a queue manager. If at the end you decide you need a queue manager, I would suggest looking towards AWS Batch.</p>
<p>[Single vs multiple training jobs]
Since you need to run 10s of thousands job I assume you are training fairly lightweight models, so to save on time, you would be better off reusing instances for multiple training jobs. (Otherwise, with 20 instances limit, you need 500 rounds of training, with a 3 min start time - depending on instance type - you need 25 hours just for the wait time. Depending on the complexity of each individual model, this 25hours might be significant or totally acceptable).</p>
<p>[Instance limit increase]
You can always ask for a limit increase, but going from a limit of 20 to 10k at once is likely that will not be accepted by the AWS support team, unless you are part of an organisation with a track record of usage on AWS, in which case this might be fine.</p>
<p>[One possible option] (Assuming multiple lightweight models)
You could create a single training job, with instance count, the number of instances available to you.
Inside the training job, your code can run a for loop and perform all the individual training jobs you need.</p>
<p>In this case, you will need to know which which instance is which so you can make the split of the HPOs. SageMaker writes this information on the file: <a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/your-algorithms-training-algo-running-container.html#your-algorithms-training-algo-running-container-dist-training"" rel=""nofollow noreferrer"">/opt/ml/input/config/resourceconfig.json</a> so using that you can easily have each instance run a subset of the trainings required.</p>
<p>Another thing to think of, is if you need to save the generated models (which you probably need). You can either save everything in the output model directory - standard SM approach- but this would zip all models in a model.tar.gz file.
If you don't want this, and prefer to have each model individually saved, I'd suggest using the <a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/model-checkpoints.html"" rel=""nofollow noreferrer"">checkpoints</a> directory that will sync anything written there to your s3 location.</p>
","18309229",2
1622,71973994,2,71962260,2022-04-22 19:57:49,4,"<p>With some suggestions provided in the comments, I think I managed to make my demo pipeline work. I will first include the updated code:</p>
<pre class=""lang-py prettyprint-override""><code>from kfp.v2 import compiler
from kfp.v2.dsl import pipeline, component, Dataset, Input, Output
from datetime import datetime
from google.cloud import aiplatform
from typing import NamedTuple


# Importing 'COMPONENTS' of the 'PIPELINE'

@component(
    packages_to_install=[
        &quot;google-cloud-storage&quot;,
        &quot;pandas&quot;,
    ],
    base_image=&quot;python:3.9&quot;,
    output_component_file=&quot;get_data.yaml&quot;
)
def get_data(
    bucket: str,
    url: str,
    dataset: Output[Dataset],
):
    &quot;&quot;&quot;Reads a csv file, from some location in Cloud Storage&quot;&quot;&quot;
    import ast
    import pandas as pd
    from google.cloud import storage
    
    # 'Pulling' demo .csv data from a know location in GCS
    storage_client = storage.Client(&quot;my-project&quot;)
    bucket = storage_client.get_bucket(bucket)
    blob = bucket.blob(url)
    blob.download_to_filename('localdf.csv')
    
    # Reading the pulled demo .csv data
    df = pd.read_csv('localdf.csv', compression='zip')
    df['new_skills'] = df['new_skills'].apply(ast.literal_eval)
    df.to_csv(dataset.path + &quot;.csv&quot; , index=False, encoding='utf-8-sig')


@component(
    packages_to_install=[&quot;pandas&quot;],
    base_image=&quot;python:3.9&quot;,
    output_component_file=&quot;report_data.yaml&quot;
)
def report_data(
    inputd: Input[Dataset],
) -&gt; NamedTuple(&quot;output&quot;, [(&quot;rows&quot;, int), (&quot;columns&quot;, int)]):
    &quot;&quot;&quot;From a passed csv file existing in Cloud Storage, returns its dimensions&quot;&quot;&quot;
    import pandas as pd
    
    df = pd.read_csv(inputd.path+&quot;.csv&quot;)
    
    return df.shape


# Building the 'PIPELINE'

@pipeline(
    # i.e. in my case: PIPELINE_ROOT = 'gs://my-bucket/test_vertex/pipeline_root/'
    # Can be overriden when submitting the pipeline
    pipeline_root=PIPELINE_ROOT,
    name=&quot;readcsv-pipeline&quot;,  # Your own naming for the pipeline.
)
def my_pipeline(
    url: str = &quot;test_vertex/pipeline_root/program_grouping_data.zip&quot;,
    bucket: str = &quot;my-bucket&quot;
):
    dataset_task = get_data(bucket, url)

    dimensions = report_data(
        dataset_task.output
    )
    

# Compiling the 'PIPELINE'    

compiler.Compiler().compile(
    pipeline_func=my_pipeline, package_path=&quot;pipeline_job.json&quot;
)


# Running the 'PIPELINE'

TIMESTAMP = datetime.now().strftime(&quot;%Y%m%d%H%M%S&quot;)

run1 = aiplatform.PipelineJob(
    display_name=&quot;my-pipeline&quot;,
    template_path=&quot;pipeline_job.json&quot;,
    job_id=&quot;mlmd-pipeline-small-{0}&quot;.format(TIMESTAMP),
    parameter_values={
        &quot;url&quot;: &quot;test_vertex/pipeline_root/program_grouping_data.zip&quot;,
        &quot;bucket&quot;: &quot;my-bucket&quot;
    },
    enable_caching=True,
)

# Submitting the 'PIPELINE'

run1.submit()
</code></pre>
<p>Now, I will add some complementary comments, which in sum, managed to solve my problem:</p>
<ul>
<li>First, having the &quot;Logs Viewer&quot; (roles/logging.viewer) enabled for your user, will greatly help to troubleshoot any existing error in your pipeline (Note: that role worked for me, however you might want to look for a better matching role for you own purposes <a href=""https://cloud.google.com/logging/docs/access-control?&amp;_ga=2.212939101.-833851896.1650314090&amp;_gac=1.182768084.1650632490.Cj0KCQjwpImTBhCmARIsAKr58cw9X0TGy2YeN-CFsM4RvEXDH_vL54Ce1ECrWh4_aJNoPEhgtXusUhwaAlIUEALw_wcB#permissions_and_roles"" rel=""nofollow noreferrer"">here</a>). Those errors will appear as &quot;Logs&quot;, which can be accessed by clicking the corresponding button:</li>
</ul>
<p><a href=""https://i.stack.imgur.com/jpgfU.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/jpgfU.png"" alt=""enter image description here"" /></a></p>
<ul>
<li>NOTE: In the picture above, when the &quot;Logs&quot; are displayed, it might be helpful to carefully check each log (close to the time when you created you pipeline), as generally each eof them corresponds with a single warning or error line:</li>
</ul>
<p><a href=""https://i.stack.imgur.com/CpTkZ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/CpTkZ.png"" alt=""Verte AI Pipelines Logs"" /></a></p>
<ul>
<li>Second, the output of my pipeline was a tuple. In my original approach, I just returned the plain tuple, but it is advised to return a <a href=""https://docs.python.org/3/library/typing.html#typing.NamedTuple"" rel=""nofollow noreferrer"">NamedTuple</a> instead. In general, if you need to input / output one or more &quot;<em>small values</em>&quot; (int or str, for any reason), pick a NamedTuple to do so.</li>
<li>Third, when the connection between your pipelines is <code>Input[Dataset]</code> or <code>Ouput[Dataset]</code>, adding the file extension is needed (and quite easy to forget). Take for instance the ouput of the <code>get_data</code> component, and notice how the data is recorded by specifically adding the file extension, i.e. <code>dataset.path + &quot;.csv&quot;</code>.</li>
</ul>
<p>Of course, this is a very tiny example, and projects can easily scale to huge projects, however as some sort of &quot;Hello Vertex AI Pipelines&quot; it will work well.</p>
<p>Thank you.</p>
","16706763",1
1623,71982829,2,71953876,2022-04-23 19:07:50,0,"<p>Basically this was an issue related to IAM.
Running cdk program requires bootstrapping it using the command <code>cdk bootstrap</code>
After running this command cdk was creating a bunch of roles out of which one role will be related to cloudformation's execution role. Something like</p>
<blockquote>
<p>cdk-serialnumber-cfn-exec-role-Id-region</p>
</blockquote>
<p>Now this role was used by cloudformation to run the stack.</p>
<p>Using sagemaker from console automatically adds the role associated with domain/user at</p>
<blockquote>
<p>ServiceCatalog -&gt; Portfolios -&gt; Imported -&gt; Amazon SageMaker Solutions and ML Ops products -&gt; Groups, roles, and users</p>
</blockquote>
<p>Thats was the reason why product id was accessible from console.</p>
<p>After adding the role created by cdk bootsrap to the above path I was able to run my stack.</p>
","10214532",0
1624,71984684,2,71708147,2022-04-24 00:46:51,5,"<p>Where do you run <code>mlflow ui</code> command?</p>
<p>I think if you pass tracking ui path in the arguments, it would work:</p>
<pre class=""lang-bash prettyprint-override""><code>mlflow ui --backend-store-uri file:///Users/Swapnil/Documents/LocalPython/MLFLowDemo/mlrun
</code></pre>
","11232272",1
1625,72008705,2,71858668,2022-04-26 05:16:05,1,"<p>To resolve this <code>ModuleNotFoundError: No module named 'tensorflow_hub'</code>  error, try following ways:</p>
<ul>
<li>Try installing/upgrading the latest version of <code>tensorflow</code> and <code>tensorflow-hub</code> and then import:</li>
</ul>
<pre><code>!pip install --upgrade tensorflow

!pip install --upgrade tensorflow_hub

import tensorflow as tf

import tensorflow_hub as hub
</code></pre>
<ul>
<li>Install the current environment as a new kernel:</li>
</ul>
<pre><code>python3 -m ipykernel install --user --name=testenvironment
</code></pre>
<p>You can refer to <a href=""https://stackoverflow.com/questions/63884339/modulenotfounderror-no-module-named-tensorflow-hub"">ModuleNotFoundError: No module named 'tensorflow_hub', No module named 'tensorflow_hub'</a> and <a href=""https://github.com/tensorflow/hub/issues/767"" rel=""nofollow noreferrer"">How to use Tensorflow Hub Model?</a></p>
","9303470",0
1626,72008858,2,71985302,2022-04-26 05:35:47,0,"<ul>
<li>Run mlflow on a remote server</li>
</ul>
<pre><code>mlflow server --host 0.0.0.0 --port 8888
</code></pre>
<ul>
<li>Configure ssh local port forward on your local machine, like this:
<a href=""https://stackoverflow.com/questions/9146457/ssh-port-forwarding-in-a-ssh-config-file"">SSH Port forwarding in a ~/.ssh/config file?</a></li>
</ul>
<pre><code>LocalForward 8888 your_remote_machine_addr:8888
</code></pre>
<ul>
<li>Connect to localhost:8888 on your browser</li>
</ul>
","11705689",0
1627,72013381,2,72013380,2022-04-26 11:46:38,0,"<p>The documentation of <a href=""https://www.mlflow.org/docs/latest/python_api/mlflow.pyfunc.html"" rel=""nofollow noreferrer"">mlflow.pyfunc</a> says:</p>
<blockquote>
<p>The python_function model flavor serves as a default model interface for MLflow Python models. Any MLflow Python model is expected to be loadable as a python_function model.</p>
</blockquote>
<p>This means you can use <code>mlflow.pyfunc.load_model</code>  to load all mlflow supported flavors.</p>
","4390189",0
1628,72017723,2,72007159,2022-04-26 16:51:22,3,"<p>Have you got a chance to take a look at this <a href=""https://www.tensorflow.org/tfx/tutorials/tfx/components#examplegen"" rel=""nofollow noreferrer"">section in tutorials</a>, which explains how to display the artifacts of <code>ExampleGen</code> component? You can modify the code below (Source: <a href=""https://www.tensorflow.org/tfx/tutorials/tfx/components"" rel=""nofollow noreferrer"">TFX Tutorial</a>) to achieve the same.</p>
<pre class=""lang-py prettyprint-override""><code># Get the URI of the output artifact representing the training examples, which is a directory
train_uri = os.path.join(example_gen.outputs['examples'].get()[0].uri, 'Split-train')

# Get the list of files in this directory (all compressed TFRecord files)
tfrecord_filenames = [os.path.join(train_uri, name)
                      for name in os.listdir(train_uri)]

# Create a `TFRecordDataset` to read these files
dataset = tf.data.TFRecordDataset(tfrecord_filenames, compression_type=&quot;GZIP&quot;)

# Iterate over the first 3 records and decode them.
for tfrecord in dataset.take(3):
  serialized_example = tfrecord.numpy()
  example = tf.train.Example()
  example.ParseFromString(serialized_example)
  pp.pprint(example)

</code></pre>
","user11530462",0
1629,72037140,2,72035391,2022-04-28 01:58:04,0,"<p>Change the &quot;GlobalParameter&quot; value to any floating number other than 1.0 or even you can remove it and execute. Sometimes, Global parameter will cause the issue. Check the below documentation.</p>
<p><a href=""https://learn.microsoft.com/en-us/answers/questions/746784/azure-ml-studio-error-while-testing-real-time-endp.html"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/answers/questions/746784/azure-ml-studio-error-while-testing-real-time-endp.html</a></p>
","18428148",0
1630,72038875,2,72038060,2022-04-28 06:19:43,1,"<p>As you are using Anaconda Navigator, the IDLE will be either Jupyter Notebook or Spyder. DLIB is a regular upgrading library for facial recognition. It cannot be installed on the base environment. @Avinash mentioned regarding two more libraries <em><strong>cmake</strong></em> and <em><strong>face_recognition</strong></em> the major issue for is DLIB.</p>
<p>Follow the below steps to install DLIB using Anaconda Navigator</p>
<ol>
<li><p>Open <strong>Anaconda Navigator</strong> or <strong>Open Anaconda Prompt</strong></p>
</li>
<li><p>Click on <strong>Environments</strong> (For anaconda navigator UI)</p>
</li>
<li><p>Click on <strong>&quot;Create&quot;</strong> and to create a new environment (For anaconda navigator UI)</p>
</li>
<li><p>Select the <strong>python version</strong>. (For anaconda navigator UI)</p>
</li>
<li><p>Click on create. (For anaconda navigator UI)</p>
</li>
<li><p>Open anaconda navigator</p>
</li>
<li><p>Enter the following command to navigate from base environment to virtual environment just created</p>
</li>
<li><p>Enter the following command to install DLIB</p>
</li>
</ol>
<p>If you are using normal <em><strong>Python IDLE</strong></em>, use the following procedure.</p>
<ol>
<li><strong>Virtual Environment creation using command prompt</strong></li>
</ol>
<p>Syntax: <code>python3 -m venv [Virtual Environment Name]</code></p>
<p>Code: <code>python3 -m venv dlib</code></p>
<ol start=""2"">
<li><strong>Activate Virtual Environment</strong></li>
</ol>
<p>Syntax: <code>.\[Virtual Environment Folder Name]\Scripts\activate</code></p>
<p>Code: <code>.\dlib\Scripts\activate</code></p>
<p>or you can directly mention as</p>
<pre><code>activate dlib
</code></pre>
<ol start=""3"">
<li><p><strong>Finally you will be shifter from base environment to virtual environment</strong></p>
</li>
<li><p><strong>Deactivating virtual environment</strong></p>
<p><code>deactivate</code></p>
</li>
</ol>
<p>If you are not having proper UI access, then directly go to <strong>Anaconda Navigator</strong> and use the below command for virtual environment creation.</p>
<ol>
<li><p><strong>Open Anaconda Prompt</strong></p>
</li>
<li><p><strong>Check for conda installed in path</strong></p>
<p><code>conda -V</code></p>
</li>
<li><p><strong>Check if conda is updated or not</strong></p>
<p><code>conda update conda</code></p>
</li>
<li><p><strong>Create a virtual environment</strong></p>
</li>
</ol>
<p>syntax: <code>conda create --name [Virtual Environment Name] python=[Version you want to install]</code></p>
<p>code: <code>conda create --name dlib python=3.8</code></p>
<p>Installing DLIB in virtual environment.</p>
<pre><code>pip install dlib
</code></pre>
<p>remaining activating and deactivating are same.</p>
","18428148",1
1631,72048755,2,72039147,2022-04-28 18:40:15,2,"<p>Amazon SageMaker model monitor only supports metrics that are defined <a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/model-monitor-model-quality-metrics.html"" rel=""nofollow noreferrer"">here</a> out of the box.
If you need to include another metric such as MAPE (Mean Absolute Percentage Error) in your case, you will have to rely on BYOC approach, note that with this approach you cannot &quot;add&quot; a metric to the available list, unfortunately you will have to implement the entire suite of metrics yourself. I understand this is not ideal for customers, I'd encourage you to reach out to your AWS account manager to create a request to add MAPE (Mean Absolute Percentage Error) as a supported metric in the long run. I've made a note of it as well and will rely it back to the team.</p>
<p>In the meantime, you can find examples on how to BYOC <a href=""https://aws.amazon.com/blogs/machine-learning/detect-nlp-data-drift-using-custom-amazon-sagemaker-model-monitor/"" rel=""nofollow noreferrer"">here</a>.</p>
<p>I work for AWS but my opinions are my own.</p>
<p>Thanks,
Raghu</p>
","6411548",0
1632,72051491,2,72032469,2022-04-29 00:13:05,1,"<p>Have you tested this model locally? How does inference work with your TF model locally? This should show you how the input needs to be formatted for inference with that model in specific. Application/x-image data format should be fine. Do you have a custom inference script? Check out this link here for adding an inference script with will let you control pre/post processing and you can log each line to capture the error: <a href=""https://github.com/aws/sagemaker-tensorflow-serving-container"" rel=""nofollow noreferrer"">https://github.com/aws/sagemaker-tensorflow-serving-container</a>.</p>
","16504640",0
1633,72052218,2,72039744,2022-04-29 02:39:16,1,"<p>Sagemaker Debugger is only to monitor the training jobs.</p>
<p><a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/train-debugger.html"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/sagemaker/latest/dg/train-debugger.html</a></p>
<p>I dont think you can use it on Endpoints.</p>
<p>The script that you have provided is used both for training and inference. The container used by the estimator will take care of what functions to run. So it is not possible to debug the script directly. But what are you debugging in the code ? Training part or the inference part ?</p>
<p>While creating the estimator we need to give either the entry_point or the source directory. If you are using the &quot;entry_point&quot; then the value should be relative path to the file, if you are using &quot;source_dir&quot; then you should be able to give an S3 path. So before running the estimator, you can programmatically tar the files and upload it to S3 and then use the S3 path in the estimator.</p>
","7849350",6
1634,72064907,2,71870508,2022-04-30 00:49:46,0,"<p>Using &quot;latest&quot; it not suggested as per documentation(see note): <a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-algo-docker-registry-paths.html"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-algo-docker-registry-paths.html</a></p>
<p>Use specific versions as they are more stable.</p>
","7849350",2
1635,72065007,2,71777914,2022-04-30 01:17:31,0,"<p>I think it is not possible to change the log streams names for any of the SageMaker services.</p>
","7849350",1
1636,72065187,2,68687729,2022-04-30 02:10:19,0,"<p>I would suggest to look into the CloudWatch logs of the endpoint to see if there are any invocations reaching the endpoint.</p>
<p>If yes, see if they are sending a response back without any errors in the same log file.</p>
","7849350",0
1637,72068142,2,72068059,2022-04-30 11:32:19,1,"<p>I found the solution <a href=""https://github.com/aws/amazon-sagemaker-examples/issues/2952"" rel=""nofollow noreferrer"">here</a>. I will leave it in case someone come accross the same issue.</p>
","283538",0
1638,72089736,2,72058686,2022-05-02 16:41:33,0,"<p>I've solved this problem - I used sagemaker.model.model to load in the model data I already had and I called the deploy method on the aforementioned model object to deploy it. Further, I had the inference script and the model file in the same place as the notebook and directly called them, as this gave me an error earlier as well.</p>
","11525534",0
1639,72090702,2,72090460,2022-05-02 18:12:15,0,"<p>Not sure why, but this helped me solve this acute problem:</p>
<pre><code>pip3 install --upgrade setuptools
</code></pre>
<p>And then I did the:</p>
<pre><code>pip3 install kfp --upgrade --user
</code></pre>
","3102968",0
1640,72095701,2,71851981,2022-05-03 06:49:22,1,"<p>You cannot select the files.</p>
<p>But if you have a folder within a bucket then you can select that folder which consists of the input data.</p>
<p>In the video they selected the bucket but not the files.</p>
","7849350",1
1641,72101027,2,72097296,2022-05-03 14:32:47,0,"<p>Something along those lines appears to work fine:</p>
<pre><code>local_model_path = &quot;model.tar.gz&quot;
with tarfile.open(local_model_path) as tar:
    tar.extractall()

model = xgb.XGBRegressor() 
model.load_model(model_file_name)     
</code></pre>
<p>model can then be used as usual - model.tar.gz is an artifcat coming from sagemaker.</p>
","283538",0
1642,72107202,2,72099790,2022-05-04 01:59:22,1,"<p>In the sample notebook, the model is trained within SageMaker. So it is created with certain environment variables like the &quot;SAGEMAKER_PROGRAM&quot;(I think, need to check the documentation) with value set to entry point script.</p>
<p>But while you are creating the model with models trained outside the SageMaker you need to add those environment variables.</p>
<p>Without an entry point script SageMaker is not in a position to know what to do with the request.</p>
","7849350",0
1643,72107244,2,72096297,2022-05-04 02:05:55,1,"<p>It is not possible through HPO.</p>
<p>You need to add additional step in your workflow to achieve cross-validation.</p>
","7849350",1
1644,72121801,2,72083832,2022-05-05 04:03:52,0,"<blockquote>
<p>What signal name should I use?</p>
</blockquote>
<p>You can use <code>PipelineChangeEvent</code> category of <code>AmlPipelineEvent</code> table to view events when ML pipeline draft or endpoint or module are accessed (read, created, or deleted).</p>
<p>For example, according to <a href=""https://learn.microsoft.com/en-us/azure/machine-learning/monitor-azure-machine-learning#analyzing-logs"" rel=""nofollow noreferrer"">documentation</a>, use <code>AmlComputeJobEvent</code> to get failed jobs in the last five days:</p>
<pre><code>AmlComputeJobEvent
| where TimeGenerated &gt; ago(5d) and EventType == &quot;JobFailed&quot;
| project  TimeGenerated , ClusterId , EventType , ExecutionState , ToolType
</code></pre>
<p><strong>Updated answer:</strong></p>
<p>According to <a href=""https://stackoverflow.com/users/897665/laurynas-g"">Laurynas G</a>:</p>
<pre><code>AmlRunStatusChangedEvent 
| where Status == &quot;Failed&quot; or Status == &quot;Canceled&quot;
</code></pre>
<p>You can refer to <a href=""https://learn.microsoft.com/en-us/azure/machine-learning/monitor-azure-machine-learning#analyzing-logs"" rel=""nofollow noreferrer"">Monitor Azure Machine Learning</a>, <a href=""https://learn.microsoft.com/en-us/azure/machine-learning/how-to-log-view-metrics"" rel=""nofollow noreferrer"">Log &amp; view metrics and log files</a> and <a href=""https://learn.microsoft.com/en-us/azure/machine-learning/how-to-debug-pipelines"" rel=""nofollow noreferrer"">Troubleshooting machine learning pipelines</a></p>
","9303470",3
1645,72123273,2,72120382,2022-05-05 07:15:00,2,"<p><a href=""https://docs.aws.amazon.com/eventbridge/latest/userguide/eb-transform-target-input.html"" rel=""nofollow noreferrer"">Input transformers</a> manipulate the event payload that EventBridge sends to the target.  Transforms consist of (1) an &quot;input path&quot; that maps substitution variable names to JSON-paths in the event and (2) a &quot;template&quot; that references the substitution variables.</p>
<p>Input path:</p>
<pre class=""lang-json prettyprint-override""><code>{
  &quot;detail-bucket-name&quot;: &quot;$.detail.bucket.name&quot;,
  &quot;detail-object-key&quot;: &quot;$.detail.object.key&quot;
}
</code></pre>
<p>Input template that concatenates the s3 url and outputs it along with the original event payload:</p>
<pre class=""lang-json prettyprint-override""><code>{
  &quot;s3Url&quot;: &quot;s3://&lt;detail-bucket-name&gt;/&lt;detail-object-key&gt;&quot;,
  &quot;original&quot;: &quot;$&quot;
}
</code></pre>
<p>Define the transform in the EventBridge console by editing the rule: <code>Rule &gt; Select Targets &gt; Additional Settings</code>.</p>
","1103511",10
1646,72130874,2,71395641,2022-05-05 16:50:06,0,"<p>You can get that information like this:
<a href=""https://github.com/kubeflow/pipelines/issues/4327#issuecomment-687255001"" rel=""nofollow noreferrer"">https://github.com/kubeflow/pipelines/issues/4327#issuecomment-687255001</a></p>
<p>component_name: This can be checked in the yaml definition of the pipeline, under <code>templates.name</code> (search for the component containing the output you want)</p>
<p>artifact_name: This can also be checked in the yaml definition of the pipeline, under that same component on the <code>outputs</code> attribute</p>
<p>Once you got these two parameters, you can use the functions as described in the above url:</p>
<pre><code>#!/usr/bin/env python3

import json
import tarfile
from base64 import b64decode
from io import BytesIO

import kfp


def get_node_id(*, run_id: str, component_name: str, client: kfp.Client):
    run = client.runs.get_run(run_id)
    workflow = json.loads(run.pipeline_runtime.workflow_manifest)
    nodes = workflow[&quot;status&quot;][&quot;nodes&quot;]
    for node_id, node_info in nodes.items():
        if node_info[&quot;displayName&quot;] == component_name:
            return node_id
    else:
        raise RuntimeError(f&quot;Unable to find node_id for Component '{component_name}'&quot;)


def get_artifact(*, run_id: str, node_id: str, artifact_name: str, client: kfp.Client):
    artifact = client.runs.read_artifact(run_id, node_id, artifact_name)
    # Artifacts are returned as base64-encoded .tar.gz strings
    data = b64decode(artifact.data)
    io_buffer = BytesIO()
    io_buffer.write(data)
    io_buffer.seek(0)
    data = None
    with tarfile.open(fileobj=io_buffer) as tar:
        member_names = tar.getnames()
        if len(member_names) == 1:
            data = tar.extractfile(member_names[0]).read().decode('utf-8')
        else:
            # Is it possible for KFP artifacts to have multiple members?
            data = {}
            for member_name in member_names:
                data[member_name] = tar.extractfile(member_name).read().decode('utf-8')
    return data


if __name__ == &quot;__main__&quot;:
    run_id = &quot;e498b0da-036e-4e81-84e9-6e9c6e64960b&quot;
    component_name = &quot;my-component&quot;
    # For an output variable named &quot;output_data&quot;
    artifact_name = &quot;my-component-output_data&quot;

    client = kfp.Client()
    node_id = get_node_id(run_id=run_id, component_name=component_name, client=client)
    artifact = get_artifact(
        run_id=run_id, node_id=node_id, artifact_name=artifact_name, client=client,
    )
    # Do something with artifact ...
</code></pre>
","19045311",0
1647,72138421,2,72133111,2022-05-06 08:27:02,1,"<blockquote>
<p>What does this mean? It keeps running, but I don't know if it is a bad message. It takes soo long, and I don't want to lose the processing time.</p>
</blockquote>
<ul>
<li><code>Reconnecting terminal</code> message can appear for multiple reasons like intermittent connectivity issues, unused active terminal sessions, processing of different size/format of data.</li>
<li>Make sure you close any unused terminal sessions to preserve your compute instance's resources. Idle terminals may impact the performance of compute instances.</li>
</ul>
<p>You can refer to <a href=""https://learn.microsoft.com/en-us/azure/machine-learning/how-to-access-terminal#manage-terminal-sessions"" rel=""nofollow noreferrer"">Access a compute instance terminal in your workspace</a>, <a href=""https://learn.microsoft.com/en-us/azure/machine-learning/concept-optimize-data-processing"" rel=""nofollow noreferrer"">Optimize data processing with Azure Machine Learning</a> and <a href=""https://www.youtube.com/watch?v=kiScfw9i4FM"" rel=""nofollow noreferrer"">Azure ML: Speed up processing time</a></p>
","9303470",1
1648,72167122,2,72166808,2022-05-09 04:51:23,1,"<p>The issue raised is very rare and there are less chances of getting the success rate even after the proper installation of libraries. When the code was deployed in Azure Machine Learning some of the issues might be resolved. Checkout the following steps to be taken care of:</p>
<ol>
<li>Check with the version of Open CV</li>
</ol>
<p><code>import cv2</code></p>
<p><code>cv2.__version__</code></p>
<ol start=""2"">
<li>After installation, implement the following steps</li>
</ol>
<p>these steps are very much time taking.</p>
<pre><code>%cd /content
!git clone https://github.com/opencv/opencv
!git clone https://github.com/opencv_contrib
!mkdir /content/build
%cd /content/build
!cmake -DOPENCV_EXTRA_MODULES_PATH=/content/opencv_contrib/modules  -DBUILD_SHARED_LIBS=OFF  -DBUILD_TESTS=OFF  -DBUILD_PERF_TESTS=OFF -DBUILD_EXAMPLES=OFF -DWITH_OPENEXR=OFF -DWITH_CUDA=ON -DWITH_CUBLAS=ON -DWITH_CUDNN=ON -DOPENCV_DNN_CUDA=ON /content/opencv
!make -j8 install
</code></pre>
<ol start=""3"">
<li>Check the version of Open CV again.</li>
</ol>
","18428148",1
1649,72170407,2,72170169,2022-05-09 10:19:09,1,"<p>The requirement of using .nii or other file formats in Azure auto ML is a challenging task. Unfortunately, Auto ML image input format will be using in only JSON format. Kindly check the <a href=""https://learn.microsoft.com/en-us/azure/machine-learning/reference-automl-images-schema"" rel=""nofollow noreferrer"">document</a></p>
<p>Answering regarding requirement of .nii format of dataset, there are different file format convertors available like &quot;<em><strong><a href=""http://medicalimageconverter.com/?msclkid=403b597ecf8111ecac65b3422c7b95b5"" rel=""nofollow noreferrer"">Medical Image Convertor</a></strong></em>&quot;. This software is commercial and can be used for 10days for free. Convert .nii file formats into JPG and proceed with the general documentation provided in the top of the answer.</p>
","18428148",1
1650,72202090,2,71679081,2022-05-11 13:38:19,0,"<p>You can set:</p>
<pre><code>os.environ['MLFLOW_TRACKING_INSECURE_TLS'] = 'true'
</code></pre>
<p>And then try to get your cert-chain straight from there for production use.</p>
<p>Also see Documentation: <a href=""https://mlflow.org/docs/latest/tracking.html#id19"" rel=""nofollow noreferrer"">https://mlflow.org/docs/latest/tracking.html#id19</a></p>
","9240308",0
1651,72203950,2,72203674,2022-05-11 15:43:32,0,"<p>CDK L1 Constructs correspond 1:1 to a CloudFormation resource of the same name. The construct props match the resouce properties.  The go-to source is therefore the CloudFormation docs.</p>
<p>The <code>AWS::SageMaker::Pipeline</code> <a href=""https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-sagemaker-pipeline.html#aws-resource-sagemaker-pipeline--examples"" rel=""nofollow noreferrer"">docs have a more complete example</a>.</p>
","1103511",4
1652,72210518,2,72210450,2022-05-12 05:32:02,0,"<p>According to the requirements, there is no procedure developed to create/delete workspaces through PowerShell in machine learning studio. For reference of creation of workspaces, you can check the below link and the point to be noted is we can create/delete workspaces using <em><strong>Az</strong></em></p>
<p>Here is the table link to check PowerShell support table</p>
<p><a href=""https://learn.microsoft.com/en-us/previous-versions/azure/machine-learning/classic/powershell-module"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/previous-versions/azure/machine-learning/classic/powershell-module</a></p>
<p><a href=""https://i.stack.imgur.com/PGfhb.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/PGfhb.png"" alt=""enter image description here"" /></a></p>
","18428148",0
1653,72211127,2,72210628,2022-05-12 06:43:57,0,"<p>The general monitoring is available and supportive in AKS, but the out of the box implementation was not supportive unfortunately. Check the below documentation and screenshot to refer supportive formats of monitoring in AKS</p>
<p><a href=""https://i.stack.imgur.com/0uxW5.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/0uxW5.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://learn.microsoft.com/en-us/azure/machine-learning/concept-endpoints"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/machine-learning/concept-endpoints</a></p>
","18428148",1
1654,72232763,2,72214443,2022-05-13 16:32:09,1,"<p>The 'Outline' tab (Table of Contents extension in Jupyter) is not available for Studio yet.</p>
<p><a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/nbi.html"" rel=""nofollow noreferrer"">SageMaker notebook instances</a> come with the extension prebuilt.</p>
","2458691",1
1655,72242854,2,72044181,2022-05-14 18:17:36,0,"<p>The issue mentioned above has been resolved. The part where &quot;DEMO XG-BOOST&quot; run doesn't end has been resolved by selecting &quot;Use emissary executor&quot; option while creating the pipeline.</p>
<p><a href=""https://i.stack.imgur.com/77j3P.png"" rel=""nofollow noreferrer"">see the snapshot</a></p>
<p>When we launch the pipeline with this setting, it solved the issue &amp; we can run complete pipelines now.</p>
<p><strong>More details:</strong>
We took support from GCP &amp; they have mentioned that the issue might have been caused by a recent upgrade of the GKE cluster which removes the docker runtime (<a href=""https://www.kubeflow.org/docs/components/pipelines/installation/choose-executor/#docker-executor"" rel=""nofollow noreferrer"">https://www.kubeflow.org/docs/components/pipelines/installation/choose-executor/#docker-executor</a>). Namely, the Docker executor is the default workflow executor and depends on docker container runtime, which is deprecated on Kubernetes 1.20+. We were using a GKE cluster whose version was 1.21.6. Hence the issue. So, we used the documentation (<a href=""https://www.kubeflow.org/docs/components/pipelines/installation/choose-executor/#migrate-to-emissary-executor"" rel=""nofollow noreferrer"">https://www.kubeflow.org/docs/components/pipelines/installation/choose-executor/#migrate-to-emissary-executor</a>) &amp; migrated to the Emissary executor (instead of Docker) which has solved our issue.</p>
","18962876",0
1656,72252259,2,72250896,2022-05-15 21:19:52,1,"<p>Finally, after struggling for a long time, I found the answer !</p>
<p>The crux is in the documentation <a href=""https://learn.microsoft.com/en-us/powershell/module/microsoft.powershell.utility/invoke-restmethod?view=powershell-7.2"" rel=""nofollow noreferrer"">here</a>.
Especially this section</p>
<p><a href=""https://i.stack.imgur.com/h0gwk.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/h0gwk.png"" alt=""enter image description here"" /></a></p>
<p>So, if you want to pass on a body for your &quot;get&quot; method in powershell, pass it as a hashtable.</p>
<p>So, finally the answer is</p>
<pre><code>$query=@{&quot;filter&quot;=&quot;name='model_name'&quot;;&quot;order_by&quot;=@(&quot;version DESC&quot;); &quot;max_results&quot;=1};
$searchuri=&quot;baseurl/api/2.0/preview/mlflow/model-versions/search&quot;

$resp=Invoke-RestMethod -Uri $searchuri -Headers $auth -Method Get -Body $query
</code></pre>
<p>Hope this helps someone looking for something similar.</p>
","4772836",3
1657,72256550,2,72166920,2022-05-16 08:57:22,2,"<p>For any future viewers this works</p>
<pre class=""lang-py prettyprint-override""><code>raw_image_dataset = tf.map_fn(fn = lambda x : tf.io.parse_tensor(x[0], tf.uint8, name=None), elems = raw_image_dataset, fn_output_signature = tf.TensorSpec((28,28),dtype=tf.uint8,    name=None), infer_shape = True)
    raw_image_dataset = tf.cast(raw_image_dataset, tf.int64)
    outputs[_IMAGE_KEY] = raw_image_dataset
</code></pre>
","18498366",0
1658,72274864,2,72101578,2022-05-17 13:24:24,1,"<p>If anyone gets this same problem, this solved it. I had to create a tf.train.Example() and set the data correctly</p>
<pre><code>example = tf.train.Example()
example_bytes = str.encode(input_data)
example.features.feature['utterance'].bytes_list.value.extend([example_bytes])
inputs = [
    httpclient.InferInput('examples', [1], &quot;BYTES&quot;),
]
inputs[0].set_data_from_numpy(np.asarray(example.SerializeToString()).reshape([1]), binary_data=False)
    
</code></pre>
","10408318",0
1659,72277021,2,72073763,2022-05-17 15:43:32,2,"<p>The custom component is defined as a Python function with a <code>@kfp.v2.dsl.component</code> decorator.</p>
<p>The <code>@component</code> decorator specifies three optional arguments: the base container image to use; any packages to install; and the yaml file to which to write the component specification.</p>
<p>The component function, <code>classif_model_eval_metrics</code>, has some input parameters.  The model parameter is an input <code>kfp.v2.dsl.Model artifact</code>.</p>
<p>The two function args, <code>metrics</code> and <code>metricsc</code>, are component Outputs, in this case of types Metrics and ClassificationMetrics. They’re not explicitly passed as inputs to the component step, but rather are automatically instantiated and can be used in the component.</p>
<pre><code>@component(
    base_image=&quot;gcr.io/deeplearning-platform-release/tf2-cpu.2-3:latest&quot;,
    output_component_file=&quot;tables_eval_component.yaml&quot;,
    packages_to_install=[&quot;google-cloud-aiplatform&quot;],
)
def classif_model_eval_metrics(
    project: str,
    location: str,  # &quot;us-central1&quot;,
    api_endpoint: str,  # &quot;us-central1-aiplatform.googleapis.com&quot;,
    thresholds_dict_str: str,
    model: Input[Model],
    metrics: Output[Metrics],
    metricsc: Output[ClassificationMetrics],
)
</code></pre>
<p>For example, in the function below, we’re calling <code>metricsc.log_roc_curve()</code> and <code>metricsc.log_confusion_matrix()</code> to render these visualizations in the Pipelines UI. These Output params become component outputs when the component is compiled, and can be consumed by other pipeline steps.</p>
<pre><code>def log_metrics(metrics_list, metricsc):
        ...
        metricsc.log_roc_curve(fpr, tpr, thresholds)
        ...
        metricsc.log_confusion_matrix(
            annotations,
            test_confusion_matrix[&quot;rows&quot;],
        )
</code></pre>
<p>For more information you can refer to this <a href=""https://cloud.google.com/blog/topics/developers-practitioners/use-vertex-pipelines-build-automl-classification-end-end-workflow"" rel=""nofollow noreferrer"">document</a>.</p>
","15747414",0
1660,72277293,2,72266041,2022-05-17 16:02:38,0,"<p>Turns out Sagemaker already decompress the <code>.tar.gz</code> file automatically.
So I can just read the folder exactly like before.</p>
","3669997",0
1661,72281612,2,72256288,2022-05-17 23:10:20,1,"<p><code>CreatePresignedDomainUrl</code> statement allows the role to launch a SageMaker Studio app (and hence the <code>domain-id/user-profile</code> ARN). Opening SageMaker notebook instance does not need the presigned domain url permission.</p>
<p>You'll need to make sure you're tagging the notebook with an OwnerRole key, with value = userid (not username). In addition, you'll need to use the <code>sagemaker:ResourceTag</code> (instead of <code>aws:ResourceTag</code>).</p>
<p>See the <a href=""https://docs.aws.amazon.com/service-authorization/latest/reference/list_amazonsagemaker.html"" rel=""nofollow noreferrer"">service authorization page</a> for a complete list of actions and condition keys.</p>
","2458691",1
1662,72321948,2,72319750,2022-05-20 16:15:17,1,"<p>A quick way to diagnose this would be using <a href=""https://aws.amazon.com/cloudtrail/"" rel=""nofollow noreferrer"">CloudTrail</a>. Go to the CloudTrail console in the shared account, choose &quot;Event History&quot; section on the left. You can then filter by &quot;Event name&quot;: &quot;StartNotebookInstance&quot; and you'll be able to see which identity made the call.</p>
<p>Afaik, no AWS service would automatically start a notebook instance.</p>
","2458691",1
1663,72389875,2,72376401,2022-05-26 09:48:08,0,"<p>I believe I managed to solve the problem - even though I encountered some serious issues. :)</p>
<ol>
<li>As described here <a href=""https://learn.microsoft.com/en-us/azure/machine-learning/how-to-deploy-advanced-entry-script"" rel=""nofollow noreferrer"">here</a> - I edited the <code>score.py</code> script:</li>
</ol>
<pre><code>import joblib
from azureml.core.model import Model
import numpy as np
import json
import pandas as pd
import numpy as np

from inference_schema.schema_decorators import input_schema, output_schema
from inference_schema.parameter_types.numpy_parameter_type import NumpyParameterType
from inference_schema.parameter_types.pandas_parameter_type import PandasParameterType
from inference_schema.parameter_types.standard_py_parameter_type import StandardPythonParameterType
    
data_sample = PandasParameterType(pd.DataFrame({'age': pd.Series([0], dtype='int64'),
                                                'sex': pd.Series(['example_value'], dtype='object'),
                                                'cp': pd.Series(['example_value'], dtype='object'),
                                                'trestbps': pd.Series([0], dtype='int64'),
                                                'chol': pd.Series([0], dtype='int64'),
                                                'fbs': pd.Series(['example_value'], dtype='object'),
                                                'restecg': pd.Series(['example_value'], dtype='object'),
                                                'thalach': pd.Series([0], dtype='int64'),
                                                'exang': pd.Series(['example_value'], dtype='object'),
                                                'oldpeak': pd.Series([0.0], dtype='float64'),
                                                'slope': pd.Series(['example_value'], dtype='object'),
                                                'ca': pd.Series(['example_value'], dtype='object'),
                                                'thal': pd.Series(['example_value'], dtype='object')}))

input_sample = StandardPythonParameterType({'data': data_sample})
result_sample = NumpyParameterType(np.array([0]))
output_sample = StandardPythonParameterType({'Results':result_sample})

def init():
    global model
    # Example when the model is a file
    model_path = Model.get_model_path('log') # logistic
    print('Model Path is  ', model_path)
    model = joblib.load(model_path)

@input_schema('Inputs', input_sample)
@output_schema(output_sample)
def run(Inputs):
    try:
        data = Inputs['data']
        result = model.predict_proba(data)
        return result.tolist()
    except Exception as e:
        error = str(e)
        return error
</code></pre>
<ol start=""2"">
<li>In the deployment step I adjusted the <code>CondaDependencies</code>:</li>
</ol>
<pre><code># to install required packages
env = Environment('env')
cd = CondaDependencies.create(pip_packages=['pandas==1.1.5', 'azureml-defaults','joblib==0.17.0', 'inference-schema==1.3.0'], conda_packages = ['scikit-learn==0.22.2.post1'])
env.python.conda_dependencies = cd
# Register environment to re-use later
env.register(workspace = ws)
print('Registered Environment')
</code></pre>
<p>as</p>
<p>a) It is necessary to include <code>inference-schema</code> in the <code>Dependencies</code> file
b) I downgraded <code>scikit-learn</code> to <code>scikit-learn==0.22.2.post1</code> version because of <a href=""https://github.com/hyperopt/hyperopt/issues/668"" rel=""nofollow noreferrer"">this issue</a></p>
<p>Now, when I feed the model with new data:</p>
<pre><code>new_data = {
  &quot;Inputs&quot;: {
    &quot;data&quot;: [
      {
        &quot;age&quot;: 71,
        &quot;sex&quot;: &quot;0&quot;,
        &quot;cp&quot;: &quot;0&quot;,
        &quot;trestbps&quot;: 112,
        &quot;chol&quot;: 203,
        &quot;fbs&quot;: &quot;0&quot;,
        &quot;restecg&quot;: &quot;1&quot;,
        &quot;thalach&quot;: 185,
        &quot;exang&quot;: &quot;0&quot;,
        &quot;oldpeak&quot;: 0.1,
        &quot;slope&quot;: &quot;2&quot;,
        &quot;ca&quot;: &quot;0&quot;,
        &quot;thal&quot;: &quot;2&quot;
      }
    ]
  }
}
</code></pre>
<p>And use it for prediction:</p>
<pre><code>import json
import requests
data = new_data
headers = {'Content-Type':'application/json'}
r = requests.post(url, str.encode(json.dumps(data)), headers = headers)
print(r.status_code)
print(r.json())
</code></pre>
<p>I get:</p>
<p><code>200 [[0.02325369841858338, 0.9767463015814166]]</code></p>
<p>Uff! Maybe someone will benefit from my painful learning path! :)</p>
","9088176",0
1664,72400539,2,72399408,2022-05-27 04:42:21,1,"<blockquote>
<p>Unfortunately, Azure Resource Graph Explorer doesn't provide any query
to get any compute related information from both, Azure Machine
Learning and Databricks.</p>
</blockquote>
<p>Though Azure Resource Graph Explorer supports join functionality, allowing for more advanced exploration of your Azure environment by enabling you to correlate between resources and their properties. But these services only applicable on few Azure resources like VM, storage account, Cosmos DB, SQL databases, Network Security Groups, public IP addresses, etc.</p>
<p><strong>Hence, there is no such Kusto query available in Azure Resource Graph Explorer which can list compute instance size of Machine Learning service and Databricks.</strong></p>
<p><strong>Workarounds</strong></p>
<p>Machine Learning Service</p>
<p>For machine learning service you can manage the compute instance directly from ML service by using Python SDK. Refer <a href=""https://learn.microsoft.com/en-us/azure/machine-learning/how-to-create-manage-compute-instance?tabs=python#manage"" rel=""nofollow noreferrer"">Python SDK azureml v1</a> to know more.</p>
<p>Azure Databricks</p>
<p>Cluster is the computational resource in Databricks. You can <strong>filter the cluster list</strong> from Databricks UI and manage the same. Features like cluster configuration, cluster cloning, access control, etc. are available which you can used based on your requirement. For more details, please check <a href=""https://learn.microsoft.com/en-us/azure/databricks/clusters/clusters-manage#filter-cluster-list"" rel=""nofollow noreferrer"">here</a>.</p>
","15969041",0
1665,72409316,2,72408785,2022-05-27 17:51:48,2,"<p>MLRun has several different ways to run a piece of code. At this time, the following runtimes are supported:</p>
<ul>
<li>Batch runtimes
<ul>
<li><strong>local</strong> - execute a Python or shell program in your local environment (i.e. Jupyter, IDE, etc.)</li>
<li><strong>job</strong> - run the code in a Kubernetes Pod</li>
<li><strong>dask</strong> - run the code as a Dask Distributed job (over Kubernetes)</li>
<li><strong>mpijob</strong> - run distributed jobs and Horovod over the MPI job operator, used mainly for deep learning jobs</li>
<li><strong>spark</strong> - run the job as a Spark job (using Spark Kubernetes Operator)</li>
<li><strong>remote-spark</strong> - run the job on a remote Spark service/cluster (e.g. Iguazio Spark service)</li>
</ul>
</li>
<li>Real-time runtimes
<ul>
<li><strong>nuclio</strong> - real-time serverless functions over Nuclio</li>
<li><strong>serving</strong> - higher level real-time Graph (DAG) over one or more Nuclio functions</li>
</ul>
</li>
</ul>
<p>If you are interested in learning more about each runtime, see the <a href=""https://docs.mlrun.org/en/latest/concepts/functions-overview.html"" rel=""nofollow noreferrer"">documentation</a>.</p>
","16359976",0
1666,72409920,2,72408377,2022-05-27 18:53:13,5,"<p>In the Iguazio, when you create a Dask cluster, you don't need to worry about lower-level <code>dask_kubernetes</code> related stuff.
You just need to specify the min and max number of workers like below</p>
<pre><code># create an mlrun function which will init the dask cluster
dask_cluster_name = &quot;dask-cluster&quot;
dask_cluster = mlrun.new_function(dask_cluster_name, kind='dask', image='mlrun/ml-models')
dask_cluster.apply(mlrun.mount_v3io())

# set range for # of replicas with replicas and max_replicas
dask_cluster.spec.min_replicas = 1
dask_cluster.spec.max_replicas = 100
</code></pre>
<p>Depending on your workload, the cluster will scale up and down between the min and max number of workers. We bake in the adaptive deployments of the Dask cluster so it results in both faster analyses that give users much more power, but with much less pressure on computational resources.</p>
","8350820",1
1667,72418673,2,72392070,2022-05-28 19:27:05,1,"<p>The concurrency settings for TorchServe DLC are controlled by such mechanisms as # of workers, which can be set by defining the appropriate variables, such as <code>SAGEMAKER_TS_*</code>, and <code>SAGEMAKER_MODEL_*</code> (see, e.g., <a href=""https://github.com/pytorch/serve/blob/master/docs/configuration.md"" rel=""nofollow noreferrer"">this page</a> for details on their meaning and implications).</p>
<p>While the latter are agnostic to any particular serving stack and are defined in the <a href=""https://github.com/aws/sagemaker-inference-toolkit"" rel=""nofollow noreferrer"">SageMaker Inference Toolkit</a>, the former are TorchServe-specific and are defined in <a href=""https://github.com/aws/sagemaker-pytorch-inference-toolkit"" rel=""nofollow noreferrer"">TorchServe Inference Toolkit</a>. Moreover, since the TorchServe Inference Toolkit is built on top of the SageMaker Inference Toolkit, there is a non-trivial interplay between these two sets of params.</p>
<p>Thus you may also want to experiment with such params as, e.g., <code>SAGEMAKER_MODEL_SERVER_WORKERS</code> to properly set up the concurrency setting of the SageMaker Async Endpoint.</p>
","8468326",1
1668,72432182,2,72431938,2022-05-30 09:38:56,0,"<p>Youre logging a single value into log_metrics and i dont think thats correct based on the implementation of log_metric and log_metrics in the documentation:</p>
<p><a href=""https://www.mlflow.org/docs/latest/python_api/mlflow.html#mlflow.log_metric"" rel=""nofollow noreferrer"">https://www.mlflow.org/docs/latest/python_api/mlflow.html#mlflow.log_metric</a> and
<a href=""https://www.mlflow.org/docs/latest/python_api/mlflow.html#mlflow.log_metrics"" rel=""nofollow noreferrer"">https://www.mlflow.org/docs/latest/python_api/mlflow.html#mlflow.log_metrics</a></p>
<p>So i would suggest to maybe change the &quot;log_metrics&quot; to &quot;log_metric&quot; and leave the tolist out</p>
","5625096",0
1669,72454566,2,72454565,2022-05-31 22:09:34,0,"<p>Yes, this is possible. A common development pattern with the Iguazio platform is to utilize a local version of MLRun and Nuclio on a laptop/workstation and move/execute jobs on the cluster at a later point.</p>
<p>There are two main options for installing MLRun and Nuclio on a local environment:</p>
<ul>
<li><code>docker-compose</code> - Simpler and easier to get up and running, however restricted to running jobs within the environment it was executed in (i.e. Jupyter or IDE). This means you cannot specify resources like CPU/MEM/GPU to run a particular job. This approach is great for quickly getting up and running. Instructions can be found <a href=""https://docs.mlrun.org/en/latest/install/local-docker.html"" rel=""nofollow noreferrer"">here</a>.</li>
<li><code>Kubernetes</code> - More complex to get up and running, but allows for running jobs in their own containers with specified CPU/MEM/GPU resources. This approach is a better for better emulating capabilities of the Iguazio platform in a local environment. Instructions can be found <a href=""https://docs.mlrun.org/en/latest/install/kubernetes.html"" rel=""nofollow noreferrer"">here</a>.</li>
</ul>
<p>Once you have installed MLRun and Nuclio using one of the above options and have created a job/function you can test it locally as well as deploy to the Iguazio cluster directly from your local development environment:</p>
<ul>
<li>To run your job locally, utilize the <code>local=True</code> flag when specifying your MLRun function like in the <a href=""https://docs.mlrun.org/en/latest/tutorial/01-mlrun-basics.html#running-the-mlrun-function-locally"" rel=""nofollow noreferrer"">Quick-Start guide</a>.</li>
<li>To run your job remotely, specify the required environment files to allow connectivity to the Iguazio cluster as specified in this <a href=""https://docs.mlrun.org/en/latest/install/remote.html#load-the-configuration-and-credential-environmental-variables-from-file"" rel=""nofollow noreferrer"">guide</a>, and run your job with <code>local=False</code></li>
</ul>
","16359976",0
1670,72458838,2,72448994,2022-06-01 08:44:03,1,"<p>You're probably referring to <a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/object-detection.html"" rel=""nofollow noreferrer"">this</a> object detection algorithm which is part of of <a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/algos.html"" rel=""nofollow noreferrer"">Amazon SageMaker built-in algorithms</a>. <strong>Built-in algorithms must be trained on the cloud</strong>.<br />
If you're bringing your own Tensorflow or PyTorch model, you could use SageMaker training jobs to train either on the cloud or locally as @kirit noted.</p>
<p>I would also look at <a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/studio-jumpstart.html"" rel=""nofollow noreferrer"">SageMaker JumpStart</a> for a wide variety of object detection algorithm which are TF/PT based.</p>
","121956",0
1671,72469296,2,72419908,2022-06-02 00:17:48,1,"<p><code>json.dumps</code> will not convert your array to a dict structure and serialize it to a JSON String.</p>
<p>What data type is <code>prediction</code> ? Have you tested making sure <code>prediction</code> is a dict?</p>
<p>You can confirm the data type by adding <code>print(type(prediction))</code> to see the data type in the CloudWatch Logs.</p>
<p>If prediction is a <code>list</code> you can test the following:</p>
<pre><code>def output_fn(prediction, accept):
    if accept == &quot;application/json&quot;:

        my_dict = {'output': prediction}
        return json.dumps(my_dict), mimetype=accept)

    raise RuntimeException(&quot;{} accept type is not supported by this script.&quot;.format(accept))
</code></pre>
<p><code>DataProcessing</code> and <code>JoinSource</code> are used to associate the data that is relevant to the prediction results in the output. It is not meant to be used to match the input and output format.</p>
","9796588",2
1672,72479832,2,72479831,2022-06-02 17:02:19,0,"<p>There are two ways of using Spark in Iguazio:</p>
<ol>
<li>Create a standalone Spark cluster via the Iguazio UI (like you found on the services page). This is a persistent cluster that you can associate with multiple jobs, Jupyter notebooks, etc. This is a good choice for long running computations with a static pool of resources. An overview of the Spark service in Iguazio can be found <a href=""https://www.iguazio.com/docs/latest-release/services/app-services/spark/"" rel=""nofollow noreferrer"">here</a> along with some ingestion <a href=""https://www.iguazio.com/docs/latest-release/data-layer/spark-data-ingestion-qs/"" rel=""nofollow noreferrer"">examples</a>.
<ul>
<li>When creating a JupyterLab instance in the UI, there is an option to associate it with an existing Spark cluster. This lets you use PySpark out of the box</li>
</ul>
</li>
<li>Create an ephemeral Spark cluster via the Spark Operator. This is a temporary cluster that only exists for the duration of the job. This is a good choice for shorter one-off jobs with a static or variable pool of resources. The Spark Operator runtime is usually the better option if you don't need a persistent Spark cluster. Some examples of using the Spark operator on Iguazio can be found <a href=""https://docs.mlrun.org/en/latest/runtimes/spark-operator.html"" rel=""nofollow noreferrer"">here</a> as well as below.</li>
</ol>
<pre class=""lang-py prettyprint-override""><code>import mlrun
import os

# set up new spark function with spark operator
# command will use our spark code which needs to be located on our file system
# the name param can have only non capital letters (k8s convention)
sj = mlrun.new_function(kind='spark', command='spark_read_csv.py', name='sparkreadcsv') 

# set spark driver config (gpu_type &amp; gpus=&lt;number_of_gpus&gt;  supported too)
sj.with_driver_limits(cpu=&quot;1300m&quot;)
sj.with_driver_requests(cpu=1, mem=&quot;512m&quot;) 

# set spark executor config (gpu_type &amp; gpus=&lt;number_of_gpus&gt; are supported too)
sj.with_executor_limits(cpu=&quot;1400m&quot;)
sj.with_executor_requests(cpu=1, mem=&quot;512m&quot;)

# adds fuse, daemon &amp; iguazio's jars support
sj.with_igz_spark() 

# set spark driver volume mount
# sj.function.with_driver_host_path_volume(&quot;/host/path&quot;, &quot;/mount/path&quot;)

# set spark executor volume mount
# sj.function.with_executor_host_path_volume(&quot;/host/path&quot;, &quot;/mount/path&quot;)

# args are also supported
sj.spec.args = ['-spark.eventLog.enabled','true']

# add python module
sj.spec.build.commands = ['pip install matplotlib']

# Number of executors
sj.spec.replicas = 2 

# Rebuilds the image with MLRun - needed in order to support artifactlogging etc
sj.deploy()

# Run task while setting the artifact path on which our run artifact (in any) will be saved
sj.run(artifact_path='/User')
</code></pre>
<p>Where the <code>spark_read_csv.py</code> file looks like:</p>
<pre class=""lang-py prettyprint-override""><code>from pyspark.sql import SparkSession
from mlrun import get_or_create_ctx

context = get_or_create_ctx(&quot;spark-function&quot;)

# build spark session
spark = SparkSession.builder.appName(&quot;Spark job&quot;).getOrCreate()

# read csv
df = spark.read.load('iris.csv', format=&quot;csv&quot;,
                     sep=&quot;,&quot;, header=&quot;true&quot;)

# sample for logging
df_to_log = df.describe().toPandas()

# log final report
context.log_dataset(&quot;df_sample&quot;,
                     df=df_to_log,
                     format=&quot;csv&quot;)
spark.stop()
</code></pre>
","16359976",0
1673,72481059,2,72463927,2022-06-02 18:53:50,3,"<p>You need to deploy the default image to the cluster docker registry. There is one image for remote spark and one image for spark operator. Those images contain all the necessary dependencies for remote Spark and Spark Operator.</p>
<p>See the code below.</p>
<pre><code># This section has to be invoked once per MLRun/Iguazio upgrade
from mlrun.runtimes import RemoteSparkRuntime
RemoteSparkRuntime.deploy_default_image()
from mlrun.runtimes import Spark3Runtime
Spark3Runtime.deploy_default_image()
</code></pre>
<p>Once these images are deployed (to the cluster docker registry), your function with function spec <code>“use_default_image” = True</code> will be able to pull the image and deploy.</p>
","8350820",1
1674,72486796,2,72473641,2022-06-03 08:28:54,0,"<p>I solved my problem. It is necessary, that all files (executable scripts, 'dvc.yaml', 'params.yaml') be tracked by git. In this case <code>dvc exp run</code> command works correctly.</p>
","16207818",0
1675,72487285,2,72473317,2022-06-03 09:09:10,4,"<p>After searching through the internet and combining a few concepts, I was able to solve the problem that I had asked. In Keras, we can create custom callbacks that can be called at various points (start/end of epoch, batch, etc) during training, testing, and prediction phase of a model.</p>
<p>So, I created a Keras custom callback to store loss/accuracy values after each epoch as mlflow metrics like below.</p>
<pre><code>class CustomCallback(keras.callbacks.Callback):
    def on_epoch_end(self, epoch, logs=None):
        mlflow.log_metrics({
           &quot;loss&quot;: logs[&quot;loss&quot;],
           &quot;sparse_categorical_accuracy&quot;:
               logs[&quot;sparse_categorical_accuracy&quot;],
           &quot;val_loss&quot;: logs[&quot;val_loss&quot;],
           &quot;val_sparse_categorical_accuracy&quot;:
               logs[&quot;val_sparse_categorical_accuracy&quot;],
    })
</code></pre>
<p>I called this above callback during training of my model like below.</p>
<pre><code>history = model.fit(
    features_train,
    labels_train,
    batch_size=BATCH_SIZE,
    epochs=EPOCHS,
    callbacks=[CustomCallback()],
    validation_split=0.2
)
</code></pre>
<p>The keras custom callback stored all the values during training after each epoch which I was able to see as a graph in mlflow UI like below.
<a href=""https://i.stack.imgur.com/wWzYV.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/wWzYV.png"" alt=""loss and val_loss graph"" /></a></p>
","6812618",0
1676,72489362,2,72474007,2022-06-03 12:08:40,1,"<p>Got to know that KServe is not comaptible with Kubeflow version 1.4 and works for Kubeflow version &gt;=1.5.</p>
<p>Switching to kfserving 0.6 resolved my issue.</p>
","19129542",0
1677,72491717,2,72490682,2022-06-03 15:14:54,1,"<p>Since the documentation is lacking a bit of clarity, in order to have this work as in the example, you would first have to create the Service Catalog product in Terraform as well, e.g.:</p>
<pre><code>resource &quot;aws_servicecatalog_product&quot; &quot;example&quot; {
  name  = &quot;example&quot;
  owner = [aws_security_group.example.id] # &lt;---- This would need to be created first
  type  = aws_subnet.main.id # &lt;---- This would need to be created first

  provisioning_artifact_parameters {
    template_url = &quot;https://s3.amazonaws.com/cf-templates-ozkq9d3hgiq2-us-east-1/temp1.json&quot;
  }

  tags = {
    foo = &quot;bar&quot;
  }
}
</code></pre>
<p>You can reference it then in the SageMaker project the same way as in the example:</p>
<pre><code>resource &quot;aws_sagemaker_project&quot; &quot;example&quot; {
  project_name = &quot;example&quot;

  service_catalog_provisioning_details {
    product_id = aws_servicecatalog_product.example.id
  }
}
</code></pre>
<p>Each of the resources that gets created has a set of attributes that can be accessed as needed by other resources, data sources or outputs. In order to understand how this works, I strongly suggest reading the documentation about referencing values [1]. Since you already created the Service Catalog product, the only thing you need to do is provide the string value for the product ID:</p>
<pre><code>resource &quot;aws_sagemaker_project&quot; &quot;example&quot; {
  project_name = &quot;example&quot;

  service_catalog_provisioning_details {
    product_id = &quot;prod-xxxxxxxxxxxxx&quot;
  }
}
</code></pre>
<p>When I can't understand what value is expected by an argument (e.g., <code>product_id</code> in this case), I usually read the docs and look for examples like in [2]. Note: That example is CloudFormation, but it can help you understand what type of a value is expected (e.g., string, number, bool).</p>
<p>You could also import the created Service Catalog product into Terraform so you can manage it with IaC [3]. You should understand all the implications of <code>terraform import</code> though before trying it [4].</p>
<hr />
<p>[1] <a href=""https://www.terraform.io/language/expressions/references"" rel=""nofollow noreferrer"">https://www.terraform.io/language/expressions/references</a></p>
<p>[2] <a href=""https://docs.amazonaws.cn/en_us/AWSCloudFormation/latest/UserGuide/aws-resource-sagemaker-project.html#aws-resource-sagemaker-project--examples--SageMaker_Project_Example"" rel=""nofollow noreferrer"">https://docs.amazonaws.cn/en_us/AWSCloudFormation/latest/UserGuide/aws-resource-sagemaker-project.html#aws-resource-sagemaker-project--examples--SageMaker_Project_Example</a></p>
<p>[3] <a href=""https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/servicecatalog_product#import"" rel=""nofollow noreferrer"">https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/servicecatalog_product#import</a></p>
<p>[4] <a href=""https://www.terraform.io/cli/commands/import"" rel=""nofollow noreferrer"">https://www.terraform.io/cli/commands/import</a></p>
","8343484",0
1678,72492007,2,72491505,2022-06-03 15:38:44,1,"<p>Asynchronous inference could be a good option for this use case. There is a blog published by AWS that talks about how you can do this.</p>
<p><a href=""https://aws.amazon.com/blogs/machine-learning/run-computer-vision-inference-on-large-videos-with-amazon-sagemaker-asynchronous-endpoints/"" rel=""nofollow noreferrer"">https://aws.amazon.com/blogs/machine-learning/run-computer-vision-inference-on-large-videos-with-amazon-sagemaker-asynchronous-endpoints/</a></p>
","16247336",0
1679,72492930,2,72478572,2022-06-03 17:01:26,1,"<p>see this - <a href=""https://stackoverflow.com/questions/68750375/no-matching-distribution-found-for-pandas-1-3-1"">No matching distribution found for pandas==1.3.1</a></p>
<p>The latest version to support python 3.6 is 1.1.5.</p>
<p>You can create a new conda environment with python version &gt;= 3.7 in your existing notebook, or move to notebooks with Amazon Linux 2 (see <a href=""https://aws.amazon.com/blogs/machine-learning/amazon-sagemaker-notebook-instance-now-supports-amazon-linux-2/"" rel=""nofollow noreferrer"">blog post</a>). In the AL2 notebooks, <code>conda_python3</code> kernels come with Python 3.8.</p>
","2458691",0
1680,72550911,2,72376872,2022-06-08 19:01:50,1,"<p><code>your_xgboost_abalone_script.py</code> can be created locally. The path you provide is relative to where the code is running.</p>
<p>I.e. <code>your_xgboost_abalone_script.py</code> can be located in the same directory where you are running the SageMaker SDK (&quot;source code&quot;).</p>
<p>For example if you have <code>your_xgboost_abalone_script.py</code> in the same directory as the source code:</p>
<pre><code>.
├── source_code.py
└── your_xgboost_abalone_script.py
</code></pre>
<p>Then you can point to this file exactly how the documentation depicts:</p>
<pre><code>estimator = XGBoost(entry_point = &quot;your_xgboost_abalone_script.py&quot;, 
.
.
.
)
</code></pre>
<p>The SDK will take <code>your_xgboost_abalone_script.py</code> repackage it into a model tar ball and upload it to S3 on your behalf.</p>
","9796588",1
1681,72565146,2,72538301,2022-06-09 18:40:55,0,"<p>Do you have a SageMaker real-time endpoint created? One route is using an AWS SDK (Ex: boto3 for Python) and coupling it with a Lambda function to invoke that endpoint. The Lambda function can grab data from S3 or Athena or whatever data source using the SDK and then invoke that endpoint.</p>
","16504640",1
1682,72576799,2,72551630,2022-06-10 15:37:00,2,"<p>After running <code>dvc repro</code> in <code>test-service</code>, a new <code>dvc.lock</code> will be created, containing the file hashes relative to your pipeline (i.e. the hash for <code>models/model.pkl</code> etc).</p>
<p>If you're running a shared cache, <code>inference-service</code> should have access to the updated <code>dvc.lock</code>. If that is present, it will be sufficient to run <code>dvc checkout</code> to populate the workspace with the files corresponding to the hashes in the shared cache.</p>
","19001541",3
1683,72580645,2,72580594,2022-06-10 23:28:53,1,"<p>You are just missing the pyyaml module</p>
<p>Install it by:</p>
<pre><code>pip install pyyaml
</code></pre>
","15341276",1
1684,72597768,2,72425907,2022-06-13 04:29:29,2,"<p>The <code>tag_constants</code> is in <code>tf.compat.v1.saved_model</code>.</p>
<p>To resolve the error replace this line</p>
<pre><code>tag=[tf.saved_model.tag_constants.SERVING]
</code></pre>
<p>with this</p>
<pre><code>tag=[tf.compat.v1.saved_model.tag_constants.SERVING]
</code></pre>
<p>Please refer <a href=""https://www.tensorflow.org/api_docs/python/tf/compat/v1/saved_model/tag_constants"" rel=""nofollow noreferrer"">this</a> for more details.</p>
","user11530462",0
1685,72606145,2,72480662,2022-06-13 16:29:59,3,"<p>Inside the Iguazio Dashboard, navigate to the job summary of the job you want to reproduce.
In the right corner, there is a menu that has a Rerun option. See the screenshot below:
<a href=""https://i.stack.imgur.com/2qjU4.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/2qjU4.png"" alt=""enter image description here"" /></a>
When you select that, by default, it keeps the same task config (params, inputs, mounts, etc), but you can also modify them as well. This is the easiest and fastest way to reproduce an experiment, but it can also be done programmatically using the SDK.</p>
","8350820",1
1686,72613045,2,72604450,2022-06-14 07:29:05,1,"<p>I found the problem. Like I wrote in the EDIT1 of my post, with further observations, the parquet files were missing in the docker container. That was strange because I was copying the entire folder in my Dockerfile.</p>
<p>I then realized that I was hitting this problem <a href=""https://github.com/moby/buildkit/issues/1366"" rel=""nofollow noreferrer"">mentioned here</a>. File paths exceeding 260 characters, silently fail and do not get copied over to the docker container. This was really frustrating because nothing failed during build and then during run, it gave me that cryptic error of &quot;unable to infer schema for parquet&quot;, essentially because the parquet files were not copied over during docker build.</p>
","4772836",0
1687,72639737,2,72637756,2022-06-16 02:15:08,1,"<p>By checking AKS webservice class, we can do the multiple services links to single AKS cluster. The endpoint management was described in <a href=""https://learn.microsoft.com/en-us/python/api/azureml-core/azureml.core.webservice.aks.akswebservice?view=azure-ml-py"" rel=""nofollow noreferrer"">document</a>, but this is representing one-to-one cluster and service. For multiple workspaces refer <a href=""https://learn.microsoft.com/en-us/python/api/azureml-core/azureml.core.webservice.aksendpoint?view=azure-ml-py#azureml-core-webservice-aksendpoint-create-version"" rel=""nofollow noreferrer"">document</a>.</p>
<p>Regarding <strong><code>azureml-fe</code></strong>. There will be one <strong>azureml-fe</strong> for one cluster. That means, when we are using different workspaces for deployment into one AKS, then only <strong>one</strong> <strong>azureml-fe</strong> and can be considered to take <strong>one certificate.</strong></p>
<p><a href=""https://learn.microsoft.com/en-us/azure/machine-learning/v1/how-to-deploy-azure-kubernetes-service?tabs=python#azure-ml-router"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/machine-learning/v1/how-to-deploy-azure-kubernetes-service?tabs=python#azure-ml-router</a></p>
","18428148",1
1688,72640097,2,72622280,2022-06-16 03:22:11,0,"<p>Assuming bar/ is the dataset directory you're incrementally adding to, you can instead</p>
<pre><code>dvc add bar
</code></pre>
<p>This creates a bar.dvc file and writes to .gitignore at the top level.</p>
<p>When you update content in bar/, <code>dvc add</code> it again or use <code>dvc commit</code> to register the new dataset version. The new files get added to the project cache and the .dvc file gets an updated <code>md5</code> hash that identifies to the latest directory structure.</p>
<p>Some docs:<br />
<a href=""https://dvc.org/doc/start/data-management#making-changes"" rel=""nofollow noreferrer"">https://dvc.org/doc/start/data-management#making-changes</a><br />
<a href=""https://dvc.org/doc/command-reference/add"" rel=""nofollow noreferrer"">https://dvc.org/doc/command-reference/add</a><br />
<a href=""https://dvc.org/doc/user-guide/project-structure/internal-files#structure-of-the-cache-directory"" rel=""nofollow noreferrer"">https://dvc.org/doc/user-guide/project-structure/internal-files#structure-of-the-cache-directory</a></p>
","761963",1
1689,72645302,2,72633246,2022-06-16 11:44:57,0,"<p>Try to run the same training script outside of a SageMaker training job and see what happens.<br />
If the error doesn't happen on a standalone script, try to run it as a <a href=""https://aws.amazon.com/blogs/machine-learning/use-the-amazon-sagemaker-local-mode-to-train-on-your-notebook-instance/"" rel=""nofollow noreferrer"">Local SageMaker training job</a>, so you can reproduce it in seconds instead of minutes, and potentially use a debugger to figure out what is the problem.</p>
","121956",0
1690,72653489,2,72641789,2022-06-17 01:37:08,0,"<p>Issue was resolved. I did not change anything in the service principal and running it on second day using same yml got through the issue. I guess there might be some propagation issue, but longer than usual.</p>
","12051503",0
1691,72653569,2,65664382,2022-06-17 01:52:44,1,"<p>I experienced this same issue. To resolve you just need to request a limit increase via <a href=""https://us-east-1.console.aws.amazon.com/support/home#/case/create?issueType=service-limit-increase"" rel=""nofollow noreferrer"">https://us-east-1.console.aws.amazon.com/support/home#/case/create?issueType=service-limit-increase</a></p>
<ul>
<li><strong>Limit type</strong>: SageMaker Studio</li>
<li><strong>Limit</strong>: Max User Profiles Per Account</li>
<li><strong>New Limit</strong>: 1 (or more)</li>
</ul>
","758542",0
1692,72655193,2,72094768,2022-06-17 06:32:21,2,"<p>Each pipeline run is automatically logged to Google Logging, and so are also the failed pipeline runs.
The error logs also contain information about the pipeline and the component that failed.</p>
<p>We can use this information to monitor our logs and set up an alert via email for example.</p>
<p>The logs for our Vertex AI Pipeline runs we get with the following filter</p>
<p>resource.type=”aiplatform.googleapis.com/PipelineJob”
severity=(ERROR OR CRITICAL OR ALERT OR EMERGENCY)</p>
<p><a href=""https://i.stack.imgur.com/e4jFR.png"" rel=""nofollow noreferrer"">Vertex AI Pipeline Logs</a></p>
<p>Based on those logs you can set up log-based alerts <a href=""https://cloud.google.com/logging/docs/alerting/log-based-alerts"" rel=""nofollow noreferrer"">https://cloud.google.com/logging/docs/alerting/log-based-alerts</a>. Notifications via email, Slack, SMS, and many more are possible.</p>
<p>source:
<a href=""https://medium.com/google-cloud/google-vertex-ai-the-easiest-way-to-run-ml-pipelines-3a41c5ed153"" rel=""nofollow noreferrer"">https://medium.com/google-cloud/google-vertex-ai-the-easiest-way-to-run-ml-pipelines-3a41c5ed153</a></p>
","19356695",0
1693,72657564,2,72097417,2022-06-17 09:56:35,3,"<p>I fixed this with</p>
<pre class=""lang-bash prettyprint-override""><code># Note: add sudo if needed:
ln -fs /lib/x86_64-linux-gnu/libncursesw.so.6 /opt/conda/lib/libncursesw.so.6
</code></pre>
","511069",1
1694,72657672,2,72411618,2022-06-17 10:05:21,3,"<p>You should use psycopg2 instead, e.g.:</p>
<p><code>postgresql+psycopg2://&lt;username&gt;:&lt;password&gt;@/&lt;dbname&gt;?host=/cloudsql/&lt;my-project&gt;:&lt;us-central1&gt;:&lt;dbinstance&gt;</code></p>
<p>It works for me, with versions:</p>
<p>mlflow==1.26.1</p>
<p>psycopg2-binary==2.9.3</p>
","3525017",3
1695,72661168,2,72648719,2022-06-17 14:46:48,3,"<p>A job stuck in this status is usually a Kubernetes issue. The reason there is no logs in the Iguazio dashboard for the job is because the pod never started, which is where the logs come from. You can navigate to the web shell / Jupyter service in Iguazio and use kubectl commands to find out what is going on in Kubernetes. Usually, I see this when there is an issue with the docker image for the pod, it either can’t be found or has bugs.</p>
<p>In a terminal: doing <code>kubectl get pods</code> and find your pod. It usually has <code>ImagePullBackOff</code>, or <code>CrashLoopBackOff</code> or some similar error. Check the docker image which is usually the culprit. You can kill the pod in Kubernetes, which in turn will error the job out. You can also “abort” the job from the menu in the dashboard under that specific job.</p>
","8350820",0
1696,72666514,2,72651603,2022-06-18 03:39:31,2,"<p>Stage 3 is manual so you can't really codify it or automate it, nor guarantee its reproducibility (due to possible human error). But there's a way to get you as close as possible:</p>
<p>You could replace it with a helper script that just checks whether all the labels are annotated. If so, output a text file with content &quot;green&quot;, otherwise &quot;red&quot; (for example) and error out.</p>
<p>Stage 4 should depend on both the inputs from stages 2 and 3, so it will only run if BOTH the face crops changed AND if they are thoroughly annotated.
Internally, it first checks the semaphore file (from 3) and dies on red. On green, it trains the model :)</p>
<p>The <a href=""https://dvc.org/doc/command-reference/dag#directed-acyclic-graph"" rel=""nofollow noreferrer"">DAG</a> looks like this:</p>
<pre><code>          +-----------+       
          | 1-acquire |       
          +-----------+       
                *          
                *          
                *          
          +---------+       
          | 2-xform |       
          +---------+       
 you      **        **     
   --&gt;  **            **   
       *                ** 
+---------+               *
| 3-check |             ** 
+---------+           **   
          **        **     
            **    **       
              *  *         
          +---------+      
          | 4-train |      
          +---------+      
</code></pre>
<blockquote>
<p>re randomness: while not ideal, non-determinism technically only <a href=""https://dvc.org/doc/command-reference/run#avoiding-unexpected-behavior"" rel=""nofollow noreferrer"">affects intermediate stages</a> of the pipeline, because it causes everything after that to always run. In this case, since it's in the last stage, it won't affect DVC's job.</p>
</blockquote>
","761963",0
1697,72666551,2,72665109,2022-06-18 03:51:17,3,"<p>Stage <code>load_extract_save</code> both outputs and depends on the same path (<code>artifacts/data</code>). That's a cycle.</p>
<p>Pipeline structures should be <a href=""https://dvc.org/doc/command-reference/dag#directed-acyclic-graph"" rel=""nofollow noreferrer"">directed <strong>acyclical</strong> graphs</a>, otherwise <code>dvc repro</code> could execute that stage over and over forever.</p>
","761963",0
1698,72681014,2,72659937,2022-06-19 23:45:28,0,"<p>The following workflow is the official practice to be followed to achieve the task required.</p>
<ol>
<li>Starting with the architecture mentioned below</li>
</ol>
<p><a href=""https://i.stack.imgur.com/KdRUa.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/KdRUa.png"" alt=""enter image description here"" /></a></p>
<ul>
<li>we need to have a specific data store to handle the dataset</li>
<li>Perform the regular code modifications using the IDE like Jupyter Notebook or VS Code</li>
<li>Train and test the model</li>
<li>To register and operate on the model, deploy the model image as a web service and operate the rest.</li>
</ul>
<ol start=""2"">
<li><strong>Configure the CI Pipeline:</strong></li>
</ol>
<ul>
<li><p>Follow the below steps to complete the procedure</p>
<p><strong>Before implementation:</strong></p>
<pre><code>- We need azure subscription enabled account
- DevOps activation must be activated.
</code></pre>
</li>
<li><p>Open DevOps portal with enabled SSO</p>
</li>
<li><p>Navigate to <strong>Pipeline -&gt; Builds -&gt; Choose the model which was created -&gt; Click on EDIT</strong>
<a href=""https://i.stack.imgur.com/yUVZl.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/yUVZl.png"" alt=""enter image description here"" /></a></p>
</li>
<li><p>Build pipeline will be looking like below screen
<a href=""https://i.stack.imgur.com/VSKJq.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/VSKJq.png"" alt=""enter image description here"" /></a></p>
</li>
<li><p>We need to use Anaconda distribution for this example to get all the dependencies.</p>
</li>
<li><p>To install environment dependencies, check the <a href=""https://learn.microsoft.com/en-us/azure/devops/pipelines/tasks/package/conda-environment?view=azure-devops&amp;viewFallbackFrom=azdevops"" rel=""nofollow noreferrer"">link</a></p>
</li>
<li><p>Use the python environment, under <strong>Install Requirements</strong> in user setup.</p>
</li>
<li><p>Select <strong>create or get workspace</strong> select your account subscription as mentioned in below screen</p>
</li>
</ul>
<p><a href=""https://i.stack.imgur.com/vt0el.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/vt0el.png"" alt=""enter image description here"" /></a></p>
<ul>
<li>Save the changes happened in other tasks and all those muse be in same subscription.
<a href=""https://i.stack.imgur.com/WJxCL.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/WJxCL.png"" alt=""enter image description here"" /></a></li>
</ul>
<p>The entire CI/CD procedure and solution was documented in <a href=""https://www.azuredevopslabs.com/labs/vstsextend/aml/#author-praneet-singh-solanki"" rel=""nofollow noreferrer"">link</a></p>
<p><strong>Document Credit: Praneet Singh Solanki</strong></p>
","18428148",1
1699,72695538,2,72695360,2022-06-21 05:01:52,1,"<blockquote>
<p>If so... how should I decide for wich packages use conda and for wich use pip?</p>
</blockquote>
<p>According to <a href=""https://learn.microsoft.com/en-us/python/api/azureml-core/azureml.core.conda_dependencies.condadependencies?view=azure-ml-py#remarks"" rel=""nofollow noreferrer"">documentation</a>:</p>
<blockquote>
<p>If your dependency is available through both Conda and pip (from PyPi), use the Conda version, as Conda packages typically come with pre-built binaries that make installation more reliable.</p>
</blockquote>
<p>For example:</p>
<pre><code>from azureml.core.environment import CondaDependencies

   myenv = Environment(name=&quot;myenv&quot;)
   conda_dep = CondaDependencies()
   conda_dep.add_conda_package(&quot;scikit-learn&quot;)
</code></pre>
<p>References: <a href=""https://www.anaconda.com/blog/understanding-conda-and-pip"" rel=""nofollow noreferrer"">Understanding Conda and Pip</a>, <a href=""https://stackoverflow.com/questions/64979752/unable-to-access-python-packages-installed-in-azure-ml"">Unable to access python packages installed in Azure ML</a> and <a href=""https://stackoverflow.com/questions/67639665/azure-ml-not-able-to-create-conda-environment-exit-code-15"">Azure ML not able to create conda environment</a></p>
","9303470",0
1700,72706723,2,72663159,2022-06-21 20:23:35,1,"<p>Got the answer from Quicksight Community. Pasting it here.</p>
<p>For segmentation, you can use the calculated field which you created for average score .</p>
<pre><code>avg_score = avgOver(Score,[Name],PRE_AGG)
</code></pre>
<p>Segment</p>
<pre><code>ifelse
(
    {avg_score}&gt;= 98,'A',
    {avg_score}&gt;= 95,'B',
    {avg_score}&gt;= 90,'C',
    {avg_score}&gt;= 80,'D',
    'E'
)
</code></pre>
<p>The survey id can be used to get the distinct count per individual.</p>
","18356464",0
1701,72707669,2,72697889,2022-06-21 22:16:08,1,"<p>The infrastructure code is the Python code that you want to write for the resources you want to provision with SageMaker. In the example you provided for example the infra code they have is creating a Lambda function. You can do this locally on your machine, the question is what do you want to achieve with SageMaker? If you want to create an endpoint then following the CDK Python docs with SageMaker to identify the steps for creating an endpoint. Here's two guides, the first is an introduction to the AWS CDK and getting started. The second is an example of using the CDK with SageMaker to create an endpoint for  inference.</p>
<p>CDK Python Starter: <a href=""https://towardsdatascience.com/build-your-first-aws-cdk-project-18b1fee2ed2d"" rel=""nofollow noreferrer"">https://towardsdatascience.com/build-your-first-aws-cdk-project-18b1fee2ed2d</a>
CDK SageMaker Example: <a href=""https://github.com/philschmid/cdk-samples/tree/master/sagemaker-serverless-huggingface-endpoint"" rel=""nofollow noreferrer"">https://github.com/philschmid/cdk-samples/tree/master/sagemaker-serverless-huggingface-endpoint</a></p>
","16504640",1
1702,72719904,2,72690551,2022-06-22 17:45:42,2,"<p>You can use Kubernetes secret in an Nuclio function. There are several steps to set this up.</p>
<ol>
<li><p>create a Kubernetes secret, simple example using kubectl like this:</p>
<p><code>kubectl create secret generic db-user-pass --from-literal=username=devuser --from-literal=password='&lt;A-Password-Here&gt;'</code></p>
</li>
<li><p>Then create an Nuclio function with set_env_from_secret like this:</p>
<p><code>fn = mlrun.code_to_function(&quot;nuclio-with-secret&quot;, kind='nuclio', image='mlrun/mlrun', handler=&quot;handler&quot;) fn.set_env_from_secret(&quot;a-secret-name&quot;, &quot;db-user-pass&quot;, &quot;password&quot;) fn.apply(mlrun.auto_mount()) fn.deploy()</code></p>
</li>
<li><p>In your Nuclio function, you can use the secret like this:</p>
<p><code>the_secret_inside_nuclio_to_use = os.getenv('a-secret-name')</code></p>
</li>
</ol>
","8350820",0
1703,72721227,2,72718114,2022-06-22 19:47:12,2,"<p>There are actually different grafana dashboards being used in Iguazio. The grafana dashboard you are referring to is for the platform users and is found on the services page. This includes all your model monitoring dashboards as you mentioned.
There is another grafana service that is used by the Iguazio system which comes pre-installed. You can access it by navigating to the Clusters page, under the Application tab, Status Dashboard column there is an icon that is a link which takes you to that grafana service. See this screenshot:
<a href=""https://i.stack.imgur.com/iRwoC.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/iRwoC.png"" alt=""enter image description here"" /></a>
There you will find about 15 dashboards related to kubernetes, NGINX, GPUs, Nuclio, and Iguazio services resources monitoring. This constitutes the &quot;cluster-level&quot; monitoring that you were referring to.</p>
","8350820",0
1704,72721755,2,72721109,2022-06-22 20:40:47,0,"<p>The problem was in the <code>docker-compose.yaml</code>, in the <code>environment</code> section of oauth2_proxy` service.</p>
<p>I was passing the <code>OAUTH2_EXTRA_JWT_ISSUERS</code> but the correct variable name is <code>OAUTH2_PROXY_EXTRA_JWT_ISSUERS</code>. After fixing this everything is working perfect.</p>
","7705692",0
1705,72722150,2,72694707,2022-06-22 21:24:47,3,"<p>You have set <code>autolog</code> and you are also logging the model explicitly. Remove one and then try.</p>
","2986344",0
1706,72722324,2,72454747,2022-06-22 21:43:18,1,"<p>It turns out this was caused by using different versions of mlflow. The model was uploaded to registry with the newest version but was loaded with a previous one. When updated the server to load it, it now works! :)</p>
","2411048",0
1707,72729521,2,72729267,2022-06-23 11:38:25,1,"<p>There is a chance of auto scaling for the normal services in azure cloud services, that means for stipulated time you can increase or decrease as mentioned in the <a href=""https://learn.microsoft.com/en-us/azure/cloud-services/cloud-services-how-to-scale-portal"" rel=""nofollow noreferrer"">link</a>.</p>
<p>When it comes for vCPU which is cannot be performed automatically. vCPU can be scaled up based on the request criteria and in the same manner we need to request the support team to scale those down to the normal.</p>
<p><strong>There is no specific procedure to make the auto scaling for vCPU operations. We can increase the capacity of core, but to reduce to the normal, we need to approach the support system for manual changing. You can change it from 10 cores to next level 16 cores, but cannot be performed automatic scaling down from 16 cores to 10 cores.</strong></p>
","18428148",1
1708,72733261,2,72693169,2022-06-23 16:01:25,0,"<p>pvolumes={&quot;/mnt/positive/&quot;: vop.volume}) and pvolumes={&quot;/mnt/negative/&quot;: vop.volume}) was creating two separate pvc's.</p>
","14889342",1
1709,72738711,2,72738710,2022-06-24 03:34:04,1,"<p>Since the API is IP whitelist secured, you will need to add the public IP of your Sagemaker instance to the whitelist in order to connect to the API from the notebook.</p>
<ol>
<li><p>Open a Sagemaker notebook instance, and run the following command in a cell.</p>
<p><code>!curl ifconfig.me</code></p>
<p>This will return an IP address such as <code>13.232.97.17</code>.</p>
</li>
<li><p>Go to your cluster's security group from EC2 console page. It will have a name such as <code>eks-cluster-sg-cluserName-uniqueID</code>.</p>
</li>
<li><p>Edit the security group's inbound rules, and add a new entry for the sagemaker notebook instance.</p>
<pre><code>   Type          Protocol
   Custom TCP    TCP    &lt;port_number&gt;    Custom    13.232.97.17/32
</code></pre>
<p>and add an appropriate description.</p>
</li>
</ol>
<p>Now, you should be able to access the API via the notebook.</p>
","10052646",0
1710,72748673,2,72712449,2022-06-24 19:36:07,0,"<p>So, <code>ImageGenerator</code> and <code>flow_from_directory</code> I continue use inside of Training step. Processing step I skip at all, just use Training, Evaluating and Register model.</p>
","6672422",0
1711,72752498,2,72746701,2022-06-25 08:50:44,0,"<p>I don't fully understand your question (can you make an example please?), specially when you say</p>
<blockquote>
<p>like how we run training jobs using in-built algorithms</p>
</blockquote>
<p>But basically you can do whatever you want in your container, as probably you already did, you have a proper <code>train</code> file in your container, which is the one that sagemaker calls as the entrypoint. In that file you can call external script (which are in your container too) and also pass parameters to your container (see how for example hyperparameters are passed). There is a quite clear documentation <a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/docker-containers-create.html"" rel=""nofollow noreferrer"">here</a>.</p>
","4267439",5
1712,72770385,2,72732616,2022-06-27 10:12:54,0,"<pre><code>scoring_step = PythonScriptStep(
    name=&quot;Scoring_Step&quot;,
    source_directory=os.getenv(&quot;DATABRICKS_NOTEBOOK_PATH&quot;, &quot;/Users/USER_NAME/source_directory&quot;),
    script_name=&quot;./scoring.py&quot;,
    arguments=[&quot;--input_dataset&quot;, ds_consumption],
    compute_target=pipeline_cluster,
    runconfig=pipeline_run_config,
    allow_reuse=False)
</code></pre>
<p>Change the above code block with below code block. It will resolve the error</p>
<pre><code>data_ref = OutputFileDatasetConfig(
    name='data_ref',
    destination=(ds, '/data')
).as_upload()


data_prep_step = PythonScriptStep(
    name='data_prep',
    script_name='pipeline_steps/data_prep.py',
    source_directory='/.',
    arguments=[
        '--main_path', main_ref,
        '--data_ref_folder', data_ref
                ],
    inputs=[main_ref, data_ref],
    outputs=[data_ref],
    runconfig=arbitrary_run_config,
    allow_reuse=False
)
</code></pre>
<p>Reference link for the <a href=""https://scoring_step%20=%20PythonScriptStep(%20%20%20%20%20name=%22Scoring_Step%22,%20%20%20%20%20source_directory=os.getenv(%22DATABRICKS_NOTEBOOK_PATH%22,%20%22/Users/USER_NAME/source_directory%22),%20%20%20%20%20script_name=%22./scoring.py%22,%20%20%20%20%20arguments=%5B%22--input_dataset%22,%20ds_consumption%5D,%20%20%20%20%20compute_target=pipeline_cluster,%20%20%20%20%20runconfig=pipeline_run_config,%20%20%20%20%20allow_reuse=False)"" rel=""nofollow noreferrer"">documentation</a></p>
","18428148",0
1713,72771390,2,72731861,2022-06-27 11:32:08,1,"<p>Two things to try:</p>
<ul>
<li><p>Like in the notebook, you should pass <code>project=&quot;your-project-name&quot;</code> like <code>wandb.sweep(sweep_configuration, project=&quot;your-project-name&quot;)</code></p>
</li>
<li><p>Have you logged in to W&amp;B (using <code>wandb.login()</code>)?</p>
</li>
</ul>
<p>Finally, once you've successfully created the sweep, you should pass the <code>sweep_id</code> and your function (here <code>train</code>) like:
<code>wandb.agent(sweep_id, train, count=5)</code></p>
","3959708",0
1714,72771593,2,72745109,2022-06-27 11:49:11,2,"<p>Additional to the <code>spark.hadoop.fs.s3a.impl</code> config parameter, you can try to also set <code>spark.hadoop.fs.s3.impl</code> to <code>org.apache.hadoop.fs.s3a.S3AFileSystem</code></p>
","11551255",0
1715,72784330,2,72760982,2022-06-28 09:54:48,1,"<p>I would try the canonical way to use tensorboard in AWS Sagemaker, it should be supported also by Studio Lab, it is described <a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/studio-tensorboard.html"" rel=""nofollow noreferrer"">here</a>. Basically install tensorboard and using the <code>EFS_PATH_LOG_DIR</code> launch tensorboard using the embedded console (you can do the following also from a cell):</p>
<pre><code>pip install tensorboard
tensorboard --logdir &lt;EFS_PATH_LOG_DIR&gt;
</code></pre>
<p>Be careful with the EFS_PATH_LOG_DIR, be sure this folder is valida path from the location you are, for example by default you are located in <code>studio-lab-user/sagemaker-studiolab-notebooks/</code> so the proper command would be <code>!tensorboard --logdir logs/fit</code>.</p>
<p>Then open a browser to:</p>
<pre><code>https://&lt;YOUR URL&gt;/studiolab/default/jupyter/proxy/6006/
</code></pre>
","4267439",5
1716,72796506,2,68736614,2022-06-29 06:06:10,0,"<p>I was also doing the similar stuff and running a sagemaker GT labeling job.
I am following the below git repo by amazon :</p>
<p><a href=""https://github.com/aws-samples/amazon-textract-transformer-pipeline"" rel=""nofollow noreferrer"">https://github.com/aws-samples/amazon-textract-transformer-pipeline</a></p>
<p>If you look into the below file in the repo:</p>
<p><a href=""https://github.com/aws-samples/amazon-textract-transformer-pipeline/blob/main/notebooks/util/smgt.py"" rel=""nofollow noreferrer"">https://github.com/aws-samples/amazon-textract-transformer-pipeline/blob/main/notebooks/util/smgt.py</a></p>
<p>at line no:349 ,you will see how they do it and you can do the similar change in your code.</p>
<p><strong>&quot;TaskTitle&quot;: &quot;Credit Card Agreement Entities&quot;</strong></p>
<p>Hope this will help.</p>
","1771338",0
1717,72803260,2,72798225,2022-06-29 14:37:17,1,"<p>I just figured out the reason. As of today, the Python versions for the data clients in <a href=""https://learn.microsoft.com/de-de/sql/machine-learning/python/setup-python-client-tools-sql?view=sql-server-ver15"" rel=""nofollow noreferrer"">https://learn.microsoft.com/de-de/sql/machine-learning/python/setup-python-client-tools-sql?view=sql-server-ver15</a> are not the newest (revoscalepy Version 9.3), while the version of Machine Learning Services that we have running in our SQL Server is already 9.4.7.
However, the revoscalepy libraries for the client and server must be the same, otherwise the deserialization fails server-sided.</p>
","19436129",0
1718,72820513,2,72780102,2022-06-30 18:31:45,0,"<p>I have missed the <code>archive_existing_versions=True</code> that comes with the <code>transition_model_version_stage</code> function.</p>
<p>This flag defaults to <code>False</code>.</p>
<p>The documentation is available here,
<br>
<a href=""https://mlflow.org/docs/latest/python_api/mlflow.tracking.html#mlflow.tracking.MlflowClient.transition_model_version_stage"" rel=""nofollow noreferrer"">https://mlflow.org/docs/latest/python_api/mlflow.tracking.html#mlflow.tracking.MlflowClient.transition_model_version_stage</a></p>
","9542989",0
1719,72829114,2,72774207,2022-07-01 12:15:12,1,"<p>They way to create model and register model on Pipelines has changed slightly with the introduction of ModelStep, also the instantiation of session_pipeline is needed. Similarly, ModelStep will be used for registering the model .
Reference: <code>https://github.com/aws/sagemaker-python-sdk/pull/3076</code>
Examples : <a href=""https://sagemaker.readthedocs.io/en/stable/workflows/pipelines/sagemaker.workflow.pipelines.html?highlight=ModelStep#sagemaker.workflow.model_step.ModelStep"" rel=""nofollow noreferrer"">https://sagemaker.readthedocs.io/en/stable/workflows/pipelines/sagemaker.workflow.pipelines.html?highlight=ModelStep#sagemaker.workflow.model_step.ModelStep</a></p>
","19460312",0
1720,72834950,2,72834714,2022-07-01 21:55:50,2,"<p>Finally you can use</p>
<pre><code>from minio import Minio
client = Minio(
    &quot;minio-service.kubeflow.svc.cluster.local:9000&quot;
    access_key=&quot;minio&quot;,
    secret_key=&quot;minio123&quot;,
    secure=False
)

obj = client.get_object(
    &quot;bucket&quot;,
    &quot;file.csv&quot;,
)

df = pd.read_csv(obj)
</code></pre>
","3672883",0
1721,72856536,2,72775967,2022-07-04 12:10:59,0,"<p>Issue has been filed in <a href=""https://github.com/rstudio/pins/issues/624"" rel=""nofollow noreferrer"">pins</a>, it seems that is not an AzureStor issue.</p>
","3002799",0
1722,72864388,2,72518344,2022-07-05 05:29:14,1,"<p>The retrieving of log run parameters like <strong>Learning Rate, or Momentum</strong> is not possible with <strong>AzureML</strong> alone. Because it was tied with <strong>MLFlow</strong> and <strong>azureml-core</strong>. without those two involvements, we cannot retrieve the log run parameters.</p>
<pre><code>pip install azureml-core mlflow azureml-mlflow
</code></pre>
<p>Need to install these three for getting run parameters. <a href=""https://learn.microsoft.com/en-us/azure/machine-learning/how-to-log-view-metrics"" rel=""nofollow noreferrer"">Link</a></p>
","18428148",0
1723,72875304,2,72874937,2022-07-05 20:42:25,0,"<p>You can use the boto3 library to do this.</p>
<p>Here is an example of pseudo code for this -</p>
<pre><code>import boto3
sm_client = boto3.client('sagemaker')
create_model_respose = sm_client.create_model(ModelName=model_name, ExecutionRoleArn=role, Containers=[container] )

create_endpoint_config_response = sm_client.create_endpoint_config(EndpointConfigName=endpoint_config_name)

create_endpoint_response = sm_client.create_endpoint(EndpointName=endpoint_name, EndpointConfigName=endpoint_config_name)
</code></pre>
","16247336",0
1724,72875857,2,72780610,2022-07-05 21:40:54,3,"<p>The problem is that you are starting the server on <code>127.0.0.1</code> and the port mapping was not pointing to this interface (socket hang up). Starting it on all interfaces <code>0.0.0.0</code> works.</p>
<p>You should just run this command in the container.</p>
<pre><code>mlflow ui -h 0.0.0.0
</code></pre>
","11232272",0
1725,72876499,2,72663991,2022-07-05 23:31:09,2,"<p>It is possible to add extra packages for training when using any of the Framework containers such as SKLearn. Kindly see this <a href=""https://sagemaker.readthedocs.io/en/stable/frameworks/sklearn/using_sklearn.html#using-third-party-libraries"" rel=""nofollow noreferrer"">link</a> for more information.</p>
<p>From a hosting perspective, you do have the ability to provide a custom entry_point / inference.py script that you can use to control model loading, pre and post processing. Please see this <a href=""https://sagemaker.readthedocs.io/en/stable/frameworks/sklearn/using_sklearn.html#load-a-model"" rel=""nofollow noreferrer"">link</a> for more information</p>
","9796588",0
1726,72878042,2,72831360,2022-07-06 04:42:30,0,"<blockquote>
<p>parser.add_argument(&quot;--data&quot;, type=str, help=&quot;path to input data&quot;) # &lt;=== Here, but I don´t know how</p>
</blockquote>
<p>To get the path to input data, according to <a href=""https://github.com/MicrosoftDocs/azure-docs/blob/main/articles/machine-learning/how-to-train-with-datasets.md"" rel=""nofollow noreferrer"">documentation</a>:</p>
<ul>
<li><p>You can get <code>--input-data</code> by ID which you can access in your training script.</p>
</li>
<li><p>Use it as <code>argument</code> on <code>mounted_input_path</code></p>
</li>
</ul>
<p>For example, try the following three code snippets taken from the <a href=""https://github.com/MicrosoftDocs/azure-docs/blob/main/articles/machine-learning/how-to-train-with-datasets.md"" rel=""nofollow noreferrer"">documentation</a>:</p>
<p><strong>Access dataset in training script:</strong></p>
<pre><code>parser = argparse.ArgumentParser()
parser.add_argument(&quot;--input-data&quot;, type=str)
args = parser.parse_args()

run = Run.get_context()
ws = run.experiment.workspace

# get the input dataset by ID
dataset = Dataset.get_by_id(ws, id=args.input_data)
</code></pre>
<p><strong>Configure the training run:</strong></p>
<pre><code>src = ScriptRunConfig(source_directory=script_folder,
                      script='train_titanic.py',
                      # pass dataset as an input with friendly name 'titanic'
                      arguments=['--input-data', titanic_ds.as_named_input('titanic')],
                      compute_target=compute_target,
                      environment=myenv)
</code></pre>
<p><strong>Pass <code>mounted_input_path</code> as argument:</strong></p>
<pre><code>mounted_input_path = sys.argv[1]
mounted_output_path = sys.argv[2]

print(&quot;Argument 1: %s&quot; % mounted_input_path)
print(&quot;Argument 2: %s&quot; % mounted_output_path)
</code></pre>
<p>References: <a href=""https://github.com/MicrosoftDocs/azure-docs/blob/main/articles/machine-learning/v1/how-to-create-register-datasets.md"" rel=""nofollow noreferrer"">How to create register dataset</a> and <a href=""https://github.com/Azure/MachineLearningNotebooks/blob/master/how-to-use-azureml/work-with-data/datasets-tutorial/scriptrun-with-data-input-output/how-to-use-scriptrun.ipynb"" rel=""nofollow noreferrer"">How to use configure a training run with data input and output</a></p>
","9303470",0
1727,72895587,2,72886409,2022-07-07 09:40:14,1,"<p>The problem is that the server is running on wrong run parameters, the <code>--default-artifact-root</code> needs to either be removed or set to <code>mlflow-artifacts:/</code>.</p>
<p>From <code>mlflow server --help</code>:</p>
<pre><code>  --default-artifact-root URI  Directory in which to store artifacts for any
                               new experiments created. For tracking server
                               backends that rely on SQL, this option is
                               required in order to store artifacts. Note that
                               this flag does not impact already-created
                               experiments with any previous configuration of
                               an MLflow server instance. By default, data
                               will be logged to the mlflow-artifacts:/ uri
                               proxy if the --serve-artifacts option is
                               enabled. Otherwise, the default location will
                               be ./mlruns.
</code></pre>
","10465165",0
1728,72909496,2,72909085,2022-07-08 09:39:48,3,"<p>you need to make sure to store your model output to the right location inside the training container. Sagemaker will upload everything that is stored in the MODEL_DIR directory. You can find the location in the ENV of the training job:</p>
<pre class=""lang-py prettyprint-override""><code>model_dir = os.environ.get(&quot;SM_MODEL_DIR&quot;)
</code></pre>
<p>Normally it is set to <code>opt/ml/model</code></p>
<p>Ref:</p>
<ul>
<li><a href=""https://github.com/aws/sagemaker-training-toolkit/blob/master/ENVIRONMENT_VARIABLES.md#sm_model_dir"" rel=""nofollow noreferrer"">https://github.com/aws/sagemaker-training-toolkit/blob/master/ENVIRONMENT_VARIABLES.md#sm_model_dir</a></li>
<li><a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/your-algorithms-training-algo-output.html"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/sagemaker/latest/dg/your-algorithms-training-algo-output.html</a></li>
</ul>
","13196719",0
1729,72914082,2,72912418,2022-07-08 16:02:18,0,"<p>See this step by step guide on how to enable TensorBoard on SageMaker Studio here: <a href=""https://github.com/anoop-ml/smstudio_tensorboard_sample"" rel=""nofollow noreferrer"">https://github.com/anoop-ml/smstudio_tensorboard_sample</a></p>
<p>Did you follow those instructions?</p>
","6360955",1
1730,72933948,2,72933908,2022-07-11 05:09:35,1,"<p>You can <code>ResourceGroupsTaggingAPI</code>'s method <code>tag_resources()</code>.<br />
This is used to apply one or more tags to the specified list of resources.</p>
<p>References:</p>
<ol>
<li><a href=""https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/resourcegroupstaggingapi.html#ResourceGroupsTaggingAPI.Client.tag_resources"" rel=""nofollow noreferrer"">Tag Resources using boto3</a></li>
<li><a href=""https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/resourcegroupstaggingapi.html#ResourceGroupsTaggingAPI.Client.untag_resources"" rel=""nofollow noreferrer"">UnTag Resources using boto3</a></li>
</ol>
","3555366",1
1731,72939180,2,72935918,2022-07-11 13:14:33,0,"<p>Configuring VPC to restrict outbound traffic is quite easy. You can start from <a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/studio-notebooks-and-internet-access.html"" rel=""nofollow noreferrer"">here</a>. There are lot of AWS Official blogs/samples written on this topic but you can start with these:
<a href=""https://aws.amazon.com/blogs/machine-learning/securing-amazon-sagemaker-studio-connectivity-using-a-private-vpc/"" rel=""nofollow noreferrer"">Securing Amazon SageMaker Studio connectivity using a private VPC</a>
<a href=""https://github.com/aws-samples/amazon-sagemaker-studio-vpc-networkfirewall"" rel=""nofollow noreferrer"">Amazon SageMaker Studio in a private VPC with NAT Gateway and Network Firewall</a></p>
<p>on the topic of sudo access, Studio uses <code>run-as</code> POSIX user/group to manage the <code>JupyterServer app</code> and <code>KernelGateWay app</code>. The JupyterServer app user is run as <code>sagemaker-user</code>, which has <code>sudo</code> permission to enable installation of yum packages, whereas the <code>KernelGateway app</code> user is run as <code>root</code> and can perform pip/conda installs, but <strong>neither</strong> can access the host instance. Apart from the default <code>run-as</code> user, the user inside the container is mapped to a <code>non-privileged user ID range</code> on the notebook instances. This is to ensure that the user can’t escalate privileges to come out of the container and perform any restricted operations in the EC2 instance.</p>
<p>In addition, SageMaker adds specific route rules to block requests to Amazon EFS and the <code>instance metadata service (IMDS)</code> from the container, and users can’t change these rules. All the inter-network traffic in Studio is TLS 1.2 encrypted, barring some intra-node traffic like communication between nodes in a distributed training or processing job and communication between a service control plane and training instances. Check out this <a href=""https://aws.amazon.com/blogs/machine-learning/dive-deep-into-amazon-sagemaker-studio-notebook-architecture/"" rel=""nofollow noreferrer"">blog</a> to understand better on How Studio runs</p>
","3856286",0
1732,72957957,2,72954467,2022-07-12 20:19:31,2,"<p>You can add the volume and volume mount to your Katib job template so that all the HPO jobs on Katib can share the same volumes. e.g.</p>
<pre><code>apiVersion: batch/v1
kind: Job
spec:
  template:
    spec:
      containers:
        - name: training-container
          image: docker.io/romeokienzler/claimed-train-mobilenet_v2:0.4
          command:
            - &quot;ipython&quot;
            - &quot;/train-mobilenet_v2.ipynb&quot;
            - &quot;optimizer=${trialParameters.optimizer}&quot;
          volumeMounts:
            - mountPath: /data/
              name: data-volume
      restartPolicy: Never
      volumes:
          - name: data-volume
            persistentVolumeClaim:
              claimName: data-pvc
</code></pre>
<p>Also make sure your pvc is read write many so the pod on different nodes can mount on the same volume at the same time.</p>
","6728439",0
1733,72997709,2,72914046,2022-07-15 17:27:05,0,"<p>Using a different cloudformation template fixed the issue. Not sure why.</p>
","19201667",0
1734,73028228,2,73025706,2022-07-18 20:26:38,0,"<p>When you're running commands from SageMaker, you're executing them as the SageMaker execution role, instead of your role. There are two options -</p>
<ol>
<li>[Straighforward solution] Add <em>ecr:InitiateLayerUpload</em> permissions to the <code>AmazonSageMaker-ExecutionRole-xxxxxxxxxxxx</code> role</li>
<li>Assume a different role using sts (in that case, <code>AmazonSageMaker-ExecutionRole-xxxxxxxxxxxx</code> needs to have permissions to assume your Admin role) and then run <code>docker push</code> command.</li>
</ol>
","2458691",0
1735,73028575,2,73024189,2022-07-18 20:59:33,1,"<p>This works (escaping backticks) -</p>
<pre><code>aws efs describe-file-systems --query &quot;FileSystems[?CreationToken==\`$DOMAIN_ID\`].FileSystemId&quot;
</code></pre>
<p>You can also use describe-domain command instead -</p>
<pre><code>$ DOMAIN_ID=$(aws sagemaker list-domains --query 'Domains[0].DomainId' | tr -d '&quot;')
$ aws sagemaker describe-domain --domain-id $DOMAIN_ID --query 'HomeEfsFileSystemId'
</code></pre>
","2458691",0
1736,73038116,2,72975189,2022-07-19 13:44:43,1,"<p>To answer your question, the source code of that class looks like:</p>
<pre><code>package com.amazonaws.sagemaker.spark.test;

import java.lang.invoke.SerializedLambda;
import org.apache.commons.cli.CommandLineParser;
import org.apache.commons.cli.ParseException;
import org.apache.commons.cli.HelpFormatter;
import org.apache.commons.cli.Option;
import org.apache.commons.cli.BasicParser;
import org.apache.commons.cli.Options;
import org.apache.spark.sql.Dataset;
import org.apache.commons.cli.CommandLine;
import org.apache.spark.sql.types.DataTypes;
import org.apache.commons.lang3.StringUtils;
import java.util.List;
import org.apache.spark.sql.SparkSession;

public class HelloJavaSparkApp
{
    public static void main(final String[] args) {
        System.out.println(&quot;Hello World, this is Java-Spark!&quot;);
        final CommandLine parsedArgs = parseArgs(args);
        final String inputPath = parsedArgs.getOptionValue(&quot;input&quot;);
        final String outputPath = parsedArgs.getOptionValue(&quot;output&quot;);
        final SparkSession spark = SparkSession.builder().appName(&quot;Hello Spark App&quot;).getOrCreate();
        System.out.println(&quot;Got a Spark session with version: &quot; + spark.version());
        System.out.println(&quot;Reading input from: &quot; + inputPath);
        final Dataset salesDF = spark.read().json(inputPath);
        salesDF.printSchema();
        salesDF.createOrReplaceTempView(&quot;sales&quot;);
        final Dataset topDF = spark.sql(&quot;SELECT date, sale FROM sales WHERE sale &gt; 750 SORT BY sale DESC&quot;);
        topDF.show();
        final Dataset avgDF = salesDF.groupBy(&quot;date&quot;, new String[0]).avg(new String[0]).orderBy(&quot;date&quot;, new String[0]);
        System.out.println(&quot;Collected average sales: &quot; + StringUtils.join((Object[])new List[] { avgDF.collectAsList() }));
        spark.sqlContext().udf().register(&quot;double&quot;, n -&gt; n + n, DataTypes.LongType);
        final Dataset saleDoubleDF = salesDF.selectExpr(new String[] { &quot;date&quot;, &quot;sale&quot;, &quot;double(sale) as sale_double&quot; }).orderBy(&quot;date&quot;, new String[] { &quot;sale&quot; });
        saleDoubleDF.show();
        System.out.println(&quot;Writing output to: &quot; + outputPath);
        saleDoubleDF.coalesce(1).write().json(outputPath);
        spark.stop();
    }
    
    private static CommandLine parseArgs(final String[] args) {
        final Options options = new Options();
        final CommandLineParser parser = (CommandLineParser)new BasicParser();
        final Option input = new Option(&quot;i&quot;, &quot;input&quot;, true, &quot;input path&quot;);
        input.setRequired(true);
        options.addOption(input);
        final Option output = new Option(&quot;o&quot;, &quot;output&quot;, true, &quot;output path&quot;);
        output.setRequired(true);
        options.addOption(output);
        try {
            return parser.parse(options, args);
        }
        catch (ParseException e) {
            new HelpFormatter().printHelp(&quot;HelloScalaSparkApp --input /opt/ml/input/foo --output /opt/ml/output/bar&quot;, options);
            throw new RuntimeException((Throwable)e);
        }
    }
}
</code></pre>
<p>At the same time, I have created a simple example that shows how to run an hello world app <a href=""https://github.com/giuseppeporcelli/sagemaker-misc-examples/blob/main/spark-jar-example.ipynb"" rel=""nofollow noreferrer"">here</a>. Please note that I have run that example on Amazon SageMaker Studio Notebooks, using the Data Science 1.0 kernel.</p>
<p>Hope this helps.</p>
","19580013",0
1737,73044244,2,73041737,2022-07-19 22:52:10,1,"<p>You can use <a href=""https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_DescribeNotebookInstance.html"" rel=""nofollow noreferrer"">DescribeNotebookInstance</a> API to get the instance size.</p>
<pre><code>sm_client = boto3.client(&quot;sagemaker&quot;)
sm.describe_notebook_instance(
    NotebookInstanceName=&lt;nb-name&gt;
)['InstanceType']
</code></pre>
","2458691",2
1738,73048122,2,72994988,2022-07-20 08:15:34,2,"<p>The proper way to do this is to use <code>mlflow.log_figure</code> as a fluent API announced in <code>MLflow 1.13.0</code>. You can read the documentation <a href=""https://www.mlflow.org/docs/latest/python_api/mlflow.html#mlflow.log_figure"" rel=""nofollow noreferrer"">here</a>. This code will do the job.</p>
<pre class=""lang-py prettyprint-override""><code>mlflow.log_figure(cm.figure_, 'test_confusion_matrix.png')
</code></pre>
<p>This function implicitly store the image, and then calls <code>log_artifact</code> against that path, something like you did.</p>
","11232272",0
1739,73051013,2,73050203,2022-07-20 11:42:34,2,"<p><strong>AFAIK</strong>, based on your scenario you can make use of <strong><code>Azure File Share</code></strong> to create data store.</p>
<p>Please <strong>note</strong> that, Azure Blob storage is suitable for uploading large amount of unstructured data whereas <strong><code>Azure File Share</code></strong> is suitable for uploading and processing the structured files in chunks (more interaction with app to share files).</p>
<blockquote>
<p>I have a folder called data with a bunch of csvs (about 80), file sizes are fairly small. This data is clean and has already been preprocessed.</p>
</blockquote>
<p>As you mentioned <strong><code>CSV</code></strong> data is clean and preprocessed it comes under structured data. So, you can make you of <strong>Azure File Share</strong> to create data store.</p>
<p>To register a data store with <strong>Azure File Share</strong> you can make use of this <a href=""https://learn.microsoft.com/en-us/python/api/azureml-core/azureml.core.datastore.datastore?view=azure-ml-py#azureml-core-datastore-datastore-register-azure-file-share"" rel=""nofollow noreferrer""><strong>MsDoc</strong></a></p>
<p>To know more about Azure File Share and Azure Blob storage, please find below <strong>links</strong>:</p>
<p><a href=""https://stackoverflow.com/questions/67217164/azure-blob-storage-or-azure-file-storage"">Azure Blob Storage or Azure File Storage by Mike</a></p>
<p><a href=""https://learn.microsoft.com/en-us/python/api/azureml-core/azureml.data.azure_storage_datastore.azurefiledatastore?view=azure-ml-py"" rel=""nofollow noreferrer"">azureml.data.azure_storage_datastore.AzureFileDatastore class - Azure Machine Learning Python | Microsoft Docs</a></p>
","18043665",0
1740,73061992,2,73061907,2022-07-21 07:06:06,2,"<p>You could use a <code>dynamic</code> block [1] in combination with <code>for_each</code> meta-argument [2]. It would look something like:</p>
<pre><code>dynamic &quot;async_inference_config&quot; {
    for_each = var.s3_output_path != null &amp;&amp; var.max_concurrent_invocations_per_instance != null ? [1] : []
    content {
    output_config {
      s3_output_path = var.s3_output_path
    }
    client_config {
      max_concurrent_invocations_per_instance = var.max_concurrent_invocations_per_instance
    }
  }
}
</code></pre>
<p>Of course, you could come up with a different variable, say <code>enable_async_inference_config</code> (probalby of type <code>bool</code>) and base the <code>for_each</code> on that, e.g.:</p>
<pre><code>dynamic &quot;async_inference_config&quot; {
    for_each = var.enable_async_inference_config ? [1] : []
    content {
    output_config {
      s3_output_path = var.s3_output_path
    }
    client_config {
      max_concurrent_invocations_per_instance = var.max_concurrent_invocations_per_instance
    }
  }
}
</code></pre>
<hr />
<p>[1] <a href=""https://www.terraform.io/language/expressions/dynamic-blocks"" rel=""nofollow noreferrer"">https://www.terraform.io/language/expressions/dynamic-blocks</a></p>
<p>[2] <a href=""https://www.terraform.io/language/meta-arguments/for_each"" rel=""nofollow noreferrer"">https://www.terraform.io/language/meta-arguments/for_each</a></p>
","8343484",13
1741,73073772,2,73042521,2022-07-21 23:57:06,1,"<p>Linear-Learner is a built in algorithm written using MX-net and the binary is also MXNET compatible. You can't use this model outside of SageMaker as there is no open source implementation for this.</p>
","19490330",0
1742,73083654,2,73058582,2022-07-22 16:50:49,0,"<p>I had to change the boundary to be this: <code> arn:aws:ecr:us-west-2:246618743249:repository/sagemaker-xgboost</code></p>
","19201667",0
1743,73102877,2,73074887,2022-07-24 23:47:46,1,"<p>It's because of the mlflow version that you mentioned in the comments. <code>mlflow.last_active_run()</code> API was introduced in <a href=""https://github.com/mlflow/mlflow/releases/tag/v1.25.0"" rel=""nofollow noreferrer"">mlflow 1.25.0
</a>. So you should upgrade the mlflow or you can use the previous version of the code available <a href=""https://github.com/mlflow/mlflow/tree/5e2cb3baef544b00a972dff9dd6fb764be20510b/examples/sklearn_autolog"" rel=""nofollow noreferrer"">here</a>.</p>
<pre><code>wget https://raw.githubusercontent.com/mlflow/mlflow/5e2cb3baef544b00a972dff9dd6fb764be20510b/examples/sklearn_autolog/utils.py
wget https://raw.githubusercontent.com/mlflow/mlflow/5e2cb3baef544b00a972dff9dd6fb764be20510b/examples/sklearn_autolog/pipeline.py
</code></pre>
","11232272",0
1744,73113886,2,73110661,2022-07-25 18:38:08,3,"<p>in my case, the issue was solved by upgrading the ml extension to <code>azure-cli-ml v2</code></p>
<p>Remove any existing installation of the of <code>ml</code> extension and also the CLI v1 <code>azure-cli-ml</code> extension:</p>
<pre><code>az extension remove -n azure-cli-ml
az extension remove -n ml
</code></pre>
<p>Now, install the ml extension:</p>
<pre><code>az extension add -n ml -y
</code></pre>
<p>which still doesn't explain why the <code>create</code> command wasn't recognized, but the v2 behavior works fine for me.</p>
","9615185",0
1745,73120660,2,71649163,2022-07-26 09:10:30,1,"<p>I know I'm a bit late to the party but here we go:</p>
<p><strong>Passing variables between AzureML Pipeline Steps</strong></p>
<p>To directly answer your question, to my knowledge it is not possible to pass variables directly between PythonScriptSteps in an AzureML Pipeline.</p>
<p>The reason for that is that the steps are executed in isolation, i.e. the code is run in different processes or even computes. The only interface a PythonScriptStep has is (a) command line arguments that need to be set prior to submission of the pipeline and (b) data.</p>
<p><strong>Using datasets to pass information between PythonScriptSteps</strong></p>
<p>As a workaround you can use PipelineData to pass data between steps.
The previously posted blog post may help: <a href=""https://vladiliescu.net/3-ways-to-pass-data-between-azure-ml-pipeline-steps/"" rel=""nofollow noreferrer"">https://vladiliescu.net/3-ways-to-pass-data-between-azure-ml-pipeline-steps/</a></p>
<p>As for your concrete problem:</p>
<pre class=""lang-py prettyprint-override""><code># pipeline.py

# This will make Azure create a unique directory on the datastore everytime the pipeline is run.
variables_data = PipelineData(&quot;variables_data&quot;, datastore=datastore)

# `variables_data` will be mounted on the target compute and a path is given as a command line argument
write_variable = PythonScriptStep(
    script_name=&quot;write_variable.py&quot;,
    arguments=[
        &quot;--data_path&quot;,
        variables_data
    ],
    outputs=[variables_data],
)

read_variable = PythonScriptStep(
    script_name=&quot;read_variable.py&quot;,
    arguments=[
        &quot;--data_path&quot;,
        variables_data
    ],
    inputs=[variables_data],
)

</code></pre>
<p>In your script you'll want to serialize the variable / object that you're trying to pass between steps:</p>
<p>(You could of course use JSON or any other serialization method)</p>
<pre class=""lang-py prettyprint-override""><code># write_variable.py

import argparse
import pickle
from pathlib import Path

parser = argparse.ArgumentParser()
parser.add_argument(&quot;--data_path&quot;)
args = parser.parse_args()

obj = [1, 2, 3, 4]

Path(args.data_path).mkdir(parents=True, exist_ok=True)
with open(args.data_path + &quot;/obj.pkl&quot;, &quot;wb&quot;) as f:
    pickle.dump(obj, f)
</code></pre>
<p>Finally, you can read the variable in the next step:</p>
<pre class=""lang-py prettyprint-override""><code># read_variable.py

import argparse
import pickle

parser = argparse.ArgumentParser()
parser.add_argument(&quot;--data_path&quot;)
args = parser.parse_args()


with open(args.data_path + &quot;/obj.pkl&quot;, &quot;rb&quot;) as f:
    obj = pickle.load(f)

print(obj)
</code></pre>
","16825536",0
1746,73134754,2,73134073,2022-07-27 08:38:47,1,"<p>AFAIK, as of now, deleting the dataset using <a href=""https://github.com/Azure/azure-sdk-for-python/search?q=delete+datasets"" rel=""nofollow noreferrer"">AzureML Python SDK</a> is not possible via <code>delete.datasets()</code>. But it might be possible via <a href=""https://github.com/Azure/azure-sdk-for-python/blob/396853110f5c15463e5a531ee759446d3389d441/sdk/ml/azure-ai-ml/azure/ai/ml/_restclient/dataset_dataplane/operations/_delete_operations.py"" rel=""nofollow noreferrer"">delete_operations.py</a></p>
<p>As suggested by <a href=""https://learn.microsoft.com/en-us/answers/questions/567611/is-there-a-way-to-delete-datasets-on-azureml.html"" rel=""nofollow noreferrer"">YutongTie</a>, you can delete the dataset using the Azure Machine Learning Studio.</p>
<p>References: <a href=""https://learn.microsoft.com/en-us/answers/questions/379022/how-to-delete-data-backing-a-dataset.html"" rel=""nofollow noreferrer"">How to Delete Data Backing a Dataset</a>, <a href=""https://learn.microsoft.com/en-us/azure/machine-learning/how-to-export-delete-data"" rel=""nofollow noreferrer"">Export or delete your Machine Learning service workspace data</a> and <a href=""https://rdrr.io/cran/AzureML/man/delete.datasets.html"" rel=""nofollow noreferrer"">R interface to AzureML - delete dataset</a></p>
","9303470",3
1747,73141189,2,73137433,2022-07-27 16:00:35,0,"<p>Okay I found the issue.
I am unnecessarily using the ScriptRunConfig which overwrites the assigned environment with some default azureml environment. I was able to see that only in the Task description in the Azure Machine Learning Studio UI.</p>
<p>I was able to just remove that part and now it works:</p>
<pre><code>task_1_run_config = RunConfiguration.load(
    os.path.join(WORKING_DIR + '/pipeline/task_runconfigs/T01_Test_Task.yml')
    ) 
task_1_py_script_step = PythonScriptStep(
    name='Task_1_Step',
    script_name='T01_Test_Task.py',
    source_directory=os.path.join(WORKING_DIR + '/pipeline/task_scripts'),
    runconfig=task_1_run_config, 
    compute_target=compute_target
)
</code></pre>
","10775269",0
1748,73145679,2,73133746,2022-07-27 23:32:23,1,"<p>In short, You can upload the notebook without any issue into SageMaker. Few things to keep in mind</p>
<ol>
<li>If you are using the pyspark library in colab and running spark locally,  you should be able to do the same by installing necessary pyspark libs in Sagemaker studio kernels. Here you will only pay for the underlying compute for the notebook instance. If you are experimenting then I would recommend you to use <a href=""https://studiolab.sagemaker.aws/"" rel=""nofollow noreferrer"">https://studiolab.sagemaker.aws/</a> to create a free account and try things out.</li>
<li>If you had a separate spark cluster setup then you may need a similar setup in AWS using EMR so that you can connect to the cluster to execute the job.</li>
</ol>
","19490330",0
1749,73152034,2,73146779,2022-07-28 11:21:24,1,"<p>The issue was raised because of missing the called parameters in the function. While doing language detection in machine learning studio, we need to assign end point and key credentials. In the code mentioned above, endpoint details were mentioned, but missed <strong>AzureKeyCredential.</strong></p>
<pre><code>endpoint = os.environ[&quot;AZURE_LANGUAGE_ENDPOINT&quot;]
key = os.environ[&quot;AZURE_LANGUAGE_KEY&quot;]
text_analytics_client = TextAnalyticsClient(endpoint=endpoint)
</code></pre>
<p>replace the above line with the code block mentioned below</p>
<pre><code>text_analytics_client = TextAnalyticsClient(endpoint=endpoint, credential= AzureKeyCredential(key))
</code></pre>
","18428148",1
1750,73160126,2,73158818,2022-07-28 23:19:51,0,"<p>For encryption of data stored in s3 ( offline store ) you need to add a field
'offline_store_kms_key_id ' to the create() method call, please refer the document below</p>
<p><a href=""https://sagemaker.readthedocs.io/en/stable/api/prep_data/feature_store.html#sagemaker.feature_store.feature_group.FeatureGroup.create"" rel=""nofollow noreferrer"">https://sagemaker.readthedocs.io/en/stable/api/prep_data/feature_store.html#sagemaker.feature_store.feature_group.FeatureGroup.create</a></p>
<p>Also please go through the below document to check the policies and also to verify if you have a symmetric customer managed keys or asymmetric customer managed keys as feature store only supports symmetric keys.</p>
<p><a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/feature-store-security.html"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/sagemaker/latest/dg/feature-store-security.html</a></p>
","19490330",2
1751,73162113,2,73127303,2022-07-29 05:46:47,1,"<p>As @Andre has said, I had to write my own function to achieve this,</p>
<pre><code>def get_model_experiment(model_name, model_version):
    # get run_id of the model version
    run_id = mlflow_client.get_model_version(model_name, model_version).run_id

    # get the experiment_id from the run_id
    experiment_id = mlflow_client.get_run(run_id).info.experiment_id

    # get the experiment name from the experiment_id
    return mlflow_client.get_experiment(experiment_id).name
</code></pre>
","9542989",0
1752,73167262,2,73166561,2022-07-29 13:07:07,1,"<p>There is an explicit procedure called a <strong>sweep job</strong>. This sweep job in <strong>hyperparameter value</strong>. We can mention the random sampling using the sweep job explicitly.</p>
<p>From azure.ai.ml.sweep import Normal, Uniform, RandomParameterSampling</p>
<pre><code>Command_job_for_sweep = command_job(
    learning_rate = Normal(mu=value, sigma=value),
    keep_probability=Uniform(min_value= your value, max_value= value),
    batch_size = Choice(value=[.your values in list]),
)

Sweep_job = command_job_sweep.sweep(
    Computer =”cluster”,
    sampling_algorithm=”random”,
    ....
)
</code></pre>
<p>This will be available in <strong>version 2 (v2)</strong> of hyperparameter tuning in random sampling.</p>
","18428148",0
1753,73172888,2,73172644,2022-07-30 04:36:22,0,"<p>There is a possibility you may be executing &quot;pip&quot; in a different environment.</p>
<p>Try executing &quot;!pip install tqdm&quot; or &quot;!pip3 install tqdm&quot; as a code cell in the Sagemaker document itself.</p>
","19651081",1
1754,73181671,2,73098701,2022-07-31 08:22:48,1,"<p>Please check my answer to a similar question about volumes: <a href=""https://stackoverflow.com/a/67898164/1497385"">https://stackoverflow.com/a/67898164/1497385</a></p>
<p>The short answer is that the usage of volumes is not a supported way of passing data between components in KFP. I'm not saying it cannot work, but if a developer goes out of the officially supported data passing method they're on their own.</p>
<p>Using KFP without KFP's data passing is pretty close to not using KFP at all...</p>
<p>Here is how to pass data properly:</p>
<pre class=""lang-py prettyprint-override""><code>from kfp.components import InputPath, OutputPath, create_component_from_func

def download_images(
    url: str,
    output_path: OutputPath(),
):
    ...
    # Create directory at output_path
    # Put all images into it

download_images_op = create_component_from_func(download_images)

def compress_images(
    input_path: InputPath(),
    output_path: OutputPath(),
):
    # read images from input_path
    # write results to output_path

compress_images_op = create_component_from_func(compress_images)

def my_pipeline():
    images = download_images_op(
        url=...,
    ).outputs[&quot;output&quot;]

    compressed_images = compress_images_op (
        input=images,
    ).outputs[&quot;output&quot;]
</code></pre>
<p>You can also find many examples of real-world components in this repo: <a href=""https://github.com/Ark-kun/pipeline_components/tree/master/components"" rel=""nofollow noreferrer"">https://github.com/Ark-kun/pipeline_components/tree/master/components</a></p>
<p>P.S. As a small team we've spent so much time answering user questions about volumes not working despite the official documentation and all samples and tutorials showing how to use proper methods and never suggesting to use volumes.
I want to understand where this comes from.
Is there some unofficial KFP tutorial on the Internet that teaches users that the users should pass data via volumes?</p>
","1497385",7
1755,73181744,2,72997776,2022-07-31 08:36:23,1,"<p>We advise you use KFP's built-in data passing methods.
This way you get reproducibility, immutability caching etc.</p>
<p>You should split your pipeline into multiple components:
Download-&gt;Preprocess-&gt;Train
This way, the outputs of the <code>Download</code> task are cached and it's never executed again. Same with the <code>Preprocess</code> task.</p>
<blockquote>
<p>downloading 1GB per each training is a lot.</p>
</blockquote>
<p>Kubernetes volumes are connected through network anyway. Getting data from one machine to another is &quot;downloading&quot; no matter how it's done.
What you want to do with volumes is actually slower. When you train for 100 epochs, with KFP data passing, the data is only downloaded/mounted once. With shared volume, the data will be downloaded 100 times.</p>
","1497385",0
1756,73181820,2,72640182,2022-07-31 08:50:07,0,"<p>I think this might just be a bug in the version of KFP v2 SDK code you're using.</p>
<p>I mostly use the stable KFPv1 methods to avoid problems.</p>
<pre class=""lang-py prettyprint-override""><code>
from kfp.components import InputPath, OutputPath, create_component_from_func


def train_xgboost_model(
    project: str, 
    bq_dataset: str, 
    test_view_name: str,
    bq_location: str,
    metrics_path: OutputPath(Metrics),
    model_path: OutputPath(Model),
):
    import json
    from pathlib import Path

    metrics = {
       ...
    }
    Path(metrics_path).write_text(json.dumps(metrics))

    dump(bst, model_path)

train_xgboost_model_op = create_component_from_func(
    func=train_xgboost_model,
    packages_to_install=[&quot;google-cloud-bigquery&quot;, &quot;xgboost&quot;, &quot;pandas&quot;, &quot;sklearn&quot;, &quot;joblib&quot;, &quot;pyarrow&quot;, &quot;db_dtypes&quot;],
    base_image=&quot;python:3.9&quot;,
    output_component_file=&quot;create_xgb_model_xgboost.yaml&quot;,
)

</code></pre>
<p>You can also find many examples of real-world components in this repo: <a href=""https://github.com/Ark-kun/pipeline_components/tree/master/components"" rel=""nofollow noreferrer"">https://github.com/Ark-kun/pipeline_components/tree/master/components</a></p>
<p>including an XGBoost trainer <a href=""https://github.com/Ark-kun/pipeline_components/blob/d8c4cf5/components/XGBoost/Train/component.py"" rel=""nofollow noreferrer"">https://github.com/Ark-kun/pipeline_components/blob/d8c4cf5/components/XGBoost/Train/component.py</a></p>
<p>and a full XGBoost pipeline: <a href=""https://github.com/Ark-kun/pipeline_components/blob/4f19be6f26eaaf85ba251110d10d103b17e54a17/samples/Google_Cloud_Vertex_AI/Train_tabular_regression_model_using_XGBoost_and_import_to_Vertex_AI/pipeline.py"" rel=""nofollow noreferrer"">https://github.com/Ark-kun/pipeline_components/blob/4f19be6f26eaaf85ba251110d10d103b17e54a17/samples/Google_Cloud_Vertex_AI/Train_tabular_regression_model_using_XGBoost_and_import_to_Vertex_AI/pipeline.py</a></p>
","1497385",0
1757,73188377,2,73185685,2022-08-01 02:41:22,0,"<p>Only <strong>tabular data</strong> can be ingested in Import data component.</p>
<p><strong>Workaround:</strong></p>
<p>You can use Azure data factory to convert json to structured data. Then you can try to import data in azure ml studio’s import data component.</p>
<p>There are following rules while importing data</p>
<p><strong>a.</strong>  All data files must have the same schema(columns and column names)</p>
<p><strong>b.</strong>   To get all files in folder given path should be folder_name/**</p>
<p><strong>c.</strong>  Data files must be encoded in unicode-8.</p>
<p>When import completes, right-click the output dataset and select Visualize to see if the data was imported successfully.</p>
","11104805",0
1758,73189283,2,73187536,2022-08-01 05:50:06,1,"<p>If we are using any <strong>extensions of SDK or Azure CLI</strong> for machine learning to detach AKS cluster, it <strong>will not work</strong> and it will not get deleted or detached. Instead, we need to use <strong>Azure CLI with AKS</strong>. There are two types of implementations we can perform.</p>
<p><strong>Python:</strong></p>
<pre><code>Aks_target.detach()
</code></pre>
<p><strong>Azure CLI:</strong></p>
<p>Before performing this step, we need to get the details of the working AKS cluster name attached to our workspace. Resource Group details and workspace name</p>
<pre><code>az ml computertarget detach -n youraksname -g yourresourcegroup -w yourworkspacename
</code></pre>
","18428148",0
1759,73189509,2,73130723,2022-08-01 06:18:41,1,"<p>The version of <strong>Matplotlib</strong> will depend on the version of python. There is no support for the latest version of matplotlib library for <strong>python version 3.8</strong>. The latest version of matplotlib can be available only up to <strong>3.7 version</strong> of python. Even though we tried to upgrade it with <strong>pip, conda</strong> , by default <strong>version of python program</strong>, it will take the supporting version of matplotlib.</p>
<p>For the current version of matplotlib in the current case of the situation, it is <strong>3.2</strong>. It is suggestable to check the version of python, whether it is 3.6 or not. <strong>Python version 3.6 supports 3.2 and 3.1 versions of matplotlib</strong>.</p>
<p>If still the python version is showing the latest version of python, try to <strong>degrade</strong> the version and check.</p>
","18428148",0
1760,73193626,2,73182806,2022-08-01 12:24:47,0,"<p>if anyone encounters the condition in which the kernel is showing 'busy' and your can't run any code, just uninstall this kerenel and reinstall one.</p>
<p>Using 'conda'-related command about which you can easily find information on the Internet</p>
","19584929",1
1761,73194611,2,73192053,2022-08-01 13:39:44,1,"<p>One of the workaround you can follow to resolve the above issue;</p>
<p>Based on this <a href=""https://github.com/Azure/azure-cli/issues/21390#issuecomment-1161782243"" rel=""nofollow noreferrer""><em><strong>GitHub issue</strong></em></a> as suggested by @<em>adba-msft</em> .</p>
<blockquote>
<p><strong>Please make sure that you have upgraded your azure cli to latest and</strong>
<strong>Azure CLI ML extension v2 is being used.</strong></p>
</blockquote>
<p>To check and upgrade the cli we can use the below <code>cmdlts</code>:</p>
<pre><code>az version

az upgrade
</code></pre>
<p><a href=""https://i.stack.imgur.com/Uopde.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Uopde.png"" alt=""enter image description here"" /></a></p>
<p>For more information please refer this similar <a href=""https://stackoverflow.com/questions/73110661/create-is-misspelled-or-not-recognized-by-the-system-on-az-ml-dataset-create""><em><strong>SO THREAD|'create' is misspelled or not recognized by the system on az ml dataset create</strong></em></a> .</p>
<p>I did observe the same issue after trying the aforementioned suggestion by @<em>Dor Lugasi-Gal</em> it works for me with (in my case <code>az ml -h</code>) after installed the extension with  <code>az extension add -n ml -y</code> can able to get the result of <code>az ml -h</code> without any error.</p>
<p><em><strong>SCREENSHOT FOR REFERENCE:-</strong></em></p>
<p><a href=""https://i.stack.imgur.com/39LHa.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/39LHa.png"" alt=""enter image description here"" /></a></p>
","15969165",0
1762,73205742,2,71321757,2022-08-02 10:10:13,0,"<p>To get the package from a Azure DevOps git repository you can change the target to the repository URL:</p>
<pre><code>ws.set_connection(
    name=&quot;ConnectionName&quot;, 
    category = &quot;PythonFeed&quot;,
    target = &quot;https://dev.azure.com/&lt;MY-ORG&gt;/&lt;MY-PROJECT&gt;/_git/&lt;MY-REPO&gt;&quot;, 
    authType = &quot;PAT&quot;, 
    value = &lt;PAT-TOKEN&gt;)
</code></pre>
<p>Note here that there is no user specified in the URL (the standard &quot;clone&quot; URL in Azure DevOps also contains &quot;DevOps-Vx@&quot;).</p>
<p>As for any other options for &quot;authType&quot;, &quot;category&quot; and &quot;valueFormat&quot;, I don't know.</p>
","18353193",0
1763,73208961,2,73208208,2022-08-02 14:04:44,0,"<p>you mean this:</p>
<pre><code>import pandas as pd

a = a.decode(encoding=&quot;utf-8&quot;).split(&quot;\n&quot;)

df = pd.DataFrame(data=a)
df.head()
</code></pre>
","13473914",0
1764,73217824,2,73216926,2022-08-03 07:29:00,1,"<p>I was able to solve this by the following steps</p>
<pre><code>model_data=output_path
from sagemaker.pytorch.model import PyTorchModel 

pytorch_model = PyTorchModel(model_data=model_data,
                             role=role,
                             framework_version=&quot;1.3.1&quot;,
                             source_dir=&quot;code&quot;,
                             py_version=&quot;py3&quot;,
                             entry_point=&quot;train_deploy.py&quot;)

predictor = pytorch_model.deploy(initial_instance_count=1, instance_type=&quot;ml.m4.2xlarge&quot;)
predictor.serializer = sagemaker.serializers.JSONSerializer()
predictor.deserializer = sagemaker.deserializers.JSONDeserializer()
result = predictor.predict(&quot;&lt;text that needs to be predicted&gt;&quot;)
print(&quot;predicted class: &quot;, np.argmax(result, axis=1))
</code></pre>
","2351492",1
1765,73221065,2,73200116,2022-08-03 11:36:34,0,"<p>Recently I have completed similar use case and here's my answers -</p>
<p><strong>Q1  : Is there need of retraining every week?</strong></p>
<p>Ans : Yes, you need to do continuous training and continuous forecasting steps (tie using sagemaker pipeline) in prod to make it work perfectly automated for stable MAE, MAPE etc.</p>
<p><strong>Q2  : How can I pass new data and forecast for next week? How to get input data from mongodb?</strong></p>
<p>Ans : You could use Lambda, or Glue job (designed for ETL so better) to drop in S3 bucket. This will could become input raw data bucket for sagemaker pipeline.</p>
<p><strong>Q3  : Can I write the predictions back into mongodb in a new table, or are they saved somewhere else first and this would have to be another lambda function?</strong></p>
<p>Ans : Yes you can, both ways.</p>
<p>I would suggest to start small i.e. First drop a csv file to s3 location in say YYMMDD folder. Use this as input and develop completely in one notebook (continuous train, continuous forecast).</p>
<p>Later, learn about pipelines - how to write different steps, pass objects between steps etc and go modify your code to fit in pipeline.</p>
<p>Create a sagemaker pipeline with steps : (Refer links below )</p>
<ol>
<li>Preprocess ( any transformations, cleansing )</li>
<li>Training ( use prebuilt image or need to build one, depends)</li>
<li>Forecast ( either do batch transform or deploy to endpoint and later delete )</li>
<li>Post processing ( if required )</li>
</ol>
<p>Take the output the from sagemaker pipelines to mongodb. <strong>Sagemaker Pipelines help automate scheduled execution using AWS Event Bridge</strong></p>
<p>Some references :</p>
<p><a href=""https://github.com/aws/amazon-sagemaker-examples/blob/main/sagemaker-pipelines/tabular/abalone_build_train_deploy/sagemaker-pipelines-preprocess-train-evaluate-batch-transform.ipynb"" rel=""nofollow noreferrer"">Example Pipelines</a> Look here in to know about pipelines</p>
<p><a href=""https://github.com/aws-samples/amazon-sagemaker-forecasting-air-pollution-with-deepar/blob/5e29057a8c9d7f8db6cb1143f7838b6614f44ef1/01_train_and_evaluate_air_quality_deepar_model.ipynb"" rel=""nofollow noreferrer"">Example1 DeepAR</a></p>
<p><a href=""https://sagemaker-examples.readthedocs.io/en/latest/introduction_to_amazon_algorithms/deepar_electricity/DeepAR-Electricity.html?highlight=deepar"" rel=""nofollow noreferrer"">Example2 DeepAR</a></p>
","7101128",0
1766,73238830,2,73232032,2022-08-04 15:50:56,3,"<p>If the Pipeline has been created, you can use the Python Boto3 SDK to make the <a href=""https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/sagemaker.html#SageMaker.Client.start_pipeline_execution"" rel=""nofollow noreferrer""><code>StartPipelineExecution</code></a> API call.</p>
<pre><code>response = client.start_pipeline_execution(
    PipelineName='string',
    PipelineExecutionDisplayName='string',
    PipelineParameters=[
        {
            'Name': 'string',
            'Value': 'string'
        },
    ],
    PipelineExecutionDescription='string',
    ClientRequestToken='string',
    ParallelismConfiguration={
        'MaxParallelExecutionSteps': 123
    }
)
</code></pre>
<p>If you prefer AWS CLI, the most basic call is:</p>
<pre><code>aws sagemaker start-pipeline-execution --pipeline-name &lt;name-of-the-pipeline&gt;
</code></pre>
","9796588",0
1767,73281215,2,73140531,2022-08-08 16:30:20,2,"<p>To anyone curious, this is how I ended up solving this issue:</p>
<p>I ran a Jupyter notebook locally to create the model artifact. Once complete, I zipped the model artifact into a tar.gz file.</p>
<pre class=""lang-py prettyprint-override""><code>from transformers import AutoModel, AutoTokenizer
from os import makedirs

saved_model_dir = 'saved_model_dir'
makedirs(saved_model_dir, exist_ok=True)

# models were obtained from https://huggingface.co/models
tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/multi-qa-MiniLM-L6-cos-v1')
model = AutoModel.from_pretrained('sentence-transformers/multi-qa-MiniLM-L6-cos-v1')

tokenizer.save_pretrained(saved_model_dir)
model.save_pretrained(saved_model_dir)
</code></pre>
<pre class=""lang-bash prettyprint-override""><code>cd saved_model_dir &amp;&amp; tar czvf ../model.tar.gz *
</code></pre>
<p>I included a script in my pipeline to then upload that artifact to S3.</p>
<pre class=""lang-bash prettyprint-override""><code>aws s3 cp path/to/model.tar.gz s3://bucket-name/prefix
</code></pre>
<p>I also created a CloudFormation template that would stand up my SageMaker resources. The tricky part of this was finding a container image to use, and a colleague was able to point me to <a href=""https://github.com/aws/deep-learning-containers/blob/master/available_images.md"" rel=""nofollow noreferrer"">this repo</a> that contained a massive list of AWS-maintained container images for deep learning and inference. From there, I just needed to select the one that fit my needs.</p>
<pre class=""lang-yaml prettyprint-override""><code>Resources:
  SageMakerModel:
    Type: AWS::SageMaker::Model
    Properties:
      PrimaryContainer:
        Image: 763104351884.dkr.ecr.us-east-1.amazonaws.com/pytorch-inference:1.12.0-cpu-py38-ubuntu20.04-sagemaker # image resource found at https://github.com/aws/deep-learning-containers/blob/master/available_images.md
        Mode: SingleModel
        ModelDataUrl: s3://path/to/model.tar.gz
      ExecutionRole: 
      ModelName: inference-model

  SageMakerEndpointConfig:
    Type: AWS::SageMaker::EndpointConfig
    Properties:
      EndpointConfigName: endpoint-config-name
      ProductionVariants:
        - ModelName: inference-model
          InitialInstanceCount: 1
          InstanceType: ml.t2.medium
          VariantName: dev
  
  SageMakerEndpoint:
    Type: AWS::SageMaker::Endpoint
    Properties:
      EndpointName: endpoint-name
      EndpointConfigName: !GetAtt SageMakerEndpointConfig.EndpointConfigName
</code></pre>
<p>Once the PyTorch model is created locally, this solution essentially automates the process of provisioning and deploying a SageMaker endpoint for inference. If I need to switch the model, I just need to run my notebook code and it will overwrite my existing model artifact. Then I can redeploy and my pipeline will re-upload the artifact to S3, modify the existing SageMaker resources, and the solution will begin operating using the new model.</p>
<p>This may not be the most elegant solution out there, so any suggestions or pointers would be much appreciated!</p>
","14633524",0
1768,73291836,2,73282180,2022-08-09 12:29:07,1,"<p><strong>start_logging</strong> will be considered as <strong>asynchronous</strong> execution as this generates the multiple interactive run sessions. In a specific experiment, there is a chance of multiple interactive sessions, that work parallelly and there will be no scenario to be followed in sequential.</p>
<p>The individual operation can be performed and recognized based on the parameters like <strong>args</strong>  and <strong>kwargs</strong>.</p>
<p>When the start_logging is called, then an interactive run like <strong>jupyter notebook</strong> was created. The complete metrics and components which are created when the start_logging was called will be utilized. When the output directory was mentioned for each interactive run, based on the args value, the output folder will be called seamlessly.</p>
<p>The following code block will help to define the operation of start_logging</p>
<pre><code>experiment = Experiment(your_workspace, &quot;your_experiment_name&quot;)
   run = experiment.start_logging(outputs=None, snapshot_directory=&quot;.&quot;, display_name=&quot;test&quot;)
   ...
   run.log_metric(&quot;Accuracy_Value&quot;, accuracy)
   run.complete()
</code></pre>
<p>the below code block will be defining the basic syntax of start_logging</p>
<pre><code>start_logging(*args, **kwargs)
</code></pre>
","18428148",2
1769,73319039,2,73318372,2022-08-11 10:18:37,1,"<p>The compatibility break is there for the newer version of the packages based on the current version of <strong>SDK</strong>. If the current SDK version is <strong>1.13.0</strong> and above, previous versions of packages are not in working stage. The compatibility issue is raising because of support of packages from SDK for different versions. It differs from version-to-version package support from <strong>SDK</strong>.</p>
<p>Because of this we are getting Module not found,  <code>ImportError and AttributeError</code>.</p>
<p>This solution depends on the AutoML SDK training version.</p>
<ul>
<li>If you are using 1.13.0 above version of SDK, update the versions of pandas to 0.25.1 and scikit-learn to 0.22.1</li>
</ul>
<p>Using the following command in  <code>BASH</code>  to upgrade the versions.</p>
<pre><code>pip install –upgrade pandas==0.25.1

pip install –upgrade sickit-learn==0.22.1

</code></pre>
<p>The generic syntax for upgrading is:</p>
<pre><code>pip install –upgrade package_name==version

</code></pre>
<ul>
<li>If the error occurs in AutoML Configuration file, then need to upgrade that also.</li>
<li>But it is suggestable to uninstall and reinstall  <code>AutoMLConfig</code>.</li>
</ul>
<pre><code>pip uninstall azureml-train automl

</code></pre>
<p>Then reinstall using the below code,</p>
<pre><code>pip install azureml-train automl

</code></pre>
<p>If you are using windows operating system, then install  <a href=""https://docs.conda.io/en/latest/miniconda.html"" rel=""nofollow noreferrer"">Miniconda</a>.</p>
<p>If you are a linux user, then using sudo or conda syntaxes for the same operation.</p>
<p>Some of the advanced libraries of computer vision supportive like TensorFlow will be installed by default. Then we need to install them from dependencies.</p>
<blockquote>
<pre><code>azureml.core.runconfig import RunConfiguration from
azureml.core.conda_dependencies import CondaDependencies run_config =
RunConfiguration() run_config.environment.python.conda_dependencies =
CondaDependencies.create(conda_packages=['tensorflow==1.12.0']) 

</code></pre>
</blockquote>
<p><a href=""https://learn.microsoft.com/en-us/azure/machine-learning/how-to-troubleshoot-auto-ml#tensorflow"" rel=""nofollow noreferrer"">Documentation</a>  credit to @Larry Franks.</p>
","18428148",0
1770,73323199,2,73320708,2022-08-11 15:21:04,6,"<p>There are two ways to set the description.</p>
<h3>1. <code>description</code> parameter</h3>
<p>You can set a description using a markdown string for your run in <code>mlflow.start_run()</code> using <code>description</code> parameter. Here is an example.</p>
<pre class=""lang-py prettyprint-override""><code>if __name__ == &quot;__main__&quot;:
    # load dataset and other stuff

    run_description = &quot;&quot;&quot;
### Header
This is a test **Bold**, *italic*, ~~strikethrough~~ text.
[And this is an example hayperlink](http://example.com/).
    &quot;&quot;&quot;

    with mlflow.start_run(description=run_description) as run:
        # train model and other stuff
</code></pre>
<h3>2. <code>mlflow.note.content</code> tag</h3>
<p>You can set/edit run names by setting the tag with the key <code>mlflow.note.content</code>, which is what the UI (currently) does under the hood.</p>
<pre class=""lang-py prettyprint-override""><code>if __name__ == &quot;__main__&quot;:
    # load dataset and other stuff

    run_description = &quot;&quot;&quot;
### Header
This is a test **Bold**, *italic*, ~~strikethrough~~ text.
[And this is an example hayperlink](http://example.com/).
    &quot;&quot;&quot;

    tags = {
        'mlflow.note.content': run_description
    }

    with mlflow.start_run(tags=tags) as run:
        # train model and other stuff
</code></pre>
<h3>Result</h3>
<p><a href=""https://i.stack.imgur.com/4zZa9.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/4zZa9.png"" alt=""output of the given example"" /></a></p>
<hr />
<p>If you set <code>description</code> parameter and <code>mlflow.note.content</code> tag in <code>mlflow.start_run()</code>, you'll get this error.</p>
<pre><code>Description is already set via the tag mlflow.note.content in tags.
Remove the key mlflow.note.content from the tags or omit the description.
</code></pre>
","11232272",2
1771,73337454,2,73303849,2022-08-12 16:50:34,0,"<p>Not at the moment. Model Monitoring is tied to SageMaker RealTime endpoints.</p>
","6411548",1
1772,73344800,2,73329904,2022-08-13 13:44:33,0,"<p><strong>Summary</strong></p>
<ul>
<li><p><a href=""https://v0-2.kubeflow.org/docs/started/getting-started-minikube/"" rel=""nofollow noreferrer"">This document</a> is deprecated. I realized the version of this site was <code>v0-2</code>.</p>
</li>
<li><p>I followed <a href=""https://qiita.com/ozora/items/961f647470869705bcb5"" rel=""nofollow noreferrer"">this Japanese document</a>.
<code>hack/setup-kubeflow.sh</code> is an installation tool. This is not mentioned in the <code>kubeflow/manifests</code> repository. This was the breakthrough for me.</p>
</li>
<li><p>I read <a href=""https://github.com/kubeflow/manifests/tree/master"" rel=""nofollow noreferrer"">this document</a> carefully. And I found the there was prerequisite for compatibility and tools I installed did not follow the requirements.</p>
</li>
<li><p>Succeeded versions and branch I tried.</p>
</li>
</ul>
<p>Ubuntu 20.0.4
minikube <em>v1.26.1</em><br />
kustomize <em>v3.2.0</em><br />
kubectl <em>v1.21.14</em><br />
kubeflow/manifests <em>v1.6-branch</em></p>
","14855070",0
1773,73349255,2,73348576,2022-08-14 05:11:32,0,"<p>I solved the issue by myself.<br />
The reason could be the allocated resources.<br />
I started kubernetes server locally again by this command:</p>
<pre class=""lang-bash prettyprint-override""><code>minikube start \
--cpus 4 --memory 20G --disk-size 50G \
--kubernetes-version v1.21.14 \
--driver=kvm2
</code></pre>
","14855070",0
1774,73351494,2,73349291,2022-08-14 12:07:09,0,"<p>I could solve the additional issue by myself.</p>
<p><code>kubectl get svc -n  istio-system</code> shows ip address of <code>istio-ingressgateway</code>.<br />
I used it then I can get 200 response and could run notebook successfully.</p>
","14855070",0
1775,73368633,2,73334915,2022-08-16 03:55:21,1,"<ul>
<li>The installation process which was taken up is not having the existing location path of requirements.txt. As it is showing the location of</li>
</ul>
<p>“<strong>/tmp/build/80754af9/arviz_1614019183254/work</strong>”.</p>
<ul>
<li><p>Mention the path of the requirements.txt file’s path before the file name while trying to run it.</p>
<p><strong><code>pip install -r /path/to/requirements.txt</code></strong></p>
</li>
</ul>
","18428148",2
1776,73374242,2,73292975,2022-08-16 12:46:57,0,"<p>So the issue seemed to be related to the IAM role. The default role (<code>ModelEndpoint-Role</code>) does not have access to write S3 files. It worked via the SDK since it uses another role in the sagemaker studio. I did not receive any error message about this.</p>
","3414626",0
1777,73418489,2,71077397,2022-08-19 14:42:46,2,"<p>So i finally figured it out and had it wrong all the time.</p>
<p>The file <code>/opt/ml/input/config/hyperparameters.json</code> is there. It just has slightly different content compared to a regular training-job. The params to be tuned as well as static params are contained there. As well as the metric-name.</p>
<p>So here is the structure, i hope it helps:</p>
<pre class=""lang-json prettyprint-override""><code>{
    '_tuning_objective_metric': 'your-metric', 
    'dynamic-param1': '0.3', 
    'dynamic-param2': '1',
    'static-param1': 'some-value', 
    'static-paramN': 'another-value'
}
</code></pre>
","10039517",0
1778,73418975,2,73418843,2022-08-19 15:22:10,2,"<p>Common problem (very common).  There are two systems named Graphviz, and you need both!
see <a href=""https://stackoverflow.com/questions/73040021/im-getting-this-issue-when-trying-to-run-the-code-i-found-on-github-pydot-and/73041302#73041302"">I&#39;m getting this issue when trying to run the code I found on GitHub. Pydot and graphivz are installed but still getting this error</a></p>
","12317235",0
1779,73423083,2,73415182,2022-08-19 23:22:51,0,"<ul>
<li>if you want to use a specific EC2 instance, use SageMaker</li>
<li>Pricing: SageMaker is pro-rated per-second while Glue has minimum charge amount (1min or 10min depending on versions). You should measure how much would a workload cost you on each platform</li>
<li>customization: in SageMaker Processing you can customize the execution environment, as you provide a Docker image (you could run more than Spark/Python, such as C++ or R)</li>
</ul>
","5331834",0
1780,73442465,2,73440722,2022-08-22 08:52:23,0,"<p>I managed to get it to work by changing how I call the forecast model.</p>
<p>Taking into account these variables:</p>
<ul>
<li>X : time values</li>
<li>y : target values</li>
<li>df: df[[x,y]]</li>
</ul>
<p>For a univariate series, instead of using this:</p>
<p><strong>model.forecast(x, y)</strong></p>
<p>I need to call:</p>
<p><strong>model.forecast(df, y)</strong></p>
<p>Remember that to call forecast you need to supply the arguments in a dataframe or in numpy array</p>
","9220866",0
1781,73443845,2,73431378,2022-08-22 10:44:12,0,"<p>I suspect there is an issue with string concatenation in <code>plt.imsave</code> because the environment variable <code>SM_OUTPUT_DATA_DIR</code> by default points to <code>/opt/ml/output/data</code> (that's the actual value of <code>args.output_data_dir</code>, since you don't pass this parameter) so the outcome is something like <code>/opt/ml/output/dataplot1.jpg</code>. The same happen if you use the <code>model_dir</code> in the same way. I'd rather use something like <code>os.path.join</code> like you're already doing for the model. <a href=""https://nono.ma/sagemaker-model-dir-output-dir-and-output-data-dir-parameters"" rel=""nofollow noreferrer"">here</a> a nice exaplaination about these folders and environment variables in sagemaker.</p>
","4267439",2
1782,73445520,2,73435172,2022-08-22 12:58:00,2,"<p>Good timing! A DVC-Hydra integration is in development. You can see the proposal in <a href=""https://github.com/iterative/dvc/discussions/7044#discussioncomment-3271855"" rel=""nofollow noreferrer"">https://github.com/iterative/dvc/discussions/7044#discussioncomment-3271855</a> and the development progress in <a href=""https://github.com/iterative/dvc/pull/8093"" rel=""nofollow noreferrer"">https://github.com/iterative/dvc/pull/8093</a>. This should allow you to take a Hydra config, pass your Hydra overrides via <code>dvc exp run --set-param=&lt;hydra_overrides&gt;</code>, and capture the output with DVC.</p>
","19820556",0
1783,73457679,2,73146282,2022-08-23 11:12:11,0,"<p>I think an issue with your code is that you don't provide the output as a file (try using an <code>OutputPath</code>).</p>
<p>From <a href=""https://v1-5-branch.kubeflow.org/docs/components/pipelines/sdk/output-viewer/#v1-sdk-writing-out-metadata-for-the-output-viewers"" rel=""nofollow noreferrer"">Kubeflow docs</a>:</p>
<blockquote>
<p>The component must also export a file output artifact with an artifact
name of mlpipeline-ui-metadata, or else the Kubeflow Pipelines UI will
not render the visualization.</p>
</blockquote>
<blockquote>
<p>...
If the component writes such a file to its container filesystem, the
Kubeflow Pipelines system extracts the file, and the Kubeflow
Pipelines UI uses the file to generate the specified viewer(s). The
metadata specifies where to load the artifact data from. The Kubeflow
Pipelines UI loads the data into memory and renders it.</p>
</blockquote>
<p>The <a href=""https://v1-5-branch.kubeflow.org/docs/components/pipelines/sdk/output-viewer/#confusion-matrix-1"" rel=""nofollow noreferrer"">Kubeflow docs</a> also provide an example, which is working for me:</p>
<pre><code>def confusion_matrix_viz(mlpipeline_ui_metadata_path: kfp.components.OutputPath()):
  import json
    
  metadata = {
    'outputs' : [{
      'type': 'confusion_matrix',
      'format': 'csv',
      'schema': [
        {'name': 'target', 'type': 'CATEGORY'},
        {'name': 'predicted', 'type': 'CATEGORY'},
        {'name': 'count', 'type': 'NUMBER'},
      ],
      'source': &lt;CONFUSION_MATRIX_CSV_FILE&gt;,
      # Convert vocab to string because for bealean values we want &quot;True|False&quot; to match csv data.
      'labels': list(map(str, vocab)),
    }]
  }

  with open(mlpipeline_ui_metadata_path, 'w') as metadata_file:
    json.dump(metadata, metadata_file)
</code></pre>
<p>Accordingly, you might try modifying your code like so:</p>
<pre><code>import kfp
import kfp.dsl as dsl
from kfp.components import create_component_from_func    

@create_component_from_func
def confusion_visualization(
    matrix_uri: str = 'https://raw.githubusercontent.com/kubeflow/pipelines/master/samples/core/visualization/confusion_matrix.csv',
    mlpipeline_ui_metadata_path: kfp.components.OutputPath()
):
    &quot;&quot;&quot;Provide confusion matrix csv file to visualize as metrics.&quot;&quot;&quot;
    import json

    metadata = {
        'outputs' : [{
          'type': 'confusion_matrix',
          'format': 'csv',
          'schema': [
            {'name': 'target', 'type': 'CATEGORY'},
            {'name': 'predicted', 'type': 'CATEGORY'},
            {'name': 'count', 'type': 'NUMBER'},
          ],
          'source': matrix_uri,
          'labels': ['rose', 'lily', 'iris'],
        }]
    }
    
    print('Printing the metadata')
    print(metadata)

    with open(mlpipeline_ui_metadata_path, 'w') as metadata_file:
        json.dump(metadata, metadata_file)

@dsl.pipeline(
    name='confusion-matrix-pipeline',
    description='A sample pipeline to generate Confusion Matrix for UI visualization.'
)
def confusion_matrix_pipeline():
    confusion_visualization_task = confusion_visualization('results.json')
    
    
client = kfp.Client()
client.create_run_from_pipeline_func(
    confusion_matrix_pipeline,
    arguments={}
)
</code></pre>
","2625096",0
1784,73477654,2,73383302,2022-08-24 17:54:09,2,"<p>When your endpoint comes up, <code>model_fn</code> is invoked so that your model is loaded. When you invoke the endpoint, <code>input_fn</code> is called so that your input payload is parsed, immediately after that, <code>predict_fn</code> is called so that a prediction is generated, and then <code>output_fn</code> is called to parse the prediction before returning it to the caller.</p>
","16247336",0
1785,73477678,2,73397959,2022-08-24 17:56:04,0,"<p>The connections are indicated data dependencies between steps, not only the order of execution. This is what you see AbaloneProcess connected to AbaloneEval, since the output of AbaloneProcess is used in AbaloneEval.</p>
","16247336",0
1786,73479776,2,73310895,2022-08-24 21:42:03,0,"<p>This can't be done in SageMaker Pipelines at the moment.</p>
","16247336",0
1787,73491447,2,73471486,2022-08-25 17:19:20,1,"<p>with Pandas you can save to S3 directly (<a href=""https://stackoverflow.com/a/56275519/121956"">relevant answer</a>). For example:</p>
<pre><code>import pandas as pd
df = pd.DataFrame( [ [1, 1, 1], [2, 2, 2] ], columns=['a', 'b', 'c'])
df.to_csv('s3://test-bucket2342343//tmp.csv', index=False)
</code></pre>
<p>Or, use what you currently do and delete the local files:</p>
<pre><code>import os
os.remove('train.csv')
</code></pre>
","121956",0
1788,73511077,2,73501103,2022-08-27 12:38:02,1,"<p>The bad request in MLFlow after successful running the job is because of not giving proper API permissions for the application.</p>
<p><a href=""https://i.stack.imgur.com/rP6Ja.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/rP6Ja.png"" alt=""enter image description here"" /></a></p>
<p>Search for <strong>MLFLOW</strong></p>
<p><a href=""https://i.stack.imgur.com/TGU2C.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/TGU2C.png"" alt=""enter image description here"" /></a></p>
<p><strong>Scroll down</strong></p>
<p><a href=""https://i.stack.imgur.com/s50AL.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/s50AL.png"" alt=""enter image description here"" /></a></p>
<p>Click on View API Permissions</p>
<p><a href=""https://i.stack.imgur.com/f7Txf.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/f7Txf.png"" alt=""enter image description here"" /></a></p>
<p>Under API permissions, assign the permissions according to the application running region and requirements. Checkout the <a href=""https://learn.microsoft.com/en-us/azure/machine-learning/how-to-manage-models-mlflow"" rel=""nofollow noreferrer"">document</a> for further information.</p>
","18428148",0
1789,73530903,2,73520188,2022-08-29 15:11:22,2,"<p>What is your client side code where you are invoking the endpoint? You should also be properly serializing the data on the client side and handling it in your inference script. Example:</p>
<pre><code>import json
data = json.loads(json.dumps(request_body))
payload = json.dumps(data)
response = client.invoke_endpoint(
    EndpointName=endpoint_name,
    ContentType=content_type,
    Body=payload)
result = json.loads(response['Body'].read().decode())['Output']
result
</code></pre>
<p>Make sure to also specify your content_type appropriately &quot;application/jsonlines&quot;.</p>
","16504640",2
1790,73530957,2,73494998,2022-08-29 15:15:45,1,"<p>Whichever application you have that is taking in your input payload, you want to configure that payload in a format that your model will understand. For example the JSON you have you will want to serialize and format properly for ingestion. An example of sending a request to API GW using plain JavaScript:</p>
<pre><code>var requestData = document.getElementById(&quot;textInput&quot;).value;
    console.log(requestData);
    fetch('https://c24ge3u77j.execute-api.us-east-1.amazonaws.com/prod/helloworld',{
        method: 'POST',
        body: JSON.stringify
        ({text: requestData}),
        headers: {
            'Content-type': 'application/json; charset=UTF-8'
        }
    })
</code></pre>
<p>As long as the payload is something your model can infer upon you can simply pass in the JSON as is.</p>
","16504640",0
1791,73566709,2,73565648,2022-09-01 08:59:16,3,"<p>The files shown are completely untracked. They are shown in both SCM trees so you can add them to either Git or DVC using inline actions.
Once the files are tracked by one of the tools they should only show up under the appropriate tree.</p>
","19894323",2
1792,73570927,2,73567221,2022-09-01 14:19:38,1,"<p>When you open the file, a file object is created. It has a tiny memory footprint. The dataset values aren't read into memory until you access them.</p>
<p>You are returning <code>data</code> as a NumPy array. That loads the entire dataset into memory. (NOTE: the <code>.get()</code> method you are using is deprecated. Current syntax is provided in the example.)</p>
<p>As an alternative to returning an array, you can create a dataset object (which also has a small memory foorprint). When you do, the data is read into memory as you need it. Dataset objects behave like NumPy arrays. (Use of a dataset object vs NumPy array depends on downstream usage. Frequently you don't need an array, but sometimes they are required.) Also, if chunked I/O was enabled when the dataset was created, datasets are read in chunks.</p>
<p>Differences shown below. Note, I used Python's file context manager to open the file. It avoids problems if the file isn't closed properly (you forget or the program exits prematurely).</p>
<pre><code>dataset_path_in_h5=&quot;/Mode1/SingleFault/SimulationCompleted/IDV2/Mode1_IDVInfo_2_100/Run1/processdata&quot;
s3 = s3fs.S3FileSystem()
with h5py.File(s3.open(s3url,'rb'), 'r') as h5_file:
     # your way to get a numpy array -- .get() is depreciated:
     data = h5_file.get(dataset_path_in_h5)
     # this is the preferred syntax to return an array:
     data_arr = h5_file[dataset_path_in_h5][()]
     # this returns a h5py dataset object:
     data_ds = h5_file[dataset_path_in_h5]  # deleted [()] 
</code></pre>
","10462884",0
1793,73579937,2,70553701,2022-09-02 08:41:26,4,"<p>You fix this by:</p>
<p><code>!pip uninstall opencv-python -y</code></p>
<p>Then use the <strong>headless</strong> version instead with studio:</p>
<p><code>!pip install opencv-python-headless</code></p>
<p>Ref: <a href=""https://github.com/aws/amazon-sagemaker-examples/issues/1773"" rel=""nofollow noreferrer"">https://github.com/aws/amazon-sagemaker-examples/issues/1773</a></p>
","758542",0
1794,73581284,2,73580703,2022-09-02 10:34:04,1,"<h3>Solution 1</h3>
<p><code>.py</code> files weren't running as scripts.</p>
<p>They need to be; if you want to run one <code>.py</code> file per <code>stage</code> in <code>dvc.yaml</code>.</p>
<p>To do so, you want to append <strong>Boiler-plate code</strong>, at the bottom of each <code>.py</code> file.</p>
<pre><code>if __name__ == &quot;__main__&quot;:
    # invoke primary function() in .py file, w/ params
</code></pre>
<h3>Solution 2</h3>
<pre><code>chmod 777 ....py
</code></pre>
<h3>Soution 3</h3>
<p>I forgot the <code>python</code> in <code>cmd:</code></p>
<pre><code>  load_data:
    cmd: python pdl1_lung_model/load_data.py
</code></pre>
","16852041",0
1795,73595062,2,73592834,2022-09-03 19:51:55,0,"<p>Instead of recreating the History object, what I did was read the .log file using pandas package, <code>read_csv</code> method, and create a DataFrame data structure with the wanted columns and plot it. Code below:</p>
<pre><code>history = pd.read_csv('history.log')
history_acc = pd.DataFrame(history, columns=[&quot;accuracy&quot;, &quot;val_accuracy&quot;])
history_loss = pd.DataFrame(history, columns=[&quot;loss&quot;, &quot;val_loss&quot;])
plot_accuracy(history_acc,'plot title...')
plot_loss(history_loss,'plot title...')
</code></pre>
<br>
<pre><code>def plot_accuracy(history,title):
    plt.title(title)
    plt.plot(history)
    plt.ylabel('accuracy')
    plt.xlabel('epoch')
    plt.legend(['train_accuracy', 'validation_accuracy'], loc='best')
    plt.show()
def plot_loss(history,title):
    plt.title(title)
    plt.plot(history)
    plt.ylabel('loss')
    plt.xlabel('epoch')
    plt.legend(['train_loss', 'validation_loss'], loc='best')
    plt.show()
</code></pre>
<p>Hope this helps someone having the same issue as I did in the future.</p>
","10541761",0
1796,73612166,2,73611956,2022-09-05 16:14:09,1,"<p>One option you have is to use the <a href=""https://www.terraform.io/language/meta-arguments/lifecycle"" rel=""nofollow noreferrer"">lifecycle meta argument</a> to ignore out-of-band changes to the resource.</p>
<pre><code>  lifecycle {
    ignore_changes = [
      default_user_settings
    ]
  }
</code></pre>
","2081835",0
1797,73626029,2,73326411,2022-09-06 17:52:33,0,"<p>The problem was it was still processing. Thee job lib being produced was about 27GB that's why it looked like it was stuck but actually, it was writing files.</p>
","5368122",0
1798,73628463,2,73592371,2022-09-06 22:47:48,1,"<p>Im not sure I understand the comparison to a Tuning Job.</p>
<p>Based on what you have described, in this case the <code>preprocessing.py</code> is actually stored locally. The SageMaker SDK will upload it to S3 for the remote Processing Job to access it. I suggest launching the Job and then taking a look at the inputs in the SageMaker Console.</p>
<p>If you wanted to test the Processing Job locally you can do so using <a href=""https://sagemaker.readthedocs.io/en/stable/overview.html#local-mode"" rel=""nofollow noreferrer"">Local Mode</a>. This will basically imitate the Job locally which aids in debugging the script before kicking off a remote Processing Job. Kindly note docker is required to make use of Local Mode.</p>
<p>Example code for local mode:</p>
<pre><code>from sagemaker.local import LocalSession
from sagemaker.processing import ScriptProcessor, ProcessingInput, ProcessingOutput

sagemaker_session = LocalSession()
sagemaker_session.config = {'local': {'local_code': True}}

# For local training a dummy role will be sufficient
role = 'arn:aws:iam::111111111111:role/service-role/AmazonSageMaker-ExecutionRole-20200101T000001'

processor = ScriptProcessor(command=['python3'],
                    image_uri='sagemaker-scikit-learn-processing-local',
                    role=role,
                    instance_count=1,
                    instance_type='local')

processor.run(code='processing_script.py',
                    inputs=[ProcessingInput(
                        source='./input_data/',
                        destination='/opt/ml/processing/input_data/')],
                    outputs=[ProcessingOutput(
                        output_name='word_count_data',
                        source='/opt/ml/processing/processed_data/')],
                    arguments=['job-type', 'word-count']
                    )

preprocessing_job_description = processor.jobs[-1].describe()
output_config = preprocessing_job_description['ProcessingOutputConfig']

print(output_config)

for output in output_config['Outputs']:
    if output['OutputName'] == 'word_count_data':
        word_count_data_file = output['S3Output']['S3Uri']

print('Output file is located on: {}'.format(word_count_data_file))


</code></pre>
","9796588",4
1799,73628568,2,73595234,2022-09-06 23:10:50,0,"<p>The reason you do not have the option of <code>source_dir</code> is due to the fact that you are now trying to deploy the model using boto3 instead of using the SageMaker Python SDK which you used initially.</p>
<p>You can deploy your model to an Async Endpoint using the SDK as you did previously. Only difference is you need an <code>AsyncInferenceConfig</code>.</p>
<p>You can use something like:</p>
<pre><code>from sagemaker.tensorflow.serving import Model

tensorflow_serving_model = Model(model_data=model_artifact,
                                 entry_point = 'inference.py',
                                 source_dir = 'code',
                                 role=role,
                                 framework_version='2.3',
                                 sagemaker_session=sagemaker_session)


</code></pre>
<pre><code>from sagemaker.async_inference.async_inference_config import AsyncInferenceConfig

async_config = AsyncInferenceConfig(
    output_path=f&quot;s3://{s3_bucket}/{bucket_prefix}/output&quot;,
    max_concurrent_invocations_per_instance=4,
    # Optionally specify Amazon SNS topics
    # notification_config = {
    # &quot;SuccessTopic&quot;: &quot;arn:aws:sns:&lt;aws-region&gt;:&lt;account-id&gt;:&lt;topic-name&gt;&quot;,
    # &quot;ErrorTopic&quot;: &quot;arn:aws:sns:&lt;aws-region&gt;:&lt;account-id&gt;:&lt;topic-name&gt;&quot;,
    # }
)
</code></pre>
<pre><code>endpoint_name = resource_name.format(&quot;Endpoint&quot;, datetime.now().strftime(&quot;%Y-%m-%d-%H-%M-%S&quot;))

async_predictor = tensorflow_serving_model.deploy(
    async_inference_config=async_config,
    instance_type=&quot;ml.m5.xlarge&quot;,
    initial_instance_count=1,
    endpoint_name=endpoint_name,
)
</code></pre>
","9796588",0
1800,73631901,2,71425842,2022-09-07 07:53:48,0,"<p>Answering my own question.</p>
<p>I ended up using Sagemaker Processing jobs for this. As initially suggested by the other answer. I found this library developed a few months ago: <a href=""https://github.com/aws-samples/sagemaker-run-notebook"" rel=""nofollow noreferrer"">Sagemaker run notebook</a>, which helped still keep my notebook structure and cells as I had them, and be able to run it using Sagemaker run notebook using a bigger instance, and modifying the notebook in a smaller one.</p>
<p>The output of each cell was saved, along the plots I had, in S3 as a jupyter notebook.</p>
<p>I see that no constant support is given to the library, but you can fork it and make changes to it, and use it as per your requirements. For example, creating a docker container based on your needs.</p>
","12282834",1
1801,73651883,2,73651368,2022-09-08 15:48:16,-1,"<p>No, not possible yet. <a href=""https://sagemaker.readthedocs.io/en/stable/overview.html#local-mode"" rel=""nofollow noreferrer"">local mode</a> does not support the distributed training with <code>local_gpu</code>for Gzip compression, Pipe Mode, or manifest files for inputs</p>
","19947257",0
1802,73652132,2,73645084,2022-09-08 16:08:29,1,"<p>Considering the following example code for <a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/processing-job-frameworks-hugging-face.html"" rel=""nofollow noreferrer"">HuggingFaceProcessor</a>:</p>
<p>If you have 100 large files in S3 and use a ProcessingInput with <a href=""https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_ProcessingS3Input.html#:%7E:text=S3DataDistributionType"" rel=""nofollow noreferrer"">s3_data_distribution_type</a>=&quot;ShardedByS3Key&quot; (instead of FullyReplicated), the objects in your S3 prefix will be sharded and distributed to your instances.</p>
<p>For example, if you have 100 large files and want to filter records from them using HuggingFace on 5 instances, the s3_data_distribution_type=&quot;ShardedByS3Key&quot; will put 20 objects on each instance, and each instance can read the files from its own path, filter out records, and write (uniquely named) files to the output paths, and SageMaker Processing will put the filtered files in S3.</p>
<p>However, if your filtering criteria is stateful or depends on doing a full pass over the dataset first (such as: filtering outliers based on mean and standard deviation on a feature - in case of using SKLean Processor for example): you'll need to pass that information in to the job so each instance can know how to filter. To send information to the instances launched, you have to use the <code>/opt/ml/config/resourceconfig.json</code> <a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/build-your-own-processing-container.html#byoc-config"" rel=""nofollow noreferrer"">file</a>:</p>
<p><code>{ &quot;current_host&quot;: &quot;algo-1&quot;, &quot;hosts&quot;: [&quot;algo-1&quot;,&quot;algo-2&quot;,&quot;algo-3&quot;] }</code></p>
","4116247",0
1803,73652628,2,73651050,2022-09-08 16:54:50,2,"<p>This happens since DVC is not using <code>MLProject</code>'s config when it clones and does <code>dvc fetch</code> in the <code>DataProject2</code> during the <code>import</code>. And it doesn't know where it can find the token (clearly, it's not in the Git repo, right?).</p>
<p>There are a few ways to specify it: <a href=""https://dvc.org/doc/command-reference/config#--system"" rel=""nofollow noreferrer""><code>global/system</code> configs</a> and/or <a href=""https://dvc.org/doc/command-reference/remote/modify#authenticate-with-environment-variables"" rel=""nofollow noreferrer"">environment variables</a>.</p>
<p>To implement the first option:</p>
<p>On a machine where you do <code>dvc import</code>, you could create a remote in the <code>--global</code>, or <code>--system</code> configs with the same name and specify the token there. Global config fields will be merged with the config in the <code>DataProject2</code> repo when DVC is pulling data to import.</p>
<pre class=""lang-bash prettyprint-override""><code>dvc remote add --global &lt;DataProject2-remote-name&gt; azure://DataProject2/storage
dvc remote modify --global &lt;DataProject2-remote-name&gt; account_name &lt;name&gt;
dvc remote modify --global &lt;DataProject2-remote-name&gt; sas_token &lt;token&gt;
</code></pre>
<p>The second option:</p>
<pre class=""lang-bash prettyprint-override""><code>export AZURE_STORAGE_SAS_TOKEN='mysecret'
export AZURE_STORAGE_ACCOUNT='myaccount'
</code></pre>
<p>Please give it a try, let me know if that works or not.</p>
","298182",2
1804,73653570,2,69538469,2022-09-08 18:24:57,0,"<p>I've been passing environment variables along with the Docker image URL when creating the Training job using the SageMaker Python SDK. Documentation of the <code>train</code> method states that:</p>
<pre><code>environment (dict[str, str]) : Environment variables to be set for
            use during training job (default: ``None``): 
</code></pre>
<p>For reference, the <a href=""https://github.com/aws/sagemaker-python-sdk/blob/5bc3ccf/src/sagemaker/session.py#L569"" rel=""nofollow noreferrer"">SDK source</a>.</p>
<p>Because the SDK is a wrapper on top of <a href=""https://pypi.org/project/boto3/"" rel=""nofollow noreferrer"">Boto3</a>, I'm pretty sure that the same can be implemented with Boto3 alone, and that there is an equivalent for every other <a href=""https://aws.amazon.com/developer/tools/#SDKs"" rel=""nofollow noreferrer"">Amazon Services SDK</a>.</p>
","368544",0
1805,73655239,2,73654460,2022-09-08 21:33:47,0,"<p>this was solved by ...</p>
<pre><code>classpath = &quot;:&quot;.join(sagemaker_pyspark.classpath_jars())
conf = SparkConf() \
    .set(&quot;spark.driver.extraClassPath&quot;, classpath)
sc = SparkContext(conf=conf)
</code></pre>
","1358817",0
1806,73665076,2,73621850,2022-09-09 16:30:27,1,"<p>You can pass artifacts between processing jobs the same way you do it between Processing and Training jobs.</p>
<p>Take the output of a processing job and supply it to <code>ProcessingInput</code> of the next processing job.</p>
","16247336",0
1807,73666111,2,73646830,2022-09-09 18:17:38,1,"<p>You can find information on how the distributions are compared here (see <code>distribution_constraints</code> in the table):</p>
<p><a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/model-monitor-byoc-constraints.html"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/sagemaker/latest/dg/model-monitor-byoc-constraints.html</a></p>
<p>You can change the threshold in the constraint file to what you would like.</p>
<p>The baseline computes baseline schema constraints and statistics for each feature using Deequ. If youwould like more details you can take a look at the implementation here:</p>
<p><a href=""https://github.com/awslabs/deequ/blob/master/src/main/scala/com/amazon/deequ/analyzers/Distance.scala"" rel=""nofollow noreferrer"">https://github.com/awslabs/deequ/blob/master/src/main/scala/com/amazon/deequ/analyzers/Distance.scala</a></p>
","9796588",2
1808,73674347,2,73663585,2022-09-10 18:36:59,1,"<p>setting up training in VPC including specifying security groups is documented here: <a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/train-vpc.html#train-vpc-groups"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/sagemaker/latest/dg/train-vpc.html#train-vpc-groups</a></p>
<p>Normally you would allow all communication between the training nodes. To do this you specify the security group source and destination to the name of the security group itself, and allow all IPv4 traffic. If you want to figure out what ports are used, you could: 1/ define the permissive security group. 2/ Turn on VPC flow logs 3/ run training. 4/ examine VPC Flow logs 5/ update the security group only to the required ports.</p>
<p>I must say restricting communication between the training nodes might be an extreme, so I would challenge the customer why it's really needed, as all nodes carry the same job, have the same IAM role, and are transiate by nature.</p>
","8754522",0
1809,73708031,2,73676684,2022-09-13 18:56:21,1,"<p>SageMaker Studio does not support Docker, since the Studio apps are containers themselves. You can use the SageMaker Docker Build tool to build docker images from Studio (uses CodeBuild in the backend). See the blog <a href=""https://aws.amazon.com/blogs/machine-learning/using-the-amazon-sagemaker-studio-image-build-cli-to-build-container-images-from-your-studio-notebooks/"" rel=""nofollow noreferrer"">Using the Amazon SageMaker Studio Image Build CLI to build container images from your Studio notebooks</a> and the <a href=""https://github.com/aws-samples/sagemaker-studio-image-build-cli"" rel=""nofollow noreferrer"">Github repo</a> for details.</p>
","2458691",0
1810,73712101,2,73685446,2022-09-14 05:53:44,1,"<p>To install <code>tesseract</code> in SageMaker you can simply follow the instructions here: <a href=""https://tesseract-ocr.github.io/tessdoc/Compiling.html#linux"" rel=""nofollow noreferrer"">https://tesseract-ocr.github.io/tessdoc/Compiling.html#linux</a></p>
","9695060",0
1811,73715029,2,73700203,2022-09-14 10:05:11,0,"<p>In my case, the problem was in <code>dvc.yaml</code>.</p>
<p>For a few <code>stages</code>, I had cyclical dependencies, where a file-path was mentioned in both the <code>deps</code> and <code>outs</code>.</p>
","16852041",0
1812,73716368,2,73702976,2022-09-14 11:50:02,0,"<p>We can download the output of the model in a repository and make them as the source file for the later steps as required. The below code block can be incorporated in the same pipeline which is being used now.</p>
<p>To download the model output:</p>
<pre><code>train_step = pipeline_run1.find_step_run('train.py')

if train_step:
    train_step_obj = train_step[0] 
    train_step_obj.get_output_data('processed_data1').download(&quot;./outputs&quot;) # download the output to current directory
</code></pre>
<p>after downloading the model, then use that as the parent source directory in source_directory</p>
<pre><code>from azureml.core import ScriptRunConfig, Experiment
   # create or load an experiment
   experiment = Experiment(workspace, 'MyExperiment')
   # create or retrieve a compute target
   cluster = workspace.compute_targets['MyCluster']
   # create or retrieve an environment
   env = Environment.get(ws, name='MyEnvironment')
   # configure and submit your training run
   config = ScriptRunConfig(source_directory='.',
                            command=['python', 'train.py'],
                            compute_target=cluster,
                            environment=env)
   script_run = experiment.submit(config)
</code></pre>
","18428148",4
1813,73724158,2,73676483,2022-09-14 23:27:55,0,"<p>Yeah you should be able to use both CPU's and GPU's with Horovod on Amazon SageMaker. Please follow the below example for the same</p>
<p><a href=""https://github.com/aws/amazon-sagemaker-examples/blob/main/sagemaker-python-sdk/tensorflow_script_mode_horovod/tensorflow_script_mode_horovod.ipynb"" rel=""nofollow noreferrer"">https://github.com/aws/amazon-sagemaker-examples/blob/main/sagemaker-python-sdk/tensorflow_script_mode_horovod/tensorflow_script_mode_horovod.ipynb</a></p>
","19490330",0
1814,73724240,2,73674278,2022-09-14 23:50:08,1,"<p>Yes SageMaker XGBoost supports distributed training. If you set instance count &gt; 1, SageMaker XGBoost will distribute the files from S3 to individual instances and perform distributed training. This, however, requires number of files on S3 &gt;= number of instances. Otherwise, you will be charged for using two training instances without the benefit of using distributed training.</p>
<p>You can find an example here</p>
<p><a href=""https://github.com/aws/amazon-sagemaker-examples/blob/main/introduction_to_amazon_algorithms/xgboost_abalone/xgboost_abalone_dist_script_mode.ipynb"" rel=""nofollow noreferrer"">https://github.com/aws/amazon-sagemaker-examples/blob/main/introduction_to_amazon_algorithms/xgboost_abalone/xgboost_abalone_dist_script_mode.ipynb</a></p>
","19490330",1
1815,73726514,2,73661090,2022-09-15 06:29:02,0,"<p>This permission denied on resource issue can be resolved by using import statement:</p>
<pre><code>from google.cloud import aiplatform_v1 as aiplatform
</code></pre>
","15747414",0
1816,73736143,2,73713177,2022-09-15 18:56:30,0,"<p>Yes you can run the model monitoring job manually by configuring the process job with appropriate parameters. Please find an example in the reference <a href=""https://github.com/aws-samples/amazon-sagemaker-mlops-workshop/blob/main/labs/05_model_monitor/monitoringjob_utils.py"" rel=""nofollow noreferrer"">here</a></p>
","6411548",1
1817,73740171,2,73732523,2022-09-16 05:20:24,1,"<p>Auto ML can stratify the data when performing cross-validation. The following procedure needs to be followed to perform cross-validation</p>
<p><a href=""https://i.stack.imgur.com/mXJsi.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/mXJsi.png"" alt=""enter image description here"" /></a></p>
<p>Create the workspace resource.</p>
<p><a href=""https://i.stack.imgur.com/U7nTZ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/U7nTZ.png"" alt=""enter image description here"" /></a></p>
<p>After giving all the details, click on create</p>
<p><a href=""https://i.stack.imgur.com/6L0kr.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/6L0kr.png"" alt=""enter image description here"" /></a></p>
<p>Launch the Studio and go to AutoML and click on New Automated ML job</p>
<p><a href=""https://i.stack.imgur.com/pWeRj.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/pWeRj.png"" alt=""enter image description here"" /></a></p>
<p>Upload the dataset from here and give the basic details required.</p>
<p><a href=""https://i.stack.imgur.com/T3TIN.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/T3TIN.png"" alt=""enter image description here"" /></a></p>
<p>Dataset uploaded with some basic categories</p>
<p><a href=""https://i.stack.imgur.com/74wW8.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/74wW8.png"" alt=""enter image description here"" /></a></p>
<p>After uploading dataset use that dataset for the prediction model performance</p>
<p><a href=""https://i.stack.imgur.com/9n9dg.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/9n9dg.png"" alt=""enter image description here"" /></a></p>
<p>Here for prediction, we can choose the k-fold cross validation for validation type and number of cross validations as 5. There is no split we are performing. The model will perform according to the validation requirements.</p>
","18428148",0
1818,73753411,2,73694705,2022-09-17 08:41:16,1,"<p>The <a href=""https://github.com/aws/sagemaker-pytorch-training-toolkit/"" rel=""nofollow noreferrer"">SageMaker PyTorch Training Toolkit</a> repository used to be the repository for the Sagemaker Pytorch Training Containers, and similarly the <a href=""https://github.com/aws/sagemaker-pytorch-inference-toolkit/"" rel=""nofollow noreferrer"">SageMaker PyTorch Inference Toolkit</a> was the repository for the SageMaker PyTorch Inference containers.</p>
<p>At some point, AWS has started to directly use the DockerFiles of the Deep Learning containers from the <a href=""https://github.com/aws/deep-learning-containers"" rel=""nofollow noreferrer"">AWS Deep Learning Containers</a> repository so the repositories above were renamed because now AWS has used them to build a library that gets installed into the DL containers to make them SageMaker-compatible for training.</p>
<p>Example: From here <a href=""https://github.com/aws/sagemaker-pytorch-training-toolkit/blob/master/setup.py"" rel=""nofollow noreferrer"">https://github.com/aws/sagemaker-pytorch-training-toolkit/blob/master/setup.py</a> Example of building a package that then gets installed in the DL container here: <a href=""https://github.com/aws/deep-learning-containers/blob/1596489c9002cea08f8a2a7d2f4642c4b3727d52/pytorch/training/docker/1.6.0/py3/Dockerfile.cpu#L112"" rel=""nofollow noreferrer"">https://github.com/aws/deep-learning-containers/blob/1596489c9002cea08f8a2a7d2f4642c4b3727d52/pytorch/training/docker/1.6.0/py3/Dockerfile.cpu#L112</a></p>
","3203213",0
1819,73759159,2,73731665,2022-09-17 23:24:54,1,"<p>Yes on all nitro-backed instances EBS volumes that are exposed as NVMe block devices.</p>
<p>In the Sagemaker Python SDK, you can specify the <code>volume_size</code> of the <code>SM_TRAINING_CHANNEL</code> path - the EBS (NVMe backed) will be in that path and when you go to actually run you pass the <code>--train_dir</code> path to your code.</p>
<p>Code example below:</p>
<pre><code>def main(aws_region,s3_location,instance_cout):
    estimator = TensorFlow(
        train_instance_type='ml.p3.16xlarge',
            **train_volume_size=200,**
        train_instance_count=int(instance_count),
        framework_version='2.2',
            py_version='py3',
        image_name=&quot;231748552833.dkr.ecr.%s.amazonaws.com/sage-py3-tf-hvd:latest&quot;%aws_region,
</code></pre>
<p>And then in your entry script</p>
<pre><code>train_dir = os.environ.get('SM_CHANNEL_TRAIN')
subprocess.call(['python','-W ignore', 'deep-learning-models/legacy/models/resnet/tensorflow2/train_tf2_resnet.py', \
            &quot;--data_dir=%s&quot;%train_dir, \
</code></pre>
","3203213",0
1820,73759186,2,73751712,2022-09-17 23:32:34,1,"<p>1.<code>from sagemaker.debugger import ProfilerConfig</code></p>
<p><code>profiler_config = ProfilerConfig( framework_profile_params=FrameworkProfile(start_step=1, num_steps=2) )</code></p>
<p>2.
<code>from sagemaker.debugger import TensorBoardOutputConfig</code></p>
<p><code>tensorboard_output_config = TensorBoardOutputConfig(s3_output_path= &lt;&lt; add your bucket name an folder &gt;&gt; )</code></p>
<ol start=""3"">
<li><p>In your estimator - specify :  <code>profiler_config= profiler_config</code> and <code>tensorboard_output_config=tensorboard_output_config</code></p>
</li>
<li><p>Train your model</p>
</li>
<li><p>Go to the s3 bucket specified  for your training job name that is assigned in Sagemaker . You should see a report under <strong>rule-output</strong> &gt; <strong>ProfilerReport</strong> *** &gt; <strong>profiler-output/</strong> &gt; <strong>profiler-report.html</strong></p>
</li>
</ol>
","11976344",0
1821,73773007,2,73773006,2022-09-19 11:56:33,0,"<p>It was worked when I changed the num_classes=11. Because it adds one label &quot;0&quot; by default.</p>
<p>0-&quot;BACKGROUND&quot;
1-&quot;Button_damage&quot;
2-&quot;Cracks&quot;
3-&quot;Edge_damage&quot;
4-&quot;Frame_damage&quot;
5-&quot;Hinge_damage&quot;
6-&quot;Screen_damage&quot;
7-&quot;Good_Button&quot;
8-&quot;Good_Hinge&quot;
9-&quot;Good_screen&quot;
10-&quot;Good_frame&quot;</p>
","18553059",0
1822,73780683,2,73621446,2022-09-20 01:11:58,0,"<p>I was able to solve this using the following approach,</p>
<pre><code>def log_model(model, artifact_path):
    model_class = get_model_class(model).split('.')[0]

    try:
        log_model = getattr(mlflow, model_class).log_model
        log_model(model, artifact_path)
    except AttributeError:
        logger.info('The log_model function is not available as expected!')

def get_model_class(model):
    klass = model.__class__
    module = klass.__module__

    if module == 'builtins':
        return klass.__qualname__
    return module + '.' + klass.__qualname__
</code></pre>
<p>From what I have seen, this will be able to handle most cases. The <code>get_model_class()</code> method will return the class used to develop the model and based on this, we can use the <code>getattr()</code> method to extract the relevant <code>log_model()</code> method.</p>
","9542989",0
1823,73782945,2,73760033,2022-09-20 07:20:53,1,"<p>The way the path was mentioned is not accurate. The datastore path will be different manner.
Replace the below code for the small change in the calling path.</p>
<p><a href=""https://i.stack.imgur.com/4o1WU.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/4o1WU.png"" alt=""enter image description here"" /></a></p>
<pre><code>from azureml.core import Workspace
ws = Workspace.from_config()
datastore = ws.get_default_datastore()
    
datastore.upload_files('./Users/foldername/filename.csv',
                  target_path=’your targetfolder',
                  overwrite=True)
</code></pre>
<p>We need to call all the parent folders before the folder.  <strong><code>“./”</code></strong> is the way we can call the dataset from datastore.</p>
","18428148",0
1824,73787057,2,73781018,2022-09-20 12:53:03,0,"<p>set following in the code preprocess.py solved the issue:</p>
<pre><code>os.environ['AWS_DEFAULT_REGION'] = 'us-west-2' 
</code></pre>
","6760729",0
1825,73793067,2,73677347,2022-09-20 22:00:20,0,"<p>I went through the LightGBM section of SageMaker documentation and there are no references that it supports distributed training. One of the example <a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/lightgbm.html"" rel=""nofollow noreferrer"">here</a> uses single instance type. Also looked at lightGBM documentation <a href=""https://lightgbm.readthedocs.io/en/latest/Parallel-Learning-Guide.html"" rel=""nofollow noreferrer"">here</a> . Here are the parameters that you need to specify</p>
<p>tree_learner=your_parallel_algorithm,</p>
<p>num_machines=your_num_machines,</p>
<p>Given I couldnt find any reference of above in SageMaker documentation, I assume its not supported.</p>
","20046744",0
1826,73797261,2,73720626,2022-09-21 08:01:03,2,"<p>The following is the procedure of the math operation in Azure ML designer to select the column to be implemented. The following procedure will help to give the column name as well as we can also give the index number of the column. This answer contains both the procedures.</p>
<p><a href=""https://i.stack.imgur.com/9YV8f.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/9YV8f.png"" alt=""enter image description here"" /></a></p>
<p>We can click on edit column.</p>
<p><a href=""https://i.stack.imgur.com/bAnfq.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/bAnfq.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.stack.imgur.com/cZFfF.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/cZFfF.png"" alt=""enter image description here"" /></a></p>
<p>Based on the dataset which the experiment was running, both are options are mentioned in the above screen. We can choose either of the options.</p>
<p><a href=""https://i.stack.imgur.com/PblH7.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/PblH7.png"" alt=""enter image description here"" /></a></p>
<p>To access the data, right click and go to access data and click on result_dataset</p>
<p><a href=""https://i.stack.imgur.com/8jHz5.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/8jHz5.png"" alt=""enter image description here"" /></a></p>
<p>The following page will open and click on any file mentioned in the box</p>
<p><a href=""https://i.stack.imgur.com/4jWxT.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/4jWxT.png"" alt=""enter image description here"" /></a></p>
<p>Click on download and open in the editor according to your wish.</p>
<p><a href=""https://i.stack.imgur.com/wfGPV.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/wfGPV.png"" alt=""enter image description here"" /></a></p>
<p>It looks like the above result screen.
The below screens are the designer created.</p>
<p><a href=""https://i.stack.imgur.com/bTWGR.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/bTWGR.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.stack.imgur.com/6kUAw.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/6kUAw.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.stack.imgur.com/I2Ej1.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/I2Ej1.png"" alt=""enter image description here"" /></a></p>
<p>To check the final model result. Go to evaluate model and get the results in visualization manner.</p>
","18428148",1
1827,73820535,2,73788252,2022-09-22 20:50:01,1,"<p>Neo is optimizing inference using compilation, which is different and often orthogonal to compression</p>
<ul>
<li><p><strong>compilation</strong> makes inference faster and lighter by specializing the prediction application, notably: (1) changing the environment in which the model runs, in particular replacing training frameworks by the least amount of necessary math libraries, (2) optimizing the model graph to be prediction-only and grouping together operators that can be, (3) specializing the runtime to use best the specific hardware and instructions available on a given target machine. Compilation is not supposed to change the model math, thereby doesn't change its footprint on disk</p>
</li>
<li><p><strong>compression</strong> makes inference faster by removing model weights or making them smaller (quantization). Weights can be removed by pruning (dropping weights that do not influence much results or distillation (training a small model to mimic a big model).</p>
</li>
</ul>
<p>At the time of this writing, SageMaker Neo is a managed compilation service. That being said, compilation and compression can be combined, and you can prune or distill your network before feeding it to Neo.</p>
<p>SageMaker Neo covers a large grid of hardware targets and model architectures, and consequently leverages numerous backends and optimizations. Neo internals are publicly documented in many places:</p>
<ul>
<li><p>According to <a href=""https://aws.amazon.com/blogs/machine-learning/unlock-performance-gains-with-xgboost-amazon-sagemaker-neo-and-serverless-artillery/"" rel=""nofollow noreferrer"">this blog</a>, Neo uses <a href=""https://treelite.readthedocs.io/en/latest/"" rel=""nofollow noreferrer""><strong>Treelite</strong></a> for tree models optimization (<a href=""https://mlsys.org/Conferences/doc/2018/196.pdf"" rel=""nofollow noreferrer""><em>Treelite: toolbox for decision tree deployment</em></a>, Cho et Li)</p>
</li>
<li><p>According to its <a href=""https://aws.amazon.com/sagemaker/neo/"" rel=""nofollow noreferrer"">landing page</a>, Neo uses <a href=""https://tvm.apache.org/"" rel=""nofollow noreferrer""><strong>Apache TVM</strong></a> too. TVM is the leading open-source compiler, developed by <a href=""https://tqchen.com/"" rel=""nofollow noreferrer"">Tianqi Chen</a> and the <a href=""https://github.com/dmlc"" rel=""nofollow noreferrer"">DMLC</a> community (that also co-authored <a href=""https://arxiv.org/abs/1603.02754"" rel=""nofollow noreferrer"">XGBoost</a> and <a href=""https://arxiv.org/abs/1512.01274"" rel=""nofollow noreferrer"">MXNet</a>). TVM tricks are abundantly documented in <a href=""https://arxiv.org/pdf/1802.04799.pdf"" rel=""nofollow noreferrer""><em>TVM: An Automated End-to-End Optimizing Compiler for Deep Learning</em></a> (Chen et al)</p>
</li>
<li><p>According to <a href=""https://aws.amazon.com/blogs/machine-learning/amazon-sagemaker-neo-makes-it-easier-to-get-faster-inference-for-more-ml-models-with-nvidia-tensorrt/"" rel=""nofollow noreferrer"">this blog</a>, Neo also sometimes leverages <a href=""https://developer.nvidia.com/tensorrt"" rel=""nofollow noreferrer""><strong>NVIDIA TensorRT</strong></a>, the official inference optimization stack from NVIDIA</p>
</li>
<li><p>Neo also uses a number of Amazon-developed optimization:</p>
<ul>
<li><a href=""https://arxiv.org/pdf/1907.02154.pdf"" rel=""nofollow noreferrer""><em>A Unified Optimization Approach for CNN Model Inference on Integrated GPUs</em></a> (Wang et al): <em>&quot;Our work is already deployed
in Amazon SageMaker Neo Service&quot;</em></li>
<li><a href=""https://www.usenix.org/system/files/atc19-liu-yizhi.pdf"" rel=""nofollow noreferrer""><em>Optimizing CNN Model Inference on CPUs</em></a> (Liu et al)<em>&quot;NeoCPU is used in Amazon SageMaker Neo Service&quot;</em></li>
</ul>
</li>
</ul>
","5331834",0
1828,73825605,2,73728499,2022-09-23 09:22:37,2,"<p>Since I found an answer to my own question I will post it here for those who encounter the same problem.</p>
<p>I ended up re-writing the code of my deployment script using the <code>boto3</code> SDK rather than the <code>sagemaker</code> SDK (or a mix of both as some documentation suggest which may even be outdated on the AWS website).</p>
<p>Here's the whole script that shows how to create a sagemaker model object, an endpoint configuration and an endpoint to deploy the model on for the first time. In addition, it shows how to update the endpoint with a newer model and new model artifacts like a new inference script (which was the aim of my question)</p>
<p>Here's the code to do all 3 in case you want to bring your own model and update it safely in production on sagemaker through the <code>boto3</code> API:</p>
<pre class=""lang-py prettyprint-override""><code>import boto3
import time
from datetime import datetime
from sagemaker import image_uris
from fileManager import *  # this is a local script for helper functions

# name of zipped model and zipped inference code
CODE_TAR = 'your_inference_code_and_other_artifacts.tar.gz'
MODEL_TAR = 'your_saved_xgboost_model.tar.gz'

# sagemaker params
smClient = boto3.client('sagemaker')
smRole = &lt;your_sagemaker_role&gt;
bucket = sagemaker.Session().default_bucket()

# deploy algorithm
class Deployer:

    def __init__(self, modelName, deployRetrained=False):
        self.modelName=modelName
        self.deployRetrained = deployRetrained
        self.prefix = &lt;S3_model_path_prefix&gt;
    
    def deploy(self):
        '''
        Main method to create a sagemaker model, create an endpoint configuration and deploy the model. If deployRetrained
        param is set to True, this method will update an already existing endpoint.
        '''
        # define model name and endpoint name to be used for model deployment/update
        model_name = self.modelName + &lt;any_suffix&gt;
        endpoint_config_name = self.modelName + '-%s' %datetime.now().strftime('%Y-%m-%d-%HH%M')
        endpoint_name = self.modelName
        
        # deploy model for the first time
        if not self.deployRetrained:
            print('Deploying for the first time')

            # here you should copy and zip the model dependencies that you may have (such as preprocessors, inference code, config code...)
            # mine were zipped into the file called CODE_TAR

            # upload model and model artifacts needed for inference to S3
            uploadFile(list_files=[MODEL_TAR, CODE_TAR], prefix = self.prefix)

            # create sagemaker model and endpoint configuration
            self.createSagemakerModel(model_name)
            self.createEndpointConfig(endpoint_config_name, model_name)

            # deploy model and wait while endpoint is being created
            self.createEndpoint(endpoint_name, endpoint_config_name)
            self.waitWhileCreating(endpoint_name)
        
        # update model
        else:
            print('Updating existing model')

            # upload model and model artifacts needed for inference (here the old ones are replaced)
            # make sure to make a backup in S3 if you would like to keep the older models
            # we replace the old ones and keep the same names to avoid having to recreate a sagemaker model with a different name for the update!
            uploadFile(list_files=[MODEL_TAR, CODE_TAR], prefix = self.prefix)

            # create a new endpoint config that takes the new model
            self.createEndpointConfig(endpoint_config_name, model_name)

            # update endpoint
            self.updateEndpoint(endpoint_name, endpoint_config_name)

            # wait while endpoint updates then delete outdated endpoint config once it is InService
            self.waitWhileCreating(endpoint_name)
            self.deleteOutdatedEndpointConfig(model_name, endpoint_config_name)

    def createSagemakerModel(self, model_name):
        ''' 
        Create a new sagemaker Model object with an xgboost container and an entry point for inference using boto3 API
        '''
        # Retrieve that inference image (container)
        docker_container = image_uris.retrieve(region=region, framework='xgboost', version='1.5-1')

        # Relative S3 path to pre-trained model to create S3 model URI
        model_s3_key = f'{self.prefix}/'+ MODEL_TAR

        # Combine bucket name, model file name, and relate S3 path to create S3 model URI
        model_url = f's3://{bucket}/{model_s3_key}'

        # S3 path to the necessary inference code
        code_url = f's3://{bucket}/{self.prefix}/{CODE_TAR}'
        
        # Create a sagemaker Model object with all its artifacts
        smClient.create_model(
            ModelName = model_name,
            ExecutionRoleArn = smRole,
            PrimaryContainer = {
                'Image': docker_container,
                'ModelDataUrl': model_url,
                'Environment': {
                    'SAGEMAKER_PROGRAM': 'inference.py', #inference.py is at the root of my zipped CODE_TAR
                    'SAGEMAKER_SUBMIT_DIRECTORY': code_url,
                }
            }
        )
    
    def createEndpointConfig(self, endpoint_config_name, model_name):
        ''' 
        Create an endpoint configuration (only for boto3 sdk procedure) and set production variants parameters.
        Each retraining procedure will induce a new variant name based on the endpoint configuration name.
        '''
        smClient.create_endpoint_config(
            EndpointConfigName=endpoint_config_name,
            ProductionVariants=[
                {
                    'VariantName': endpoint_config_name,
                    'ModelName': model_name,
                    'InstanceType': INSTANCE_TYPE,
                    'InitialInstanceCount': 1
                }
            ]
        )

    def createEndpoint(self, endpoint_name, endpoint_config_name):
        '''
        Deploy the model to an endpoint
        '''
        smClient.create_endpoint(
            EndpointName=endpoint_name,
            EndpointConfigName=endpoint_config_name)
    
    def deleteOutdatedEndpointConfig(self, name_check, current_endpoint_config):
        '''
        Automatically detect and delete endpoint configurations that contain a string 'name_check'. This method can be used
        after a retrain procedure to delete all previous endpoint configurations but keep the current one named 'current_endpoint_config'.
        '''
        # get a list of all available endpoint configurations
        all_configs = smClient.list_endpoint_configs()['EndpointConfigs']

        # loop over the names of endpoint configs
        names_list = []
        for config_dict in all_configs:
            endpoint_config_name = config_dict['EndpointConfigName']

            # get only endpoint configs that contain name_check in them and save names to a list
            if name_check in endpoint_config_name:
                names_list.append(endpoint_config_name)
        
        # remove the current endpoint configuration from the list (we do not want to detele this one since it is live)
        names_list.remove(current_endpoint_config)

        for name in names_list:
            try:
                smClient.delete_endpoint_config(EndpointConfigName=name)
                print('Deleted endpoint configuration for %s' %name)
            except:
                print('INFO : No endpoint configuration was found for %s' %endpoint_config_name)

    def updateEndpoint(self, endpoint_name, endpoint_config_name):
        ''' 
        Update existing endpoint with a new retrained model
        '''
        smClient.update_endpoint(
            EndpointName=endpoint_name,
            EndpointConfigName=endpoint_config_name,
            RetainAllVariantProperties=True)
    
    def waitWhileCreating(self, endpoint_name):
        ''' 
        While the endpoint is being created or updated sleep for 60 seconds.
        '''
        # wait while creating or updating endpoint
        status = smClient.describe_endpoint(EndpointName=endpoint_name)['EndpointStatus']
        print('Status: %s' %status)
        while status != 'InService' and status !='Failed':
            time.sleep(60)
            status = smClient.describe_endpoint(EndpointName=endpoint_name)['EndpointStatus']
            print('Status: %s' %status)
        
        # in case of a deployment failure raise an error
        if status == 'Failed':
            raise ValueError('Endpoint failed to deploy')

if __name__==&quot;__main__&quot;:
    deployer = Deployer('MyDeployedModel', deployRetrained=True)
    deployer.deploy()
</code></pre>
<p>Final comments :</p>
<ul>
<li><p>The sagemaker <a href=""https://docs.amazonaws.cn/en_us/sagemaker/latest/dg/realtime-endpoints-deployment.html"" rel=""nofollow noreferrer"">documentation</a> mentions all this but fails to state that you can provide an 'entry_point' to the <code>create_model</code> method as well as a 'source_dir' for inference dependencies (e.g. normalization artifacts, inference scripts and so on). It can be done as seen in <code>PrimaryContainer</code> argument.</p>
</li>
<li><p>my <code>fileManager.py</code> script just contains basic functions to make tar files, upload and download to and from my S3 paths. To simplify the class, I have not included them in.</p>
</li>
<li><p>The method <code>deleteOutdatedEndpointConfig</code> may seem like a bit of an overkill with unnecessary loops and checks, I do so because I have multiple endpoint configurations to handle and wanted to remove the ones that weren't live AND contain the string <code>name_check</code> (I do not know the exact name of the configuration since there is a datetime suffix). Feel free to simplify it or remove it all together.</p>
</li>
</ul>
<p>Hope it helps.</p>
","17804398",3
1829,73831449,2,73760705,2022-09-23 18:04:32,1,"<p>I use Comet.ml and it addresses all 3 of your points.</p>
<ol>
<li><p>With Comet, parameter tracking is as easy as calling the <code>experiment.log_parameter</code> function. You can also use the diff tool to compare two experiments by their hyper parameters and even group experiments by hyper-paremeters!</p>
</li>
<li><p>Comet has the concept of artifacts. You can upload your dataset as an artifact and version it. You can also have <a href=""https://www.comet.com/docs/v2/guides/data-management/remote-artifacts/#access-download-an-s3-or-gcs-remote-artifact-asset:%7E:text=remote%20Artifact%20Asset-,%C2%B6,-If%20the%20Artifact"" rel=""nofollow noreferrer"">remote artifacts</a>!</p>
</li>
<li><p>Comet has a feature call the <a href=""https://www.comet.com/site/blog/introducing-comets-new-image-panel/"" rel=""nofollow noreferrer"">Image Panel</a>. This allows users to visualize their model performance on the data across different experiment runs. For the object detection use case, use <code>experiment.log_image</code> to log your images in which you have you drawn your model predicted bounding box on! You will then see in the Image Panel different experiments and how they each one of them are drawing their predictions side by side</p>
</li>
</ol>
","20072493",0
1830,73833223,2,73724586,2022-09-23 21:39:23,0,"<p>The error is because of having different subscription dependencies to the workspace. The compute instances will be having different choices to make while creating. Like standard V2 version. Based on the subscription the options will be enabled. Kindly check with the dependencies available and make them to enable in your work.</p>
","19084040",1
1831,73833447,2,73829280,2022-09-23 22:14:00,4,"<p>Important notes to add here where both serving stacks differ:</p>
<p>TorchServe does not provide the Instance Groups feature that Triton does (that is, stacking many copies of the same model or even different models onto the same GPU). This is a major advantage for both realtime and batch use-cases, as the performance increase is almost proportional to the model replication count (i.e. 2 copies of the model get you almost twice the throughput and half the latency; check out a BERT benchmark of this here). Hard to match a feature that is almost like having 2+ GPU's for the price of one.
if you are deploying PyTorch DL models, odds are you often want to accelerate them with GPU's. TensorRT (TRT) is a compiler developed by NVIDIA that automatically quantizes and optimizes your model graph, which represents another huge speed up, depending on GPU architecture and model. It is understandably so probably the best way of automatically optimizing your model to run efficiently on GPU's and make good use of TensorCores. Triton has native integration to run TensorRT engines as they're called (even automatically converting your model to a TRT engine via config file), while TorchServe does not (even though you can use TRT engines with it).
There is more parity between both when it comes to other important serving features: both have dynamic batching support, you can define inference DAG's with both (not sure if the latter works with TorchServe on SageMaker without a big hassle), and both support custom code/handlers instead of just being able to serve a model's forward function.</p>
<p>Finally, MME on GPU (coming shortly) will be based on Triton, which is a valid argument for customers to get familiar with it so that they can quickly leverage this new feature for cost-optimization.</p>
<p>Bottom line I think that Triton is just as easy (if not easier) ot use, a lot more optimized/integrated for taking full advantage of the underlying hardware (and will be updated to keep being that way as newer GPU architectures are released, enabling an easy move to them), and in general blows TorchServe out of the water performance-wise when its optimization features are used in combination.</p>
","6411548",1
1832,73837113,2,73833320,2022-09-24 12:08:23,1,"<p>Creating a sample pipeline with designer with mathematical format.</p>
<p><a href=""https://i.stack.imgur.com/kCx3A.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/kCx3A.png"" alt=""enter image description here"" /></a></p>
<p>We need to create a compute instance.</p>
<p><a href=""https://i.stack.imgur.com/7bQA5.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/7bQA5.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.stack.imgur.com/NQWZV.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/NQWZV.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.stack.imgur.com/MpxPY.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/MpxPY.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.stack.imgur.com/bAfEv.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/bAfEv.png"" alt=""enter image description here"" /></a></p>
<p>Assign the compute instance and click on create</p>
<p><a href=""https://i.stack.imgur.com/wIS0d.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/wIS0d.png"" alt=""enter image description here"" /></a></p>
<p>Now the import data warning will be removed. In the same manner, we will be getting similar error in other pills too.</p>
<p><a href=""https://i.stack.imgur.com/zFK74.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/zFK74.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.stack.imgur.com/Yk5gY.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Yk5gY.png"" alt=""enter image description here"" /></a></p>
<p>Create a mathematical format. If not needed for your case, try to remove that math operation and give the remaining.</p>
<p><a href=""https://i.stack.imgur.com/uhKlv.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/uhKlv.png"" alt=""enter image description here"" /></a></p>
<p>Assign the column set. Select any option according to the requirement.</p>
<p><a href=""https://i.stack.imgur.com/mcNZe.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/mcNZe.png"" alt=""enter image description here"" /></a></p>
<p>Finally, we can find the pills which have no warning or error.</p>
","18428148",1
1833,73847130,2,73643715,2022-09-25 19:00:40,0,"<p>It looks like the reason I was not able to log my experiments to MLflow is because I have missed the configuration steps that need to be performed before this can be achieved.</p>
<p>The document given below is a good guide on what needs to be done,
<br>
<a href=""https://learn.microsoft.com/en-us/azure/databricks/applications/mlflow/access-hosted-tracking-server"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/databricks/applications/mlflow/access-hosted-tracking-server</a></p>
<p>In essence, the following steps must be performed when trying to access the MLflow tracking server from outside of Databricks,</p>
<ol>
<li>Configure your environment to access your Azure Databricks hosted
MLflow tracking server</li>
<li>Configure MLflow applications to log to Databricks</li>
<li>Configure the MLflow CLI</li>
</ol>
","9542989",0
1834,73857250,2,73646137,2022-09-26 16:37:16,1,"<p>There is currently not an integration between Serverless Inference and Neo. AWS Lambda and SageMaker Serverless Inference are separate features. Lambda is its own service and that's why it's a compatible option for the TargetDevice. SageMaker Serverless Inference is a hosting option on SageMaker that integrates with Lambda.</p>
","16504640",0
1835,73870545,2,73824524,2022-09-27 16:02:04,0,"<p>If workspace and storage are private, you need to make egress_public_network_access flag disabled. This flag is required to establish private endpoint connections from managed online deployment to your private resources. Do not forget to approve PE connections.</p>
<p>Doc for Managed Online Endpoint network isolation
<a href=""https://learn.microsoft.com/en-us/azure/machine-learning/how-to-secure-online-endpoint?tabs=model"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/machine-learning/how-to-secure-online-endpoint?tabs=model</a></p>
","20102563",3
1836,73873448,2,73865465,2022-09-27 20:25:17,0,"<p>This can be done with the <a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/model-registry.html"" rel=""nofollow noreferrer"">SageMaker Model Registry</a>. You can register the output of your training job and automate the deployment by integrating CI/CD.</p>
<p>After you've created a model group in the registry, you can use SageMaker Studio to set up a deployment pipeline for you.</p>
<p>First create a project:</p>
<p><a href=""https://i.stack.imgur.com/2YryQ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/2YryQ.png"" alt=""Create SageMaker project"" /></a></p>
<p>Select the deployment option:
<a href=""https://i.stack.imgur.com/XdBvK.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/XdBvK.png"" alt=""enter image description here"" /></a></p>
<p>This will automatically create a CodePipeline and a CodeCommit repo. The repo houses the build spec and the default <code>README.md</code> has some good info on what SageMaker has created for you. Even if you want to customize the CI/CD portion, the default SageMaker projects are a good place to start.</p>
<p><a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-projects-walkthrough.html#sagemaker-proejcts-walkthrough-create"" rel=""nofollow noreferrer"">This walkthrough</a> of a SageMaker project might be useful for you. The model registry plays nicely with SageMaker pipelines as you can simply add a step to register a trained model.</p>
","20092202",0
1837,73880942,2,73584269,2022-09-28 11:52:49,2,"<ol>
<li>The Vertex AI Model Monitoring jobs are logged as part of Cloud Logging <a href=""https://cloud.google.com/vertex-ai/docs/model-monitoring/using-model-monitoring#cloud-logging-info"" rel=""nofollow noreferrer"">1</a>.</li>
<li>You can react to those loggings using log-based alerts <a href=""https://cloud.google.com/logging/docs/alerting/log-based-alerts"" rel=""nofollow noreferrer"">2</a>. For that, you need to configure a notification channel to PubSub <a href=""https://cloud.google.com/monitoring/api/ref_v3/rest/v3/projects.notificationChannels#NotificationChannel"" rel=""nofollow noreferrer"">3</a></li>
<li>Based on those PubSub message you can trigger a Cloud Function <a href=""https://cloud.google.com/functions/docs/calling/pubsub"" rel=""nofollow noreferrer"">4</a></li>
<li>The Cloud Function can initiate the Vertex AI Pipeline run to re-train the model <a href=""https://cloud.google.com/vertex-ai/docs/reference/rest/v1/projects.locations.pipelineJobs/create"" rel=""nofollow noreferrer"">5</a></li>
</ol>
<p><a href=""https://cloud.google.com/vertex-ai/docs/model-monitoring/using-model-monitoring#cloud-logging-info"" rel=""nofollow noreferrer"">https://cloud.google.com/vertex-ai/docs/model-monitoring/using-model-monitoring#cloud-logging-info</a></p>
<p><a href=""https://cloud.google.com/logging/docs/alerting/log-based-alerts"" rel=""nofollow noreferrer"">https://cloud.google.com/logging/docs/alerting/log-based-alerts</a></p>
<p><a href=""https://cloud.google.com/monitoring/api/ref_v3/rest/v3/projects.notificationChannels#NotificationChannel"" rel=""nofollow noreferrer"">https://cloud.google.com/monitoring/api/ref_v3/rest/v3/projects.notificationChannels#NotificationChannel</a></p>
<p><a href=""https://cloud.google.com/functions/docs/calling/pubsub"" rel=""nofollow noreferrer"">https://cloud.google.com/functions/docs/calling/pubsub</a></p>
<p><a href=""https://cloud.google.com/vertex-ai/docs/reference/rest/v1/projects.locations.pipelineJobs/create"" rel=""nofollow noreferrer"">https://cloud.google.com/vertex-ai/docs/reference/rest/v1/projects.locations.pipelineJobs/create</a></p>
","19356695",0
1838,73882530,2,73721496,2022-09-28 13:44:07,0,"<p>See comment from @durga_sury above.... quoted below.</p>
<blockquote>
<p>You won't need the estimator if you already have a trained model. Any chance it's set as an async endpoint? On the UI when creating the endpoint, if you've entered in params in the Async Invocation Config section, it might be set up as an async endpoint. Otherwise, any chance you can share the code/model? Hard to debug otherwise. –
durga_sury
Sep 21 at 22:18</p>
</blockquote>
<p>Seems just adding async options makes the endpoint async! After that you will get the error if you call it for real-time prediction.</p>
","3549576",0
1839,73938436,2,73842088,2022-10-03 16:44:00,1,"<ol>
<li>SageMaker Inference Pipeline is a functionality of SageMaker hosting whereby you can create a serial inference pipeline (chain of containers) on an endpoint and/or Batch Transform Job.</li>
</ol>
<p>With regards to the <a href=""https://sagemaker-examples.readthedocs.io/en/latest/sagemaker-python-sdk/scikit_learn_inference_pipeline/Inference%20Pipeline%20with%20Scikit-learn%20and%20Linear%20Learner.html"" rel=""nofollow noreferrer"">link</a> you shared, a common pattern is to use two containers where one container hosts the Scikit-learn model which will act as the pre-processing step before passing the request onto the second container which hosts the model either on an endpoint or Batch Transform Job.</p>
<ol start=""2"">
<li><p>The <code>SKLearnProcessor</code> is used to kick off a SKLearn Processing Job. You can use the SKLearnProcessor with a processing script to process your data. As such, SKLearnProcessor cannot be used in a Serial Inference Pipeline (<code>sagemaker.pipeline.PipelineModel</code>).</p>
</li>
<li><p>As stated above <code>SKLearnProcessor</code> is designed to kick off a SageMaker Processing Job that makes use of the Scikit-learn container that can be used for data pre- or post-processing and model evaluation workloads. Kindly see this <a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/use-scikit-learn-processing-container.html"" rel=""nofollow noreferrer"">link</a> for more information.</p>
</li>
<li><p>Are you are trying to decide whether to process your data with  <code>SKLearnProcessor</code> (Processing Job) or make use of a <code>PipelineModel</code> that contains a preprocessing step in a Batch Transform Job?</p>
</li>
</ol>
<p>If so, making the decision depends on your use case. If you were to use use a Processing Job (<code>SKLearnProcessor</code>) then the Job would need be to kicked off before the Batch Transform Job. Once the Processing Job has completed you can then kick of the Batch Transform Job with the output of the Processing Job as input to the Batch Transform Job.</p>
<p>On the other hand, if you were to use Serial Inference Pipeline (<code>sagemaker.pipeline.PipelineModel</code>) then you would just need to make sure that the first container preprocesses the request to make sure it is compliant with what the model expects. This option would entail the processing being done on a request(s) basis within the Batch Transform Job itself.</p>
","9796588",1
1840,73944990,2,73942818,2022-10-04 08:23:24,1,"<p>Follow these steps:</p>
<ol>
<li>Open a support ticket to increase <code>Longest run time for a training job</code>
to 2419200 seconds (28 days). (this can't be adjusted using the service quotas in AWS Web console).</li>
<li>Using the SageMaker Python SDK, when creating an <a href=""https://sagemaker.readthedocs.io/en/stable/api/training/estimators.html#sagemaker.estimator.Estimator"" rel=""nofollow noreferrer"">Estimator</a>, set <code>max_run=2419200</code>.</li>
<li>Implement <a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/model-checkpoints.html"" rel=""nofollow noreferrer"">Resume from checkpoints</a> in your training script.</li>
</ol>
<p>Also, the questions in @rok's answer are very relevant to consider.</p>
","121956",0
1841,73962352,2,73949317,2022-10-05 14:58:17,1,"<p>You are probably referring to <a href=""https://aws.amazon.com/sagemaker/neo/"" rel=""nofollow noreferrer"">SageMaker Neo</a> which is used for inference, rather to SageMaker Training Compiler (used for training).</p>
<p>SageMaker Neo can compile the model to a target CPU architecture, AppRunner is a high level service for light containers designed with simplicity as a primary goal. In App Runner you can <a href=""https://docs.aws.amazon.com/apprunner/latest/api/API_InstanceConfiguration.html"" rel=""nofollow noreferrer"">set the num of vCPUs</a>, I couldn't find any guarantees on which CPU model will be used to run the container (no surprise really). In practice, you could try optimizing for Intel instructions set, and see if the resulting runtime is supported on app runner.<br />
Or if you really need more control and performance, switch to AWS Fargate and explicitly choose x86 or ARM.</p>
","121956",0
1842,73969416,2,73908239,2022-10-06 06:25:56,1,"<p>As @misha130 suggested, changing the default container image <code>mcr.microsoft.com/mlops:latest</code> defined in the Azure DevOps pipeline into another one having a more recent Azure CLI (az) solved the problem:</p>
<pre><code>resources:
  containers:
  - container: my-mlops
    image: mcr.microsoft.com/azure-dev-cli-apps:latest
</code></pre>
<p>However, that image did not have azureml Python components pre-installed, so I also needed to add these installation commands into the pipeline:</p>
<pre><code>  pip3 install azureml
  pip3 install azureml-core
  pip3 install azureml-pipeline
</code></pre>
<p>After that, I was able to run <code>az ml</code> commands (after <code>az extension add -n ml -y</code>) and also run python3 code with azureml imports from my Azure DevOps pipeline.</p>
","2516371",1
1843,73970010,2,73623659,2022-10-06 07:25:08,1,"<p>One option is to bake your custom module into a custom container image. Then you can use your customer image for the component as:</p>
<pre class=""lang-py prettyprint-override""><code>@component(
    base_image='gcr.io/my-custom-image',
    packages_to_intall = [
        &quot;pandas&quot;,
        &quot;sklearn&quot;,
    ],
)
def train_xgb_model(...):
    ...
</code></pre>
<p>In fact if you go this route, you might want to bake <code>pandas</code> and <code>sklearn</code> into your custom container as well.</p>
<p>Alternatives include hosting your <code>mycustomlibrary</code> somewhere on the internet, it can be a GitHub repo for instance. And then you can install it as follows:</p>
<pre class=""lang-py prettyprint-override""><code>@component(
    packages_to_intall = [
        &quot;pandas&quot;,
        &quot;sklearn&quot;,
        &quot;git+https://my-repo/mycustomlibrary.git&quot;,
    ],
)
def train_xgb_model(...):
    ...
</code></pre>
<p>Note that what specified in <code>packages_to_install</code> is passed to <code>pip install</code> command. And <code>pip</code> allows installing from various sources. For example:
<a href=""https://packaging.python.org/en/latest/tutorials/installing-packages/#installing-from-vcs"" rel=""nofollow noreferrer"">https://packaging.python.org/en/latest/tutorials/installing-packages/#installing-from-vcs</a></p>
","20172751",0
1844,73980940,2,73967547,2022-10-06 23:31:06,1,"<p>There are a couple of things wrong with your current approach, therefore there is not a single problem/issue to fix. Instead, I'll attempt to provide some guidance on how to deploy a custom container.</p>
<h2>Using a custom docker image via requirements.txt</h2>
<p>The simplest way of achieving this is to extend an existing sagemaker container. Scikit-Learn images are good base images to extend; however I recommend to use a <code>requirements.txt</code> file with your desired packages, and save the file in the same directory as your training script. Sagemaker will then install your requirements without the need of changing the docker file for the container. Please look <a href=""https://sagemaker.readthedocs.io/en/stable/frameworks/sklearn/using_sklearn.html#using-third-party-libraries"" rel=""nofollow noreferrer"">sagemaker docs</a> for more information.</p>
<h3>Extending a prebuilt container with env variables and Dockefile</h3>
<p>Your current Dockerfile is not complete, unless you omitted the complete dockerfile for brevity.
As mentioned above, based on your apparent requirements, I do not recommend extending a container by modifying the dockerfile, as you'll see there are multiple requirements to accomplish this vs implementing a requirements.txt.</p>
<p>To extend a pre-built SageMaker image, you need to set the following environment variables within your Dockerfile</p>
<ul>
<li><code>SAGEMAKER_SUBMIT_DIRECTORY</code>: The directory within the container in
which the Python script for training is located.</li>
<li><code>SAGEMAKER_PROGRAM</code>: The Python script that should be invoked and used
as the entry point for training.</li>
</ul>
<h4>Create and Upload the Dockerfile and Python Training Scripts</h4>
<p>Your dockerfile should follow the structure below:</p>
<pre><code># Downloads ssagemaker's  base image
FROM 683313688378.dkr.ecr.us-east-1.amazonaws.com/sagemaker-scikit-learn:0.20.0-cpu-py3

ENV PATH=&quot;/opt/ml/code:${PATH}&quot;

# install your package 
RUN pip install &lt;library&gt;

# this environment variable is used by the SageMaker image container to determine our user code directory.
ENV SAGEMAKER_SUBMIT_DIRECTORY /opt/ml/code

# /opt/ml and all subdirectories are utilized by SageMaker, use the /code subdirectory to store your user code.
COPY train.py /opt/ml/code/train.py

# Defines train.py as script entrypoint 
ENV SAGEMAKER_PROGRAM train.py
</code></pre>
<h4>Build the container</h4>
<p>In the same mdirectory as your dockerfile log in to Docker to access the base container:</p>
<pre><code>! aws ecr get-login-password --region us-east-1 | docker login --username AWS --password-stdin 763104351884.dkr.ecr.us-east-1.amazonaws.com
</code></pre>
<p>To build the Docker container, run the following Docker build command, including the space followed by a period at the end:</p>
<pre><code>! docker build -t sklearn-extended-container-test .
</code></pre>
<p>At this point you should be able to test the docker container locally from your sagemaker notebook or notebook studio</p>
<h4>Push the container to ECR</h4>
<pre><code>%%sh

# Specify an algorithm name
algorithm_name=&lt;some name&gt;

account=$(aws sts get-caller-identity --query Account --output text)

# Get the region defined in the current configuration (default to us-west-2 if none defined)
region=$(aws configure get region)

fullname=&quot;${account}.dkr.ecr.${region}.amazonaws.com/${algorithm_name}:latest&quot;

# If the repository doesn't exist in ECR, create it.

aws ecr describe-repositories --repository-names &quot;${algorithm_name}&quot; &gt; /dev/null 2&gt;&amp;1
if [ $? -ne 0 ]
then
aws ecr create-repository --repository-name &quot;${algorithm_name}&quot; &gt; /dev/null
fi

# Log into Docker
aws ecr get-login-password --region ${region}|docker login --username AWS --password-stdin ${fullname}

# Build the docker image locally with the image name and then push it to ECR
# with the full name.

docker build -t ${algorithm_name} .
docker tag ${algorithm_name} ${fullname}

docker push ${fullname}
</code></pre>
<p>After pushing the image you can retrieve the image under:</p>
<pre><code>ecr_image='{}.dkr.ecr.{}.amazonaws.com/{}:latest'.format(account, region, algorithm_name)
</code></pre>
<p>For a detailed description please refer to: <a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/prebuilt-containers-extend.html"" rel=""nofollow noreferrer"">Sagemaker Dev Guide</a></p>
<h2>Scikit-learn images and input channels</h2>
<p>If you are planning on training/fitting the model with any type of data (text/csv or application/x-protobuf), I strongly suggest you read <a href=""https://sagemaker.readthedocs.io/en/stable/frameworks/sklearn/using_sklearn.html#id2"" rel=""nofollow noreferrer"">SageMaker SKLearn Estimator Doc</a>. Note that sagemaker's implementation of this particular type of estimator <code>sagemaker.sklearn.estimator.SKLearn</code> uses the same <code>sagemaker.estimator</code> framework you are using.
You should be declaring your input channel directories and passing the data when you call the <code>fit()</code> method. Sagemaker will copy the object into the local directory of the container and it will be accessible &quot;locally&quot;. Definitely avoid  reading objects outside the container (s3), this is just bad practice and it will cause problems.</p>
<h2>Training.py</h2>
<p>If you plan on deploying your model (serve) your training script should have these three functions:</p>
<ul>
<li><p><code>input_fn</code>: Takes request data and deserializes the data into an object
for prediction.</p>
</li>
<li><p><code>predict_fn</code>: Takes the deserialized request object and performs<br />
inference against the loaded model.</p>
</li>
<li><p><code>output_fn</code>: Takes the result of prediction and serializes this<br />
according to the response content type.</p>
</li>
</ul>
<p>You can also omit these and serve the model with a specific inference script instead of the train. But based on your training script it seems your use case is very basic and you can do all in one entry_point.</p>
<p>Good luck!</p>
","4032338",1
1845,73985124,2,73969207,2022-10-07 09:30:15,0,"<p>Thanks all for taking time in going through my query.</p>
<p>I tried using one more method which was using <a href=""https://github.com/aws-samples/easy-amazon-sagemaker-deployments#installing-the-ezsmdeploy-python-sdk"" rel=""nofollow noreferrer"">ezsmdeploy</a> and it worked. I would suggest that approach if you already have a pre-trained model and dont want to deal with docker.</p>
","13278726",0
1846,73986489,2,73985582,2022-10-07 11:29:08,0,"<p>Please make sure you are installing the package at the correct <a href=""https://stackoverflow.com/questions/34900042/why-would-i-add-python-to-path"">PATH</a>.</p>
<p>For example, if you have different versions of Python installed on your computer, installing packages can get a little tricky and thus, <a href=""https://stackoverflow.com/questions/41573587/what-is-the-difference-between-venv-pyvenv-pyenv-virtualenv-virtualenvwrappe?rq=1"">it's recommended to use a virtual environment</a> to keep packages or different installations properly organised on your computer.</p>
<p><a href=""https://stackoverflow.com/questions/73985582/no-module-named-mlflow-sklearn-mlflow-is-not-a-package#comment130634441_73985582"">Since you are using a conda environment</a>, I would suggest to use <code>conda install mlflow</code> with <a href=""https://docs.conda.io/projects/conda/en/latest/user-guide/concepts/channels.html#what-is-a-conda-channel"" rel=""nofollow noreferrer"">an appropriate channel</a> instead of <code>pip install mlflow</code> i.e. <strong><code>conda install -c conda-forge mlflow</code></strong>.</p>
<p>For more details, please check <a href=""https://anaconda.org/conda-forge/mlflow"" rel=""nofollow noreferrer"">https://anaconda.org/conda-forge/mlflow</a>.</p>
","7789963",0
1847,73989628,2,73952416,2022-10-07 15:43:56,1,"<p>Self-answer with my current solution.</p>
<p>My difficulty using DVC with Slurm jobs is that DVC runs stage commands serially (unless you get into queuing experiments, which introduces celery, which would be another queue on top of Slurm ... yikes.). If the stage commands run in the background, however, DVC will chug merrily along. But, you now have to manually enforce the DAG. I did this with advisory file system locking. You also don't want to run DVC commit until the backgrounded commands have completed.</p>
<p>Here's a pipeline with three stages (minimal working examples of <code>&lt;CMD&gt;</code> given below), note that the DAG allows stages <code>one</code> and <code>three</code> to run in parallel while <code>two</code> must run after <code>one</code>.</p>
<pre><code>stages:
  one:
    cmd: flock lock/a &lt;ONE&gt; &amp;
    outs:
    - one.txt
  two:
    cmd: flock lock/a &lt;TWO&gt; &amp;
    deps:
    - one.txt
    outs:
    - two.txt
  three:
    cmd: flock lock/b &lt;THREE&gt; &amp;
    outs:
    - three.txt
</code></pre>
<p>The <code>lock/a</code> and <code>lock/b</code> files are created by the <code>flock</code> command and correspond to the two separate branches of the DAG. Using <code>flock</code> may not be the ultimate solution; the release order of multiple stage commands waiting on the same lock is unclear to me.</p>
<p>Wrap your <code>dvc repro</code> command in a script something like this:</p>
<pre><code>#!/bin/sh
set -e
mkdir lock
dvc repro --no-commit
for item in lock/*
do
    flock $item rm $item
done
rmdir lock
</code></pre>
<p>This script would be your <code>sbatch</code> submission script, but I'm leaving all that out. I'll also leave out the <code>srun</code> part of the minimal working example below, but you'd need them for Slurm in your stage commands.</p>
<p>When you <code>source job.sh</code> (or <code>sbatch job.sh</code>), the commands all fire into the background and DVC exits. The flock mechanism takes over for releasing commands to run, and the script exits after all locks are released (and cleaned up). You would then run <code>dvc commit</code>.</p>
<p>Here's an example that works without Slurm:</p>
<pre><code>stages:
  one:
    cmd: flock lock/a ./stamp.sh &lt;/dev/null &gt;one.txt &amp;
    outs:
    - one.txt
  two:
    cmd: flock lock/a ./stamp.sh &lt;one.txt &gt;two.txt &amp;
    deps:
    - one.txt
    outs:
    - two.txt
  three:
    cmd: flock lock/b ./stamp.sh &lt;/dev/null &gt;three.txt &amp;
    outs:
    - three.txt
</code></pre>
<p>With executable <code>stamp.sh</code>:</p>
<pre><code>#!/bin/sh
echo &quot;time now is $(date +'%T')&quot;
read line
echo $line | sed -e &quot;s/now is/then was/&quot;
sleep 10
</code></pre>
<p>Some results:</p>
<pre><code>% source job.sh
Running stage 'three':                                                
&gt; flock lock/b ./stamp.sh &lt;/dev/null &gt;three.txt &amp;
WARNING: 'three.txt' is empty.                                        

Running stage 'one':
&gt; flock lock/a ./stamp.sh &lt;/dev/null &gt;one.txt &amp;
WARNING: 'one.txt' is empty.                                          

Running stage 'two':
&gt; flock lock/a ./stamp.sh &lt;one.txt &gt;two.txt &amp;
WARNING: 'two.txt' is empty.                                          
Updating lock file 'dvc.lock'

To track the changes with git, run:

    git add dvc.lock

To enable auto staging, run:

    dvc config core.autostage true
Use `dvc push` to send your updates to remote storage.
% grep &quot;time&quot; *.txt
one.txt:time now is 11:38:58
three.txt:time now is 11:38:58
two.txt:time now is 11:39:08
two.txt:time then was 11:38:58
</code></pre>
","687112",0
1848,73996825,2,69662004,2022-10-08 12:04:12,4,"<p>You do not need to specify the callback URL for the workforce. It is sufficient to specify the following in order to create the <code>aws_cognito_user_pool_client</code> resource:</p>
<pre><code>callback_urls = [
    &quot;https://${aws_cognito_user_pool_domain.domain&gt;.cloudfront_distribution_arn}&quot;,
]
</code></pre>
<p>Then you reference the user pool client in your workforce definition:</p>
<pre><code>resource &quot;aws_sagemaker_workforce&quot; &quot;...&quot; {
    workforce_name = &quot;...&quot;

    cognito_config {
        client_id = aws_cognito_user_pool_client.&lt;client_name&gt;.id
        user_pool = aws_cognito_user_pool_domain.&lt;domain_name&gt;.user_pool_id
    }
}
</code></pre>
<p>Existence of the callback URLs can be proven after applying the terraform configuration by running <code>aws cognito-idp describe-user-pool-client --user-pool-id &lt;pool_id&gt; --client-id &lt;client_id&gt;</code>:</p>
<pre><code>&quot;UserPoolClient&quot;: {
    ...
    &quot;CallbackURLs&quot;: [
        &quot;https://____.cloudfront.net&quot;,
        &quot;https://____.labeling.eu-central-1.sagemaker.aws/oauth2/idpresponse&quot;
    ],
    &quot;LogoutURLs&quot;: [
        &quot;https://____.labeling.eu-central-1.sagemaker.aws/logout&quot;
    ],
</code></pre>
<p>It seems as terraform itself does not do anything special on workforce creation (see <a href=""https://github.com/hashicorp/terraform-provider-aws/blob/main/internal/service/sagemaker/workforce.go"" rel=""nofollow noreferrer"">https://github.com/hashicorp/terraform-provider-aws/blob/main/internal/service/sagemaker/workforce.go</a>). So the callback urls seem to be added by AWS SageMaker itself.</p>
<p>This means that you have to instruct terraform to ignore changes on those attributes in the <code>aws_cognito_user_pool_client</code> configuration:</p>
<pre><code>lifecycle {
    ignore_changes = [
        callback_urls, logout_urls
    ]
}
</code></pre>
","20191189",0
1849,74012740,2,74010810,2022-10-10 09:26:58,1,"<p>They key difference between those methods is just the version of the SDK/API that they are using.</p>
<ul>
<li>Code from <code>azure.ai.ml</code> is coming from the Python SDK V2, contained in the Python package <code>azure-ai-ml</code>. More details about, including how to install it, can be found <a href=""https://learn.microsoft.com/en-us/python/api/overview/azure/ml/installv2?view=azure-ml-py"" rel=""nofollow noreferrer"">here</a></li>
<li>The other code snippet you posted uses the Python SDK V1, contained in the package <code>azureml-core</code>. Instructions <a href=""https://learn.microsoft.com/en-us/python/api/overview/azure/ml/install?view=azure-ml-py"" rel=""nofollow noreferrer"">here</a>.</li>
</ul>
<p>I would suggest using the V2 SDK, it is clearly more future-proof (though of course you may hit some snags for now because the V2 SDK is still in preview mode). Our team are also now switching from V1 to V2.</p>
","5979993",2
1850,74016270,2,74015742,2022-10-10 14:13:32,1,"<blockquote>
<p>Does DVC saves all the different versions of the dataset?</p>
</blockquote>
<p>Yes, it works on a file level. Please find more details here <a href=""https://stackoverflow.com/questions/60365473/by-how-much-can-i-approx-reduce-disk-volume-by-using-dvc/60366262#60366262"">By how much can i approx. reduce disk volume by using dvc?</a></p>
<p>You can control though which version to keep / save.</p>
<blockquote>
<p>Does DVC support all data files format (csv, feather)?</p>
</blockquote>
<p>Yes, it's format-agnostic. It doesn't matter which format to use. It also means it doesn't do anything specific to CSV. It won't be trying to compress it, or calculate some diff in a smart way.</p>
<blockquote>
<p>Can the usage of DVC with the could, lead to extra costs, since it increase the frequency of the communication with the cloud?</p>
</blockquote>
<p>I would not worry about communication costs (unless you move millions or billions of files). But saving multiple versions of a file leads to paying for both of those versions.</p>
<blockquote>
<p>Is there a limitation or disadvantages of the tool when working with large data files(100GB+)?</p>
</blockquote>
<p>It has additional cost of calculating the file hash (<code>md5</code>) to use as a key in its storage. If file is large that takes some extra time to do. Still, saving those files to the cloud and back should be more expensive.</p>
<p>I didn't run benchmarks, but I also can imagine there are some tools like <code>s5cmd</code>, etc that specialize in optimizing data transfer speeds in such cases. DVC doesn't do any tricks for this at the moment.</p>
","298182",2
1851,74020576,2,73980007,2022-10-10 20:54:41,0,"<p>Sagemaker SDK implementation of XGBOOST follows <code>dmlc/xgboost</code>. So you can just pass the <code>eval_metric</code> as you would with xgboost.ai.</p>
<pre><code>xgb_model.fit(trainData, targetVar, early_stopping_rounds=10, 
eval_metric=['mae', 'merror', 'aucpr'], eval_set=[(valData, valTarget)])
</code></pre>
<p>In the example above, we are passing three evaluation metrics. However, if you are trying to pass custom metrics then the above implementation wouldn't work.</p>
","4032338",1
1852,74027480,2,74017026,2022-10-11 11:37:36,2,"<p>I figured out that in order for this to work, I had to switch to using a <a href=""https://cloud.google.com/iam/docs/service-accounts"" rel=""nofollow noreferrer"">service account</a>.</p>
","6367928",0
1853,74067496,2,74064418,2022-10-14 09:52:06,1,"<p>Create a Machine learning studio resource group and workspace. Upload the dataset for several times and it will be updated with versions with the same name.</p>
<p><a href=""https://i.stack.imgur.com/qhxOI.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/qhxOI.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.stack.imgur.com/XhBlh.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/XhBlh.png"" alt=""enter image description here"" /></a></p>
<p>Use the below code block to get the versions of the dataset uploaded and information about those versions.</p>
<p><strong>Code block 1</strong></p>
<pre><code>from azureml.core import Dataset
Diabetes1234 = Dataset.get_all(workspace = ws)
counts = Diabetes1234['Diabetes123'].version
versions = [Dataset.get_by_name(workspace = ws, name = 'Diabetes123', version = v) for v in range(1,counts+1)]
</code></pre>
<p><strong>Code block 2</strong></p>
<pre><code>versions
</code></pre>
<p><strong>Output</strong></p>
<pre><code>[{
   &quot;source&quot;: [
     &quot;('workspaceblobstore', 'UI/2022-10-14_055538_UTC/')&quot;
   ],
   &quot;definition&quot;: [
     &quot;GetDatastoreFiles&quot;,
     &quot;ParseDelimited&quot;,
     &quot;DropColumns&quot;,
     &quot;SetColumnTypes&quot;
   ],
   &quot;registration&quot;: {
     &quot;id&quot;: &quot;Your ID&quot;,
     &quot;name&quot;: &quot;Diabetes123&quot;,
     &quot;version&quot;: 1,
     &quot;workspace&quot;: &quot;Workspace.create(name='cancerset', subscription_id=your subscription ID', resource_group='your resource group')&quot;
   }
 },
 {
   &quot;source&quot;: [
     &quot;('workspaceblobstore', 'UI/2022-10-14_055914_UTC/')&quot;
   ],
   &quot;definition&quot;: [
     &quot;GetDatastoreFiles&quot;,
     &quot;ParseDelimited&quot;,
     &quot;DropColumns&quot;,
     &quot;SetColumnTypes&quot;
   ],
   &quot;registration&quot;: {
     &quot;id&quot;: &quot; Your ID &quot;,
     &quot;name&quot;: &quot;Diabetes123&quot;,
     &quot;version&quot;: 2,
     &quot;workspace&quot;: &quot;Workspace.create(name='cancerset', subscription_id=your subscription ID', resource_group='your resource group')&quot;
   }
 },
 {
   &quot;source&quot;: [
     &quot;('workspaceblobstore', 'UI/2022-10-14_060011_UTC/')&quot;
   ],
   &quot;definition&quot;: [
     &quot;GetDatastoreFiles&quot;,
     &quot;ParseDelimited&quot;,
     &quot;DropColumns&quot;,
     &quot;SetColumnTypes&quot;
   ],
   &quot;registration&quot;: {
     &quot;id&quot;: &quot; Your ID &quot;,
     &quot;name&quot;: &quot;Diabetes123&quot;,
     &quot;version&quot;: 3,
     &quot;workspace&quot;: &quot;Workspace.create(name='cancerset', subscription_id=your subscription ID', resource_group='your resource group')&quot;
   }
 },
 {
   &quot;source&quot;: [
     &quot;('workspaceblobstore', 'UI/2022-10-14_070300_UTC/')&quot;
   ],
   &quot;definition&quot;: [
     &quot;GetDatastoreFiles&quot;,
     &quot;ParseDelimited&quot;,
     &quot;DropColumns&quot;,
     &quot;SetColumnTypes&quot;
   ],
   &quot;registration&quot;: {
     &quot;id&quot;: &quot; Your ID &quot;,
     &quot;name&quot;: &quot;Diabetes123&quot;,
     &quot;version&quot;: 4,
     &quot;workspace&quot;: &quot;Workspace.create(name='cancerset', subscription_id=your subscription ID', resource_group='your resource group')&quot;
   }
 },
 {
   &quot;source&quot;: [
     &quot;('workspaceblobstore', 'UI/2022-10-14_093655_UTC/')&quot;
   ],
   &quot;definition&quot;: [
     &quot;GetDatastoreFiles&quot;,
     &quot;ParseDelimited&quot;,
     &quot;DropColumns&quot;,
     &quot;SetColumnTypes&quot;
   ],
   &quot;registration&quot;: {
     &quot;id&quot;: &quot; Your ID &quot;,
     &quot;name&quot;: &quot;Diabetes123&quot;,
     &quot;version&quot;: 5,
     &quot;workspace&quot;: &quot;Workspace.create(name='cancerset', subscription_id=your subscription ID', resource_group='your resource group')&quot;
   }
 }]
</code></pre>
<p>To get the last before the latest version. Use the below code block.</p>
<p><strong>Code Block:</strong></p>
<pre><code>versions[-2]
</code></pre>
<p><strong>Output</strong></p>
<pre><code>{
  &quot;source&quot;: [
    &quot;('workspaceblobstore', 'UI/2022-10-14_070300_UTC/')&quot;
  ],
  &quot;definition&quot;: [
    &quot;GetDatastoreFiles&quot;,
    &quot;ParseDelimited&quot;,
    &quot;DropColumns&quot;,
    &quot;SetColumnTypes&quot;
  ],
  &quot;registration&quot;: {
    &quot;id&quot;: &quot;your ID&quot;,
    &quot;name&quot;: &quot;Diabetes123&quot;,
    &quot;version&quot;: 4,
    &quot;workspace&quot;: &quot;Workspace.create(name='cancerset', subscription_id=your subscription ID', resource_group='your resource group')&quot;
  }
}
</code></pre>
","18428148",0
1854,74081856,2,74012546,2022-10-15 18:02:39,1,"<p><strong>Background</strong>: Following best practices, in general, of using feature names (e.g., column names of a dataframe pandas), these should be without spaces between them.</p>
<h1>Base case</h1>
<p>To <strong>bypass your problem</strong>, you can use a string as a parameter where each element is a single feature.</p>
<pre class=""lang-py prettyprint-override""><code>features = &quot;feature_0 feature_1 feature_2&quot;
</code></pre>
<p>and then, use it normally with <a href=""https://sagemaker.readthedocs.io/en/stable/workflows/pipelines/sagemaker.workflow.pipelines.html#sagemaker.workflow.parameters.ParameterString"" rel=""nofollow noreferrer"">ParameterString</a>.</p>
<p><em>If it cannot be that way, I recommend inserting a specific separation pattern between names instead of space and splitting the whole string into features list later.</em></p>
<p>At this point, in the training script you pass the parameter to the ArgumentParser which you can configure to have the space-separated word string reprocessed into a list of individual words.</p>
<pre class=""lang-py prettyprint-override""><code>import argparse

if __name__ == &quot;__main__&quot;:

    parser = argparse.ArgumentParser()

    parser.add_argument(
        &quot;--features&quot;,
        nargs=&quot;*&quot;,
        type=str,
        default=[]
    )

    args, _ = parser.parse_known_args()
</code></pre>
<h2>Extra case</h2>
<p>Should the string mistakenly be interpreted as a list directly when passing the argument to a pipeline component (e.g., to a preprocessor), the latter can be reworked with an input reinterpretation function.</p>
<pre class=""lang-py prettyprint-override""><code>import itertools

def decode_list_of_strings_input(str_input: str) -&gt; []:
    str_input = [s.split() for s in str_input]
    return list(itertools.chain.from_iterable(str_input))
</code></pre>
<p>Here is an example of the use of this code:</p>
<pre class=""lang-py prettyprint-override""><code>features = ['a b c']
features = decode_list_of_strings_input(features)

print(features)
&gt;&gt;&gt; ['a', 'b', 'c']
</code></pre>
","20249888",0
1855,74083890,2,73433402,2022-10-16 00:29:29,1,"<p>If you want to pass default pipeline parameters for your pipeline. You can do it directly in the function definition of the pipeline like this:</p>
<pre><code>@dsl.pipeline(name='name')
def my_pipeline(a=1, b='default value'):
  # pipeline code
  pass
</code></pre>
<p>For a minimal example of a full pipeline, you can take a look at <a href=""https://medium.com/@trannhatquang1104/minimal-kubeflow-pipeline-example-f13cd5dc84ac"" rel=""nofollow noreferrer"">this</a>.
From my experience working with Kubeflow, a lot of the documentation are wrong or not up-to-date. Many times, you have to find a workaround.</p>
","12915862",0
1856,74084475,2,74084385,2022-10-16 03:48:51,1,"<p>Regarding your first question, here are two methods to update your Azure ML dataset with a new version using a CSV file stored in Blob Storage:</p>
<h2>Method 1:</h2>
<pre class=""lang-py prettyprint-override""><code>from azure.ai.ml.entities import Data
from azure.ai.ml.constants import AssetTypes

blob_url = 'https://sampleazurestorage.blob.core.windows.net/data/my-sample-data.csv'

my_dataset = Data(
    path=blob_url ,
    type=AssetTypes.MLTABLE,
    description=&quot;a description for your dataset&quot;,
    name=&quot;dataset_name&quot;,
    version='&lt;new_version&gt;'
)

ml_client.data.create_or_update(my_dataset)
</code></pre>
<h2>Method 2:</h2>
<pre class=""lang-py prettyprint-override""><code>import azureml.core
from azureml.core import Dataset, Workspace

ws = Workspace.from_config()
datastore = ws.get_default_datastore()

blob_url = 'https://sampleazurestorage.blob.core.windows.net/data/my-sample-data.csv'

my_dataset = Dataset.File.from_delimited_files(path=blob_url)
my_dataset.register(
    workspace=ws,
    name=&quot;dataset_name&quot;,
    description=&quot;a description for your dataset&quot;,
    create_new_version=True
)
</code></pre>
<p>If you want to update the dataset using a pandas DataFrame:</p>
<pre class=""lang-py prettyprint-override""><code>my_df = ...  # the variable that contains the new dataset in a DataFrame
my_dataset = Dataset.File.from_pandas_dataframe(dataframe=my_df)
my_dataset.register(
    ...
)
</code></pre>
<p>Regarding your second question:</p>
<blockquote>
<p>Above we see that version 2 is the latest, but I want to change the latest to version 1</p>
</blockquote>
<p>It is not possible since 'latest' always points to the last (latest) uploaded version of the dataset with the given name. So, if you want a specific or latest version, you should change the <code>version</code> parameter in the <code>Data</code> class in the &quot;Method 1&quot; code snippet.</p>
","13476175",0
1857,74084517,2,73972077,2022-10-16 04:00:05,1,"<p>Currently, it is required that all the necessary files and objects related to the endpoint be placed in the <code>source_directory</code>:</p>
<pre class=""lang-py prettyprint-override""><code>inference_config = InferenceConfig(
    environment=env,
    source_directory='./endpoint_source',
    entry_script=&quot;./score.py&quot;,
)
</code></pre>
<p>One workaround is to upload your JSON file somewhere else, e.g., on the Blob Storage, and download it in the <code>init()</code> function of your entry script. For example:</p>
<p>score.py:</p>
<pre class=""lang-py prettyprint-override""><code>import requests


def init():
    &quot;&quot;&quot;
    This function is called when the container is initialized/started,
    typically after create/update of the deployment.
    &quot;&quot;&quot;
    global model
    
    # things related to initializing the model
    model = ...  
    
    # download your JSON file
    json_file_rul = 'https://sampleazurestorage.blob.core.windows.net/data/my-configs.json'
    response = requests.get(json_file_rul)
    open('my_configs.json', &quot;wb&quot;).write(response.content)
</code></pre>
","13476175",0
1858,74084559,2,73703577,2022-10-16 04:11:42,0,"<p>Assuming you have a script <strong>my_module.py</strong> that contains your custom model class <code>MyBertModel</code>, you should place the script under the <code>source_directory</code> of the endpoint and import the class in the entry script.</p>
<p>So, if this is your inference configuration:</p>
<pre class=""lang-py prettyprint-override""><code>inference_config = InferenceConfig(
    environment=env,
    source_directory='./endpoint_source',
    entry_script=&quot;./score.py&quot;,
)
</code></pre>
<p>This is how <strong>./endpoint_source/</strong> should look like:</p>
<pre><code>.
└── endpoint_source
    ├── score.py
    ├── my_module.py
    └── ...
</code></pre>
<p>And this is how your entry script <code>score.py</code> should look like:</p>
<pre class=""lang-py prettyprint-override""><code>from my_module import MyBertModel

# other imports
...

def init():
    global model
    
    # load the model here, from pickle file or using the custom class
    model = ...
   

def run(data):
    # use the model here to predict on data
    ...
</code></pre>
","13476175",0
1859,74086732,2,73790017,2022-10-16 11:28:47,0,"<p>This feature, in sagemaker-python-sdk, was introduced starting with <a href=""https://github.com/aws/sagemaker-python-sdk/releases/tag/v2.90.0"" rel=""nofollow noreferrer"">release 2.90.0</a> (16 May 2022).</p>
<p>I suggest always keeping sagemaker updated, they come out with updates very frequently and quite important:</p>
<pre><code>pip install -U sagemaker
</code></pre>
","20249888",0
1860,74092465,2,74076365,2022-10-17 03:58:53,0,"<p>For anyone else who might be struggling with the same issue, I found the culprit. It's the <code>SchemaGen</code> class. This is how I was instantiating its object:</p>
<pre class=""lang-py prettyprint-override""><code>schema_gen = tfx.components.SchemaGen(
    statistics=statistics_gen.outputs['statistics'],
    infer_feature_shape=False)
</code></pre>
<p>I don't know what's the use case for asking <code>SchemaGen</code> class not to infer the shape of the features but the tutorial I was following had it set to <code>False</code> and I had just copied and pasted the same thing. Comparing with some other tutorials, I realized that it could be the reason why I was getting <code>SparseTensor</code>.</p>
<p>So, if you let <code>SchemaGen</code> infer the shape of your features or you load a hand crafted schema in which you've set the shapes yourself, you'll be getting a <code>Tensor</code> in your <code>preprocessing_fn</code>. But if the shapes are not set, the features will be instances of <code>SparseTensor</code>.</p>
<p>For the sake of completeness, this is the fixed snippet:</p>
<pre class=""lang-py prettyprint-override""><code>schema_gen = tfx.components.SchemaGen(
    statistics=statistics_gen.outputs['statistics'],
    infer_feature_shape=True)
</code></pre>
","866082",0
1861,74101397,2,74069566,2022-10-17 18:06:48,1,"<p>Ive finally figured it out, saving the model as</p>
<pre><code>pickle.dump(model.get_booster(), f)
</code></pre>
<p>instead of just</p>
<pre><code>pickle.dump(model, f)
</code></pre>
<p>fixes the issue</p>
","13360530",1
1862,74119697,2,73953744,2022-10-19 03:56:00,2,"<p>I spent some time investigating this and my conclusion is that individual components are not meant to be unit tested by kfp's design. That means that you must rely on unit testing each component's logic, wrapping each piece of that logic in a component, and then testing the end-to-end functionality of the kfp pipeline.</p>
<p>I agree that it would be quite cool if there were a way to easily mock Inputs and Outputs but I dug quite deep and it does not seem like this is an intended use (or an easy hack) at this point in time.</p>
","6809571",2
1863,74124021,2,74123333,2022-10-19 10:46:33,1,"<p>The body read by S3 is in bytes, in the case of XML it will be of the form <code>b&quot;your_xml_content&quot;</code>.</p>
<p>So you need to read this type of data and then, using any xml manipulation library, read in its content and navigate through it.</p>
<p>A complete example is this:</p>
<pre class=""lang-py prettyprint-override""><code>from lxml import etree
import boto3

xml_bytes = boto3.resource('s3').Object(bucket, my_file).get()['Body'].read()
doc = etree.XML(xml_bytes)
</code></pre>
<hr />
<p>Assuming your file is in this form:</p>
<pre class=""lang-xml prettyprint-override""><code>&lt;note&gt;
    &lt;to&gt;Tove&lt;/to&gt;
    &lt;from&gt;Jani&lt;/from&gt;
    &lt;heading&gt;Reminder&lt;/heading&gt;
    &lt;body&gt;Don't forget me this weekend!&lt;/body&gt;
&lt;/note&gt;
</code></pre>
<p>you can navigate the object with a simple for loop:</p>
<pre class=""lang-py prettyprint-override""><code>for element in doc.iter():  # remove .iter() if you don't want the root
    print(&quot;%s - %s&quot; % (element.tag, element.text))
</code></pre>
<p>output will be:</p>
<pre><code>note - 
        
to - Tove
from - Jani
heading - Reminder
body - Don't forget me this weekend!
</code></pre>
","20249888",0
1864,74154157,2,72580651,2022-10-21 13:11:59,0,"<p>Suggestion to structure your file structure as follows;</p>
<pre><code>Project
 -Dockerfile
 -src
    -a.py
    -b.py
    -c.py
 -packages
sagemaker_script.py
build_push_script.sh
Data
 -train
 -test
 -validate
</code></pre>
<p>Then you can run from the Project with a command like <code>docker ... Dockerfile</code>
and inside the Dockerfile you can simply put <code>COPY ./src/* /opt/ml/code/</code></p>
","18809515",1
1865,74160221,2,74118116,2022-10-22 00:12:52,2,"<p>SageMaker endpoints are REST endpoints. You can however make gRPC connections within the container. You cannot make the InvokeEndpoint API call via gRPC.</p>
<p>If you are using the SageMaker TensorFlow container, you need to pass an inference.py script that contains the logic to make the gRPC request to TFS.</p>
<p>Kindly see this example <a href=""https://github.com/aws-samples/amazon-sagemaker-tensorflow-serving-grpc/blob/main/image-classification/code/inference.py"" rel=""nofollow noreferrer"">inference.py</a> script that makes a gRPC prediction against TensorFlow Serving.</p>
","9796588",0
1866,74179281,2,74178509,2022-10-24 09:46:13,1,"<p>I had the same problem as you just recently and I can tell you I solved it by recreating the <a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/gs-studio-onboard.html"" rel=""nofollow noreferrer"">Amazon SageMaker Domain</a> with the <strong>Standard setup</strong> configuration (so, not Quick setup).</p>
<p>Inside it you have to specify all the correct configurations such as IAM entries, VPC / security groups and all information blocking the display of Studio resources.</p>
<p>In the quick, in fact, they are not asked and you run into this problem you show.</p>
<p>To <strong>delete the domain and recreate a new one</strong>, it is necessary to:</p>
<ul>
<li>Delete all jupyter server apps</li>
<li>Delete all user profiles</li>
<li>Delete the domain</li>
</ul>
","20249888",0
1867,74180279,2,74179120,2022-10-24 11:14:49,1,"<p>The reason of shortening or technically trimming your input argument is that the bash variable is split at the <code>&amp;</code> level. so all the rest of your sas url goes as &quot;commands&quot; or other &quot;arguments&quot;. Apparently that is how azure parses it.</p>
<p>eg:</p>
<pre><code>python3 test_input.py --blob_sas_url &quot;somepath/to/storage/account/file.txt?sv=2022-01-01&amp;sr=b&amp;sig=SOmethingwd21dd1&quot;
&gt;&gt;&gt; output:  somepath/to/storage/account/file.txt?sv=2022-01-01&amp;sr=b&amp;sig=SOmethingwd21dd1

python3 test_input.py --blob_sas_url somepath/to/storage/account/file.txt?sv=2022-01-01&amp;sr=b&amp;sig=SOmethingwd21dd1
&gt;&gt;&gt; output:  
[1] 1961
[2] 1962
[2]+  Done                    sr=b
</code></pre>
<p>so you just need to quote your Azure variable in your step command like follows:</p>
<p><code>python3 your_python_script.py --blob_sas_url &quot;$(azml.sasURL)&quot;</code></p>
","5534881",0
1868,74189113,2,74127921,2022-10-25 04:25:02,0,"<p>The classification model has <code>predict_proba</code> method which gives you the class probabailities. You need to use that like</p>
<pre><code>ypred_probabilities = classifer.predict_proba(Xtest)
ypred = classifer.predict(Xtest)
probability_score = np.max(ypred_probabilities, axis=1)
</code></pre>
<p>Now, return this score as one of the output in the form of json like:</p>
<pre><code>result = {
           &quot;class&quot; : ypred,
           &quot;probability_score&quot;: probability_score
         }
</code></pre>
","10199456",0
1869,74197963,2,74195903,2022-10-25 17:23:26,2,"<p>I have had not time to try this out, but I would assume that the <code>score.py</code> file (and any files in the same directory) will not be in the <code>os.getcwd()</code>.</p>
<p>I would try to use <code>__file__</code> to get the path of <code>score.py</code>:</p>
<p>so, instead of:</p>
<pre class=""lang-py prettyprint-override""><code>current_directory = os.getcwd()
file_name=&quot;model_adjustments.json&quot;
json_file=os.path.join(current_directory,file_name)
</code></pre>
<p>rather try:</p>
<pre class=""lang-py prettyprint-override""><code>import pathlib
model_directory = pathlib.Path(os.path.realpath(__file__)).parent.resolve()
file_name=&quot;model_adjustments.json&quot;
json_file=os.path.join(model_directory,file_name)
</code></pre>
<p>(Note: <code>realpath</code> is there to make sure symlinks are properly resolved)</p>
","8821969",0
1870,74202801,2,74201422,2022-10-26 04:44:47,1,"<p>SageMaker local mode is designed to imitate the hosted environment. As such, every time you deploy/update a new container is run.</p>
<p>For faster development, I usually bake all the packages I can into the container which stops the need for installation on every deploy.</p>
<p>I.e. You can extend the SageMaker PyTorch container and bake your packages into it instead of using a requirements.txt. You can then push the image to ECR and specify it in the <code>PyTorchModel</code></p>
<p><a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/prebuilt-containers-extend.html"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/sagemaker/latest/dg/prebuilt-containers-extend.html</a></p>
<p>In your <code>PyTorchModel</code>:</p>
<pre><code>model_instance = PyTorchModel(
    image_uri = &lt;YourImageECRURI&gt;,
    model_data=model_tar_path,
    role=role,
    source_dir=&quot;code&quot;,
    entry_point=&quot;inference.py&quot;,
    framework_version=&quot;1.8&quot;,
    py_version=&quot;py3&quot;
)
</code></pre>
","9796588",2
1871,74209602,2,73458933,2022-10-26 14:36:28,1,"<p>I finally fixed this.</p>
<p>The trick is to not chose Pipeline Endpoint ID but to choose Pipeline ID.</p>
<p>Pipeline ID can be parametrized and I have set up this to come from a global parameter. Therefore I do not need to find the right level of identation everytime</p>
<p><a href=""https://i.stack.imgur.com/6dRJ3.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/6dRJ3.png"" alt=""enter image description here"" /></a></p>
<p>Then:</p>
<p><a href=""https://i.stack.imgur.com/HgbvY.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/HgbvY.png"" alt=""enter image description here"" /></a></p>
<p>Later you add the global parameters to your ARM template:</p>
<p><a href=""https://i.stack.imgur.com/c9oN8.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/c9oN8.png"" alt=""enter image description here"" /></a></p>
<p>And in the parameter template you add:</p>
<pre><code>&quot;Microsoft.DataFactory/factories&quot;: {
        &quot;properties&quot;: {
            &quot;globalParameters&quot;: {
                &quot;*&quot;: {
                    &quot;value&quot;: &quot;=&quot;
                }
            },
            &quot;globalConfigurations&quot;: {
                &quot;*&quot;: &quot;=&quot;
            },
            &quot;encryption&quot;: {
                &quot;*&quot;: &quot;=&quot;,
                &quot;identity&quot;: {
                    &quot;*&quot;: &quot;=&quot;
                }
            }
        }
&quot;Microsoft.DataFactory/factories/globalparameters&quot;: {
    &quot;properties&quot;: {
        &quot;*&quot;: {
            &quot;value&quot;: &quot;=&quot;
        }
    }
}
</code></pre>
<p>Finally I wrote a python CLI tool to get the latest pipeline ID for a given published pipeline id:</p>
<pre><code>import argparse
from azureml.pipeline.core import PipelineEndpoint, PublishedPipeline, Pipeline
from azureml.core import Workspace
from env_variables import Env
from manage_workspace import get_workspace


def get_latest_published_endpoint(ws : Workspace, pipeline_name : str) -&gt; str:
    &quot;&quot;&quot;
    Get the latest published endpoint given a machine learning pipeline name.
    The function is used to update the pipeline id in ADF deploy pipeline

    Parameters
    ------
    ws : azureml.core.Workspace
        A workspace object to use to search for the models
    pipeline_name : str
        A string containing the pipeline name to retrieve the latest version

    Returns
    -------
    pipeline_name : azureml.pipeline.core.PipelineEndpoint
        The pipeline name to retrieve the last version
    &quot;&quot;&quot;
    pipeline_endpoint = PipelineEndpoint.get(workspace=ws, name=pipeline_name)
    endpoint_id = pipeline_endpoint.get_pipeline().id # this gives back the pipeline id
    # pipeline_endpoint.id gives back the pipeline endpoint id which can not be set
    # as dynamic parameter in ADF in an easy way

    return endpoint_id

if __name__ == &quot;__main__&quot;:
    parser = argparse.ArgumentParser()
    parser.add_argument(&quot;--monitoring_pipeline_name&quot;, type=str,
                        help=&quot;Pipeline Name to get endpoint id&quot;,
                        default='yourmonitoringpipeline')
    parser.add_argument(&quot;--training_pipeline_name&quot;, type=str,
                        help=&quot;Pipeline Name to get endpoint id&quot;,
                        default='yourtrainingpipeline')
    parser.add_argument(&quot;--scoring_pipeline_name&quot;, type=str,
                        help=&quot;Pipeline Name to get endpoint id&quot;,
                        default='yourscoringpipeline')
    args, _ = parser.parse_known_args()
    e = Env()

    ws = get_workspace(e.workspace_name, e.subscription_id, e.resource_group)  # type: ignore
    latest_monitoring_endpoint = get_latest_published_endpoint(ws, pipeline_name=args.monitoring_pipeline_name)  # type: ignore
    latest_training_endpoint = get_latest_published_endpoint(ws, pipeline_name=args.training_pipeline_name) # type: ignore
    latest_scoring_endpoint = get_latest_published_endpoint(ws, pipeline_name=args.scoring_pipeline_name) # type: ignore
    print('##vso[task.setvariable variable=MONITORING_PIPELINE_ID;]%s' % (latest_monitoring_endpoint))
    print('##vso[task.setvariable variable=TRAINING_PIPELINE_ID;]%s' % (latest_training_endpoint))
    print('##vso[task.setvariable variable=SCORING_PIPELINE_ID;]%s' % (latest_scoring_endpoint))
</code></pre>
<p>By printing the variables in these way they are added to environment variables that later I can pick in the ARM deploy step:</p>
<p><a href=""https://i.stack.imgur.com/gS6Uo.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/gS6Uo.png"" alt=""enter image description here"" /></a></p>
<p>And then we have our desired setup:</p>
<p><a href=""https://i.stack.imgur.com/FKO8Z.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/FKO8Z.png"" alt=""enter image description here"" /></a></p>
<p>Different pipeline IDs for different environments.</p>
<p>Maybe material for a blog post as it works like charm.</p>
","9521072",0
1872,74217525,2,74123559,2022-10-27 06:13:24,1,"<p>To install local libraries on azure ML environment, we need to create the Data Science VM on the local machine and connect it with the workspace subscription details.</p>
<p>Create ML studio workspace and download the JSON file which consists of the details of the workspace need to be connected to the VM.</p>
<ol>
<li>Create a python virtual environment on the local machine</li>
<li>Either virtualenv or conda is fine</li>
<li>Activate the virtual environment</li>
<li>Install the Azure Machine Learning Python SDK</li>
<li>Configure the Azure ML Studio with local machine</li>
</ol>
<p><a href=""https://i.stack.imgur.com/96fCK.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/96fCK.png"" alt=""enter image description here"" /></a></p>
<ol start=""6"">
<li>Open the Jupyter Notebook installed and configured. Create a virtual environment.</li>
<li><code>conda install notebook ipykernel</code> – enable all the ipykernal things</li>
<li><code>ipython kernel install --user --name &lt;myenv&gt;</code> --display-name &quot;Python (myenv)&quot; – creating a kernel</li>
<li>Launch the jupyter notebook server</li>
</ol>
<p>Now we need to get the ARM template by following the procedure taken from <a href=""https://learn.microsoft.com/en-us/azure/machine-learning/data-science-virtual-machine/dsvm-tutorial-resource-manager"" rel=""nofollow noreferrer"">MS Docs</a></p>
<p>Using the following code block create Windows DSVM</p>
<pre><code>az vm create --resource-group YOUR-RESOURCE-GROUP-NAME --name YOUR-VM-NAME --image microsoft-dsvm:dsvm-windows:server-2016:latest --admin-username YOUR-USERNAME --admin-password YOUR-PASSWORD --authentication-type password
</code></pre>
<p>Using the following code block create Ubuntu DSVM</p>
<pre><code>az vm create --resource-group YOUR-RESOURCE-GROUP-NAME --name YOUR-VM-NAME --image microsoft-dsvm:linux-data-science-vm-ubuntu:linuxdsvmubuntu:latest --admin-username YOUR-USERNAME --admin-password YOUR-PASSWORD --generate-ssh-keys --authentication-type password
</code></pre>
<p>Create conda environment:</p>
<pre><code>conda create -n py310 python=310
</code></pre>
<p>Activate the environment and install the libraries locally which are directly impacted in azure ml platform</p>
<pre><code>conda activate py310
pip install azure-ai-ml
</code></pre>
","18428148",1
1873,74224927,2,74224196,2022-10-27 15:55:50,2,"<p>If you take a close look at the signature of the abstract method <code>predict()</code> in the <a href=""https://github.com/mlflow/mlflow/blob/master/mlflow/pyfunc/model.py#L88"" rel=""nofollow noreferrer"">mlflow.pyfunc.PythonModel</a> class that you are extending, you will see that has 3 parameters:</p>
<pre class=""lang-py prettyprint-override""><code>def predict(self, context, model_input):
</code></pre>
<p>So, if you change your simple class to have the extra parameter <code>context</code>, your example should work:</p>
<pre class=""lang-py prettyprint-override""><code>class PredictSpeciality(mlflow.pyfunc.PythonModel):
    def fit(self):
        print('fit')
        d = {'col1': [1, 2], 'col2': [3, 4]}
        df = pd.DataFrame(data=d)
        return df
           
    def predict(self, context, X, y=None):
        print('predict')
        print(X.shape)
        return 
</code></pre>
<p>To elaborate a bit more on what is going on here:
There are 2 classes at play: <a href=""https://github.com/mlflow/mlflow/blob/master/mlflow/pyfunc/model.py#L62-L96"" rel=""nofollow noreferrer"">mlflow.pyfunc.PythonModel</a> and <a href=""https://github.com/mlflow/mlflow/blob/master/mlflow/pyfunc/__init__.py#L363-L442"" rel=""nofollow noreferrer"">mlflow.pyfunc.PyFuncModel</a>.</p>
<p>The mlflow.pyfunc.PythonModel is being wrapped by the mlflow.pyfunc.PyFuncModel. The former is doing the actual work and the latter is dealing with the metadata, packaging, conda environment, etc. In the <a href=""https://github.com/mlflow/mlflow/blob/master/mlflow/pyfunc/__init__.py#L21-L30"" rel=""nofollow noreferrer"">documentation</a> it is explained like so:</p>
<blockquote>
<p>Python function models are loaded as an instance of
mlflow.pyfunc.PyFuncModel, which is an MLflow wrapper around the model
implementation and model metadata (MLmodel file).</p>
</blockquote>
<p>Unfortunately, the <a href=""https://github.com/mlflow/mlflow/blob/master/mlflow/pyfunc/__init__.py#L367-L369"" rel=""nofollow noreferrer"">documentation also states</a> that you cannot create a PyFuncModel directly, but only</p>
<blockquote>
<p>Wrapper around model implementation and metadata. This class is not meant to be constructed directly. Instead, instances of this class are constructed and returned from <code>mlflow.pyfunc.load_model()</code>.</p>
</blockquote>
<p>I find that quite limiting and am unsure why it was designed this way, however, there are 2 things that you can do here:</p>
<ol>
<li>Pass in an extra parameter when directly dealing with your wrapped class:</li>
</ol>
<pre><code>   m.predict(None, df)
</code></pre>
<ol start=""2"">
<li>Save and load the model to get an mlflow.pyfunc.PyFuncModel:</li>
</ol>
<pre><code>   mlflow.pyfunc.save_model(path=&quot;temp_model&quot;, python_model=m)
   m2 = mlflow.pyfunc.load_model(&quot;temp_model&quot;)
   m2.predict(df)
</code></pre>
<p>I know it isn't elegant, but I actually have been using #2 in the past. It would be good if someone from the MLFlow team could comment on why direct creation of a mlflow.pyfunc.PyFuncModel is not supported.</p>
","8821969",3
1874,74225173,2,74224896,2022-10-27 16:14:23,0,"<p>The <code>mlflow.pyfunc.log_model</code> function's <code>artifact_path</code> parameter, is <a href=""https://github.com/mlflow/mlflow/blob/master/mlflow/pyfunc/__init__.py#L1369"" rel=""nofollow noreferrer"">defined as</a> :</p>
<pre><code>:param artifact_path: The run-relative artifact path to which to log the Python model.
</code></pre>
<p>That means, it is just a name that should identify the model in the context of that run and hence cannot be an absolute path like what you passed in. Try something short like <code>add5_model</code>.</p>
<pre class=""lang-py prettyprint-override""><code>reg_model_name = &quot;ml_flow_AddN_test&quot;

mlflow.pyfunc.log_model(artifact_path= &quot;add5_model&quot;,
                        python_model = add5_model,
                        registered_model_name=reg_model_name)
</code></pre>
","8821969",0
1875,74231618,2,74229203,2022-10-28 06:53:15,1,"<p>Based on the type of inference job, you have 2 options you can follow to fulfil your requirement.</p>
<h3>0. Upload during inference (less correct)</h3>
<p>This is the easiest method, but less clean. In fact, you make a step that should not be included in the inference. The sagemaker inference has the task of producing a result and giving it in response when the model is invoked. Saving an output is postponed to <a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/batch-transform.html"" rel=""nofollow noreferrer"">batch transform jobs</a>.</p>
<p>By using boto3 (you may need to install it from requirements.txt) within your inference script, you can proceed to invoke the <a href=""https://boto3.amazonaws.com/v1/documentation/api/latest/guide/s3-uploading-files.html"" rel=""nofollow noreferrer"">API</a> to make any upload of any file to any allowed bucket.</p>
<p>E.g.:</p>
<pre class=""lang-py prettyprint-override""><code>import boto3
s3 = boto3.resource('s3') # instantiate S3 client 

def output_fn(prediction, content_type):
    # ...
    s3.meta.client.upload_file(file_name, 'YOUR_S3_BUCKET_NAME', 'DESIRED_S3_OBJECT_NAME')
    # ...
    return json.dumps(['Image saved!'])
</code></pre>
<h3>1. Using post-processing job (proper way)</h3>
<p>This approach is cleaner, but requires a subsequent component downstream of the inference.</p>
<p>You can use batch transform to save the tensors/bytearray and immediately afterwards a small post-processing job (<a href=""https://sagemaker.readthedocs.io/en/stable/amazon_sagemaker_processing.html"" rel=""nofollow noreferrer"">Amazon SageMaker Processing in general</a>) to convert all the outputs of the previous job into images.</p>
<p>From an architectural point of view, this is the most correct solution as you can easily specify parameters as storage buckets. And this is a task that must be configurable and external to the model endpoint.</p>
<p>This diagram should make the architecture clearer:
<img src=""https://i.stack.imgur.com/xfqik.png""/></p>
","20249888",0
1876,74238068,2,74208931,2022-10-28 16:08:52,1,"<p>If you enable data capture SageMaker will append the inference ID:</p>
<p>Assuming your output is JSON the data capture will append the ID for example something like:</p>
<pre><code>{&quot;output&quot;: 0, &quot;SageMakerInferenceId&quot;: &quot;1f1d57b1-2e6f-488c-8c30-db4e6d757861&quot;, &quot;SageMakerInferenceTime&quot;: &quot;2022-08-30T00:49:15Z&quot;}
{&quot;output&quot;: 1, &quot;SageMakerInferenceId&quot;: &quot;22445434-0c67-45e9-bb4d-bd1bf26561e6&quot;, &quot;SageMakerInferenceTime&quot;: &quot;2022-08-30T00:49:15Z&quot;}
</code></pre>
","9796588",0
1877,74238293,2,74223523,2022-10-28 16:30:05,2,"<p>Yeah you could make use of a either a Training Job or Processing Job (assuming the ML jobs are for transient training and/or processing).</p>
<p>The benefit of using SageMaker over ECS is that SageMaker manages the infrastructure. The Jobs are also transient and as such will be killed after training/processing while your artifacts will be automatically saved to S3.</p>
<p>With SageMaker Training or Processing Jobs all you need to do is bring your container (sitting in ECR) and kick off the Job with a single API (<a href=""https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_CreateTransformJob.html"" rel=""nofollow noreferrer"">CreateTrainingJob</a>, <a href=""https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_CreateProcessingJob.html"" rel=""nofollow noreferrer"">CreateProcessingJob</a>)</p>
","9796588",0
1878,74249675,2,73425020,2022-10-29 23:58:48,0,"<p>This could not be a perfect solution but I solved the issue by doing 2 things:</p>
<ul>
<li>avoid to use &quot;latest&quot; tag</li>
<li>load docker images on minikube</li>
</ul>
<pre class=""lang-bash prettyprint-override""><code>minikube image load image_name:tag_name  
minikube image load image_name:tag_name
</code></pre>
","14855070",0
1879,74249979,2,74249458,2022-10-30 01:41:02,1,"<p>I found the Access Key and the Secret Key.</p>
<ul>
<li>Access Key: <code>minio</code></li>
<li>Secret Key: <code>minio123</code></li>
</ul>
<p>Ref</p>
<ul>
<li><a href=""https://github.com/kubeflow/pipelines/blob/master/developer_guide.md"" rel=""nofollow noreferrer"">https://github.com/kubeflow/pipelines/blob/master/developer_guide.md</a></li>
</ul>
","14855070",0
1880,74257228,2,74232315,2022-10-30 22:33:55,0,"<p>Local file cannot be directly passed between component. You can either serialize it to a string or wrap it into an Artifact. For Artifact, you can first pass the <code>url</code> or <code>path</code> of the Artifact output to a container argument, which allows you to write content to this file inside the container:</p>
<p><a href=""https://www.kubeflow.org/docs/components/pipelines/v2/author-a-pipeline/components/#3-custom-container-components"" rel=""nofollow noreferrer"">https://www.kubeflow.org/docs/components/pipelines/v2/author-a-pipeline/components/#3-custom-container-components</a></p>
","8691531",2
1881,74257446,2,74236782,2022-10-30 23:27:03,2,"<p><strong>Disclaimer</strong>: I'm co-founder at Iterative, we are authors of DVC. My response doesn't come from my experience with all the tools mentioned above. I took this as an opportunity to try build a template for this use case in the DVC ecosystem and share this in case it's useful for anyone.</p>
<p>Here is the GitHub repo, I've built (Note: it's a template, not a real ML project, scripts are artificially simplified to show the essence of the multi model evaluation):</p>
<p><a href=""https://github.com/shcheklein/ensemble-dvc-template"" rel=""nofollow noreferrer"">DVC Model Ensemble</a></p>
<p>I've put together an extensive README with a few videos of CLI, VS Code, Studio tools.</p>
<p>The core part of the repo is this <a href=""https://dvc.org/doc/user-guide/pipelines"" rel=""nofollow noreferrer"">DVC pipeline</a>, that &quot;trains&quot; multiple models, collects their metrics, and then runs <code>evaluation</code> stage to &quot;reduce&quot; those metrics into the final one.</p>
<pre class=""lang-yaml prettyprint-override""><code>stages:
  train:
    foreach:
      - model-1
      - model-2
    do:
      cmd: python train.py
      wdir: ${item}
      params:
        - params.yaml:
      deps:
      - train.py
      - data
      outs:
      - model.pkl:
          cache: false
      metrics:
      - ../dvclive/${item}/metrics.json:
          cache: false
      plots:
      - ../dvclive/${item}/plots/metrics/acc.tsv:
          cache: false
          x: step
          y: acc
  evaluate:
    cmd: python evaluate.py
    deps: 
    - dvclive
    metrics:
    - evaluation/metrics.json:
        cache: false
</code></pre>
<p>It describes how to build and connect different things in the project, also makes the project &quot;runnable&quot; and reproducible. It can scale to any number of models (the first <code>foreach</code> clause).</p>
<p>Please, let me know if that fits your scenario and/or you have more requirements, happy to learn mode and iterate on it :)</p>
","298182",2
1882,74268390,2,74267692,2022-10-31 19:42:28,1,"<p>Most of the approaches in SHAP do require a background/baseline dataset.  It is only the TreeSHAP (to my knowledge) that can do without it (by using instead information stored in the trees themselves to know about how to &quot;integrate out features&quot; that are masked).  The Clarify documentation says it uses Kernel SHAP, so a background dataset is required.  However, notice that they will compute one for you if <code>baseline=None</code>, using clustering on the background data available to Clarify from your training the model in the first place.</p>
","10495893",0
1883,74294502,2,74286611,2022-11-02 19:09:28,3,"<p>You can use the top-level <a href=""https://dvc.org/doc/user-guide/project-structure/dvcyaml-files#variables"" rel=""nofollow noreferrer""><code>vars</code></a> of <code>dvc.yaml</code>:</p>
<p>Given <code>config/my_params.json</code>:</p>
<pre class=""lang-json prettyprint-override""><code>{&quot;group&quot;: {&quot;foo&quot;: 1, &quot;bar&quot;: 2}}
</code></pre>
<p>Your <code>dvc.yaml</code> would look like:</p>
<pre class=""lang-yaml prettyprint-override""><code>vars:
  - config/my_params.json

stages:
  echo-group:
    cmd: echo ${group}
</code></pre>
<p>Resulting in:</p>
<pre><code>$ dvc repro
Running stage 'echo-group':                                                  
&gt; echo --foo 1 --bar 2
--foo 1 --bar 2
Generating lock file 'dvc.lock'                                       
Updating lock file 'dvc.lock'
</code></pre>
","6076911",0
1884,74309108,2,74308758,2022-11-03 20:19:49,1,"<h2>Pipeline Model (sequential models)</h2>
<p>There is a specific mode in SageMaker:
Look at <a href=""https://sagemaker.readthedocs.io/en/stable/api/inference/pipeline.html"" rel=""nofollow noreferrer"">PipelineModel</a>.</p>
<blockquote>
<p>You can pass a list of sagemaker.Model objects in the order you want
the inference to happen.</p>
</blockquote>
<p>This is an official AWS example to follow:
<a href=""https://github.com/aws/amazon-sagemaker-examples/blob/main/sagemaker-pipelines/tabular/train-register-deploy-pipeline-model/train%20register%20and%20deploy%20a%20pipeline%20model.ipynb"" rel=""nofollow noreferrer"">Train register and deploy a pipeline model</a></p>
<pre class=""lang-py prettyprint-override""><code>from sagemaker import PipelineModel

pipeline_model = PipelineModel(
    models=[model_0, model_1, ...],
    role=role,
    sagemaker_session=pipeline_session
)
</code></pre>
<p>It works like a normal SageMaker Model, in fact you have the normal <a href=""https://sagemaker.readthedocs.io/en/stable/api/inference/pipeline.html#sagemaker.pipeline.PipelineModel.deploy"" rel=""nofollow noreferrer"">deployment method</a>.</p>
<p>You can also follow this <a href=""https://dev.to/aws-builders/deploying-the-xgboost-model-to-aws-from-locally-developed-artifacts-adding-inference-pipeline-fa5"" rel=""nofollow noreferrer"">guide</a> that show how to deploy an Xgboost model binary built for a developer, where a post-processing layer is added through an inference pipeline in sagemaker, deploying an endpoint.</p>
","20249888",0
1885,74342432,2,74338552,2022-11-07 05:50:28,0,"<p><code>wait_for_completion()</code> – stops the execution of the process of creation of the environment, until all the dependencies are satisfies. If the dependencies are not satisfactory, it will throw the error. Separate the dependencies from the creation process. Then we can avoid the method overlapping.</p>
<p><a href=""https://i.stack.imgur.com/EPgXr.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/EPgXr.png"" alt=""enter image description here"" /></a></p>
<p>I tried to reproduce the issue. Solved the error.</p>
","18428148",1
1886,74345200,2,74289302,2022-11-07 10:33:42,1,"<p>The default storage mechanism according to the UI in azure ML is blob storage. The output will be available in metrics folder in the container</p>
<p>Checkout the screens below</p>
<p><a href=""https://i.stack.imgur.com/1UTSb.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/1UTSb.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.stack.imgur.com/PQxog.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/PQxog.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.stack.imgur.com/UrVRH.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/UrVRH.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.stack.imgur.com/Yqdwc.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Yqdwc.png"" alt=""enter image description here"" /></a></p>
<p>Download the JSON which contains the output information.</p>
<p><a href=""https://i.stack.imgur.com/KxGfW.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/KxGfW.png"" alt=""enter image description here"" /></a></p>
","18428148",0
1887,74345786,2,74188251,2022-11-07 11:22:16,1,"<p>You can pass the <code>epochs</code> attribute in <code>custom_config</code> dict as shown in <a href=""https://www.tensorflow.org/tfx/tutorials/tfx/recommenders#training_the_model"" rel=""nofollow noreferrer"">example notebook</a>.</p>
<p>Example code:</p>
<pre><code>trainer = tfx.components.Trainer(
    module_file=os.path.abspath(_trainer_module_file),
    examples=ratings_transform.outputs['transformed_examples'],
    transform_graph=ratings_transform.outputs['transform_graph'],
    schema=ratings_transform.outputs['post_transform_schema'],
    train_args=tfx.proto.TrainArgs(num_steps=500),
    eval_args=tfx.proto.EvalArgs(num_steps=10),
    custom_config={
        'epochs':5,
        'movies':movies_transform.outputs['transformed_examples'],
        'movie_schema':movies_transform.outputs['post_transform_schema'],
        'ratings':ratings_transform.outputs['transformed_examples'],
        'ratings_schema':ratings_transform.outputs['post_transform_schema']
        })

context.run(trainer, enable_cache=False)
</code></pre>
","user11530462",0
1888,74353751,2,74352961,2022-11-07 22:41:29,1,"<p>You have to add line to the first cell (or before the first plt.show() call) of the notebook:</p>
<pre class=""lang-py prettyprint-override""><code>%matplotlib inline
</code></pre>
<p>This a is a <a href=""https://ipython.readthedocs.io/en/stable/interactive/tutorial.html#magics-explained"" rel=""nofollow noreferrer"">magic function</a> in IPython.</p>
<p>According to <a href=""https://ipython.readthedocs.io/en/stable/interactive/plotting.html"" rel=""nofollow noreferrer"">documentation</a>:</p>
<blockquote>
<p>With this backend, the output of plotting commands is displayed inline
within frontends like the Jupyter notebook, directly below the code
cell that produced it. The resulting plots will then also be stored in
the notebook document.</p>
</blockquote>
","20249888",0
1889,74360860,2,74181670,2022-11-08 12:34:41,0,"<p>Debian is not supportive of all the .net versions. Debian 11 will function with .Net core 3.1, and .Net 6.</p>
<p>The following versions of .NET are no longer supported:</p>
<p>•   .NET 5</p>
<p>•   .NET Core 3.0</p>
<p>•   .NET Core 2.2</p>
<p>•   .NET Core 2.1</p>
<p>•   .NET Core 2.0</p>
<p>Before installing .Net, run the commands which are mentioned below to add the Microsoft package signing key to your list of trusted keys.</p>
<pre><code>wget https://packages.microsoft.com/config/debian/11/packages-microsoft-prod.deb -O packages-microsoft-prod.deb
sudo dpkg -i packages-microsoft-prod.deb
rm packages-microsoft-prod.deb
</code></pre>
<p>Install the .Net SDK allows you to install .Net 6.0.</p>
<pre><code>sudo apt-get update &amp;&amp; \
  sudo apt-get install -y dotnet-sdk-6.0
</code></pre>
<p>using the following commands we can work with the Debian 11 connectivity.</p>
","18428148",0
1890,74361200,2,74328992,2022-11-08 13:02:37,1,"<p>Since MLFlow has a standardized <a href=""https://www.mlflow.org/docs/latest/models.html#storage-format"" rel=""nofollow noreferrer"">model storage format</a>, you just need to bring over the model files and start using them with the MLFlow package. In addition, you can register the model to the workspace's model registry using <code>mlflow.register_model()</code> and then use it from there. These would be the steps:</p>
<ol>
<li>On the AzureML side, I assume that you have an MLFlow model saved to disk (using <code>mlflow.sklearn.save_model()</code> or <code>mlflow.sklearn.autolog</code> -- or some other <code>mlflow.&lt;flavor&gt;</code>). That should give you a folder that contains an <code>MLModel</code> file, and, depending on the flavor of the model a few more files -- like the below:</li>
</ol>
<pre><code>mlflow-model
├── MLmodel
├── conda.yaml
├── model.pkl
└── requirements.txt
</code></pre>
<blockquote>
<p>Note: You can download the model from the AzureML Workspace using the
<a href=""https://learn.microsoft.com/en-us/azure/machine-learning/how-to-configure-cli?tabs=public"" rel=""nofollow noreferrer"">v2 CLI</a> like so: <code>az ml model download  --name &lt;model_name&gt; --version &lt;model_version&gt;</code></p>
</blockquote>
<ol start=""2"">
<li>Open a Databricks Notebook and make sure it has <code>mlflow</code> installed</li>
</ol>
<pre class=""lang-py prettyprint-override""><code>%pip install mlflow
</code></pre>
<ol start=""3"">
<li><p>Upload the MLFlow model files to the <code>dbfs</code> connected to the cluster
<a href=""https://i.stack.imgur.com/2ZXyf.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/2ZXyf.png"" alt=""enter image description here"" /></a></p>
</li>
<li><p>In the Notebook, register the model using MLFlow (adjust the <code>dbfs:</code> path to the location where the model was uploaded to).</p>
</li>
</ol>
<pre class=""lang-py prettyprint-override""><code>import mlflow

model_version = mlflow.register_model(&quot;dbfs:/FileStore/shared_uploads/mlflow-model/&quot;, &quot;AzureMLModel&quot;)

</code></pre>
<p>Now your model is registered in the Workspace's model registry like any model that was created from a Databricks session. So, you can access it from the registry like so:</p>
<pre class=""lang-py prettyprint-override""><code>model = mlflow.pyfunc.load_model(f&quot;models:/AzureMLModel/{model_version.version}&quot;)

input_example = {
   &quot;sepal_length&quot;: [5.1,4.8],
   &quot;sepal_width&quot;: [3.5,4.4],
   &quot;petal_length&quot;: [1.4,2.0],
   &quot;petal_width&quot;: [0.2,0.1]
 }
model.predict(input_example)
</code></pre>
<p><a href=""https://i.stack.imgur.com/m2Iv3.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/m2Iv3.png"" alt=""enter image description here"" /></a></p>
<p>Or use the model as a <code>spark_udf</code>:</p>
<pre class=""lang-py prettyprint-override""><code>import pandas as pd
model_udf = mlflow.pyfunc.spark_udf(spark=spark, model_uri=f&quot;models:/AzureMLModel/{model_version.version}&quot;, result_type='string' )
spark_df = spark.createDataFrame(pd.DataFrame(input_example))
spark_df = spark_df.withColumn('foo', model_udf())
display(spark_df)
</code></pre>
<p><a href=""https://i.stack.imgur.com/3KKL7.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/3KKL7.png"" alt=""enter image description here"" /></a></p>
<blockquote>
<p>Note that I am using <code>mlflow.pyfunc</code> to load the model since every
MLFlow model needs to support the <code>pyfunc</code> flavor. That way, you don't
need to worry about the native flavor of the model.</p>
</blockquote>
","8821969",0
1891,74361833,2,74357085,2022-11-08 13:50:09,0,"<p>The problem was indeed an wrong .tar.gz architecture
The model.tar.gz recovered from tfhub is</p>
<ul>
<li>model.tar.gz -&gt; 5.tar -&gt; variables dir and saved_model.pb</li>
</ul>
<p>While SageMaker is waiting for</p>
<ul>
<li>model.tar.gz -&gt; 5.tar -&gt; <strong>5</strong> -&gt; variables dir and saved_model.pb</li>
</ul>
<p>A simple re-packaging saved the day</p>
","20446922",0
1892,74365125,2,74364253,2022-11-08 17:50:28,0,"<p>Turns out this is the expected behavior: <code>At the end of the test serverless-artillery will generate a report of the test. Please note that this report is generated only for small load. See here for details.</code> (<a href=""https://www.npmjs.com/package/serverless-artillery#4-invoke"" rel=""nofollow noreferrer"">source</a>, <a href=""https://www.npmjs.com/package/serverless-artillery#providing-a-data-store-to-view-the-results-of-your-performance-test"" rel=""nofollow noreferrer"">here takes you here</a>).</p>
","5599955",0
1893,74367192,2,74058573,2022-11-08 21:07:25,1,"<p>Processing Jobs cannot pass <code>ExtraArgs={'ACL':'bucket-owner-full-control'})</code> for <code>S3Output</code>.</p>
<p>You could look at making the upload in the script running in the Processing Job yourself.</p>
","9796588",0
1894,74371369,2,74337674,2022-11-09 07:40:30,0,"<p>It is probably a misunderstanding (or bug) of sagemaker.</p>
<p>As a cheat, you could recreate the configuration with the same name and put a simple echo in it.</p>
<p>For example:</p>
<pre class=""lang-bash prettyprint-override""><code>#!/bin/bash
set -e

echo &quot;Nothing to do&quot;
</code></pre>
","20249888",0
1895,74372098,2,74255238,2022-11-09 08:49:33,0,"<p><a href=""https://repost.aws/questions/QUTEYNFSxHTK2Z2JbHM8yUfw/sage-maker-neo-compilation-unable-to-neo-compile-for-fp-16-and-int-8-precision"" rel=""nofollow noreferrer"">Received a response from an AWS source</a></p>
<p>&quot;Unfortunately Neo doesn't support quantization for Jetson Devices. It means you can only compile FP32 models and they will be FP32 after compilation.</p>
<p>I know this is not what you're looking for, but FYI, Neo supports int8 model optimization only for TFLite and targeting CPU not GPU. Check here some supported models: <a href=""https://docs.amazonaws.cn/en_us/sagemaker/latest/dg/neo-supported-edge-tested-models.html%22"" rel=""nofollow noreferrer"">https://docs.amazonaws.cn/en_us/sagemaker/latest/dg/neo-supported-edge-tested-models.html&quot;</a></p>
","10977777",0
1896,74373225,2,74341988,2022-11-09 10:21:03,1,"<p>First, we need to create the designer manually and assign the deployment manually for the first time. Input for the next iteration will be the web service input. For that we need to get the web service output to be attached with the evaluation model metrics. For the next time it will be running in repeated state.</p>
<p>Follow the steps to make the automation for the case of designer deployment from the designer itself.</p>
<p><a href=""https://i.stack.imgur.com/BfjNh.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/BfjNh.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.stack.imgur.com/p6Bka.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/p6Bka.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.stack.imgur.com/EILZl.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/EILZl.png"" alt=""enter image description here"" /></a></p>
<p>The above images are not being connected to the web service input for the automation.</p>
<p><a href=""https://i.stack.imgur.com/jhahX.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/jhahX.png"" alt=""enter image description here"" /></a></p>
<p>The above image will be taking the result of automation of the web service to perform the continuous deployment.</p>
<p>Connect to the compute target.</p>
<p><a href=""https://i.stack.imgur.com/GM73T.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/GM73T.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.stack.imgur.com/ylERb.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ylERb.png"" alt=""enter image description here"" /></a></p>
<p>Create new pipeline job</p>
<p>To deploy we need to have an inference cluster. It will be AKS by default.</p>
<p><a href=""https://i.stack.imgur.com/0MqZS.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/0MqZS.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.stack.imgur.com/GOA8r.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/GOA8r.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.stack.imgur.com/RYaRt.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/RYaRt.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.stack.imgur.com/ZY9HM.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ZY9HM.png"" alt=""enter image description here"" /></a></p>
<p>We will get the inference pipeline like above. Get the details of the web service endpoint and deploy it with the web service. First time have to do it manually, next time it will work automatically.</p>
<p><a href=""https://i.stack.imgur.com/Vscfz.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Vscfz.png"" alt=""enter image description here"" /></a></p>
<p>Above is the total structure will look like in designer for automation of the deployment.</p>
","18428148",4
1897,74375680,2,74278491,2022-11-09 13:32:14,0,"<h1>Solution</h1>
<p><a href=""https://pip.pypa.io/en/stable/cli/pip_check/"" rel=""nofollow noreferrer""><code>pip check</code></a> checks whether dependencies for installed packages are compatible.</p>
<pre><code>(venv) me@ubuntu-pcs:~/PycharmProjects/project$ pip check
# ...
aiobotocore 2.4.0 has requirement botocore&lt;1.27.60,&gt;=1.27.59, but you have botocore 1.29.0.
</code></pre>
<p>I had installed packages with conflicting dependencies. i.e. Packages that required different, but default, versions of <code>botocore</code>.</p>
<p>Hence why recreating <code>venv</code> didn't work.</p>
<p>In my case, I had to:</p>
<pre><code>conda install botocore==1.27.59
dvc push
</code></pre>
<p>And <code>dvc push</code> was working again :)</p>
","16852041",0
1898,74377608,2,74345000,2022-11-09 15:40:30,0,"<p>It seems to be the compromised image/environment.</p>
<p>I recommend directly using the most up-to-date image that includes python 3.10:</p>
<p><code>Data Science 3.0</code></p>
","20249888",0
1899,74384278,2,74342053,2022-11-10 04:49:06,0,"<p>Directly we cannot install TensorFlow in designer. Instead, we can call the node of the algorithm which includes TensorFlow internally. For example, I am performing Image classification using DenseNet. Checkout the following flow.</p>
<p><a href=""https://i.stack.imgur.com/69PwJ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/69PwJ.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.stack.imgur.com/rAi4o.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/rAi4o.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.stack.imgur.com/0Oggl.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/0Oggl.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.stack.imgur.com/7aaXC.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/7aaXC.png"" alt=""enter image description here"" /></a></p>
<p>This below screen is the complete picture of the flow in the designer.</p>
<p><a href=""https://i.stack.imgur.com/98hjL.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/98hjL.png"" alt=""enter image description here"" /></a></p>
","18428148",2
1900,74384896,2,74343072,2022-11-10 06:22:16,0,"<p>We can deploy using batch or real-time endpoint. Created a sample application with multiple models in the single pipeline. We need to get the individual model result into registration and then click on deploy in the model's section to get deployed from the pipelines perspective.</p>
<p><a href=""https://i.stack.imgur.com/P5qp4.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/P5qp4.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.stack.imgur.com/ZJKus.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ZJKus.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.stack.imgur.com/K3VpI.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/K3VpI.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.stack.imgur.com/RSerZ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/RSerZ.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.stack.imgur.com/f7tG4.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/f7tG4.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.stack.imgur.com/4ZcP1.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/4ZcP1.png"" alt=""enter image description here"" /></a></p>
<p>We can create any type of clusters or instances for deployment.</p>
<p><a href=""https://i.stack.imgur.com/Os0HF.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Os0HF.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.stack.imgur.com/sfigt.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/sfigt.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.stack.imgur.com/OnGzJ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/OnGzJ.png"" alt=""enter image description here"" /></a></p>
<p>Using any kind of real-time endpoint and deploy the model.</p>
","18428148",0
1901,74401281,2,74400188,2022-11-11 10:39:52,0,"<p>This is probably not the most elegant solution to this, but it works for me so I'll use it until I get a better solution. The solution was to return the path with the object on my <code>DataSet</code> implementation, I doubt that this would generalize for other datasets like SQL queries for example, but since I know that I have to be dealing with a file here, works fine. Here is my implementation:</p>
<pre><code>from kedro.io import AbstractDataSet
from spacy.tokens import DocBin
from dataclasses import dataclass
from typing import Union
from pathlib import Path


@dataclass
class DocBinModel:
    filepath: Path
    docbin: DocBin


class SpacyDocBinDataSet(AbstractDataSet):
    def __init__(self, filepath, save_args=None, load_args=None):
        self._filepath = filepath
        self._save_args = save_args or {}
        self._load_args = load_args or {}

    def _describe(self):
        return dict(
            filepath=self._filepath,
            save_args=self._save_args,
            load_args=self._load_args,
        )

    def _load(self):
        with open(self._filepath, &quot;rb&quot;) as f:
            docbin = DocBin().from_bytes(f.read())
        
        return DocBinModel(self._filepath, docbin)

    def _save(self, data: Union[DocBin, DocBinModel]):
        if isinstance(data, DocBinModel):
            data = data.docbin
        data.to_disk(self._filepath)

    def _exists(self):
        return Path(self._filepath).exists()
</code></pre>
","4534466",0
1902,74401447,2,74401139,2022-11-11 10:54:53,1,"<h2>Pretrained models</h2>
<p>If you want to use a pretrained model, you don't have to go through the estimator because you already have the artifact of the model.</p>
<p>You can see a guide here: <a href=""https://sagemaker-examples.readthedocs.io/en/latest/sagemaker-script-mode/pytorch_bert/deploy_bert_outputs.html#Host-a-Pretrained-Model-on-SageMaker"" rel=""nofollow noreferrer"">Host a Pretrained Model on SageMaker</a></p>
<p>In a nutshell, the turn it takes is to load the pre-trained model and package it in the usual <code>model.tar.gz</code> archive (which would be created following normal training with the estimator). If a custom inference script is required, there is the step of repacking the model.
At this point, it loads the uri of the model artifact and deploys the endpoint.</p>
<h4>Extra: Using JumpStart (on SageMaker Studio)</h4>
<p>A more interactive way is to use <a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/studio-jumpstart.html"" rel=""nofollow noreferrer"">SageMaker JumpStart</a> on <a href=""https://aws.amazon.com/it/sagemaker/studio/"" rel=""nofollow noreferrer"">SageMaker Studio</a>:<a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/algos.html#algorithms-built-in-jumpstart"" rel=""nofollow noreferrer"">Use Amazon SageMaker Built-in Algorithms or Pre-trained Models</a></p>
<hr />
<h2>Built-in algorithms (not pretrained)</h2>
<p>If the model is not pre-trained (so, will be trained), the <a href=""https://sagemaker.readthedocs.io/en/stable/api/training/estimators.html"" rel=""nofollow noreferrer"">Estimator</a> (a high level interface for SageMaker training) must be used by definition.</p>
<p>See <a href=""https://docs.amazonaws.cn/en_us/sagemaker/latest/dg/model-parallel-sm-sdk.html"" rel=""nofollow noreferrer"">Launch a Training Job Using the SageMaker Python SDK</a>:</p>
<blockquote>
<p>The SageMaker Python SDK supports managed training of models with ML
frameworks such as TensorFlow and PyTorch. To launch a training job
using one of these frameworks, you define a SageMaker TensorFlow
estimator, a SageMaker PyTorch estimator, or a SageMaker generic
Estimator to use the modified training script and model parallelism
configuration</p>
</blockquote>
","20249888",0
1903,74405255,2,74396941,2022-11-11 16:09:15,1,"<p>As ascertained in the comments, the instance on which the model runs is CPU-based.</p>
<p>This happens because when the model is deployed, it already assumes that the model has been created with the precise configuration.</p>
<p>We can try to make the container for the model explicit like this:</p>
<pre class=""lang-py prettyprint-override""><code>import sagemaker
from sagemaker.model import Model

# this retrieves 'pytorch-inference:1.12.0-gpu-py38'
inf_img_uri = sagemaker.image_uris.retrieve(
    framework='pytorch',
    region=region,
    image_scope='inference',
    version=&quot;1.12.0&quot;,
    instance_type='ml.g4dn.xlarge',
    py_version='py38'
)

pytorch_model = Model(
    image_uri=inf_img_uri,
    model_data=model_data,
    role=role,
    entry_point='inference.py',
    source_dir='code',
    code_location='s3://staging',
    name='Staging-Model'
)
</code></pre>
<p>If you are executing this within a pipeline, you may need a <a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/build-and-manage-steps.html#step-type-model"" rel=""nofollow noreferrer"">model creation step</a> before deployment.</p>
","20249888",0
1904,74439458,2,74315922,2022-11-15 00:38:24,0,"<p>It is not currently possible to attach an existing EFS to a new domain. If you have multiple users and data, I'd recommend the steps in the <a href=""https://docs.aws.amazon.com/whitepapers/latest/sagemaker-studio-admin-best-practices/appendix.html#studio-domain-backup-and-recovery"" rel=""nofollow noreferrer"">Backup and recovery</a> section here.</p>
<p>If it is only a handful user profiles, you can simply download the files/use S3 as an intermediate storage.</p>
","2458691",0
1905,74444530,2,73788620,2022-11-15 11:03:46,1,"<p>It is possible to do that via the notebook and re-assigning of the validation dataset to the dataset on which you want to perform validation.</p>
","11858026",0
1906,74447216,2,73906466,2022-11-15 14:23:22,0,"<p>A working MLTable file to convert string columns from parquet files looks like this:</p>
<pre class=""lang-yaml prettyprint-override""><code>--- 
type: mltable
paths: 
  - pattern: ./*.parquet
transformations: 
  - read_parquet: 
      include_path_column: false
  - convert_column_types:
      - columns: column_a
        column_type:
          datetime:
            formats:
              - &quot;%Y-%m-%d %H:%M:%S&quot;
  - convert_column_types:
    - columns: column_b
      column_type:
        datetime:
          formats:
            - &quot;%Y-%m-%d %H:%M:%S&quot;
</code></pre>
<p>(By the way, at the moment of writing this specifying multiple columns as array did not work, e.g. <code>columns: [column_a, column_b]</code>)</p>
","7641854",0
1907,74455281,2,74406041,2022-11-16 04:42:50,0,"<p>It seems you are working on V2 and you want to do workspace sharing, but there are some changes between CLI V1 and V2.</p>
<p>To migrate from Azure Machine Learning V1 to V2, you need to upgrade <strong>az ml workspace share</strong> commands to equivalent <strong>az role assignment create</strong> commands.</p>
<p>Please refer to here - <a href=""https://learn.microsoft.com/en-us/cli/azure/role/assignment?view=azure-cli-latest#az-role-assignment-create"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/cli/azure/role/assignment?view=azure-cli-latest#az-role-assignment-create</a>
Create a new role assignment for a user, group, or service principal.</p>
<pre><code>az role assignment create --role
                          [--assignee]
                          [--assignee-object-id]
                          [--assignee-principal-type {ForeignGroup, Group, ServicePrincipal, User}]
                          [--condition]
                          [--condition-version]
                          [--description]
                          [--name]
                          [--resource-group]
                          [--scope]
</code></pre>
<p>There are some examples for how to use it here for your reference - <a href=""https://learn.microsoft.com/en-us/cli/azure/role/assignment?view=azure-cli-latest#az-role-assignment-create-examples"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/cli/azure/role/assignment?view=azure-cli-latest#az-role-assignment-create-examples</a></p>
<p>I hope this helps!</p>
","9598801",0
1908,74457886,2,74445639,2022-11-16 09:16:49,1,"<p>once you retrieve your model from your workspace you  can do the following</p>
<pre><code>model.download(exist_ok=True)
</code></pre>
<p>This is shown in the docs- <a href=""https://learn.microsoft.com/en-us/python/api/azureml-core/azureml.core.model.model?view=azure-ml-py#azureml-core-model-model-download"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/python/api/azureml-core/azureml.core.model.model?view=azure-ml-py#azureml-core-model-model-download</a></p>
<p>This answer form @Ninja_coder for this question might be helpful- <a href=""https://stackoverflow.com/questions/60841319/model-get-model-pathmodel-name-model-throws-an-error-model-not-found-in-cac"">Model.get_model_path(model_name=&quot;model&quot;) throws an error: Model not found in cache or in root at</a></p>
","12711660",0
1909,74458476,2,74375517,2022-11-16 09:59:25,1,"<p>There is a way to make your endpoint private by using a VPN for your workspace</p>
<p>&quot;Azure Private Link enables you to connect to your workspace using a private endpoint. The private endpoint is a set of private IP addresses within your virtual network&quot;</p>
<p>docs has more detail about this - <a href=""https://learn.microsoft.com/en-us/azure/machine-learning/how-to-configure-private-link?tabs=cli"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/machine-learning/how-to-configure-private-link?tabs=cli</a></p>
<p>If it is not possible to use a vpn, another work around could be to deploy your models using azure function apps.</p>
<p>function apps have different levels of authentication and therefore is much more secure then a public endpoint that anyone can access.</p>
<p>function app security- <a href=""https://learn.microsoft.com/en-us/azure/azure-functions/security-concepts?tabs=v4"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/azure-functions/security-concepts?tabs=v4</a></p>
<p>beginners guide to function app- <a href=""https://learn.microsoft.com/en-us/azure/azure-functions/functions-reference-python?tabs=asgi%2Capplication-level&amp;pivots=python-mode-configuration"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/azure-functions/functions-reference-python?tabs=asgi%2Capplication-level&amp;pivots=python-mode-configuration</a></p>
","12711660",0
1910,74464590,2,74464062,2022-11-16 17:15:05,0,"<p>You will need to read content first then use pickle to load the content and create data frame</p>
<pre><code>test_features = s3.get_object(Bucket=bucket, Key= key)
body = test_features['Body'].read()
test_data = pickle.loads(body)
print(type(test_data))
</code></pre>
","6512807",2
1911,74465314,2,74449124,2022-11-16 18:12:52,1,"<p>Finally got my problem resolved. Turns out AWS put my account on hold pending verification. Once I submitted the relevant documents, AWS reviewed my account and removed the temporary hold.</p>
","14686347",0
1912,74466400,2,74465467,2022-11-16 19:52:01,1,"<p>I think this approach is conceptually wrong.</p>
<p>Files within sagemaker jobs (whether training or otherwise) should be passed during machine initialization. Imagine you have to create a job with 10 machines, do you want to read the file 10 times or replicate it directly by having it read once?</p>
<p>In the case of the training job, they should be passed into the fit (in the case of direct code like yours) or as TrainingInput in the case of pipeline.</p>
<p>You can follow this official AWS example: &quot;<a href=""https://sagemaker-examples.readthedocs.io/en/latest/frameworks/pytorch/get_started_mnist_train_outputs.html"" rel=""nofollow noreferrer"">Train an MNIST model with PyTorch</a>&quot;</p>
<p>However, the important part is simply passing a dictionary of input channels to the fit:</p>
<pre class=""lang-py prettyprint-override""><code>pytorch_estimator.fit({'training': s3_input_train})
</code></pre>
<p>You can put the name of the channel (in this case 'train') any way you want. The path s3 will be the one in your df.csv.</p>
<p>Within your script.py, you can read the df.csv directly between environment variables (or at least be able to specify it between argparse). Generic code with this default will suffice:</p>
<pre class=""lang-py prettyprint-override""><code>parser.add_argument(&quot;--train&quot;, type=str, default=os.environ[&quot;SM_CHANNEL_TRAINING&quot;])
</code></pre>
<p>It follows the nomenclature &quot;SM_CHANNEL_&quot; + your_channel_name.
So if you had put <code>&quot;train&quot;: s3_path</code>, the variable would have been called <code>SM_CHANNEL_TRAIN</code>.</p>
<p>Then you can read your file directly by pointing to the path corresponding to that environment variable.</p>
","20249888",2
1913,74506995,2,74384817,2022-11-20 09:38:03,0,"<p>I changed the code to below, replacing Sagwmaker.pytorch.Pytorch with sagemaker.estimator.Estimator, providing reference to the relevant Docker Image URI.</p>
<pre class=""lang-py prettyprint-override""><code>from sagemaker.estimator import Estimator

TRAIN_IMAGE_URI= &quot;763104351884.dkr.ecr.ap-south-1.amazonaws.com/pytorch-training:1.12.1-cpu-py38&quot;

trainer = Estimator(
    image_uri = TRAIN_IMAGE_URI,
    entry_point=&quot;train.py&quot;,
    source_dir=&quot;source_dir&quot;,  # directory of your training script
    role=role,
    base_job_name = base_job_name,
    instance_type= &quot;local&quot;,
    instance_count=1,
    output_path=output_path,
    hyperparameters = hyperparameters
)
trainer.fit()

</code></pre>
<p>This seems to work perfectly in my case.
Although I still do not understand why.</p>
","19655788",0
1914,74524683,2,74523342,2022-11-21 20:42:16,0,"<p>The Lake Database contents are stored as Parquet files and exposed via your <a href=""https://learn.microsoft.com/en-us/azure/synapse-analytics/sql/on-demand-workspace-overview"" rel=""nofollow noreferrer"">Serverless SQL endpoint</a> as <a href=""https://learn.microsoft.com/en-us/azure/synapse-analytics/sql/develop-tables-external-tables?tabs=native"" rel=""nofollow noreferrer"">External Tables</a>, so you can technically just query them via the endpoint. This is true for any tool or service that can connect to SQL, like Power BI, SSMS, Azure Machine Learning, etc.</p>
<p><strong>WARNING, HERE THERE BE DRAGONS:</strong> Due to the manner in which the serverless engine allocates memory for text queries, using this approach may result in significant performance issues, <em>up to and including service interruption</em>. Speaking from personal experience, this approach is NOT recommended. I recommend that you limit use of the Lake Database for Spark workloads or very limited investigation in the SQL pool. Fortunately there are a couple ways to sidestep these problems.</p>
<p><strong>Approach 1:</strong> Read directly from your Lake Database's storage location. This will be in your workspace's root container (declared at creation time) under the following path structure:</p>
<p><em>synapse/workspaces/{workspacename}/warehouse/{databasename}.db/{tablename}/</em></p>
<p>These are just Parquet files, so there are no special rules about accessing them directly.</p>
<p><strong>Approach 2:</strong> You can also create Views over your Lake Database (External Table) in a serverless database and <a href=""https://learn.microsoft.com/en-us/azure/synapse-analytics/sql/develop-tables-external-tables?tabs=native"" rel=""nofollow noreferrer"">use the WITH clause to explicitly assign properly sized schemas</a>. Similarly, you can ignore the External Table altogether and use OPENROWSET over the same storage mentioned above. I recommend this approach if you need to access your Lake Database via the SQL Endpoint.</p>
","75838",0
1915,74526033,2,74522541,2022-11-21 23:49:35,1,"<p>Its <strong>not possibile</strong> through IAM. There is no condition in sagemaker's IAM policies for kernel count. You can check available condition keys for sagemaker <a href=""https://docs.aws.amazon.com/service-authorization/latest/reference/list_amazonsagemaker.html#amazonsagemaker-policy-keys"" rel=""nofollow noreferrer"">here</a>.</p>
<p>You would have to develop a fully custom solution for monitoring user activities in sagemaker, and taking corrective actions based on your own criteria.</p>
","248823",0
1916,74528436,2,74528389,2022-11-22 06:43:40,1,"<p>You could try reshaping before calling <code>model.predict</code>:</p>
<pre><code>x = pd.DataFrame(xtest2)

model.predict(tf.keras.layers.Reshape((28, 28, 1))(x))
</code></pre>
","9657861",0
1917,74529120,2,74509933,2022-11-22 07:53:32,0,"<p>Update: I was able to resolve this issue</p>
<hr />
<p>The actual problem is that the endpoint is not able to <strong>ping</strong> the container. It is because, when there are multiple containers, every container is using some dynamic port for communication and endpoint needs to know which port every container is using. Hence, we need to write a custom code to replace the port value[8080] in nginx.conf file from the value which is inside the ['SAGEMAKER_BIND_TO_PORT'] environment variable.</p>
<p>The code to do the above thing is referenced from this sagemker example: <a href=""https://github.com/aws/amazon-sagemaker-examples/tree/main/contrib/inference_pipeline_custom_containers/containers"" rel=""nofollow noreferrer"">https://github.com/aws/amazon-sagemaker-examples/tree/main/contrib/inference_pipeline_custom_containers/containers</a></p>
<p>In the serve file, use the below <strong>start_server()</strong> function:</p>
<pre><code>def start_server():
    print('Starting the inference server with {} workers.'.format(model_server_workers))

    # link the log streams to stdout/err so they will be logged to the container logs
    subprocess.check_call(['ln', '-sf', '/dev/stdout', '/var/log/nginx/access.log'])
    subprocess.check_call(['ln', '-sf', '/dev/stderr', '/var/log/nginx/error.log'])
    
    port = os.environ.get(&quot;SAGEMAKER_BIND_TO_PORT&quot;, 8080)
    print(&quot;using port: &quot;, port)
    with open(&quot;nginx.conf.template&quot;) as nginx_template:
        template = Template(nginx_template.read())    
    nginx_conf = open(&quot;/opt/program/nginx.conf&quot;, &quot;w&quot;)
    nginx_conf.write(template.substitute(port=port))
    nginx_conf.close()

    nginx = subprocess.Popen(['nginx', '-c', '/opt/program/nginx.conf'])
    gunicorn = subprocess.Popen(['gunicorn',
                                 '--timeout', str(model_server_timeout),
                                 '-k', 'sync',
                                 '-b', 'unix:/tmp/gunicorn.sock',
                                 '-w', str(model_server_workers),
                                 'wsgi:app'])

    signal.signal(signal.SIGTERM, lambda a, b: sigterm_handler(nginx.pid, gunicorn.pid))

    # If either subprocess exits, so do we.
    pids = set([nginx.pid, gunicorn.pid])
    while True:
        pid, _ = os.wait()
        if pid in pids:
            break

    sigterm_handler(nginx.pid, gunicorn.pid)
    print('Inference server exiting')

</code></pre>
<p>Instead of nginx.conf use nginx.conf.template which will in-turn create the nginx.conf file with proper port:</p>
<pre><code>worker_processes 1;
daemon off; # Prevent forking


pid /tmp/nginx.pid;
error_log /var/log/nginx/error.log;

events {
  # defaults
}

http {
  include /etc/nginx/mime.types;
  default_type application/octet-stream;
  access_log /var/log/nginx/access.log combined;
  
  upstream gunicorn {
    server unix:/tmp/gunicorn.sock;
  }

  server {
    listen $port deferred;
    client_max_body_size 5m;

    keepalive_timeout 5;
    proxy_read_timeout 1200s;

    location ~ ^/(ping|invocations) {
      proxy_set_header X-Forwarded-For $$proxy_add_x_forwarded_for;
      proxy_set_header Host $$http_host;
      proxy_redirect off;
      proxy_pass http://gunicorn;
    }

    location / {
      return 404 &quot;{}&quot;;
    }
  }
}
</code></pre>
","10551921",0
1918,74534282,2,74524941,2022-11-22 14:31:26,0,"<p>Jupyter usually has a lot of overhead - also your syntax has three levels of <code>for</code> loops. In the python world, the lesser the <code>for</code> loops the better - also, <code>binary</code> data is almost always faster. So, a number of suggestions:</p>
<ul>
<li>restructure your for loops, use some specialized lib from pypi for file fs</li>
<li>change language? use <code>bash</code> script</li>
<li>multi threading is a way indeed</li>
<li>caching, use <code>redis</code> or other fast data structures to &quot;read-in&quot; data</li>
<li><code>golang</code> is comparatively easier to jump from python, also has good multi threading support - my 2 cents: its worth a try at least</li>
</ul>
","8689681",1
1919,74544565,2,74543837,2022-11-23 09:36:10,0,"<h3>Nature of the problem</h3>
<p>This happens by default because your experiment tries to <a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/automatic-model-tuning-ex-bucket.html"" rel=""nofollow noreferrer"">upload the source code</a> into its default bucket (which is the jumpstart bucket).</p>
<p>You can check the default bucket assigned to pipeline_session by printing <a href=""https://sagemaker.readthedocs.io/en/stable/api/utility/session.html#sagemaker.session.Session.default_bucket"" rel=""nofollow noreferrer"">pipeline_session.default_bucket()</a>.</p>
<p>Evidently you do not have the correct <a href=""https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_examples.html#policy_library_S3"" rel=""nofollow noreferrer"">write permissions on that bucket</a> and I encourage you to check them.</p>
<p>When you comment the entry_point, it doesn't give you that error precisely because it doesn't have anything to load. However, the moment it tries to do inference, it does not find the script clearly.</p>
<hr />
<h3>One possible quick and controllable solution</h3>
<p>If you want to apply a cheat to verify what I told you, try putting the <code>code_location</code> parameter in the <a href=""https://sagemaker.readthedocs.io/en/stable/api/inference/model.html"" rel=""nofollow noreferrer"">Model</a>.</p>
<p>This way you can control exactly where your pipeline step goes to write. You will clearly need to specify the s3 uri of the desired destination folder.</p>
<blockquote>
<p><strong>code_location</strong> (str) – Name of the S3 bucket where custom code is
uploaded (default: None). If not specified, the default bucket created
by sagemaker.session.Session is used.</p>
</blockquote>
","20249888",5
1920,74551264,2,74550808,2022-11-23 18:05:01,2,"<p>For each job within the pipeline you should have separate requirements (so you install only the stuff you need in each step and have full control over it).</p>
<p>To do this, you need to use the <code>source_dir</code> parameter:</p>
<blockquote>
<p><strong>source_dir</strong> (str or PipelineVariable) – Path (absolute, relative or an
S3 URI) to a directory with any other training source code
dependencies aside from the entry point file (default: None). If
source_dir is an S3 URI, it must point to a tar.gz file. Structure
within this directory are preserved when training on Amazon SageMaker.</p>
</blockquote>
<p>Look at the <a href=""https://sagemaker.readthedocs.io/en/stable/api/training/processing.html#sagemaker.processing.FrameworkProcessor.run"" rel=""nofollow noreferrer"">documentation in general for Processing</a> (you have to use <a href=""https://sagemaker.readthedocs.io/en/stable/api/training/processing.html#sagemaker.processing.FrameworkProcessor"" rel=""nofollow noreferrer"">FrameworkProcessor</a>).</p>
<p>Within the specified folder, there must be the script (in your case preprocess.py), any other files/modules that may be needed, and the <code>requirements.txt</code> file.</p>
<p>The structure of the folder then will be:</p>
<pre><code>BASE_DIR/
|- requirements.txt
|- preprocess.py
</code></pre>
<p>It is the common requirements file, nothing different. And it will be used automatically at the start of the instance, without any instruction needed.</p>
<hr />
<p>So, your code becomes:</p>
<pre class=""lang-py prettyprint-override""><code>from sagemaker.processing import FrameworkProcessor
from sagemaker.sklearn import SKLearn
from sagemaker.workflow.steps import ProcessingStep
from sagemaker.processing import ProcessingInput, ProcessingOutput


sklearn_processor = FrameworkProcessor(
    estimator_cls=SKLearn,
    framework_version='0.23-1',
    instance_type=processing_instance_type,
    instance_count=processing_instance_count,
    base_job_name=f&quot;{base_job_prefix}/job-name&quot;,
    sagemaker_session=pipeline_session,
    role=role
)

step_args = sklearn_processor.run(
    outputs=[
        ProcessingOutput(output_name=&quot;train&quot;, source=&quot;/opt/ml/processing/train&quot;),
        ProcessingOutput(output_name=&quot;validation&quot;, source=&quot;/opt/ml/processing/validation&quot;),
        ProcessingOutput(output_name=&quot;test&quot;, source=&quot;/opt/ml/processing/test&quot;),
    ],
    code=&quot;preprocess.py&quot;,
    source_dir=BASE_DIR,
    arguments=[&quot;--input-data&quot;, input_data],
)

step_process = ProcessingStep(
    name=&quot;PreprocessSidData&quot;,
    step_args=step_args
)
</code></pre>
<p>Note that I changed both the <code>code</code> parameter and the <code>source_dir</code>. It's a good practice to keep the folders for the various steps separate so you have a requirements.txt for each and don't create overlaps.</p>
","20249888",8
1921,74552275,2,74551835,2022-11-23 19:48:45,1,"<p>You have to specify the <code>source_dir</code>. Within your script then you can import the modules as you normally do.</p>
<blockquote>
<p><strong>source_dir</strong> (str or PipelineVariable) – Path (absolute, relative or an
S3 URI) to a directory with any other training source code
dependencies aside from the entry point file (default: None). If
source_dir is an S3 URI, it must point to a tar.gz file. Structure
within this directory are preserved when training on Amazon SageMaker.</p>
</blockquote>
<p>Look at the <a href=""https://sagemaker.readthedocs.io/en/stable/api/training/processing.html#sagemaker.processing.FrameworkProcessor.run"" rel=""nofollow noreferrer"">documentation in general for Processing</a> (you have to use <a href=""https://sagemaker.readthedocs.io/en/stable/api/training/processing.html#sagemaker.processing.FrameworkProcessor"" rel=""nofollow noreferrer"">FrameworkProcessor</a> and not the specific ones like SKLearnProcessor).</p>
<p>P.S.: The answer is similar to that of the question &quot;<a href=""https://stackoverflow.com/questions/74550808/how-to-install-additional-packages-in-sagemaker-pipeline/74551264#74551264"">How to install additional packages in sagemaker pipeline</a>&quot;.</p>
<p>Within the specified folder, there must be the script (in your case preprocess.py), any other files/modules that may be needed, and also eventually the requirements.txt file.</p>
<p>The structure of the folder then will be:</p>
<pre><code>BASE_DIR/
|─ helper_functions/
|  |─ your_utils.py
|─ requirements.txt
|─ preprocess.py
</code></pre>
<p>Within your preprocess.py, you will call the scripts in a simple way with:</p>
<pre class=""lang-py prettyprint-override""><code>from helper_functions.your_utils import your_class, your_func
</code></pre>
<hr />
<p>So, your code becomes:</p>
<pre class=""lang-py prettyprint-override""><code>from sagemaker.processing import FrameworkProcessor
from sagemaker.sklearn import SKLearn
from sagemaker.workflow.steps import ProcessingStep
from sagemaker.processing import ProcessingInput, ProcessingOutput

BASE_DIR = your_script_dir_path

sklearn_processor = FrameworkProcessor(
    estimator_cls=SKLearn,
    framework_version=framework_version,
    instance_type=processing_instance_type,
    instance_count=processing_instance_count,
    base_job_name=base_job_name,
    sagemaker_session=pipeline_session,
    role=role
)

step_args = sklearn_processor.run(
    inputs=[your_inputs],
    outputs=[your_outputs],
    code=&quot;preprocess.py&quot;,
    source_dir=BASE_DIR,
    arguments=[your_arguments],
)

step_process = ProcessingStep(
    name=&quot;ProcessingName&quot;,
    step_args=step_args
)
</code></pre>
<p>It's a good practice to keep the folders for the various steps separate for each and don't create overlaps.</p>
","20249888",7
1922,74552643,2,74405489,2022-11-23 20:28:32,1,"<p>The SageMaker SDK will always repackage the tar ball to include the <code>inference.py</code> script and then re-upload the tar ball to S3.</p>
<p>In general, SageMaker Framework containers will install the packages specified in the <code>requirements.txt</code> file.</p>
<p>If you do not want this to occur you can leave out the <code>requirements.txt</code> file and extend the <code>sagemaker-scikit-learn:0.23-1-cpu-py</code> container to include all the necessary dependencies. That way the packages will be baked into the image and every time you kick off a Batch Transform Job the packages will not be installed again.</p>
<p><a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/prebuilt-containers-extend.html"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/sagemaker/latest/dg/prebuilt-containers-extend.html</a></p>
","9796588",2
1923,74553419,2,74495545,2022-11-23 21:58:19,0,"<p><code>$[1:]</code>: indicates that we are excluding column 0 (the 'ID') before processing the inferences and keeping everything from column 1 to the last column (all the features)</p>
<p>Kindly see <a href=""https://github.com/aws/amazon-sagemaker-examples/blob/93163a8a75529984a7f8b0093d75e11f3f6cfb6a/sagemaker_batch_transform/batch_transform_associate_predictions_with_input/Batch%20Transform%20-%20breast%20cancer%20prediction%20with%20high%20level%20SDK.ipynb"" rel=""nofollow noreferrer"">this example</a> that showcases how to make use of input/output/joining filter feature of Batch Transform.</p>
","9796588",0
1924,74555094,2,74546629,2022-11-24 02:52:42,2,"<p>Currently it's not supported, we define search_space for hyperparameter sweep in inputs of train_model and call train_model.sweep() to create a sweep node based on train_model with specific run settings.</p>
<p><a href=""https://i.stack.imgur.com/33ZS8.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/33ZS8.png"" alt=""enter image description here"" /></a></p>
","11297406",0
1925,74555998,2,74519733,2022-11-24 05:31:02,1,"<p>we cannot edit the PYTHONPATH inside the default pipeline. Instead, we can create the Data Science VM using the ARM template and make the custom modifications inside the current working directory.</p>
<p><strong>Ubuntu Based:</strong></p>
<pre><code># create a Ubuntu Data Science VM in your resource group
az vm create --resource-group YOUR-RESOURCE-GROUP-NAME --name YOUR-VM-NAME --image microsoft-dsvm:linux-data-science-vm-ubuntu:linuxdsvmubuntu:latest --admin-username YOUR-USERNAME --admin-password YOUR-PASSWORD --generate-ssh-keys --authentication-type password
</code></pre>
<p><strong>Windows Based:</strong></p>
<pre><code># create a Windows Server 2016 DSVM in your resource group
az vm create --resource-group YOUR-RESOURCE-GROUP-NAME --name YOUR-VM-NAME --image microsoft-dsvm:dsvm-windows:server-2016:latest --admin-username YOUR-USERNAME --admin-password YOUR-PASSWORD --authentication-type password
</code></pre>
<p><strong>Create a conda environment for azure machine learning</strong></p>
<pre><code>**conda create -n py310 python=310**
</code></pre>
<p><strong>Activate and Install</strong></p>
<pre><code>conda activate py310
pip install azure-ai-ml
</code></pre>
<p><strong>To deploy:</strong></p>
<p>To deploy the template, we need to use the following procedure mentioned in below link.</p>
<p><a href=""https://learn.microsoft.com/en-us/azure/machine-learning/data-science-virtual-machine/dsvm-tutorial-resource-manager"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/machine-learning/data-science-virtual-machine/dsvm-tutorial-resource-manager</a></p>
","18428148",0
1926,74563657,2,74562532,2022-11-24 16:18:23,3,"<p>You need to save (commit and push) your results, otherwise neither you, GitLab, nor Iterative Studio will be able to retrieve any results.</p>
<pre class=""lang-yaml prettyprint-override""><code>...
# add a comment to GitLab
- cml comment create report.md

# upload code/metadata
- cml pr create . # this will git commit, push, and open a merge request
# upload data/artefacts
- dvc push
</code></pre>
<p>Note for <code>dvc push</code> to work, you will need to <a href=""https://cml.dev/doc/cml-with-dvc#cloud-storage-provider-credentials"" rel=""nofollow noreferrer"">setup storage credentials</a> if you haven't done so already.</p>
","3896283",0
1927,74565125,2,74456786,2022-11-24 18:48:41,1,"<p>Check out the blog post here for hosting code-server on SageMaker Studio - <a href=""https://aws.amazon.com/blogs/machine-learning/host-code-server-on-amazon-sagemaker/"" rel=""nofollow noreferrer"">https://aws.amazon.com/blogs/machine-learning/host-code-server-on-amazon-sagemaker/</a></p>
<p>And associated git repo here - <a href=""https://github.com/aws-samples/amazon-sagemaker-codeserver"" rel=""nofollow noreferrer"">https://github.com/aws-samples/amazon-sagemaker-codeserver</a></p>
","2458691",0
1928,74569056,2,74556459,2022-11-25 06:34:44,0,"<p>found the answer to my question. Apparently, the sagemaker environment is using an old build of XGBoost, around version 0.9. As the XGboost team make constant upgrades and changes to their library, AWS was unable to keep up with it.</p>
<p>That said I was able to run my code below by downgrading the XGBoost library on my environment from 1.7 to 0.9 and it works like a charm.</p>
<pre><code>t = tarfile.open('model.tar.gz', 'r:gz')
t.extractall()
model = pkl.load('xgboost-model', 'rb')
</code></pre>
","4095707",0
1929,74576975,2,74576523,2022-11-25 18:58:57,1,"<p>Things get a bit messy when you move between between the L1 (<code>CfnModel</code>) and L2 (<code>Role</code>) <a href=""https://docs.aws.amazon.com/cdk/v2/guide/cfn_layer.html"" rel=""nofollow noreferrer"">abstraction levels</a>.  You need to use the so-called <a href=""https://docs.aws.amazon.com/cdk/v2/guide/cfn_layer.html#cfn_layer_resource"" rel=""nofollow noreferrer"">ecape hatch syntax</a>:</p>
<pre class=""lang-py prettyprint-override""><code>cfnRole = cast(iam.CfnRole, model_role.node.default_child) # cast if using typings

sagemaker_model.add_depends_on(cfnRole)
</code></pre>
","1103511",0
1930,74577898,2,74561856,2022-11-25 21:13:15,0,"<p>See some sample event patterns here - <a href=""https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-events-rule.html#aws-resource-events-rule--examples"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-events-rule.html#aws-resource-events-rule--examples</a></p>
<p>Your <code>source</code> should be a custom source, and cannot contain <code>aws.</code> (Reference -https://docs.aws.amazon.com/eventbridge/latest/userguide/eb-events.html)</p>
","2458691",0
1931,74598115,2,74596339,2022-11-28 08:53:47,0,"<p>Your task can be solved with a simple processing job.</p>
<p>By using SKLearnProcessor, you already imply the use of a scikit-learn container. Otherwise, you can use a generic ScriptProcessor and specify the most appropriate container (already existing in sagemaker, such as that of scikit-learn or one of your own completely customised).</p>
<p><strong>Within the scikit-learn container, libraries such as pandas and numpy are already present.</strong> You can look at the <a href=""https://github.com/aws/sagemaker-scikit-learn-container/blob/master/requirements.txt"" rel=""nofollow noreferrer"">complete list of requirements</a> here.</p>
<p>Below is an example of code that answers the questions on file input/output from S3:</p>
<pre class=""lang-py prettyprint-override""><code>from sagemaker.processing import ProcessingInput, ProcessingOutput
from sagemaker.sklearn.processing import SKLearnProcessor

sklearn_processor = SKLearnProcessor(
    framework_version=framework_version,  # e.g. &quot;1.0-1&quot;, 
    role=role, 
    instance_type=your_instance_type,  # e.g. 'ml.m5.large'
    base_job_name = your_base_job_name,
    instance_count=your_instance_count,  # e.g. 1
)

sklearn_processor.run(
    code=your_script_path,
    inputs=[
        ProcessingInput(
            input_name='insert-custom-name-for-first-file',
            source=first_file_s3_uri,
            destination=&quot;/opt/ml/processing/input/data&quot;,
            s3_data_type='S3Prefix',
            s3_input_mode=&quot;File&quot;
        ),
        ProcessingInput(
            input_name='insert-custom-name-for-second-file',
            source=second_file_s3_uri,
            destination=&quot;/opt/ml/processing/input/data&quot;,
            s3_data_type='S3Prefix',
            s3_input_mode=&quot;File&quot;
        )
    ],
    outputs=[
        ProcessingOutput(
            output_name=&quot;output-channel-name&quot;,
            destination=output_s3_path_uri,
            source=&quot;/opt/ml/processing/processed_data&quot;
        )
    ]
)
</code></pre>
<p><em>If both input files reside in the same folder on S3, you can directly load a single <a href=""https://sagemaker.readthedocs.io/en/stable/api/training/processing.html#sagemaker.processing.ProcessingInput"" rel=""nofollow noreferrer"">ProcessingInput</a> pointing to that folder instead of separating the two files. However, since they are only two, I recommend distinguishing them as I have done in the example.</em></p>
<hr />
<p>For dependencies: if they are modules to be loaded, you could pass them just like ProcessingInput. See the <a href=""https://sagemaker.readthedocs.io/en/stable/api/training/processing.html#sagemaker.processing.FrameworkProcessor.run"" rel=""nofollow noreferrer"">run() documentation</a> under <code>source_dir</code>, <code>dependencies</code> and <code>git_config</code>. That way you can choose the optimal configuration for your task.</p>
<hr />
<p>In conclusion, using the sklearn container directly is not wrong. Install maybe something you don't need, but it's not a lot of stuff. If you have no particular need for compatibility with other libraries, use this ready-made container. Otherwise, go with a <a href=""https://sagemaker-examples.readthedocs.io/en/latest/advanced_functionality/scikit_bring_your_own/scikit_bring_your_own.html"" rel=""nofollow noreferrer"">custom container</a>.</p>
","20249888",3
1932,74621913,2,74611142,2022-11-30 01:20:30,0,"<p>While I am not sure exactly on what out come you want when you refer to &quot;compress the metrics into a single alarm.&quot; You can look at using <a href=""https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/using-metric-math.html"" rel=""nofollow noreferrer"">metric math</a></p>
","9796588",0
1933,74624575,2,74603005,2022-11-30 08:06:28,0,"<p>The solution is to use the mlflow.client module instead of the mlflow module as stated in the documentation of the mlflow client:</p>
<blockquote>
<p>The mlflow.client module provides a Python CRUD interface to MLflow
Experiments, Runs, Model Versions, and Registered Models. This is a
lower level API that directly translates to MLflow REST API calls. For
a higher level API for managing an “active run”, use the mlflow
module.</p>
</blockquote>
<ul>
<li><a href=""https://mlflow.org/docs/latest/python_api/mlflow.client.html"" rel=""nofollow noreferrer"">mlflow client documentation</a></li>
</ul>
<p>@Andre: Thanks for pointing me in the right direction.</p>
","16766083",0
1934,74630621,2,74621585,2022-11-30 15:54:33,0,"<pre><code>account_url = &quot;https://&lt;storageaccountname&gt;.blob.core.windows.net&quot;
default_credential = DefaultAzureCredential()

# Create the BlobServiceClient object
blob_service_client = BlobServiceClient(account_url, credential=default_credential)
</code></pre>
<p>Here is the <a href=""https://learn.microsoft.com/en-us/azure/storage/blobs/storage-quickstart-blobs-python?tabs=managed-identity%2Croles-azure-portal%2Csign-in-azure-cli"" rel=""nofollow noreferrer"">document</a> for connecting the Azure Blob Storage client library for Python to manage blobs and containers. Follow these steps to install the package and try out <a href=""https://learn.microsoft.com/en-us/azure/storage/blobs/storage-quickstart-blobs-python?tabs=managed-identity%2Croles-azure-portal%2Csign-in-azure-cli"" rel=""nofollow noreferrer"">example code</a> for basic tasks in an interactive console app.</p>
","11297406",0
1935,74643104,2,74387655,2022-12-01 14:03:02,0,"<p>It seems that the problem is related to the number of parallel worker setted. If i change PARALLELISM to some other values (&lt; 8) it works.</p>
","11670984",0
1936,74645618,2,74638888,2022-12-01 17:06:20,1,"<p>You are probably looking for <a href=""https://dvc.org/doc/user-guide/project-structure/dvcyaml-files#templating"" rel=""nofollow noreferrer"">Templating</a>. The very first example templates an output file:</p>
<p><code>params.yaml</code>:</p>
<pre class=""lang-yaml prettyprint-override""><code>models:
  us:
    threshold: 10
    filename: 'model-us.hdf5'
</code></pre>
<p><code>dvc.yaml</code>:</p>
<pre class=""lang-yaml prettyprint-override""><code>stages:
  build-us:
    cmd: &gt;-
      python train.py
      --thresh ${models.us.threshold}
      --out ${models.us.filename}
    outs:
      - ${models.us.filename}:
          cache: true
</code></pre>
","298182",5
1937,74670123,2,74444916,2022-12-03 19:45:55,0,"<p>You can calculate its coherence value and compare it with previous one. See <a href=""http://svn.aksw.org/papers/2015/WSDM_Topic_Evaluation/public.pdf"" rel=""nofollow noreferrer"">Michael Roeder, Andreas Both and Alexander Hinneburg: “Exploring the space of topic coherence measures</a>, and if you're using <code>gensim</code> with python, check its implementation at <a href=""https://radimrehurek.com/gensim/models/coherencemodel.html"" rel=""nofollow noreferrer""><code>CoherenceModel</code></a>.</p>
","13727871",0
1938,74712763,2,74697155,2022-12-07 06:52:41,0,"<p>Issue got resolved, I had to register the dataset in AzureML with proper configurations</p>
","653397",0
1939,74720234,2,74717490,2022-12-07 16:54:34,3,"<p>This is a little more involved in v2 than in was in v1. The reason is that v2 makes a clear distinction between the control plane (where you start/stop your job, deploy compute, etc.) and the data plane (where you run your data science code, load data from storage, etc.).</p>
<p>Jobs can do control plane operations, but they need to do that with a proper identity that was explicitly assigned to the job by the user.</p>
<p>Let me show you the code how to do this first. This script creates an MLClient and then connects to the service using that client in order to retrieve the job's metadata from which it extracts the name of the user that submitted the job:</p>
<pre class=""lang-py prettyprint-override""><code># control_plane.py
from azure.ai.ml import MLClient
from azure.ai.ml.identity import AzureMLOnBehalfOfCredential
import os

def get_ml_client():
    uri = os.environ[&quot;MLFLOW_TRACKING_URI&quot;]
    uri_segments = uri.split(&quot;/&quot;)
    subscription_id = uri_segments[uri_segments.index(&quot;subscriptions&quot;) + 1]
    resource_group_name = uri_segments[uri_segments.index(&quot;resourceGroups&quot;) + 1]
    workspace_name = uri_segments[uri_segments.index(&quot;workspaces&quot;) + 1]
    credential = AzureMLOnBehalfOfCredential()
    client = MLClient(
        credential=credential,
        subscription_id=subscription_id,
        resource_group_name=resource_group_name,
        workspace_name=workspace_name,
    )
    return client

ml_client = get_ml_client()
this_job = ml_client.jobs.get(os.environ[&quot;MLFLOW_RUN_ID&quot;])
print(&quot;This job was created by:&quot;, this_job.creation_context.created_by)
</code></pre>
<p>As you can see, the code uses a special <code>AzureMLOnBehalfOfCredential</code> to create the MLClient. Options that you would use locally (<code>AzureCliCredential</code> or <code>InteractiveBrowserCredential</code>) won't work for a remote job since you are not authenticated through <code>az login</code> or through the browser prompt on that remote run. For your credentials to be available on the remote job, you need to run the job with <code>user_identity</code>. And you need to retrieve the corresponding credential from the environment by using the <code>AzureMLOnBehalfOfCredential</code> class.</p>
<p>So, how do you run a job with <code>user_identity</code>? Below is the yaml that will achieve it:</p>
<pre class=""lang-yaml prettyprint-override""><code>$schema: https://azuremlschemas.azureedge.net/latest/commandJob.schema.json
type: command
command: |
  pip install azure-ai-ml 
  python control_plane.py
code: code
environment: 
  image: library/python:latest
compute: azureml:cpu-cluster
identity:
  type: user_identity
</code></pre>
<p>Note the <code>identity</code> section at the bottom. Also note that I am lazy and install the azureml-ai-ml sdk as part of the job. In a real setting, I would of course create an environment with the package installed.</p>
<p>These are the valid settings for the identity type:</p>
<ul>
<li><code>aml_token</code>: this is the default which will not allow you to access the control plane</li>
<li><code>managed</code> or <code>managed_identity</code>: this means the job will be run under the given managed identity (aka compute identity). This would be accessed in your job via <code>azure.identity.ManagedIdentityCredential</code>. Of course, you need to provide the chosen compute identity with access to the workspace to be able to read job information.</li>
<li><code>user_identity</code>: this will run the job under the submitting user's identity. It is to be used with the <code>azure.ai.ml.identity.AzureMLOnBehalfOfCredential</code> credentials as shown above.</li>
</ul>
<p>So, for your use case, you have 2 options:</p>
<ol>
<li>You could run the job with <code>user_identity</code> and use the <code>AzureMLOnBehalfOfCredential</code> class to create the MLClient</li>
<li>You could create the compute with a managed identity which you give access to the workspace and then run the job with <code>managed_identity</code> and use the <code>ManagedIdentityCredential</code> class to create the MLClient</li>
</ol>
","8821969",1
1940,74729735,2,74728242,2022-12-08 11:39:11,1,"<p><strong>Try Below Solutions :</strong></p>
<p><strong>1)</strong> If a pod doesn't have sidecar.istio.io/inject annotation, it will not be injected Sidecar by default. And because of the istio-system's global-deny-all AuthorizationPolicy policy, the pod is not accessible.</p>
<blockquote>
<pre><code>$ kubectl get AuthorizationPolicy -A
NAMESPACE                   NAME                              AGE
istio-system                cluster-local-gateway             5d10h
istio-system                global-deny-all                   5d10h
istio-system                istio-ingressgateway              5d10h
...
</code></pre>
</blockquote>
<pre><code>$ kubectl get AuthorizationPolicy global-deny-all -n istio-system -o yaml 
apiVersion: security.istio.io/v1beta1
kind: AuthorizationPolicy
metadata:
  annotations:
    kubectl.kubernetes.io/last-applied-configuration: |
      {&quot;apiVersion&quot;:&quot;security.istio.io/v1beta1&quot;,&quot;kind&quot;:&quot;AuthorizationPolicy&quot;,&quot;metadata&quot;:{&quot;annotations&quot;:{},&quot;name&quot;:&quot;global-deny-all&quot;,&quot;namespace&quot;:&quot;istio-system&quot;},&quot;spec&quot;:{}}
  creationTimestamp: &quot;2021-06-11T14:51:22Z&quot;
  generation: 1
  name: global-deny-all
  namespace: istio-system
  resourceVersion: &quot;1191&quot;
  uid: e8b243c6-23c9-48a7-8947-b297ffd7d302
spec: {}
</code></pre>
<p>Set an allow-all AuthorizationPolicy, the serving pod was accessible</p>
<pre><code>apiVersion: security.istio.io/v1beta1
kind: AuthorizationPolicy
metadata:
  name: allow-all
  namespace: kubeflow-user-example-com
spec:
 rules:
 - {}
</code></pre>
<p>Note : You should not apply this AuthorizationPolicy, as it allows all traffic from anywhere to access your namespace. Which would also mean others can access your notebook instances, for example.</p>
<p>You do want to allow all communication within the namespace, so you try to add a selector for the same namespace.</p>
<pre><code>apiVersion: security.istio.io/v1beta1
kind: AuthorizationPolicy
metadata:
  name: allow-all-from-same-ns
  namespace: kubeflow-user-example-com
spec:
  action: ALLOW
  rules:
  - from:
     - source:
        namespaces: [&quot;kubeflow-user-example-com&quot;] 
</code></pre>
<p><strong>2)</strong> Disable Sidecar for the namespace using a kubectl command.
Another option would be to disable Istio injection for the xxx pods by adding the sidecar.istio.io/inject: &quot;false&quot; label to the pods.</p>
<p>You can find how to disable <a href=""https://istio.io/latest/docs/setup/additional-setup/sidecar-injection/"" rel=""nofollow noreferrer"">Istio Sidecar</a> by adding a label for the namespace.
Disable the Sidecar for the two related pods by running the commands below.</p>
<pre><code>$ kubectl label pods &lt;20210601-zjwzh-3877683498&gt; -n kubeflow-user-example-com sidecar.istio.io/inject=&quot;false&quot;  --overwrite
 
$ k label pod &lt;xxx-infraval-modelserver-72n7m&gt; -n kubeflow-user-example-com sidecar.istio.io/inject=&quot;false&quot;  --overwrite
</code></pre>
<p><strong>3)</strong> Update the ServiceRoleBinding which is actually generated/monitored/managed by the profile controller in the kubeflow namespace instead of the validating webhook.</p>
<p>The RBAC issue is based on the params.yaml in the profiles manifest folder, the rule is generated as</p>
<pre><code>request.headers[]: regal.l.c.lei@XXXX.com
</code></pre>
<p>instead of</p>
<pre><code>request.headers[kubeflow-userid]: regal.l.c.lei@XXXX.com
</code></pre>
<p>Check you may mis-configed the value as blank instead of userid-header=kubeflow-userid in the params.yaml.
Also please check the kubernetes official document how to Determine Whether a Request is Allowed or Denied for more information.</p>
","19818461",1
1941,74751115,2,74689932,2022-12-10 07:21:47,0,"<p>It seems like I  had the order of operations in the dataprep mixed up, so that I converted floats to int and replaced missing numerical values before doing a one hot encoding of a categorical varaible. That last variable did not get the proper treatment since it was created after cleaning up the rest of the data</p>
","3032519",0
1942,74768194,2,74748411,2022-12-12 08:05:32,0,"<p>Hey you can do multiple things:</p>
<ol>
<li>Either have the config file in the <code>source_dir</code>, along with the entry point. This doesn't have to be local, it can also come from a git repo, as indicated here: <a href=""https://aws.amazon.com/blogs/machine-learning/git-integration-now-available-for-amazon-sagemaker-python-sdk/"" rel=""nofollow noreferrer"">blog</a>, <a href=""https://github.com/aws/amazon-sagemaker-examples/blob/main/sagemaker-python-sdk/pytorch_lstm_word_language_model/pytorch_rnn.ipynb"" rel=""nofollow noreferrer"">demo</a></li>
<li>Or you could bring the config file via S3, using SageMaker input or checkpoint channels (<a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/model-train-storage.html"" rel=""nofollow noreferrer"">doc</a>)</li>
</ol>
","5331834",1
1943,74786644,2,74745554,2022-12-13 14:47:40,1,"<p>I’ve gotten the solution from pyg discussion <a href=""https://github.com/pyg-team/pytorch_geometric/discussions/6197"" rel=""nofollow noreferrer"">here</a></p>
<p>So basically you can get around this by iterating over all `MessagePassing layers and setting:</p>
<pre><code>loaded_model = mlflow.pytorch.load_model(logged_model)
for conv in loaded_model.conv_layers:
    conv.aggr_module = SumAggregation()
</code></pre>
<p>This should fix the problem!</p>
","15797975",0
1944,74787866,2,74787715,2022-12-13 16:18:28,0,"<p>You can use the az ml online-deployment get_logs command instead. Log Analytics support will be partial at GA. Application Insights support is not included yet.</p>
","11297406",0
1945,74825867,2,74694537,2022-12-16 14:34:20,0,"<p>I think I fixed the problem.  I changed my &quot;checkpoint_s3_bucket&quot; to the session default bucket.  Haven't gotten an error since.</p>
<pre><code>  bucket=sagemaker.Session().default_bucket()
  base_job_name=&quot;sagemaker-checkpoint-test&quot;
  checkpoint_in_bucket=&quot;checkpoints&quot;
  # The S3 URI to store the checkpoints
  checkpoint_s3_bucket=&quot;s3://{}/{}/{}&quot;.format(bucket, base_job_name, checkpoint_in_bucket)
</code></pre>
","7542403",0
1946,74833079,2,74507434,2022-12-17 09:38:05,1,"<p>The MLTable file should be named <em>exactly</em> as MLTable - no filename extension. You have MLTable.yaml, which is not allowed.</p>
","20800425",0
1947,74851644,2,74851620,2022-12-19 14:11:01,1,"<p>The files have GZ extension. That's a common extension for GZip. Whatever is in those zipped files, you need to unzip them first.</p>
<p>Other than that, use <code>{i:03d}</code> for a 3 digit number with leading zeros.</p>
","480982",0
1948,74854138,2,74853843,2022-12-19 17:47:16,1,"<p>As written in the <a href=""https://sagemaker.readthedocs.io/en/stable/amazon_sagemaker_model_building_pipeline.html#not-all-built-in-python-operations-can-be-applied-to-parameters"" rel=""nofollow noreferrer"">documentation</a>:</p>
<blockquote>
<p>Pipeline parameters can only be evaluated at run time. If a pipeline
parameter needs to be evaluated at compile time, then it will throw an
exception.</p>
</blockquote>
<h2>A way to use parameters as ProcessingStep arguments</h2>
<p>If your requirement is to use them for a pipeline step, in particular the ProcessingStep, you will have to use the run method to use the arguments (which is different from job_arguments).</p>
<p>See this <a href=""https://github.com/aws/amazon-sagemaker-examples/blob/main/sagemaker-pipelines/tabular/abalone_build_train_deploy/sagemaker-pipelines-preprocess-train-evaluate-batch-transform.ipynb"" rel=""nofollow noreferrer"">official example</a>.</p>
<blockquote>
<p>By passing the pipeline_session to the sagemaker_session, calling
.run() does not launch the processing job, it returns the arguments
needed to run the job as a step in the pipeline.</p>
</blockquote>
<pre class=""lang-py prettyprint-override""><code>step_process = ProcessingStep(
   step_args=your_processor.run(
       # ...
       arguments=[&quot;--foo&quot;, foo]
   )
)
</code></pre>
<hr />
<p>In addition, there are some limitations: <a href=""https://sagemaker.readthedocs.io/en/stable/amazon_sagemaker_model_building_pipeline.html#not-all-built-in-python-operations-can-be-applied-to-parameters"" rel=""nofollow noreferrer"">Not all built-in Python operations can be applied to parameters</a>.</p>
<p>An example taken from the link above:</p>
<pre class=""lang-py prettyprint-override""><code># An example of what not to do
my_string = &quot;s3://{}/training&quot;.format(ParameterString(name=&quot;MyBucket&quot;, default_value=&quot;&quot;))

# Another example of what not to do
int_param = str(ParameterInteger(name=&quot;MyBucket&quot;, default_value=1))

# Instead, if you want to convert the parameter to string type, do
int_param.to_string()

# A workaround is to use Join
my_string = Join(on=&quot;&quot;, values=[
    &quot;s3://&quot;,
    ParameterString(name=&quot;MyBucket&quot;, default_value=&quot;&quot;),
    &quot;/training&quot;]
)
</code></pre>
<hr />
<h2>A way to use parameters to manipulate the pipeline internally</h2>
<p>Personally, I prefer to pass the value directly when you get the pipeline definition before the start:</p>
<pre class=""lang-py prettyprint-override""><code>def get_pipeline(my_param_hardcoded, ...):

    # here you can use my_param_hardcoded
   
    my_param = ParameterString(
        name=&quot;Foo&quot;, default_value=&quot;foo&quot;
    )

   # pipeline's steps definition here

   return pipeline = Pipeline(
        name=pipeline_name,
        parameters=[my_param, ...],
        steps=[...],
        sagemaker_session=sagemaker_session,
    )
   return pipeline
</code></pre>
<pre class=""lang-py prettyprint-override""><code>pipeline = get_pipeline(my_param_hardcoded, ...)
pipeline.start(parameters=dict(Foo=my_param_hardcoded))
</code></pre>
<p>Obviously this is not a really elegant way, but I do not think it is conceptually wrong because after all it is a parameter that will be used to manipulate the pipeline and cannot be pre-processed beforehand (e.g. in a configuration file).</p>
<p>An example of use is the creation of a name which can be based on the pipeline_name (which is clearly passed in the get_pipeline() and a pipeline parameter). For example, if we wanted to create a custom name for a step, it could be given by the concatenation of the two strings, and this cannot happen at runtime but must be done with this trick.</p>
","20249888",4
1949,74856555,2,73844060,2022-12-19 22:10:07,1,"<p>A new feature allows you to <a href=""https://aws.amazon.com/blogs/machine-learning/operationalize-your-amazon-sagemaker-studio-notebooks-as-scheduled-notebook-jobs/"" rel=""nofollow noreferrer"">Operationalize your Amazon SageMaker Studio notebooks as scheduled notebook jobs</a></p>
","121956",0
1950,74883763,2,74629440,2022-12-22 03:01:04,3,"<p>Here are two ways to get the data from <code>component</code></p>
<ol>
<li>using <code>return</code> from component function</li>
<li>save and load data with <code>from kfp.components import OutputPath</code></li>
</ol>
<p>I provides examples about <code>return</code>, <code>OutputPath</code>, <code>InputPath</code> with code.</p>
<ol>
<li><p>case 1: return single value, type of <code>&lt;T&gt;</code></p>
<pre><code>from kfp.components import func_to_container_op   
# I make component using `kfp.components.func_to_container_op`

def case_1(value_1: int, value_2: int) -&gt; int:  # to use return, write '-&gt; {type}' 
    result = value_1 + value_2       

    return result    # it can be int, float, bouble, str, bool, .... 

case_1_op = func_to_container_op(func = case_1,
                                 base_image = 'case_1/tag:0.1',
                              output_component_file=&quot;case_1.component.yaml&quot;)
</code></pre>
</li>
<li><p>case 2: return multiple value</p>
<pre><code>from kfp.components import func_to_container_op   
from typing import NamedTuple

# If there are multiple return values, wrap them in a tuple.
# And use `NamedTuple`
def case_2(value_1: int) -&gt; NamedTuple('Output', [(&quot;key_of_foo&quot;, int), 
                                                  ('key_of_bar', str),
                                                  ('key_of_baz', bool)]):
    foo = value_1        # int
    bar = &quot;2&quot;            # str
    baz = True           # bool

    return (foo, bar, baz)      #  wrap them in a tuple.

case_2_op = func_to_container_op(func = case_2,
                                 base_image = 'case_2/tag:0.1',
                              output_component_file=&quot;case_2.component.yaml&quot;)
</code></pre>
</li>
<li><p>case 3: save data to file using <code>kfp.components.OutputPath</code>.</p>
<pre><code>def case_3(value_1: str, value_2: int, value_3: bool,
        file_path: OutputPath(&quot;dict&quot;)):      
        # 'OutputPath' has a naming rule: it must end with `_path`.

    import json
    exam_dict = dict(one = value_1,
                      two = [value_2, value_3])

    json.dump(exam_dict, open(file_path, &quot;w&quot;), indent=4) 

case_3_op = func_to_container_op(func = case_3,
                                 base_image = 'case_3/tag:0.1',
                              output_component_file=&quot;case_3.component.yaml&quot;)
</code></pre>
</li>
<li><p>case 4: load data from file using <code>kfp.components.InputPath</code>.</p>
</li>
</ol>
<pre><code>def case_4(data_input: InputPath(&quot;dict&quot;)): 
    import json
    
    with open(data_input, &quot;r&quot;, encoding='utf-8') as f:
        data = json.load(f)
    
    print(data)     # {'one': '2', 'two': [1, True]}
 
case_4_op = func_to_container_op(func = case_4,
                                 base_image = 'case_4/tag:0.1',
                              output_component_file=&quot;case_4.component.yaml&quot;)
</code></pre>
<p><strong>pipeline</strong></p>
<pre class=""lang-py prettyprint-override""><code>import kfp.dsl as dsl

@dsl.pipeline(name=&quot;example&quot;)
def data_example(value_1: int, value_2:int):
    _case_1_op = case_1_op(value_1, value_2)
    
    # case of input from single return
    _case_2_op = case_2_op(_case_1_op.output)   
    
    # case of input from multi value return. input value using key.
    # key name must match each key in the NamedTuple.
    _case_3_op = case_3_op(_case_2_op.outputs['key_of_bar'], _case_2_op.outputs['key_of_foo'], _case_2_op.outputs['key_of_baz'])
    
    # case of input from path. input path using key.
    # The key name must match the OutputPath name without '_path'.
    _case_4_op = case_4_op(_case_3_op.outputs[&quot;file&quot;])  
</code></pre>
<p>I'm not sure if this answer is what you asked for, but I hope this helps.</p>
","20696751",1
1951,74893920,2,74874949,2022-12-22 21:14:59,0,"<p><strong>Issue</strong></p>
<p>I got it, the problem is in invalid url, see error message <code>Not Found for url: https://mlrun-api.default-tenant.app.iguazio.prod//api/v1/client-spec</code>.</p>
<p>You can see double <strong><code>//</code></strong> and it is the issue, variable MLRUN_DB contains this url 'https://mlrun-api.default-tenant.app.iguazio.prod/'.</p>
<p><strong>Solution</strong></p>
<p>You have to remove end <strong><code>/</code></strong> in MLRUN_DB and final value will be <strong><code>MLRUN_DB=https://mlrun-api.default-tenant.app.iguazio.prod</code></strong></p>
<p>Now it works.</p>
","20266647",0
1952,74894156,2,74875985,2022-12-22 21:45:43,1,"<p>I'll build up on <a href=""https://stackoverflow.com/users/298182/shcheklein"">@Shcheklein</a>'s great answer - specifically on the 'external dependencies' proposal - and focus on your last question, i.e. <em>&quot;another way to download data automatically from Azure&quot;</em>.</p>
<h2>Assumptions</h2>
<p>Let's assume the following:</p>
<ul>
<li>We're using a DVC pipeline, specified in an existing <code>dvc.yaml</code> file. The first stage in the current pipeline is called <code>prepare</code>.</li>
<li>Our data is stored on some Azure blob storage container, in a folder named <code>dataset/</code>. This folder follows a structure of sub-folders that we'd like to keep intact.</li>
<li>The Azure blob storage container has been configured in our DVC environment as a DVC 'data remote', with name <code>myazure</code> (more info about DVC 'data remotes' <a href=""https://dvc.org/doc/command-reference/remote"" rel=""nofollow noreferrer"">here</a>)</li>
</ul>
<h2>High-level idea</h2>
<p>One possibility is to start the DVC pipeline by synchronizing a local <code>dataset/</code> folder with the <code>dataset/</code> folder on the remote container.</p>
<p>This can be achieved with a command-line tool called <a href=""https://learn.microsoft.com/en-us/azure/storage/common/storage-use-azcopy-v10?WT.mc_id=thomasmaurer-blog-thmaure#download-azcopy"" rel=""nofollow noreferrer""><code>azcopy</code></a>, which is available for Windows, Linux and macOS.
As recommended <a href=""https://learn.microsoft.com/en-us/azure/storage/common/storage-use-azcopy-v10?WT.mc_id=thomasmaurer-blog-thmaure#download-azcopy"" rel=""nofollow noreferrer"">here</a>, it is a good idea to add <code>azcopy</code> to your account or system path, so that you can call this application from any directory on your system.</p>
<p>The high-level idea is:</p>
<ol>
<li>Add an initial <code>update_dataset</code> stage to the DVC pipeline that checks if changes have been made in the remote <code>dataset/</code> directory (i.e., file additions, modifications or removals).
If changes are detected, the <code>update_datset</code> stage shall use the <code>azcopy sync [src] [dst]</code> command to apply the changes on the Azure blob storage container (the <code>[src]</code>) to the local <code>dataset/</code> folder (the <code>[dst]</code>)</li>
<li>Add a dependency between <code>update_dataset</code> and the subsequent DVC pipeline stage <code>prepare</code>, using a 'dummy' file. This file should be added to (a) the <strong>outputs</strong> of the <code>update_dataset</code> stage; and (b) the <strong>dependencies</strong> of the <code>prepare</code> stage.</li>
</ol>
<h2>Implementation</h2>
<p>This procedure has been tested on Windows 10.</p>
<ol>
<li>Add a simple <code>update_dataset</code> stage to the DVC pipeline by running:</li>
</ol>
<pre><code>$ dvc stage add -n update_dataset -d remote://myazure/dataset/ -o .dataset_updated azcopy sync \&quot;https://[account].blob.core.windows.net/[container]/dataset?[sas token]\&quot; \&quot;dataset/\&quot; --delete-destination=\&quot;true\&quot;
</code></pre>
<p>Notice how we specify the 'dummy' file <code>.dataset_updated</code> as an output of the stage.</p>
<ol start=""2"">
<li>Edit the <code>dvc.yaml</code> file directly to modify the command of the <code>update_dataset</code> stage. After the modifications, the command shall (a) create the <code>.dataset_updated</code> file after the <code>azcopy</code> command - <code>touch .dataset_updated</code> - and (b) pass the current date and time to the <code>.dataset_updated</code> file to guarantee uniqueness between different update events - <code>echo %date%-%time% &gt; .dataset_updated</code>.</li>
</ol>
<pre class=""lang-yaml prettyprint-override""><code>stages:
  update_dataset:
    cmd: azcopy sync &quot;https://[account].blob.core.windows.net/[container]/dataset?[sas token]&quot; &quot;dataset/&quot; --delete-destination=&quot;true&quot; &amp;&amp; touch .dataset_updated &amp;&amp; echo %date%-%time% &gt; .dataset_updated # updated command
    deps:
    - remote://myazure/dataset/
    outs:
    - .dataset_updated
...
</code></pre>
<p>I recommend editing the <code>dvc.yaml</code> file directly to modify the command, as I wasn't able to come up with a complete <code>dvc add stage</code> command that took care of everything in one go.
This is due to the use of multiple commands chained by <code>&amp;&amp;</code>, special characters in the Azure connection string, and the <code>echo</code> expression that needs to be evaluated dynamically.</p>
<ol start=""3"">
<li>To make the <code>prepare</code> stage depend on the <code>.dataset_updated</code> file, edit the <code>dvc.yaml</code> file directly to add the new dependency, e.g.:</li>
</ol>
<pre class=""lang-yaml prettyprint-override""><code>stages:
  prepare:
    cmd: &lt;some command&gt;
    deps:
    - .dataset_updated # add new dependency here
    - ... # all other dependencies
...
</code></pre>
<ol start=""4"">
<li>Finally, you can test different scenarios on your remote side - e.g., adding, modifying or deleting files - and check what happens when you run the DVC pipeline up till the <code>prepare</code> stage:</li>
</ol>
<pre><code>$ dvc repro prepare
</code></pre>
<h2>Notes</h2>
<ul>
<li><p>The solution presented above is very similar to the example given in DVC's <a href=""https://dvc.org/doc/user-guide/data-management/importing-external-data#examples"" rel=""nofollow noreferrer"">external dependencies documentation</a>.
Instead of the <code>az copy</code> command, it uses <code>azcopy sync</code>.
The advantage of <code>azcopy sync</code> is that it only applies the differences between your local and remote folders, instead of 'blindly' downloading everything from the remote side when differences are detected.</p>
</li>
<li><p>This example relies on a full connection string with an SAS token, but you can probably do without it if you configure <code>azcopy</code> with your credentials or fetch the appropriate values from environment variables</p>
</li>
<li><p>When defining the DVC pipeline stage, I've intentionally left out an output dependency with the local <code>dataset/</code> folder - i.e. the <code>-o dataset</code> part - as it was causing the <code>azcopy</code> command to fail. I think this is because DVC automatically clears the folders specified as output dependencies when you reproduce a stage.</p>
</li>
<li><p>When defining the <code>azcopy</code> command, I've included the <code>--delete-destination=&quot;true&quot;</code> option. This allows synchronization of deleted files, i.e. files are deleted on your local <code>dataset</code> folder if deleted on the Azure container.</p>
</li>
</ul>
","6660861",0
1953,74897859,2,74890969,2022-12-23 08:59:28,1,"<p>As shown in the documentation, 1.7 is not supported in SageMaker's native XGBoost estimator. If you require 1.7, it would be best to use your own container and run your jobs through an Estimator with your custom container.</p>
","16247336",1
1954,74905403,2,74905350,2022-12-24 03:22:25,1,"<p>DVC stage outputs are automatically tracked by DVC, you don't need to do <code>dvc add</code> on them. If you already have done it before, you can safely un-track it with  <a href=""https://dvc.org/doc/command-reference/remove#example-remove-a-dvc-file"" rel=""nofollow noreferrer""><code>dvc remove</code></a> first:</p>
<blockquote>
<p>Note that the actual output files or directories of the stage (<code>outs</code> field) are not removed by this command, unless the <code>--outs</code> option is used.</p>
</blockquote>
<p>One thing to mention / note. When you create a stage and run it, it removes outputs (unless a persistence flag is specified). This done for reproducibility, it's expected that your stage produces its outputs every time it runs.</p>
","298182",2
1955,74936828,2,74934228,2022-12-28 06:11:41,2,"<p>There is no documentation on allowed filenames in DVC, but the issue is that DVC currently uses <code>urllib.urlsplit</code> and <code>urllib.urlunsplit</code> when normalizing path names, and the newline gets removed by <code>urlsplit</code> since it's not a valid path character for RFC-compliant URLs. DVC needs to support both local paths and remote URL paths like <code>s3://bucket/object/path</code>, so currently it treats everything as a URL.</p>
<p>The intended behavior is that DVC should support any character that is valid for your local filesystem, so it seems pretty clear that this is a bug - DVC should account for invalid URL characters that are valid for local filesystems. I've opened a report which you can follow for further updates: <a href=""https://github.com/iterative/dvc-objects/issues/177"" rel=""nofollow noreferrer"">https://github.com/iterative/dvc-objects/issues/177</a></p>
","1538451",0
1956,74949802,2,74942388,2022-12-29 10:14:10,1,"<p>So the issue was that one of the columns had an extra category not seen during training and the model was not able to vectorize it.</p>
<p>The data is fed without processing and the model on SageMaker performs the vectorization and the transformation necessary based on the training process so that it fits the trained model. No need to add preprocessing steps such as Standarization and OneHotEncoder in the inference step as I was trying to do.
I solved my issue by removing the row containing the extra category (which was a simple yes) and feeding the data and removing the processing steps from <code>input_fn</code>.</p>
<p>As per the error on this post, it says 37 features when I was feeding only 16. Well, this is because the ML model in SageMaker processed the raw data and OneHotEncoded it creating more columns. Then, it encountered the processing steps I put into input_fn and tried to do it again.</p>
<p>I was using SageMaker with SKLearn models, it might be different with other models.</p>
","9808509",0
1957,74984693,2,74981058,2023-01-02 16:15:44,1,"<p>There is no any way by default to visualize for jobs on progress. You can publish metric you want to cloudwatch and visualize on cloudwatch or you some external applications like <a href=""https://neptune.ai"" rel=""nofollow noreferrer"">neptune-ai</a></p>
","5589820",2
1958,74995793,2,74805125,2023-01-03 16:07:41,0,"<p>The problem was with kfp version == 1.18.17. Everything worked perfectly fine after switching to version 1.18.16.</p>
","11774431",0
1959,74999725,2,74972158,2023-01-03 23:47:30,1,"<p>I came across the same issue trying to run a machine learning tutorial by AWS in a notebook instance. What I had to do was update sagemaker within the notebook instance like so:</p>
<pre><code>import sys
!{sys.executable} -m pip install sagemaker -U
</code></pre>
<p>Hopefully this fixes your problem :)</p>
","19954312",0
1960,75047066,2,75047065,2023-01-08 10:34:42,1,"<p>All installed git hooks will be inside <code>.git/hooks</code>. Your problem is with the <code>pre-push</code> hook:</p>
<pre><code>$ ls .git/hooks
applypatch-msg.sample      post-checkout          pre-commit               pre-push               pre-push.sample     prepare-commit-msg.sample
commit-msg.sample          post-update.sample     pre-commit.sample        pre-rebase.sample   update.sample
fsmonitor-watchman.sample  pre-applypatch.sample  pre-merge-commit.sample  pre-receive.sample
</code></pre>
<p>Remove that and you will be fine:</p>
<pre><code>$ rm -rf .git/hooks/pre-push
</code></pre>
<p>Note that other hooks like <code>dvc checkout</code> for <code>git checkout</code> still work. If you want to disable all hooks, not just dvc-installed, you can run:</p>
<pre><code>$ rm -rf .git/hooks/*
</code></pre>
","11922237",1
1961,75077647,2,75077646,2023-01-11 02:05:53,0,"<p>To solve the problem, double check to make sure you are using the right ARN in WORKTEAM_ARN to solve the error received.</p>
","11915729",0
1962,75150866,2,75150865,2023-01-17 18:39:42,0,"<p>According to the <a href=""https://boto3.amazonaws.com/v1/documentation/api/latest/guide/credentials.html#environment-variables"" rel=""nofollow noreferrer"">Environment Variables section</a> in the Boto3 official documentation, setting <code>AWS_SESSION_TOKEN</code> was also necessary.</p>
<p>With reference to <a href=""https://stackoverflow.com/a/63164723/7422352"">this answer</a> added by Stack Overflow user @Francis Lewis, I just reset the AWS access token inside the activated python environment as follows:</p>
<pre><code>export AWS_SESSION_TOKEN=&quot;&quot;
</code></pre>
<p>And the problem was solved. Thanks to @Francis Lewis!</p>
","7422352",0
1963,76224868,2,76211496,2023-05-11 06:57:11,1,"<p>Follow below steps to access dataset in multiple workspaces.</p>
<p>Here, I created the data asset named  <code>sample</code> in workspace <code>jgsML</code> as shown below.</p>
<p><img src=""https://i.imgur.com/ebJbn7a.png"" alt=""enter image description here"" /></p>
<p>Then created new workspace named <code>jgsnew</code>  and created datastore linked to the previous workspace <code>jgsML</code>.
Below is the storage account in <code>jgsML</code>.</p>
<p><img src=""https://i.imgur.com/5sd8Mrv.png"" alt=""enter image description here"" /></p>
<p>With this storage account I created data store in workspace <code>jgsnew</code>.</p>
<p><img src=""https://i.imgur.com/Q1Y5rt1.png"" alt=""enter image description here"" /></p>
<p>Click on create &gt; Give a name &gt; select storage account under workspace having data asset.</p>
<p><img src=""https://i.imgur.com/eKsx0ra.png"" alt=""enter image description here"" /></p>
<p>Then select the blob container where the file is.
To find this go data asset , there you will find storage uri as below, you can see under which container the file is there.
select that blob container while creating datastore.</p>
<p><img src=""https://i.imgur.com/8XVMRUa.png"" alt=""enter image description here"" /></p>
<p>Next select <strong>Authentication type</strong> as <strong>Account key</strong> and add your storage account key.</p>
<p>Below is the <code>sample</code> datastore I created.</p>
<p><img src=""https://i.imgur.com/v4zyZKK.png"" alt=""enter image description here"" /></p>
<p>After creating come to designer pipeline, drag and drop the
<strong>Import Data</strong> component and
Select <strong>Data source</strong> as datastore &gt; select created datastore &gt; select the path as below and click on preview schema to see columns.</p>
<p><img src=""https://i.imgur.com/h2ej4wj.png"" alt=""enter image description here"" /></p>
<p>And you will get the preview schema as below.</p>
<p><img src=""https://i.imgur.com/oSaH4ce.png"" alt=""enter image description here"" /></p>
<p>Like this you can access the data asset from multiple workspace.</p>
","21588207",0
1964,76437072,2,76436642,2023-06-09 03:41:53,2,"<p><a href=""https://dvc.org/doc/command-reference/pull"" rel=""nofollow noreferrer""><code>dvc pull</code></a> command is the same as <a href=""https://dvc.org/doc/command-reference/fetch"" rel=""nofollow noreferrer""><code>dvc fetch</code></a> + <a href=""https://dvc.org/doc/command-reference/checkout"" rel=""nofollow noreferrer""><code>dvc checkout</code></a>.</p>
<p><code>dvc fetch</code> is downloading data from the remote storage (can be S3, Google Cloud, etc) into DVC cache, while <code>checkout</code> then &quot;instantiates&quot; those file in the workspace.</p>
<p>Not a perfect comparison, but roughly you can compare <code>dvc pull</code> with <code>git pull</code>, <code>dvc fetch</code> with <code>git fetch</code>, and <code>dvc checkout</code> with <code>git checkout</code>- they serve similar purpose but for large files or directories that you want to save not in Git directly, but on the cloud, SSH, NAS server, etc.</p>
<p>Btw, besides <code>dvc add</code> you need to run <code>dvc push</code> to save your data, so that your team (or you on a different) machine could run <code>dvc pull</code> later.</p>
","298182",0
1965,76375285,2,76363301,2023-05-31 15:48:47,1,"<p>You're right about the inconsistency. In general, newer SageMaker APIs will return ResourceNotFound (404) for cases like these, while older APIs may return a ValidationException (400).</p>
<p>You can determine which APIs do and don't return ResourceNotFound exceptions by checking the API documentation. For example, <a href=""https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_DeletePipeline.html"" rel=""nofollow noreferrer"">DeletePipeline</a> has <code>ResourceNotFound</code> listed under its error types, while <a href=""https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_DeleteModelPackageGroup.html"" rel=""nofollow noreferrer"">DeleteModelPackageGroup</a> does not. If an API does not have <code>ResourceNotFound</code> listed in its error types, it will return a ValidationException instead.</p>
<p>Unfortunately for proper error handling you will need to handle this inconsistency, either by checking each API's documentation or, more easily, by writing some generic logic to handle this. For example:</p>
<pre><code>try:
  # boto3_method_call()
except ResourceNotFound:
  # 404 logic
except ClientError as error:
  if error.response['Error']['Code'] == 'ValidationException':
    404_strings = ['does not exist', 'could not find']
    if any([x in error.response['Error']['Message'] for x in 404_strings]):
      # 404 logic  
</code></pre>
","895615",1
1966,76309171,2,76307198,2023-05-22 18:58:26,1,"<p>If you can elaborate more, that would be helpful, but maybe something like this is what you are looking for?</p>
<p>dvc.yaml</p>
<pre class=""lang-yaml prettyprint-override""><code>stages:
  example:
    foreach: ${some_param}
    do:
      cmd:
        - echo ${item}

</code></pre>
<p>params.yaml</p>
<pre class=""lang-yaml prettyprint-override""><code>some_param:
  - files/file1
  - files/file2
  - files/file3

</code></pre>
","1007635",0
1967,76294966,2,76294949,2023-05-20 11:25:51,2,"<p>The issue is in the name of entity, you cannot use prefix <code>_</code>. You have to keep limits for definition of name entites/features in FeatureSet. See the <a href=""https://docs.mlrun.org/en/latest/data-prep/ingest-data-fs.html?ingest-data-using-the-feature-store"" rel=""nofollow noreferrer"">link</a> to original documentation. You can see here:</p>
<ul>
<li>Do not name columns starting with either <strong>_</strong> or <strong>aggr_</strong>. They are
reserved for internal use. See also general limitations in Attribute
name restrictions.</li>
<li>Do not name columns to match the regex pattern
<strong>.*<em>[a-z]+</em>[0-9]+[smhd]$</strong>, where <strong>[a-z]+</strong> is an aggregation name, one of:
<strong>count, sum, sqr, max, min, first, last, avg, stdvar, stddev</strong>. E.g.
x_count_1h.</li>
<li>etc.</li>
</ul>
","20266647",0
1968,76282025,2,76258863,2023-05-18 15:00:14,1,"<p>If you want you could make use of an ensemble model in Triton where the first model tokenizes the text and passes it onto the model.</p>
<p>Take a look at this link that describes the strategy: <a href=""https://blog.ml6.eu/triton-ensemble-model-for-deploying-transformers-into-production-c0f727c012e3"" rel=""nofollow noreferrer"">https://blog.ml6.eu/triton-ensemble-model-for-deploying-transformers-into-production-c0f727c012e3</a></p>
","9796588",0
1969,76259719,2,76256165,2023-05-16 05:48:10,1,"<blockquote>
<p>ValueError:azureml://subscriptions/xx/resourcegroups/xx/workspaces/xx/datastore/adls/paths/iris-processed/* is not a valid datastoreuri:azureml://subscriptions/([^/]+)/resourcegroups/([^/]+)/(?:Microsoft.MachineLearningServices/)workspaces/([^/]+)/datastores/([^/]+)/paths/(.*)</p>
</blockquote>
<p>The above error occurs when you pass the wrong parameters in the URI like (Susbcriptionid, Resource group, Workspace name, Datastore name, and path).</p>
<p>I tried with proper parameters in the Uri with the same code and got the expected results.</p>
<p>Code:</p>
<pre><code>from azureml.fsspec import AzureMachineLearningFileSystem

subscription_id = 'Subscription-id'
resource_group = 'Your-resource-group'
workspace_name = 'Workspacename'
input_datastore_name = 'datastore1'
path_on_datastore = 'folder1/'

#azureml://subscriptions/&lt;subid&gt;/resourcegroups/&lt;rgname&gt;/workspaces/&lt;workspace_name&gt;/datastore/datastorename
ds_url =  f'azureml://subscriptions/{subscription_id}/resourcegroups/{resource_group}/workspaces/{workspace_name}/datastores/{input_datastore_name}/paths/{path_on_datastore}'
fs = AzureMachineLearningFileSystem(ds_url)
f_list = fs.ls()
print(f_list)
</code></pre>
<p><strong>Output:</strong></p>
<pre><code>['datastore1/folder1/09-05-2023 (1).html', 'datastore1/folder1/09-05-2023.html', 'datastore1/folder1/10-05-2023.html', 'datastore1/folder1/10-05=2023.html', 'datastore1/folder1/11-05-2023.html', 'datastore1/folder1/12-05-2023 (1).html', 'datastore1/folder1/12-05-2023.html', 'datastore1/folder1/timezone.csv']
</code></pre>
<p><img src=""https://i.imgur.com/JIWNjfr.png"" alt=""enter image description here"" /></p>
<p><strong>Reference:</strong>
<a href=""https://stackoverflow.com/questions/75870455/is-there-a-way-to-get-list-of-folders-from-a-datastore-in-azure-ml-studio-with-p"">Is there a way to get list of folders from a datastore in Azure ML studio with Python SDK v2 - Stack Overflow</a> by khemanth958.</p>
","19144428",1
1970,76258603,2,76244257,2023-05-16 00:00:11,2,"<p>You don't need to refactor that module, just update the version to the latest one (4.0.2) you're using a really old one.
Recommendation: <br> Every time that you see a module calling in a terraform repo go to the Terraform registry and check the versions, documentation, and official examples/repos. <br>
<a href=""https://registry.terraform.io/modules/terraform-aws-modules/vpc/aws/4.0.2"" rel=""nofollow noreferrer"">https://registry.terraform.io/modules/terraform-aws-modules/vpc/aws/4.0.2</a></p>
","21885071",0
1971,76251678,2,76235292,2023-05-15 07:14:40,1,"<p>When using <code>mlflow.sklearn.log_model</code> <strong>you work with the experiment registry which is run-focused</strong> so only experiments and runs can be described and tagged.</p>
<p>If you want to set tags on models, you <strong>need to work with the model registry</strong>.</p>
<p>The solution I would recommend is to <a href=""https://mlflow.org/docs/latest/python_api/mlflow.sklearn.html#mlflow.sklearn.log_model"" rel=""nofollow noreferrer"">register the model when logging using <code>registered_model_name</code></a> (there are more fine-grained ways, too) and use <code>MLFlowClient</code> API to set custom properties (like tags) of the already registered model.</p>
<p>Here is a working example:</p>
<pre><code>import mlflow
from mlflow.client import MlflowClient

mlflow.set_tracking_uri('http://0.0.0.0:5000')

experiment_name = 'test_mlflow'
try:
    experiment_id = mlflow.create_experiment(experiment_name)
except:
    experiment_id = mlflow.get_experiment_by_name(experiment_name).experiment_id

from sklearn.linear_model import LogisticRegression
from sklearn.datasets import load_iris
from sklearn.metrics import accuracy_score

with mlflow.start_run(experiment_id = experiment_id):
    # log performance and register the model
    X, y = load_iris(return_X_y=True)
    params = {&quot;C&quot;: 0.1, &quot;random_state&quot;: 42}
    mlflow.log_params(params)
    lr = LogisticRegression(**params).fit(X, y)
    y_pred = lr.predict(X)
    mlflow.log_metric(&quot;accuracy&quot;, accuracy_score(y, y_pred))
    mlflow.sklearn.log_model(lr, 
        artifact_path=&quot;models&quot;, 
        registered_model_name='test-model'
    )
    # set extra tags on the model
    client = MlflowClient(mlflow.get_tracking_uri())
    model_info = client.get_latest_versions('test-model')[0]
    client.set_model_version_tag(
        name='test-model',
        version=model_info.version,
        key='task',
        value='regression'
    )
</code></pre>
<p>Here is the illustration</p>
<p><a href=""https://i.stack.imgur.com/GbG56.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/GbG56.png"" alt=""tagged and registered model in MLFlow"" /></a></p>
<p>See also this excellent documentation of <a href=""https://mlflow.org/docs/latest/python_api/mlflow.client.html"" rel=""nofollow noreferrer"">MLFlow Client</a>.</p>
","10710587",0
1972,76231642,2,76230401,2023-05-11 21:13:38,1,"<p>You can use EventBridge and Lambda function to start a notebook instance based on an event (like a file upload to S3, se example here - <a href=""https://aws.amazon.com/blogs/compute/using-dynamic-amazon-s3-event-handling-with-amazon-eventbridge/"" rel=""nofollow noreferrer"">https://aws.amazon.com/blogs/compute/using-dynamic-amazon-s3-event-handling-with-amazon-eventbridge/</a>), and execute the notebook (sample LCC to execute notebook on startup - <a href=""https://github.com/aws-samples/amazon-sagemaker-notebook-instance-lifecycle-config-samples/tree/master/scripts/execute-notebook-on-startup"" rel=""nofollow noreferrer"">https://github.com/aws-samples/amazon-sagemaker-notebook-instance-lifecycle-config-samples/tree/master/scripts/execute-notebook-on-startup</a>)</p>
<p>That said, I'd recommend using SageMaker jobs, since they automatically shutdown once the code is executed. If you're using notebook instances, you also have to shut down the notebooks.</p>
<p>Also, make sure you're using different S3 buckets/prefixes for input and output files, otherwise, the writing output file will trigger the Lambda function and will stay looping.</p>
","2458691",3
1973,76203452,2,76197587,2023-05-08 18:42:10,1,"<p>Unfortunately, SageMaker Studio architecture at this moment doesn't allow you to run apps through Jupyter Proxy on a kernel gateway. Therefore, running Streamlit in an image terminal is a little bit tricky.</p>
<p>One of the options is to use <a href=""https://github.com/aws-samples/sagemaker-ssh-helper"" rel=""nofollow noreferrer"">SageMaker SSH Helper</a> library. After reading and completing the &quot;Getting Started&quot; section, you will need to jump to the &quot;Local IDE integration with SageMaker Studio over SSH&quot;. Inside the script <code>sm-local-ssh-ide</code> there are <a href=""https://github.com/aws-samples/sagemaker-ssh-helper/blob/v1.10.1/sagemaker_ssh_helper/sm-local-ssh-ide#L40_L44"" rel=""nofollow noreferrer"">lines with the port forwarding instructions</a> for the SSH client.</p>
<p>You will need to add one more:</p>
<pre><code> -L localhost:8501:localhost:8501 \
</code></pre>
<p>After you complete all steps and run this script, your Streamlit app that you've started inside the SageMaker Studio image terminal will be available on your local machine at http://localhost:8501/.</p>
<p>If you want to learn more how SageMaker Studio works, I recommend this nice blog post by Giuseppe Angelo Porcelli and Vikesh Pandey: <a href=""https://aws.amazon.com/blogs/machine-learning/dive-deep-into-amazon-sagemaker-studio-notebook-architecture/"" rel=""nofollow noreferrer"">Dive deep into Amazon SageMaker Studio Notebooks architecture</a>.</p>
","18309077",0
1974,76181758,2,76176988,2023-05-05 11:35:55,2,"<p>If your RDS is only reacheable from within your VPC, your SageMaker Studio domain's <code>AppNetworkAccessType</code> needs to be <code>VpcOnly</code>, also the domain's network configuration (VPC, subnet, security-groups, and NACLs) need to allow communication between the Studio Apps and your RDS instances.</p>
<p>You can refer SageMaker <a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/infrastructure-connect-to-resources.html"" rel=""nofollow noreferrer"">documentation</a> for more info on this topic.</p>
<p>I would also suggest that you use following commands to troubleshoot networking issues: <br />
<code>dig</code>, <code>nslookup</code>, <code>telnet</code>, <code>mtr</code>, etc.</p>
","13416754",0
1975,76181503,2,76177051,2023-05-05 11:04:27,1,"<p>I'd recommend you go thorugh the AWS sample LCC script <a href=""https://github.com/aws-samples/amazon-sagemaker-notebook-instance-lifecycle-config-samples/tree/master/scripts/persistent-conda-ebs"" rel=""nofollow noreferrer"">here</a>.</p>
<p>I combined those script and ran below commands from the Terminal of my Notebook instance and it created a custom Python 3.8 kernel.</p>
<pre class=""lang-bash prettyprint-override""><code>WORKING_DIR=/home/ec2-user/SageMaker/custom-miniconda
mkdir -p &quot;$WORKING_DIR&quot;
wget https://repo.anaconda.com/miniconda/Miniconda3-4.6.14-Linux-x86_64.sh -O &quot;$WORKING_DIR/miniconda.sh&quot;
bash &quot;$WORKING_DIR/miniconda.sh&quot; -b -u -p &quot;$WORKING_DIR/miniconda&quot; 
rm -rf &quot;$WORKING_DIR/miniconda.sh&quot;


# Create a custom conda environment
source &quot;$WORKING_DIR/miniconda/bin/activate&quot;
KERNEL_NAME=&quot;custom_python&quot;
PYTHON=&quot;3.8&quot;

conda create --yes --name &quot;$KERNEL_NAME&quot; python=&quot;$PYTHON&quot;
conda activate &quot;$KERNEL_NAME&quot;

pip install --quiet ipykernel

# Customize these lines as necessary to install the required packages
conda install --yes numpy
pip install --quiet boto3


source &quot;$WORKING_DIR/miniconda/bin/activate&quot;

for env in $WORKING_DIR/miniconda/envs/*; do
    BASENAME=$(basename &quot;$env&quot;)
    source activate &quot;$BASENAME&quot;
    python -m ipykernel install --user --name &quot;$BASENAME&quot; --display-name &quot;Custom ($BASENAME)&quot;
done

# Optionally, uncomment these lines to disable SageMaker-provided Conda functionality.
# echo &quot;c.EnvironmentKernelSpecManager.use_conda_directly = False&quot; &gt;&gt; /home/ec2-user/.jupyter/jupyter_notebook_config.py
# rm /home/ec2-user/.condarc


echo &quot;Restarting the Jupyter server..&quot;
# restart command is dependent on current running Amazon Linux and JupyterLab
CURR_VERSION=$(cat /etc/os-release)
if [[ $CURR_VERSION == *$&quot;http://aws.amazon.com/amazon-linux-ami/&quot;* ]]; then
    sudo initctl restart jupyter-server --no-wait
else
    sudo systemctl --no-block restart jupyter-server.service
fi
</code></pre>
<p>Once the <code>jupyter-server</code> is restarted, you should see the new custom Kernel from the launcher:
<a href=""https://i.stack.imgur.com/EVl9R.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/EVl9R.png"" alt=""notebook launcher"" /></a></p>
<p>Opening a notebook using the custom Kernel gives me the following result:</p>
<pre class=""lang-py prettyprint-override""><code>import sys
sys.version

'3.8.13 | packaged by conda-forge | (default, Mar 25 2022, 06:04:18) \n[GCC 10.3.0]'
</code></pre>
","13416754",0
1976,76177300,2,76175487,2023-05-04 20:55:32,7,"<p>I ran into the same issue today and was scratching my head. I found adding <code>appengine-python-standard</code> to the <code>packages_to_install</code> argument in the component decorator solved the issue:</p>
<pre><code>@component(base_image=&quot;python:3.7&quot;, packages_to_install=[&quot;appengine-python-standard&quot;,...])
</code></pre>
<p>For context, I'm using KFP v1.8.20 through Vertex workbench. Fingers crossed, it works for v2.0.0-beta.15.</p>
","5422457",2
1977,76156863,2,76149416,2023-05-02 16:17:09,1,"<p>The simplest way to automate the jupyter notebook on your SageMaker Notebook instance would be to schedule a cronjob on the NB instance itself, you can do that either manually or via LCC (LifeCycle Configuration) script.</p>
<p>Another way to schedule notebook jobs is via the new <a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/create-notebook-auto-run.html"" rel=""nofollow noreferrer"">feature</a> of SageMaker Studio, which creates a Training job to execute the jupyter notebook at the scheduled times.</p>
<p>Lastly, if you have a Notebook instance up and running which contains the jupyter notebook or other scripts that you'd like to execute through Lambda, here is a solution I was able to test on my end by taking inspiration from this <a href=""https://www.linkedin.com/pulse/how-run-sagemaker-notebook-from-aws-lambdas-saurabh-aggarwal/"" rel=""nofollow noreferrer"">article</a>.</p>
<p>I created a Lambda function with Python 3.9 in the same Private subnet (with NAT GW) and same auto-referencing SecurityGroup as the NB instance (so that Lambda can connect to NB instance).
Lambda's execution role needs to have IAM permissions to create presigned url of your NB instance.</p>
<p>Lambda code to execute the jupyter notebook names <code>automated-nb.ipynb</code> inside a NB instance:</p>
<pre class=""lang-py prettyprint-override""><code>import boto3
import time
import requests
import asyncio
from websockets import connect

async def remote_script(notebook, cmd):
    sm_client = boto3.client('sagemaker')
    notebook_instance_name = notebook
    url = sm_client.create_presigned_notebook_instance_url(NotebookInstanceName=notebook_instance_name)['AuthorizedUrl']
    url_tokens = url.split('/')
    print(f&quot;AuthorizedURL: '{url}'&quot;)
    http_proto = url_tokens[0]
    http_hn = url_tokens[2].split('?')[0].split('#')[0]
    s = requests.Session()
    r = s.get(url)
    cookies = &quot;; &quot;.join(f&quot;{key}={value}&quot; for key, value in s.cookies.items())
    print(f&quot;Cookies: '{cookies}'&quot;)
    
    uri = f&quot;wss://{http_hn}/terminals/websocket/1&quot;
    print(f&quot;URI: '{uri}'&quot;)
    
    try:
        async with connect(
            uri, 
            origin=http_proto + &quot;//&quot; + http_hn, 
            extra_headers={'Cookie': cookies, 'Host': http_hn}
        ) as websocket:
            print(f&quot;Connected to WebSocket {uri}.&quot;)
            await websocket.send(cmd)
            time.sleep(1)
            websocket.close()
            print(&quot;WebSocket closed.&quot;)
    except Exception as e:
        print(&quot;Excpetion:&quot;)
        print(e)
        
def lambda_handler(event, context):
    print(&quot;About to execute `asyncio.run`...&quot;)
    asyncio.run(remote_script(notebook=&quot;&lt;My_Notebook_instance&gt;&quot;, cmd=&quot;&quot;&quot;[ &quot;stdin&quot;, &quot;jupyter nbconvert --execute --to notebook --inplace /home/ec2-user/SageMaker/automated-nb.ipynb --ExecutePreprocessor.kernel_name=python3 --ExecutePreprocessor.timeout=1500\\r&quot; ]&quot;&quot;&quot;))
    return None
</code></pre>
<p>Also, you'll need to install packages <code>requests</code> and <code>websockets</code> for the Lambda to work. You can follow the documentation <a href=""https://docs.aws.amazon.com/lambda/latest/dg/python-package.html"" rel=""nofollow noreferrer"">here</a>.
Here is how I performed it on my end:</p>
<pre class=""lang-bash prettyprint-override""><code>pip install --target ./packages requests websockets
cd packages
zip -r ../my-deployment-packages.zip .
cd ..
zip -g my-deployment-packages.zip lambda_function.py

aws lambda update-function-code --function-name &lt;My_Lambda_Function&gt; --zip-file fileb://my-deployment-packages.zip
</code></pre>
","13416754",3
1978,76117297,2,76117119,2023-04-27 06:21:58,0,"<blockquote>
<p>Body is a sensitive parameter and its value will be replaced with &quot;sensitive&quot; in string returned by InvokeEndpointOutput's String and GoString methods.</p>
</blockquote>
<p>As stated in the document, it only affects the string returned by the <code>String</code> and <code>GoString</code> methods. You still can access the <code>Body</code> field directly.</p>
<pre class=""lang-golang prettyprint-override""><code>package main

import (
    &quot;fmt&quot;

    &quot;github.com/aws/aws-sdk-go/aws&quot;
    &quot;github.com/aws/aws-sdk-go/service/sagemakerruntime&quot;
)

func main() {
    output := &amp;sagemakerruntime.InvokeEndpointOutput{
        Body:                     []byte(&quot;body data&quot;),
        ContentType:              aws.String(&quot;application/json&quot;),
        InvokedProductionVariant: aws.String(&quot;single-variant&quot;),
    }

    fmt.Println(output)
    fmt.Printf(&quot;body: %s\n&quot;, output.Body)
}
</code></pre>
<p>Output:</p>
<pre><code>{
  Body: &lt;sensitive&gt;,
  ContentType: &quot;application/json&quot;,
  InvokedProductionVariant: &quot;single-variant&quot;
}
body: body data
</code></pre>
","1369400",2
1979,76116731,2,75907677,2023-04-27 04:23:25,1,"<p>If you are using sample weights, the confusion matrix values will be calculated as the sum of the sample weights for each cell, which could result in float values. If you want to see integer values in the confusion matrix, you could try not passing any sample weights to the <code>score_classification</code> method.</p>
<pre><code>classification_metrics = list(constants.CLASSIFICATION_SCALAR_SET)
scores = scoring.score_classification(
            y_test_df.values, predicted, classification_metrics, class_labels, train_labels
        )
</code></pre>
","21262579",0
1980,76080956,2,76023766,2023-04-22 17:17:10,1,"<p>It is necessity to wait for data/schema migrations (it is the known issue <a href=""https://github.com/mlrun/mlrun/issues/3372"" rel=""nofollow noreferrer"">see</a>). You can force this migration based on MLRun SDK call, see:</p>
<pre><code>import mlrun
...
mlrun.get_run_db().trigger_migrations()
</code></pre>
<p>BTW: This issue can happened, when you migrate from old MLRun version to the new one.</p>
","20754749",0
1981,76072371,2,76065598,2023-04-21 10:41:48,0,"<p>So I have my code working from a change to how I accessed the data within Azure so I am assuming that this was the overarching issue.</p>
<p>Instead of using the <code>AzureMachineLearningFileSystem</code> I have turned to <code>adlfs.AzureBlobFileSystem</code>.</p>
<p>There is a bit more code involved to access all the correct credentials etc but isn't too verbose - and ultimately it is working :)</p>
<pre><code>import pyarrow.dataset as ds
import polars as pl
import adlfs
from azureml.core import Workspace,Datastore
from azure.mgmt.storage import StorageManagementClient
from azure.identity import DefaultAzureCredential

# Acquire a credential object
credential = DefaultAzureCredential()
# Get Workspace
ws = Workspace.from_config()
# Get specific datastore
datastore = Datastore.get(ws,'datastore_name')

# Azure Machine Learning workspace details:
subscription = ws.subscription_id
resource_group = ws.resource_group
datastore_name = datastore.account_name
container_name = datastore.container_name
path_on_datastore = f'{container_name}/path/to/data'

# Provision the storage account, starting with a management object.
storage_client = StorageManagementClient(credential, subscription)

# Retrieve the account's primary access key
keys = storage_client.storage_accounts.list_keys(resource_group, datastore_name)
key_to_access = keys.keys[0].value

# ... load your credentials and configure the filesystem
fs = adlfs.AzureBlobFileSystem(account_name=datastore_name, account_key=key_to_access)

dd = ds.dataset(path_on_datastore, filesystem=fs)

df = (
    pl.scan_pyarrow_dataset(dd)
    .select([
        'COLUMN_LIST'
    ])
    .filter((pl.col('col1')&gt;0)&amp;(pl.col('col2') &gt;= 2022))
)

grouped = (df.lazy()
    .groupby(['colA','colB'])
    .agg(
        [
            pl.n_unique('colC').alias('Blah'),
            pl.sum('colD').alias(&quot;BlahBlah&quot;),
            pl.n_unique('colE').alias('BlahBlahBlah'),
            (pl.col('colF') == &quot;C&quot;).count().alias('BlahBlahBlahBlah')
        ]
    )
).collect()

</code></pre>
<p>References for help to others:</p>
<p><strong>Pyarrow connection to Azure Blob</strong> - <a href=""https://arrow.apache.org/docs/python/filesystems.html#using-fsspec-compatible-filesystems-with-arrow"" rel=""nofollow noreferrer"">https://arrow.apache.org/docs/python/filesystems.html#using-fsspec-compatible-filesystems-with-arrow</a></p>
<p><strong>adlfs docs</strong> - <a href=""https://github.com/fsspec/adlfs"" rel=""nofollow noreferrer"">https://github.com/fsspec/adlfs</a></p>
<p><strong>Programmatically get blob connection string</strong> - <a href=""https://stackoverflow.com/questions/65588629/how-to-programmatically-retrieve-the-connection-string-from-an-azure-storage-acc"">How to programmatically retrieve the connection string from an Azure storage account in Python</a></p>
<p>Will accept my own answer for now but if there is a better way of doing this then please feel free to post and I will change the acceptance if so</p>
","12023872",0
1982,76067298,2,76000161,2023-04-20 18:42:02,0,"<p>One thing that I found about the TFMA library and TFX's Evaluator component, in general, is that the output key has to be one-dimensional, and there has to be a label key alway.
If you want to make it work for auto-encoders, instead of making changes to the <code>_input_fn</code>, in the Transform component, return the input twice with two different keys.
For example, if your input key for an image is <code>img</code>, return <code>img_input</code> and <code>img_output</code> in your Transform component. This way, you don't need to manipulate the Trainer component's <code>input_fn</code>, and in the Evaluator, you can easily use the <code>img_output</code> key as your label.
However, as mentioned earlier, this <code>img_output</code> has to be one-dimensional. if in your model, you're using Conv2D layers to encode and decode your image, I'd recommend using the one-dimensional data at first but adding a Reshape layer to make it ready for subsequent Conv2D layers.</p>
<p>Example:</p>
<pre><code>    encoder_inputs = tf.keras.Input(shape=(60,), name='input_xf')
    x = layers.Reshape((15, 4))(encoder_inputs)
    x = layers.Conv1D(filter_num*2, 3, activation=&quot;relu&quot;,
                      strides=2, padding=&quot;valid&quot;)(x)
    z = layers.Dense(latent_dim)(x)

    encoder = tf.keras.Model(encoder_inputs, [z], name=&quot;encoder&quot;)

    latent_inputs = tf.keras.Input(shape=(latent_dim,))
    x = layers.Conv1DTranspose(4, 3, padding=&quot;same&quot;)(x)
    decoder_outputs = layers.Reshape((60,))(x)
    decoder = tf.keras.Model(latent_inputs, decoder_outputs, name=&quot;decoder&quot;)

</code></pre>
","7313175",0
1983,76056853,2,76053273,2023-04-19 16:28:47,0,"<p>MLFLow uses python module <code>random</code> to randomly generate a run name, see the implementation of <code>_generate_string()</code> in <code>name_utils.py</code>. Therefore the way to set the seed for that would be to just call <code>random.seed()</code>.</p>
<p>The implementation of the transformers <code>set_seed()</code> sets a variety of random seeds, including <code>random.seed()</code>.</p>
<p>It seems to me that in order to have random run names from MLFlow, either I don't call <code>set_seed()</code>, but I manually set the seeds for Numpy and Pytorch, or I call <code>random.seed()</code> myself after calling <code>set_seed()</code>.</p>
","4262324",0
1984,76034142,2,75548031,2023-04-17 10:24:04,1,"<p>TL;DR:
<a href=""https://learn.microsoft.com/en-us/azure/machine-learning/how-to-deploy-online-endpoints?view=azureml-api-2&amp;tabs=azure-cli#register-your-model-and-environment-separately"" rel=""nofollow noreferrer"">Register your model and environment separately</a></p>
<hr />
<p>I assume you followed the <a href=""https://learn.microsoft.com/en-us/azure/machine-learning/how-to-create-component-pipeline-python?view=azureml-api-2"" rel=""nofollow noreferrer"">how to create component pipeline v2 docs</a> and created a <code>mldesigner.command_component</code> by specifying the <code>environment</code> parameter with a <code>dict</code> object or an <a href=""https://learn.microsoft.com/en-us/python/api/azure-ai-ml/azure.ai.ml.entities.environment?view=azure-python"" rel=""nofollow noreferrer"">Environment</a> object.</p>
<p>E.g.:</p>
<pre class=""lang-py prettyprint-override""><code>import mldesigner

environment=dict(
    name=&quot;my_env&quot;,
    conda_file=&quot;conda.yaml&quot;,
    image=&quot;mcr.microsoft.com/azureml/openmpi4.1.0-ubuntu20.04&quot;,
)

@mldesigner.command_component(
    name=&quot;prep_data&quot;,
    version=&quot;1&quot;,
    display_name=&quot;Prep Data&quot;,
    description=&quot;Convert data to CSV file, and split to training and test data&quot;,
    environment=environment,
)
def prepare_data_component(
    input_data: Input(type=&quot;uri_folder&quot;),
    training_data: Output(type=&quot;uri_folder&quot;),
    test_data: Output(type=&quot;uri_folder&quot;),
):
    pass
</code></pre>
<p>If you run above Python snippet in a script you will receive a warning:</p>
<pre><code>Warning: the provided asset name 'my_env' will not be used for anonymous registration
</code></pre>
<p>This means, although you specify the name it is not used since a pipeline command job has an anonymous environment.
In the <a href=""https://learn.microsoft.com/en-us/azure/machine-learning/concept-environments?view=azureml-api-2"" rel=""nofollow noreferrer"">Azure ML Environment Docs</a> we find:<br />
<em>&quot;Anonymous&quot; environments are automatically registered in your workspace when you submit an experiment. They will not be listed but may be retrieved by version.</em></p>
<p>More precisely an environment <code>CliV2AnonymousEnvironment</code> will be created with a GUID.</p>
<p>Using above component in an Azure ML pipeline won't result in an error, though. Of course your Azure DevOps pipeline will fail based on this warning.</p>
<p><strong>Solutions</strong></p>
<ol>
<li><a href=""https://learn.microsoft.com/en-us/azure/machine-learning/how-to-deploy-online-endpoints?view=azureml-api-2&amp;tabs=azure-cli#register-your-model-and-environment-separately"" rel=""nofollow noreferrer"">Register your model and environment separately</a></li>
</ol>
<p>The advised solution would be to manage your environment separately of your pipeline. So create an environment via the SDK (<a href=""https://learn.microsoft.com/en-us/azure/machine-learning/how-to-manage-environments-v2?view=azureml-api-2&amp;tabs=cli"" rel=""nofollow noreferrer"">Manage Environments V2 Docs</a>) and specify it in your command component:</p>
<pre class=""lang-py prettyprint-override""><code>environment=&quot;azureml:my_env@latest&quot;
</code></pre>
<ol start=""2"">
<li>Name your environment <code>CliV2AnonymousEnvironment</code></li>
</ol>
<p>If you would like to keep using an anonymous environment provided via the dictionary object rename it to <code>CliV2AnonymousEnvironment</code>. Using this name won't raise the error from above.</p>
<ol start=""3"">
<li>Configure your Azure DevOps pipeline not to fail on this warning</li>
</ol>
<p>E.g. (<a href=""https://stackoverflow.com/questions/64286891/ignore-supress-warning-during-azure-devops-pipeline"">Ignore/supress Warning during Azure Devops Pipeline</a>)</p>
<hr />
<p>I hope this helps, looking forward to feedback. I also do not find it obvious how all the components are linked in Azure ML.</p>
","7641854",0
1985,76008139,2,75980538,2023-04-13 17:09:33,1,"<p>Yes, you can pass the URL directly.</p>
","16247336",0
1986,76007673,2,76007568,2023-04-13 16:16:37,1,"<p>The <code>run()</code> you call seems to be incorrect choice, as <code>run()</code> is used to run the processing job directly, instead of defining the pipeline steps, which you apparently want. Use <code>ProcessingStep</code> directly and feed it with all the necessary arguments:</p>
<pre class=""lang-py prettyprint-override""><code># Setup the first step (scaling step)

...

# --&gt; Use ProcessingStep directly and provide all the args
step_process = ProcessingStep(
    name=&quot;DataProcess&quot;,
    processor=scaling_processor,
    inputs=[
        ProcessingInput(source=input_data, destination=&quot;/opt/ml/processing/input&quot;),
    ],
    outputs=[
        ProcessingOutput(output_name=&quot;scaled_data&quot;, source=&quot;/opt/ml/processing/output/scaled_data/&quot;),
        ProcessingOutput(output_name=&quot;train&quot;, source=&quot;/opt/ml/processing/output/train/&quot;),
        ProcessingOutput(output_name=&quot;test&quot;, source=&quot;/opt/ml/processing/output/test/&quot;),
    ],
    code=&quot;scripts/preprocess.py&quot;,
)

# Setup the 2nd step (RF Training-BYO mode)

...

# --&gt; Use ProcessingStep directly and provide all the args
step_train = ProcessingStep(
    name=&quot;RFTrain&quot;,
    processor=rf_processor,
    inputs=[
        ProcessingInput(source=step_process.properties.ProcessingOutputConfig.Outputs[&quot;train&quot;].S3Output.S3Uri,
                        destination=&quot;/opt/ml/processing/input/train&quot;),
        ProcessingInput(source=step_process.properties.ProcessingOutputConfig.Outputs[&quot;test&quot;].S3Output.S3Uri,
                        destination=&quot;/opt/ml/processing/input/test&quot;),
    ],
    outputs=[
        ProcessingOutput(output_name=&quot;rf_model&quot;, source=&quot;/opt/ml/processing/output/&quot;),
    ],
    code=&quot;scripts/train.py&quot;,
)
</code></pre>
","1235698",0
1987,75987829,2,75981098,2023-04-11 15:50:08,0,"<p>I've managed to download the notebook you referred to and load it on Azure Machine Learning:
<a href=""https://i.stack.imgur.com/pR3DD.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/pR3DD.png"" alt=""AML working notebook"" /></a></p>
<p>Please, take a look on the version you've downloaded to check if it looks like this:</p>
<ul>
<li>Correct json notebook structure:</li>
</ul>
<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"" data-babel=""false"">
<div class=""snippet-code"">
<pre class=""snippet-code-html lang-html prettyprint-override""><code>{
 ""cells"": [
  {
   ""cell_type"": ""code"",
   ""execution_count"": 1,
   ""id"": ""4e082724-c77a-4188-889a-fb5eb028d298"",
   ""metadata"": {},
   ""outputs"": [
    {
     ""name"": ""stdout"",
     ""output_type"": ""stream"",
     ""text"": [
...</code></pre>
</div>
</div>
</p>
<ul>
<li>Incorrect HTML downloaded from Github:</li>
</ul>
<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"" data-babel=""false"">
<div class=""snippet-code"">
<pre class=""snippet-code-html lang-html prettyprint-override""><code>&lt;!DOCTYPE html&gt;
&lt;html lang=""en"" data-color-mode=""auto"" data-light-theme=""light"" data-dark-theme=""dark"" data-a11y-animated-images=""system""&gt;
  &lt;head&gt;
    &lt;meta charset=""utf-8""&gt;
  &lt;link rel=""dns-prefetch"" href=""https://github.githubassets.com""&gt;
  &lt;link rel=""dns-prefetch"" href=""https://avatars.githubusercontent.com""&gt;
  &lt;link rel=""dns-prefetch"" href=""https://github-cloud.s3.amazonaws.com""&gt;
  &lt;link rel=""dns-prefetch"" href=""https://user-images.githubusercontent.com/""&gt;
  &lt;link rel=""preconnect"" href=""https://github.githubassets.com"" crossorigin&gt;
  &lt;link rel=""preconnect"" href=""https://avatars.githubusercontent.com""&gt;
...</code></pre>
</div>
</div>
</p>
<p>This incorrect format usually happens if you right-click the link and select the &quot;Save Link as&quot; option. As an alternative, on Chrome, go to the <a href=""https://raw.githubusercontent.com/Azure-Samples/Azure-OpenAI-Docs-Samples/main/Samples/Tutorials/Embeddings/embedding_billsum.ipynb"" rel=""nofollow noreferrer"">raw version of the file</a>, right-click and select &quot;Save as&quot;, keeping the extension ipynb. Then, in AML, it should work.</p>
","7077050",0
1988,75983179,2,75981077,2023-04-11 06:58:50,0,"<p><strong>I tried in my environment and got the below results:</strong></p>
<p>Initailly, I got the same error in my environment.</p>
<p><strong>Error:</strong></p>
<pre><code>   ModuleNotFound Error
    Input In [9], in &lt;cell line: 1&gt;()
    ----&gt; 1 import openai
    2 import re
    3 import requests
    Traceback (most recent call last)
    ModuleNotFound Error:No module named openai '
</code></pre>
<p><img src=""https://i.imgur.com/y6IxswL.png"" alt=""enter image description here"" /></p>
<p>The reason why the <strong><code>openai module</code></strong> is not found when you switch to a different kernel in Azure ML Studio is because the module is installed in the Python environment associated with the notebook's kernel. Each kernel in Azure ML Studio has its own isolated Python environment, which means that the packages you install in one kernel will not be available in another kernel.</p>
<p>You need directly install by commands.when I tried with same code and packages it executed successfully:</p>
<pre><code>!pip install openai==0.27.4
!pip install pandas==2.0.0
!pip install num2words==0.5.12
</code></pre>
<p><strong>Code:</strong></p>
<pre><code>import openai
import re
import requests
import sys
from num2words import num2words
import os
import pandas as pd
import numpy as np
from openai.embeddings_utils import get_embedding, cosine_similarity
from transformers import GPT2TokenizerFast

#API_KEY = os.getenv(&quot;AZURE_OPENAI_API_KEY&quot;) 
API_KEY = &quot;somekey&quot;
#RESOURCE_ENDPOINT = os.getenv(&quot;AZURE_OPENAI_ENDPOINT&quot;) 
RESOURCE_ENDPOINT = &quot;https://someendpoint/&quot;
openai.api_type = &quot;azure&quot;
openai.api_key = API_KEY
openai.api_base = RESOURCE_ENDPOINT
openai.api_version = &quot;2022-12-01&quot;

url = openai.api_base + &quot;/openai/deployments?api-version=2022-12-01&quot;

r = requests.get(url, headers={&quot;api-key&quot;: API_KEY})

print(r.text)
</code></pre>
<p><strong>Output:</strong></p>
<pre><code>{
  &quot;data&quot;: [
    {
      &quot;scale_settings&quot;: {
        &quot;scale_type&quot;: &quot;standard&quot;
      },
      &quot;model&quot;: &quot;gpt-35-turbo&quot;,
      &quot;owner&quot;: &quot;organization-owner&quot;,
      &quot;id&quot;: &quot;deploymentname1&quot;,
      &quot;status&quot;: &quot;succeeded&quot;,
      &quot;created_at&quot;: 16807xxx,
      &quot;updated_at&quot;: 16807xxx,
      &quot;object&quot;: &quot;deployment&quot;
    }
  ],
  &quot;object&quot;: &quot;list&quot;
}
</code></pre>
<p><img src=""https://i.imgur.com/3wqx4kB.png"" alt=""enter image description here"" /></p>
<p><strong>Reference:</strong>
<a href=""https://learn.microsoft.com/en-us/azure/machine-learning/how-to-run-jupyter-notebooks?view=azureml-api-2"" rel=""nofollow noreferrer"">Run Jupyter notebooks in your workspace - Azure Machine Learning | Microsoft Learn</a></p>
","19144428",0
1989,75983042,2,75981029,2023-04-11 06:39:30,1,"<p>You can use the python package called <code>python-dotenv</code>(<a href=""https://pypi.org/project/python-dotenv/"" rel=""nofollow noreferrer"">python-dotenv</a>), which loads the variables from <code>.env</code> files and you use those as you get environment variables like <code>os.getenv()</code>.</p>
<p>Follow below steps,</p>
<p>First, create a <code>.env</code> file and add your environment values like below.</p>
<p><a href=""https://i.stack.imgur.com/wOazS.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/wOazS.png"" alt=""enter image description here"" /></a></p>
<p>Next, upload this file into your notebook workspace.
By default, the filename starts with <code>.(dot)</code> are hidden.</p>
<p><a href=""https://i.stack.imgur.com/7L7b3.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/7L7b3.png"" alt=""enter image description here"" /></a>
Then install the <code>python-dotenv</code>  library.</p>
<pre><code>%pip install python-dotenv
</code></pre>
<p>Load the environment variables by executing below code.</p>
<pre><code>from dotenv import load_dotenv
load_dotenv()
</code></pre>
<p>Now, get the variables using <code>os.getenv()</code> function as below.</p>
<pre><code>import  os
print(os.getenv(&quot;YOUR_APT_KEY&quot;),os.getenv(&quot;YOUR_HOST&quot;))
</code></pre>
<p><a href=""https://i.stack.imgur.com/sUSsS.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/sUSsS.png"" alt=""enter image description here"" /></a></p>
<p>Here, you can see the environment variables.</p>
","21588207",0
1990,75979840,2,75947125,2023-04-10 18:51:04,2,"<p>MLflow is getting imported in both versions. But it seems that when you create a run MLFlow is not configured to run with Databricks.</p>
<p>Did you configure Databricks before running the second flow?</p>
<p>If not <a href=""https://docs.databricks.com/dev-tools/cli/index.html"" rel=""nofollow noreferrer"">this guide</a> may be helpful.</p>
<p>See the source of the MLFlow function that is causing the error <a href=""https://github.com/mlflow/mlflow/blob/master/mlflow/utils/databricks_utils.py#L386"" rel=""nofollow noreferrer"">here</a>.</p>
","20933478",0
1991,75975381,2,75961271,2023-04-10 08:11:32,2,"<p>I am using these solutions for iteration cross keys in KV storage:</p>
<p><strong>1. NoSqlTarget</strong></p>
<p>Iteration via v3io (it is not pure part of MLRun API, but it is part of MLRun distribution packages). More information about v3io python SDK <a href=""https://github.com/v3io/v3io-py"" rel=""nofollow noreferrer"">see</a> and iteration cross KV items (cursor usage) <a href=""https://github.com/v3io/v3io-py#accessing-key-values-nosql"" rel=""nofollow noreferrer"">see sample</a></p>
<pre><code>import v3io.dataplane

v3io_client = v3io.dataplane.Client(endpoint='https://v3io-webapi:8081', access_key='some_access_key')

# create a query, and use an items cursor to iterate the results
items_cursor = v3io_client.kv.new_cursor(container='users',
                                         table_path='/user-profile',
                                         attribute_names=['income'],
                                         filter_expression='income &gt; 150')

# print the output
for item in items_cursor.all():
    print(item)
</code></pre>
<p>BTW: NoSqlTarget is available only for MLRun Enterprise edition</p>
<p><strong>2. RedisTarget</strong></p>
<p>You can use easy iteration cross KV items, it is part of Redis API</p>
<pre><code>import redis

r = redis.StrictRedis(host='localhost', port=6379, db=0)
for key in r.keys('*'):
    r.delete(key)
</code></pre>
<p>It is possible to use commandline also via <code>redis-cli</code> see sample:</p>
<pre><code>redis-cli keys users*
</code></pre>
<p>or remove from redis specific keys based on list of keys:</p>
<pre><code>redis-cli keys users* | xargs redis-cli del
</code></pre>
<p>BTW: RedisTarget is available for MLRun CE and Enterprise editions</p>
","20266647",3
1992,75929365,2,75928306,2023-04-04 12:23:48,1,"<p>Replaced <code>unix_sock</code> with <code>unix_socket</code> acc. to the pymysql documentation -
<a href=""https://pymysql.readthedocs.io/en/latest/modules/connections.html"" rel=""nofollow noreferrer"">https://pymysql.readthedocs.io/en/latest/modules/connections.html</a></p>
","11168925",0
1993,75929349,2,75833450,2023-04-04 12:21:18,0,"<p>I am able to deploy via this github repo:
<a href=""https://github.com/dlabsai/mlflow-for-gcp"" rel=""nofollow noreferrer"">https://github.com/dlabsai/mlflow-for-gcp</a></p>
","11168925",0
1994,75924231,2,75923500,2023-04-03 22:38:39,3,"<p>Yes, the whole directory can be added at once and this is the recommended way to handle directories in DVC. Having 100s of <code>.dvc</code> files is discouraged and not what DVC is optimized for.</p>
<p>Here is an <a href=""https://dvc.org/doc/command-reference/add#example-directory"" rel=""nofollow noreferrer"">example</a> in the documentation. Pretty much, you can do:</p>
<pre class=""lang-bash prettyprint-override""><code>dvc add dataset
</code></pre>
<p>No matter how many files are inside the <code>dataset</code> directory, DVC will create a single <code>dataset.dvc</code> file that will handle the whole directory. Files will be cached (one time per unique file per dataset).</p>
<p>To update it later, you could run <code>dvc add</code> or <code>dvc commit</code>. To get to the previous version, you will be able to do use the same mechanics as described <a href=""https://dvc.org/doc/start/data-management/data-versioning#switching-between-versions"" rel=""nofollow noreferrer"">here</a>.</p>
<p>Here is the brief summary of <a href=""https://stackoverflow.com/questions/60365473/by-how-much-can-i-approx-reduce-disk-volume-by-using-dvc/60366262#60366262"">some technical details</a> that I recommend to read if you'd like to understand the implications better.</p>
<p>If there a lot of files inside the directory, please also read <a href=""https://dvc.org/doc/user-guide/data-management/large-dataset-optimization"" rel=""nofollow noreferrer"">Large Dataset Optimization</a>.</p>
","298182",0
1995,75916937,2,75909626,2023-04-03 07:26:12,3,"<blockquote>
<p>I want to be able to create other branches and do experiments and track their model output</p>
</blockquote>
<p>You can create a DVC Pipeline that invokes your training and do experiments/create branches with <code>dvc exp run</code> <a href=""https://dvc.org/doc/start/experiments/building-pipelines"" rel=""nofollow noreferrer"">https://dvc.org/doc/start/experiments/building-pipelines</a></p>
<blockquote>
<p>if the generated model is dumped to the same s3 path, adding the model via dvc add --external , should track each version of the model ?</p>
</blockquote>
<p>IIUC, DVC will copy the file you track with <code>dvc add -external s3://mlops-artifact/something</code> to DVC cache, so you should be allowed to get access to the file version you did <code>dvc add</code> for even after re-writing it on s3. <a href=""https://dvc.org/doc/user-guide/data-management/managing-external-data"" rel=""nofollow noreferrer"">https://dvc.org/doc/user-guide/data-management/managing-external-data</a></p>
<blockquote>
<p>next , i want to add some metadata to the output/model generated , so i downloaded GTO (<a href=""https://mlem.ai/doc/gto/user-guide/dvc/"" rel=""nofollow noreferrer"">https://mlem.ai/doc/gto/user-guide/dvc/</a>) and followed the instructions</p>
</blockquote>
<p>You don't need to run <code>dvc import-url --no-download s3://mlops-artifacts/output/sagemaker-experiment/output/model.tar</code> because it's already DVC-tracked after running <code>dvc add --external s3://mlops-artifacts/output/sagemaker-experiment/output/model.tar</code>.</p>
<blockquote>
<p>what is $Repo here</p>
</blockquote>
<p>It's an example with using shell variable $REPO. Substitute it for github repo URL (or <code>&quot;.&quot;</code> if you <code>cd</code> to your repo folder).</p>
<blockquote>
<p>how can i add version information to my models</p>
</blockquote>
<p>You can use command <code>gto register</code> to create a semantic version for your model. This creates a Git tag, which you can later reference and use to get access to the model version you need. <a href=""https://mlem.ai/doc/gto/get-started/"" rel=""nofollow noreferrer"">https://mlem.ai/doc/gto/get-started/</a></p>
<p>Note that's a $REVISION in:</p>
<pre><code>$ dvc get $REPO $ARTIFACT_PATH --rev $REVISION -o $OUTPUT_PATH
</code></pre>
","4890419",0
1996,75906365,2,75899177,2023-04-01 13:07:10,1,"<p>I did not see relevant method in MLRun 1.2.1, but you can use a few work-arounds.</p>
<p><strong>1. About ParquetTarget</strong></p>
<ul>
<li>this format is immutable</li>
<li>but in case that you use partitioning in parquet, than you can remove specific parquet file(s). E.g. if you use partitioning by years, you can easy delete file (via command line <code>rm</code>) on file system specific year and practically you delete requested content.</li>
</ul>
<p><strong>2. About NoSqlTarget</strong></p>
<ul>
<li>this is not immutable format and you can easy update value(s), but MLRun 1.2.1 is without relevant API for delete items</li>
<li>you can see persistence of each key on file systems (v3io is compatible with file system). It means you can also delete content (key or keys) via delete file(s) (again via command line <code>rm</code>).</li>
</ul>
<p><strong>3. About RedisTarget</strong></p>
<ul>
<li>it is near to NoSqlTarget (easy update of values)</li>
<li>you can see full support from redis for delete key(s)</li>
</ul>
<p>command lines:</p>
<pre><code>DEL user
redis-cli KEYS &quot;user*&quot; | xargs redis-cli DEL
</code></pre>
<p>code in python:</p>
<pre><code>import redis

r = redis.Redis()
r.delete('test')
</code></pre>
","20266647",0
1997,75903680,2,75673909,2023-04-01 00:35:00,1,"<p>I figured out why this was happening, so I will post it here.</p>
<p>In this attached Kubernetes cluster, some engineers scheduled jobs with the default Kubernetes scheduler and some scheduled AzureML jobs. AzureML uses the <a href=""https://volcano.sh"" rel=""nofollow noreferrer"">volcano scheduler</a> to schedule the jobs.</p>
<p>The default scheduler allocated GPU resources, and the volcano scheduler somehow did not have an accurate snapshot of the cluster resource.</p>
<p>Restarting the scheduler by deleting the volcano-scheduler pod fixed the problem:
<code>kubectl delete pods -n azureml -lapp=volcano-scheduler</code></p>
<p>And gathering the logs of the volcano scheduler helped understanding what was happening:
<code>kubectl logs -n azureml -lapp=volcano-scheduler -f</code></p>
","3795471",0
1998,75900314,2,75876726,2023-03-31 15:34:23,1,"<p>Here's an example of achieving this through Lifecycle scripts - <a href=""https://github.com/aws-samples/sagemaker-studio-lifecycle-config-examples/blob/main/scripts/set-git-credentials/on-jupyter-server-start.sh"" rel=""nofollow noreferrer"">https://github.com/aws-samples/sagemaker-studio-lifecycle-config-examples/blob/main/scripts/set-git-credentials/on-jupyter-server-start.sh</a></p>
<p>You can run line 52-63(after importing boto3) to get the secrets on a notebook as well.</p>
","2458691",0
1999,75898356,2,75768789,2023-03-31 12:06:35,0,"<p>Turns out that the problem was only that I created my classes inside training script and not imported it from somewhere else. After setting my classes to be imported into training, following the same folder hierarchy in inference script made it work fine.</p>
","6476801",0
2000,75891821,2,75872575,2023-03-30 18:35:29,1,"<p>In your training script you want the function for your model training. You can pass in hyperparameters via the estimator that you can read as arguments in your training script.</p>
<p>Ex:</p>
<pre><code>#Can have other hyper-params such as batch-size, which we are not defining in this case
    parser.add_argument('--epochs', type=int, default=10)
    parser.add_argument('--learning-rate', type=float, default=0.001)
    
    #sm_model_dir: model artifacts stored here after training
    #training directory has the data for the model
    parser.add_argument('--sm-model-dir', type=str, default=os.environ.get('SM_MODEL_DIR'))
    parser.add_argument('--model_dir', type=str)
    parser.add_argument('--train', type=str, default=os.environ.get('SM_CHANNEL_TRAIN'))
    
    args, _ = parser.parse_known_args()
    epochs     = args.epochs
    lr         = args.learning_rate
    model_dir  = args.model_dir
    sm_model_dir = args.sm_model_dir
    training_dir   = args.train
</code></pre>
<p>You can also define your model training function and read in any hyperparams you are passing in (ex: epochs).</p>
<pre><code>model = Sequential()
    model.add(Dense(units=4,activation='relu',input_shape=[4,]))
    model.add(Dropout(.3))
    model.add(Dense(units=3,activation='softmax'))
    model.compile(optimizer='adam',
                  loss='categorical_crossentropy',metrics=['accuracy'])
    early_stop = EarlyStopping(patience=10)
    model.fit(x=scaled_X_train, 
          y=y_train, 
          epochs=epochs,
          validation_data=(scaled_X_test, y_test), verbose=1 ,callbacks=[early_stop])
</code></pre>
<p>You can add a requirements.txt file in the same path as your training script, this will be picked up by the estimator. Generally having a folder titled as &quot;code&quot; with your train.py and requirements.txt will simplify these matters for you.</p>
<p>Here is a reference blog and code sample to also follow: <a href=""https://towardsdatascience.com/training-and-deploying-custom-tensorflow-models-with-aws-sagemaker-72027722ad76"" rel=""nofollow noreferrer"">https://towardsdatascience.com/training-and-deploying-custom-tensorflow-models-with-aws-sagemaker-72027722ad76</a></p>
","16504640",0
2001,75882614,2,75522284,2023-03-29 22:14:46,1,"<p>There are two ways to accomplish this:</p>
<ol>
<li>Create a plugin for overriding definitions of Model Registry APIs like <code>mlflow.register_model</code>: <a href=""https://mlflow.org/docs/latest/plugins.html#writing-your-own-mlflow-plugins"" rel=""nofollow noreferrer"">plugins docs</a> <a href=""https://github.com/mlflow/mlflow/blob/branch-1.5/mlflow/store/model_registry/sqlalchemy_store.py#L34"" rel=""nofollow noreferrer"">example registry plugin</a></li>
<li>Create a helper function that everyone in your org agrees to use when registering models.</li>
</ol>
<p>The helper functions could be of the following form:</p>
<pre><code>import mlflow

def check_metadata_and_register(model_uri, name, await_registration_for=300, *, tags: Optional[Dict[str, Any]] = None):
    # any metadata checks you're enforcing
    status = ...

    # one or more checks failed
    if not status:
        return status

    # forward arguments to `register_model` when all checks succeed!
    return mlflow.register_model(model_uri=model_uri, name=name, ...)
</code></pre>
","7495987",1
2002,75868337,2,75845822,2023-03-28 15:41:51,0,"<p>Managed to work around by overriding the <code>load_context</code> method in <code>mlflow.pyfunc.PythonModel</code> (to load the jar only during runtime)
<a href=""https://mlflow.org/docs/latest/models.html#example-saving-an-xgboost-model-in-mlflow-format"" rel=""nofollow noreferrer"">https://mlflow.org/docs/latest/models.html#example-saving-an-xgboost-model-in-mlflow-format</a></p>
","6540762",0
2003,75864203,2,75843768,2023-03-28 08:57:17,0,"<p>With a help from a professional I figured out next things:</p>
<ol>
<li>Poetry creates virtual env - it can be disabled with using an action, so installing poetry with <code>pip install poetry</code> is not needed anymore</li>
</ol>
<pre><code>  name: Install and configure Poetry
  uses: snok/install-poetry@v1
  with:
    version: 1.3.2
    virtualenvs-create: false
</code></pre>
<ol start=""2"">
<li>Similarly, dvc should be installed with an action</li>
</ol>
<pre><code>- uses: iterative/setup-dvc@v1
</code></pre>
<ol start=""3"">
<li>The reason why pipeline kept failing was because of the wrong path to data folder within great_expectations. As this was a school project, I was following steps of the <a href=""https://www.digitalocean.com/community/tutorials/how-to-test-your-data-with-great-expectations"" rel=""nofollow noreferrer"">tutorial</a> which automatically generated great_expectations.yml file in which it modified path to use <strong>backslash \</strong>, because I was running the setup on Windows. Github Actions pipeline is executed on Ubuntu (Linux) where it can't find the path defined with <strong>backslashes \</strong>. So the final solution is to replace <strong>backslashes \</strong> with <strong>forward slashes  /</strong> inside path in <code>great_expectations.yml &gt; my_datasource &gt; data_connectors &gt; default_inferred_data_connector_name &gt; base_directory</code>.</li>
</ol>
","13059515",0
2004,75845730,2,75831227,2023-03-26 03:34:02,1,"<p>From the <a href=""https://docs.aws.amazon.com/autoscaling/application/userguide/application-auto-scaling-target-tracking.html#target-tracking-considerations"" rel=""nofollow noreferrer"">docs</a>, &quot;A target tracking scaling policy is more aggressive in adding capacity when utilization increases than it is in removing capacity when utilization decreases.&quot;, which would explain the behavior of how fast it scales up vs down.</p>
<p>Since you wanted to scale instances proportionally with number of requests, you should look into <a href=""https://docs.aws.amazon.com/autoscaling/application/userguide/application-auto-scaling-step-scaling-policies.html#as-scaling-steps"" rel=""nofollow noreferrer"">step scaling policies</a>, which will allow you to &quot;choose scaling metrics and threshold values for the CloudWatch alarms that trigger the scaling process as well as define how your scalable target should be scaled when a threshold is in breach for a specified number of evaluation periods&quot;.</p>
<p>In this case, you wanted to scale from 0 to 1 as quickly as possible, so you would just minimize the number of evaluation periods. (consider if outliers triggering scaling is a concern based on resources at hand, likelihood/frequency of outliers, etc.)</p>
","13707954",4
2005,75834290,2,75834188,2023-03-24 13:31:59,1,"<p>In this line:</p>
<pre><code>cs.execute(&quot;select * FROM HEVOPROD_DB.HEVOPROD_SCH.PRDNEW_METERING where REQUESTED_SERVICE = '[&quot;dsp&quot;]' and storeid is not null and latitude is not null and longitude is not null &quot;)
</code></pre>
<p>You delineate the query text in double quoted strings.  Any double quote within double quoted strings must be escaped.</p>
<pre><code>cs.execute(&quot;select * FROM HEVOPROD_DB.HEVOPROD_SCH.PRDNEW_METERING where REQUESTED_SERVICE = '[\&quot;dsp\&quot;]' and storeid is not null and latitude is not null and longitude is not null &quot;)
</code></pre>
<p>A more elegant solution might be to remove the literal value from the query altogether and <a href=""https://docs.snowflake.com/developer-guide/python-connector/python-connector-example#label-python-connector-binding-data"" rel=""nofollow noreferrer"">use a parameter</a> instead</p>
<pre class=""lang-py prettyprint-override""><code>cs.execute(
  &quot;select * FROM HEVOPROD_DB.HEVOPROD_SCH.PRDNEW_METERING where REQUESTED_SERVICE = %s and storeid is not null and latitude is not null and longitude is not null &quot;,
  '[&quot;dsp&quot;]',
)
</code></pre>
","1726083",0
2006,75824757,2,75781599,2023-03-23 15:10:55,0,"<p>I saw in documentation:</p>
<ol>
<li><p>You not need V3IO_API, based on official documentation see:
<a href=""https://i.stack.imgur.com/anCi5.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/anCi5.png"" alt=""enter image description here"" /></a></p>
</li>
<li><p>If V3IO_API is required (see exception) and you setup MLRUN_DBPATH than your mlrun version contains mistake (I got this mistake in mlrun version 1.2.1)</p>
</li>
<li><p>You can see address for V3IO_API in menu <strong>MLRun platform/Services/webapi</strong> and typical value is https://<strong>webapi</strong>.default-tenant.app.iguazio-nonprod.eu.nonprod</p>
</li>
</ol>
","20266647",0
2007,75823062,2,75812404,2023-03-23 12:34:16,0,"<p>Well, it turns out I was doing the right thing, albeit in the wrong order. Here's how I fixed it:</p>
<pre><code>tmp_op = create_custom_training_job_op_from_component(component_item, network=&quot;your_vpc_network_path)
component_op = tmp_op(project=&quot;your_project&quot;, some_argument1=1, some_argument2=2, some_argument3=3)
</code></pre>
<p>At this point, I have a ContainerOP object with the proper worker_pool_specs.</p>
","17317877",0
2008,75815715,2,75812640,2023-03-22 17:54:18,1,"<p><strong>My Azure ML workspace environment:-</strong></p>
<p><img src=""https://i.imgur.com/k2abPMT.png"" alt=""enter image description here"" /></p>
<p><em><strong>Source is a part of Azure ML Environment. You can list all the Environment either by its specific name or all the list together by using the below code and the source will be appended with the environment name in the output like below:-</strong></em></p>
<p>Make sure you replace your client id and client secret with your service principal that has access to your azure ml workspace.</p>
<p><em><strong>List entire list:-</strong></em></p>
<p><strong>Code:-</strong></p>
<pre class=""lang-py prettyprint-override""><code>from azure.identity import  ClientSecretCredential

from azure.ai.ml import  MLClient

  

subscription_id = &quot;&lt;subscription-id&gt;&quot;

resource_group = &quot;siliconrg&quot;

workspace_name = &quot;siliconmlws&quot;

tenant_id = &quot;&lt;tenant-id&gt;&quot;

client_id = &quot;&lt;client-id&gt;&quot;

client_secret = &quot;&lt;client-secret&gt;&quot;

  

credentials = ClientSecretCredential(tenant_id=tenant_id, client_id=client_id, client_secret=client_secret)

  

ml_client = MLClient(credentials, subscription_id, resource_group, workspace_name)

envs = ml_client.environments.list()

for  env  in  envs:

print(env.name)
</code></pre>
<p><strong>Output:-</strong></p>
<p><img src=""https://i.imgur.com/9jA7rRj.png"" alt=""enter image description here"" /></p>
<p><em><strong>List specific environment by adding its name like below code:-</strong></em></p>
<p><strong>Code:-</strong></p>
<pre class=""lang-py prettyprint-override""><code>from azure.identity import  ClientSecretCredential

from azure.ai.ml import  MLClient

  

subscription_id = &quot;&lt;subscription-id&gt;&quot;

resource_group = &quot;siliconrg&quot;

workspace_name = &quot;siliconmlws&quot;

tenant_id = &quot;&lt;tenant-id&gt;&quot;

client_id = &quot;&lt;client-id&gt;&quot;

client_secret = &quot;&lt;client-secret&gt;&quot;

  

credentials = ClientSecretCredential(tenant_id=tenant_id, client_id=client_id, client_secret=client_secret)

  

ml_client = MLClient(credentials, subscription_id, resource_group, workspace_name)

envs = ml_client.environments.list(name=&quot;AzureML-ACPT-pytorch-1.13-py38-cuda11.7-gpu&quot;)

for  env  in  envs:

print(env.name)
</code></pre>
<p><strong>Output:-</strong></p>
<p><img src=""https://i.imgur.com/WUaRefW.png"" alt=""enter image description here"" /></p>
<p><strong>Reference:-</strong></p>
<p><a href=""https://learn.microsoft.com/en-us/azure/machine-learning/how-to-manage-environments-v2?tabs=python"" rel=""nofollow noreferrer"">Manage Azure Machine Learning environments with the CLI &amp; SDK (v2) - Azure Machine Learning | Microsoft Learn</a></p>
","20849135",0
2009,75813806,2,75805480,2023-03-22 14:50:48,2,"<p>Some high-level answers for your high-level questions.</p>
<p>DVC is designed to behave in a very similar way to git, if you are ever in dought you should run <code>dvc status</code></p>
<p>In your example, if you add a file to <code>./storage</code> then <a href=""https://dvc.org/doc/command-reference/status"" rel=""nofollow noreferrer""><code>dvc status</code></a> will show that it has been modified, you can run <code>dvc commit</code> which will update your tracking file <code>storage.dvc</code> which should be then committed with git. I would run <a href=""https://dvc.org/doc/command-reference/install"" rel=""nofollow noreferrer""><code>dvc install</code></a> to set up some pre-commit hooks which will help do this automatically for you.</p>
<p>Your <code>.dvc/config</code> should contain the bucket name and config options. So CML or anyone machine can run <code>dvc pull</code> to get the data so long as they have credentials.</p>
<p>On CML I would think of it as a collection of tools to help you interact with GitLab/GitHub you can follow one of the examples at <a href=""https://cml.dev"" rel=""nofollow noreferrer"">https://cml.dev</a>. It sounds like you are looking at the <code>cml runner launch</code> command (using GitHub as an example) can provision an ec2 instance for you and installs the GitHub Actions agent to run subsequent CI/CD jobs.</p>
","1007635",2
2010,75794621,2,75785159,2023-03-20 19:42:48,2,"<p>Here is an example that you can use:</p>
<pre><code>cml runner launch \
   ...
   --cloud=aws \
   --cloud-region=xxx \
   --cloud-type=t3.small \
   --cloud-aws-security-group=cml-test-sg \
   --cloud-aws-subnet=subnet-5d7ee225
</code></pre>
<p>You can create an AWS security group in the VPC you want to target (you need to expose SSH for cml runner) note that <code>cml-test-sg</code> is the Name of the created security in your target VPC</p>
<p>Additionally you can target a specific subnet within your VPC by also providing the id for the resource you want, here: <code>subnet-5d7ee225</code></p>
","1007635",3
2011,75753353,2,75676242,2023-03-16 07:47:35,0,"<p>My problem appeared not to be related to istio or networking. I just needed to add the prefix <code>.well-know/</code> to be skipped by the authentication service in kubeflow. It was preventing the <code>domain.com/.well-known/acme-challenge/token</code> url, so the solver pod couldn't reach it without authentication. So to overcome this, adding <code>.well-known</code> in the <code>SKIP_AUTH_URI</code> parameter found in path <code>manifests-1.6.1/common/oidc-authservice/base/params.env</code> solved the problem.</p>
","16237534",0
2012,75731332,2,75729724,2023-03-14 09:36:22,0,"<p>You can filter based on <strong>resource name instead of service name</strong> in cost analysis to view cost calculated for Azure machine learning resource.</p>
<ul>
<li>Using cost analysis, you can view the cost calculated for each resource in a pie chart representation.
<img src=""https://i.imgur.com/twf5Nf8.png"" alt=""enter image description here"" /></li>
<li>I have also tried to filter based on service name and I could not find azure machine learning name in filter.
<img src=""https://i.imgur.com/ZhyYART.png"" alt=""enter image description here"" /></li>
<li>If you want see the cost calculated for azure machine learning, <strong>select cost by resource option in cost analysis and then resources(preview) option.</strong>
<img src=""https://i.imgur.com/uXQatK0.png"" alt=""enter image description here"" /></li>
<li>Once selected resources, type machine learning in filter for rows field.
<img src=""https://i.imgur.com/vmyh4sq.png"" alt=""enter image description here"" />
In this way you can check pricing of Azure machine learning.</li>
</ul>
","20336887",0
2013,75727460,2,75701023,2023-03-13 22:27:20,0,"<p>I figured out the answer to my question, so I'm going to post in case someone else has the same issue.</p>
<p>The error was cause because I was using Databricks Runtime 10.4 LTS ML.</p>
<p>When I <strong>upgraded to 12.1 LTS ML</strong> the error when away.</p>
","8260492",0
2014,75698337,2,75695930,2023-03-10 15:59:39,1,"<p>I believe you are slightly misusing the python API see here: <a href=""https://dvc.org/doc/api-reference/get_url"" rel=""nofollow noreferrer"">https://dvc.org/doc/api-reference/get_url</a></p>
<p>It looks like you would want something like this:</p>
<pre class=""lang-py prettyprint-override""><code>import dvc.api

path = &quot;data/test.csv&quot;
remote_name = &quot;dvc-remote&quot;
repo = &quot;https://github.com/username/repo.git&quot;
version = &quot;v1&quot;

url = dvc.api.get_url(
    path=path,
    remote=remote_name,
    repo=repo,
    rev=version
)

print(url)

</code></pre>
","1007635",1
2015,75672755,2,75670745,2023-03-08 11:58:57,2,"<p>Looks like you have an older sqlite version installed. You could install older dvc version (2.45.1) or upgrade sqlite. Created <a href=""https://github.com/iterative/sqltrie/issues/8"" rel=""nofollow noreferrer"">https://github.com/iterative/sqltrie/issues/8</a> to avoid this problem in the next release.</p>
","2628602",2
2016,75665987,2,75665736,2023-03-07 18:41:36,0,"<p>The role needs access to <code>jumpstart-cache-prod-&lt;Region&gt;</code> bucket. Make sure your IAM role and VPC endpoint policy allow access to the JumpStart bucket.</p>
","9796588",2
2017,75657966,2,75651736,2023-03-07 04:09:47,1,"<p>For building the pipeline you can use SageMaker pipelines. You can find an example here - <a href=""https://github.com/aws/amazon-sagemaker-examples/blob/main/sagemaker-pipelines/tabular/abalone_build_train_deploy/sagemaker-pipelines-preprocess-train-evaluate-batch-transform.ipynb"" rel=""nofollow noreferrer"">https://github.com/aws/amazon-sagemaker-examples/blob/main/sagemaker-pipelines/tabular/abalone_build_train_deploy/sagemaker-pipelines-preprocess-train-evaluate-batch-transform.ipynb</a></p>
<p>Once you have the pipeline built you can trigger the pipeline using Amazon EventBridge integration which supports both event based or schedule based events. Check the below link to setup using event bridge
<a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/pipeline-eventbridge.html"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/sagemaker/latest/dg/pipeline-eventbridge.html</a></p>
","19490330",2
2018,75631020,2,75574144,2023-03-03 19:12:46,0,"<p>This will be implemented in the future by Kubeflow team. It's a planned feature, as seen here:</p>
<p><a href=""https://github.com/kubeflow/pipelines/issues/8899#issuecomment-1452764426"" rel=""nofollow noreferrer"">https://github.com/kubeflow/pipelines/issues/8899#issuecomment-1452764426</a></p>
","6901690",0
2019,75626020,2,75625784,2023-03-03 10:41:04,1,"<p>It was human mistake, the solution was easy and the key problem was in Spark service configuration (I configured extremely small vCPU values and it generated timeouts for Spark service):</p>
<ul>
<li>I used setting vCPU in the range 1-14 but I used default units <code>millicpu</code> (not cpu)</li>
<li>After setup correct units <code>cpu</code> and restart of Spark service, everything was fine.</li>
</ul>
<p>Wrong setting <a href=""https://i.stack.imgur.com/sNwsR.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/sNwsR.png"" alt=""enter image description here"" /></a></p>
<p>Correct setting <a href=""https://i.stack.imgur.com/R2bks.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/R2bks.png"" alt=""enter image description here"" /></a></p>
","20266647",0
2020,75608269,2,75574407,2023-03-01 19:45:39,1,"<p>The solution is easy, it is enough to switch-off preview mode based on setting <code>infer_options=0</code> in ingest method. See part of the code</p>
<pre><code>...
feature_derived.graph.to(name=&quot;calc&quot;, handler='calc')
fstore.ingest(feature_derived, dataFrm, infer_options=0)
...
</code></pre>
<p>The output has only two values (as requested):</p>
<pre><code>&gt; calc
&gt; calc 
</code></pre>
","20266647",0
2021,75593127,2,75497497,2023-02-28 14:05:24,1,"<p>As stated in my comment, the error could be very likely related to a CORS problem.</p>
<p>Please, consider configuring your REST endpoint for sending the CORS headers appropriate for your Javascript application.</p>
<p>You seem to be using API Management for exposing the REST endpoint: if that is correct, you can configure CORS by defining the appropriate <a href=""https://learn.microsoft.com/en-us/azure/api-management/api-management-policies"" rel=""nofollow noreferrer"">policy</a>.</p>
<p>This page provides the necessary information about <a href=""https://learn.microsoft.com/en-us/azure/api-management/set-edit-policies?tabs=form"" rel=""nofollow noreferrer"">how to set or edit API Management policies</a>.</p>
<p>The CORS policy is documented in <a href=""https://learn.microsoft.com/en-us/azure/api-management/cors-policy"" rel=""nofollow noreferrer"">this other page</a>. The documentation provides <a href=""https://learn.microsoft.com/en-us/azure/api-management/cors-policy#example"" rel=""nofollow noreferrer"">a sample configuration</a>, adapted for your use case:</p>
<pre class=""lang-xml prettyprint-override""><code>&lt;cors allow-credentials=&quot;true&quot;&gt;
    &lt;allowed-origins&gt;
        &lt;!-- Localhost useful for development --&gt;
        &lt;origin&gt;http://localhost:8080/&lt;/origin&gt;
        &lt;origin&gt;http://your-javascript-app-domain.com/&lt;/origin&gt;
    &lt;/allowed-origins&gt;
    &lt;allowed-methods preflight-result-max-age=&quot;300&quot;&gt;
        &lt;method&gt;POST&lt;/method&gt;
    &lt;/allowed-methods&gt;
    &lt;allowed-headers&gt;
        &lt;header&gt;content-type&lt;/header&gt;
        &lt;header&gt;accept&lt;/header&gt;
    &lt;/allowed-headers&gt;
&lt;/cors&gt;
</code></pre>
<p><a href=""https://stackoverflow.com/questions/67244072/how-configure-azure-api-management-for-cors"">This related SO question</a> could be of help as well.</p>
<p>Additionally, please, consider including an <a href=""https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/Accept"" rel=""nofollow noreferrer""><code>Accept</code> header</a> when posting your information, we have experienced problems when this header is not present when invoking an API exposed by the API Management service, I am not absolutely sure, but I think it is not included by default by <code>fetch</code> (in contrast to Postman):</p>
<pre class=""lang-js prettyprint-override""><code>fetch('http://ziggyapimanagementservice.azure-api.net/score', {
        method: 'POST',
        headers: {
            'Accept': '*/*',
            'Content-Type': 'application/json',
            'Ocp-Apim-Subscription-Key': 'cd529cc993494fdfb1530eaf04ae63dc'
        },
        body: testData
    })
        .then(response =&gt; response.json())
        .then(data =&gt; {
            console.log(data);
            const result = data.result[0]; // Get the result array from the response
            const volumeForecastElement = document.querySelector('#volume-forecast');
            volumeForecastElement.textContent = result.join(', '); // Update the text content of the &lt;b&gt; element with the result array joined by commas
            document.getElementById(&quot;result&quot;).innerHTML = result;
        })
        .catch(error =&gt; {

            document.getElementById(&quot;error&quot;).innerHTML = error.message;
            console.error(error.message)
        });
</code></pre>
","13942448",11
2022,75575987,2,75572304,2023-02-27 00:53:37,0,"<p>using private endpoints alone may not prevent data exfiltration, but it can reduce the attack surface and the chances of data exfiltration. It is recommended to use a combination of Azure Virtual Network, Azure Private Link, and <a href=""https://learn.microsoft.com/en-us/azure/governance/policy/overview"" rel=""nofollow noreferrer"">Azure Policy</a> to secure your Azure Machine Learning resources.</p>
","21262579",0
2023,75563451,2,75064559,2023-02-25 05:22:16,0,"<p>The solution was to have 2 security group rules:</p>
<ol>
<li>inbound rule that allows all traffic from all ports if the source has the same security group as the subnet (I had this already).</li>
<li>outbound rule that allows all traffic from all ports if the destination has the same security group as the subnet (I had a rule such as 0.0.0.0 that allowed all traffic but that didn't allow outbound traffic from EFA).</li>
</ol>
","11146517",6
2024,75561591,2,75560759,2023-02-24 21:37:47,2,"<p>In MLRun 1.2.1</p>
<ul>
<li>You can define user/identity and add roles only in MLRun managed version (not in CE - Community version)</li>
<li>List of roles/policies <a href=""https://www.iguazio.com/docs/latest-release/users-and-security/security/#predefined-management-policies"" rel=""nofollow noreferrer"">see</a></li>
<li>You can also see rich UI
<a href=""https://i.stack.imgur.com/ntodN.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ntodN.png"" alt=""enter image description here"" /></a></li>
</ul>
<p><a href=""https://i.stack.imgur.com/MgqCu.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/MgqCu.png"" alt=""enter image description here"" /></a></p>
","20266647",0
2025,75559235,2,75374982,2023-02-24 16:47:22,0,"<p>I resolved this using below shell command -</p>
<pre><code>pipeline_definition_json=$(aws s3 cp &quot;s3://$S3_BUCKET/&lt;folder_1&gt;/sagemaker_pipeine_definition.json&quot; - | tr -d '\n') 

aws sagemaker update-pipeline --pipeline-name $PIPELINE_NAME --role-arn $ROLE_ARN --pipeline-definition &quot;$pipeline_definition_json&quot;
</code></pre>
","15484616",0
2026,75555315,2,75553207,2023-02-24 10:19:43,1,"<p><code>lambda</code> is a reserved keyword in Python for <a href=""https://docs.python.org/3/tutorial/controlflow.html#lambda-expressions"" rel=""nofollow noreferrer"">lambda expressions</a>.</p>
<p>The way you can get around it is to rather put your function arguments in a dict and then &quot;unpack&quot; the dict into function arguments:</p>
<pre class=""lang-py prettyprint-override""><code>xgb.set_hyperparameters(**{ &quot;num_round&quot;: 2000,
                           .
                           .
                           .
                            &quot;lambda&quot;: 0.5,
                            &quot;alpha&quot;: 1
                       })
</code></pre>
","3486675",0
2027,75491351,2,75367831,2023-02-18 05:13:47,0,"<p>This seems to be an issue with the lightfm package installation. Have you tried installing this in another environment? This can also be dependent on the notebook kernel you are using. Can you try installing in a virtual environment as well?</p>
","16504640",1
2028,75487273,2,75386721,2023-02-17 17:07:10,1,"<p>The hyperparameterTuner takes an Estimator object as one of the parameters. You can keep static hyperparameters as part of the estimator something like below</p>
<pre><code>estimator = PyTorch(
    entry_point=&quot;mnist.py&quot;,
    role=role,
    py_version=&quot;py3&quot;,
    framework_version=&quot;1.8.0&quot;,
    instance_count=1,
    instance_type=&quot;ml.c5.2xlarge&quot;,
    hyperparameters={&quot;epochs&quot;: 1, &quot;backend&quot;: &quot;gloo&quot;},
)
</code></pre>
<p>Once you have the estimator initialized you can pass this to Tuner along with Parameters that has to be tuned as shown below</p>
<pre><code>hyperparameter_ranges = {
    &quot;lr&quot;: ContinuousParameter(0.001, 0.1),
    &quot;batch-size&quot;: CategoricalParameter([32, 64, 128, 256, 512]),
}
tuner = HyperparameterTuner(
    estimator,
    objective_metric_name,
    hyperparameter_ranges,
    metric_definitions,
    max_jobs=9,
    max_parallel_jobs=3,
    objective_type=objective_type,
)
</code></pre>
<p>Please refer this example for a complete solution</p>
<p><a href=""https://github.com/aws/amazon-sagemaker-examples/blob/main/hyperparameter_tuning/pytorch_mnist/hpo_pytorch_mnist.ipynb"" rel=""nofollow noreferrer"">https://github.com/aws/amazon-sagemaker-examples/blob/main/hyperparameter_tuning/pytorch_mnist/hpo_pytorch_mnist.ipynb</a></p>
","19490330",0
2029,75478362,2,75414991,2023-02-16 22:04:31,0,"<p>This is likely because the notebook instance doesn't have the NeptuneML IAM role parameter set as an environment variable.</p>
<p>Try running <code>export NEPTUNE_ML_ROLE_ARN=&lt;your neptune ml role arn&gt;</code> in the terminal of the notebook instance you're using. Alternatively you can add that  to the .bashrc file on the notebook instance.</p>
","10073288",1
2030,75470956,2,75456934,2023-02-16 10:31:07,1,"<p>Building upon the <a href=""https://stackoverflow.com/a/75470312/90580"">answer from Gili</a>.</p>
<p>Using the aws CLI and jq :</p>
<pre><code>aws --region us-east-1 pricing get-products \
  --service-code AmazonSageMaker \
  --filters Type=TERM_MATCH,Field=regionCode,Value=eu-north-1 \
  | jq -r '.PriceList[]|fromjson|select(.product.productFamily == &quot;ML Instance&quot;)|.product.attributes.instanceName'\
|sort\
|uniq

ml.c5.12xlarge
ml.c5.18xlarge
ml.c5.24xlarge
ml.c5.2xlarge
ml.c5.4xlarge
ml.c5.9xlarge
ml.c5.large
...
ml.t3.2xlarge
ml.t3.large
ml.t3.medium
ml.t3.xlarge
</code></pre>
<p>it uses</p>
<ul>
<li><code>aws pricing get-products</code>
<ul>
<li><code>--region us-east-1</code> is important because the Pricing service is not widely available</li>
<li><code>--filters Type=TERM_MATCH,Field=regionCode,Values=eu-north-1</code> to restrict the listing to products in <code>eu-north-1</code> region, the number of products for AmazonSageMaker alone across all regions is huge, so better to let AWS to filter out those early</li>
</ul>
</li>
<li><code>jq</code> is used to further filter the output, it seems it's not possible to filter by <code>productFamily</code> at the <code>aws pricing get-products</code> so we need to do it with <code>jq</code>
<ul>
<li><code>-r</code> removes the quotes from the output</li>
<li><code>.PriceList[]</code> will iterate over all the prices returned by <code>aws pricing get-products</code></li>
<li><code>fromjson</code> will parse the each string as JSON (<code>.PriceList</code> is an array of strings)</li>
<li><code>select(.product.productFamily == &quot;ML Instance&quot;)</code> will filter out all other products</li>
<li><code>.product.attributes.instanceName</code> extracts the instance type from each product.</li>
</ul>
</li>
</ul>
","90580",1
2031,75463398,2,75432878,2023-02-15 17:25:28,0,"<p>It's not available out of the box. You can use CostExplorer or the AWS Pricing API and find the approximate costs, then update the IAM role to deny <code>CreateApp</code> if the costs exceed a threshold.</p>
<p>That said, I would recommend to preventatively restrict the types of instances a user can launch using IAM policies. You can work backwards from the budget and the use cases for your Studio users and set a list of acceptable instance types for Studio (see <a href=""https://docs.aws.amazon.com/whitepapers/latest/sagemaker-studio-admin-best-practices/permissions-management.html#common-guardrails"" rel=""nofollow noreferrer"">here</a> for a sample policy). This way, your users' productivity is not affected suddenly due to them reaching the budget threshold.</p>
<p>Secondly, use the auto shutdown extension to shut down apps when they're not in use, and save on costs. See the blog <a href=""https://aws.amazon.com/blogs/machine-learning/save-costs-by-automatically-shutting-down-idle-resources-within-amazon-sagemaker-studio/"" rel=""nofollow noreferrer"">here</a> and a Lifecycle script <a href=""https://github.com/aws-samples/sagemaker-studio-lifecycle-config-examples/tree/main/scripts/install-autoshutdown-server-extension"" rel=""nofollow noreferrer"">here</a></p>
<p>Lastly, one call out - these refer to Studio usage costs only. If your users spin up training jobs and endpoints from Studio - you will have to account for those costs as well.</p>
","2458691",0
2032,75446520,2,75342508,2023-02-14 10:29:05,1,"<p>TFMA provides support for calculating metrics that were used at training time (i.e. built-in metrics) as well metrics defined after the model was saved as part of the TFMA configuration settings.<br />
<a href=""https://www.tensorflow.org/tfx/model_analysis/api_docs/python/tfma/metrics"" rel=""nofollow noreferrer"">tfma.metrics.*</a> consists of Standard TFMA metrics and plots.</p>
<p><a href=""https://www.tensorflow.org/tfx/model_analysis/api_docs/python/tfma/metrics/MeanLabel"" rel=""nofollow noreferrer"">tfma.metrics.MeanLabel</a> calculates mean label by calculating the ratio of total weighted labels and total weighted examples.</p>
<p><a href=""https://www.tensorflow.org/tfx/model_analysis/api_docs/python/tfma/metrics/MeanPrediction"" rel=""nofollow noreferrer"">tfma.metrics.MeanPrediction</a> calculates mean prediction by calculating the ratio of total weighted predictions and total weighted examples.</p>
<p><a href=""https://www.tensorflow.org/tfx/model_analysis/api_docs/python/tfma/metrics/MeanAttributions"" rel=""nofollow noreferrer"">tfma.metrics.MeanAttributions</a> calculates mean attributions by calculating contribution of each input feature to the prediction made by the model.</p>
<p>This metrics are provided in <code>metrics_specs</code> section of <code>tfma.EvalConfig</code> which holds specifications for the model, metrics, and slices that are to be evaluated. Please refer <a href=""https://www.tensorflow.org/tfx/tutorials/model_analysis/tfma_basic#setup_and_run_tfma"" rel=""nofollow noreferrer"">TFMA tutorial</a> for better understanding on using these metrics.</p>
","user11530462",0
2033,75408814,2,75407526,2023-02-10 08:53:19,0,"<p><code>StartPipelineExecution</code> is an asynchronous API that returns immediately with the execution ARN. You have several options to wait for pipeline completion with a Step Function.  Here are two:</p>
<h3>Option 1:  Polling</h3>
<p>One option is to poll for execution completion within a single State Machine.  Polling could be implemented with a <a href=""https://docs.aws.amazon.com/step-functions/latest/dg/sample-project-job-poller.html"" rel=""nofollow noreferrer"">Wait - Lambda - Choice</a> task loop.  Or by a single Lambda function with a <a href=""https://docs.aws.amazon.com/step-functions/latest/dg/callback-task-sample-sqs.html"" rel=""nofollow noreferrer"">callback pattern</a> (in which case the looping is the Lambda's job).  In both cases, the Lambda checks the status of the pipeline execution with an SDK call.</p>
<p>Visually, the State Machine looks like this:</p>
<pre><code> SM #1
[x x x S P x x x x]
</code></pre>
<p>where <code>S</code> = Sagemaker <code>StartPieplineExecution</code> task,  <code>P</code> = Poller tasks,  <code>x</code> = Other tasks.</p>
<h3>Option #2:  Event-driven</h3>
<p>A second option avoids polling.  Instead, split your State Machine in two, the first one ending with the <code>StartPieplineExecution</code> task.  Add an EventBridge rule that <a href=""https://docs.aws.amazon.com/step-functions/latest/dg/concepts-invoke-sfn.html"" rel=""nofollow noreferrer"">triggers</a> the second half of your tasks when a <a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/automating-sagemaker-with-eventbridge.html#eventbridge-pipeline"" rel=""nofollow noreferrer"">Pipeline execution state change</a> event with <code>&quot;currentPipelineExecutionStatus&quot;: &quot;Succeeded&quot;</code> is emitted.</p>
<pre><code> SM #1                                  SM #2
[x x x S] -&gt; pipeline success event -&gt; [x x x x]
</code></pre>
<p>These patterns apply more generally to orchestratiing asynchronous tasks.  See this <a href=""https://stackoverflow.com/questions/74142223/is-it-possible-to-have-an-aws-step-function-wait-until-a-rds-is-deleted-to-conti"">related question</a> for another example.</p>
","1103511",0
2034,75408787,2,75387306,2023-02-10 08:50:29,5,"<p>The problem is indeed sensitive and hard to debug. I suspect it has to do with the underlying hardware on which the docker container is deployed, not with the actual custom Docker container and its corresponding dependencies.</p>
<p>Since you have a Tesla K80, I suspect NC series video cards (upon which the environments are deployed).</p>
<p>As of writing this comment (10th of February 2023), the following observation is valid (<a href=""https://learn.microsoft.com/en-us/azure/machine-learning/resource-curated-environments"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/machine-learning/resource-curated-environments</a>):</p>
<blockquote>
<p>Note</p>
<p>Currently, due to underlying cuda and cluster incompatibilities, on NC
series only AzureML-ACPT-pytorch-1.11-py38-cuda11.3-gpu with cuda 11.3
can be used.</p>
</blockquote>
<p>Therefore, in my opinion, this can be traced back to the supported versions of CUDA + PyTorch and Python.</p>
<p>What I did in my case, I just installed my dependences via a <code>.yaml</code> dependency file when creating the environment, starting from this base image:</p>
<p>Azure container registry</p>
<pre><code>mcr.microsoft.com/azureml/curated/acpt-pytorch-1.11-py38-cuda11.3-gpu:9
</code></pre>
<p>You can start building your docker container from this URI as base image in order to work properly on Tesla K80s.</p>
<p><strong>IMPORTANT NOTE</strong> : Using this base image did work in my case, I was able to train PyTorch models.</p>
","6117017",2
2035,75393073,2,75306950,2023-02-09 00:35:03,1,"<p>It's a known issue / limitation that is being worked on at the moment:</p>
<ul>
<li><a href=""https://github.com/iterative/vscode-dvc/pull/3253"" rel=""nofollow noreferrer"">Show a setup screen when project has no commits</a></li>
<li><a href=""https://github.com/iterative/dvc/issues/8839"" rel=""nofollow noreferrer""><code>exp init</code>/<code>exp run</code>: fails if the repo has no root commit</a></li>
<li><a href=""https://github.com/iterative/dvc/issues/7548"" rel=""nofollow noreferrer""><code>exp</code>: improve ui for empty Git repos</a></li>
</ul>
<p>The simplest workaround is to create at least a single commit in the repository before running it.</p>
","298182",0
2036,75392720,2,75391866,2023-02-08 23:28:07,3,"<p>How about extending RichAsyncFunction ?</p>
<p>you can find similar example here  - <em><a href=""https://nightlies.apache.org/flink/flink-docs-master/docs/dev/datastream/operators/asyncio/#async-io-api"" rel=""nofollow noreferrer"">https://nightlies.apache.org/flink/flink-docs-master/docs/dev/datastream/operators/asyncio/#async-io-api</a></em></p>
","9125940",2
2037,75383668,2,75377986,2023-02-08 09:28:54,1,"<p>It is about access right management and you can not create/save FeatureSet in project, because you do not have enough privileges. There are at least two solutions:</p>
<p><strong>1 - Setup project owner</strong></p>
<ul>
<li>Create new project, where you will be in role project owner (in this case you have ability to create/save new FeatureSet), see</li>
<li><a href=""https://i.stack.imgur.com/oy7vV.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/oy7vV.png"" alt=""enter image description here"" /></a></li>
</ul>
<p><strong>2 - Add member of project</strong></p>
<ul>
<li>You have to add your account as new member to the existing project in relevant roles (Admin or Editor), see</li>
<li><a href=""https://i.stack.imgur.com/Nm6zQ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Nm6zQ.png"" alt=""enter image description here"" /></a></li>
</ul>
<p>Precondition: Before focus on these two solutions, please check your current roles, if you have roles Data and Developer at least.</p>
","20266647",0
2038,75358955,2,75356400,2023-02-06 08:32:21,2,"<p>There are basically 3 paths you can take, depending on the context.</p>
<h2>Parallelising function execution</h2>
<p><strong>This solution has nothing to do with SageMaker.</strong> It is applicable to any python script, regardless of the ecosystem, as long as you have the necessary resources to parallelise a task.</p>
<p>Based on the needs of your software, you have to work out whether to parallelise multi-thread or multi-process. This question may clarify some doubts in this regard: <a href=""https://stackoverflow.com/questions/3044580/multiprocessing-vs-threading-python"">Multiprocessing vs. Threading Python</a></p>
<p>Here is a simple example on how to parallelise:</p>
<pre class=""lang-py prettyprint-override""><code>from multiprocessing import Pool
import os

POOL_SIZE = os.cpu_count()

your_list = [...]

def run_function(i):
    # ...
    return your_result


if __name__ == '__main__':
    with Pool(POOL_SIZE) as pool:
        print(pool.map(run_function, your_list))
</code></pre>
<h2>Splitting input data into multiple instances</h2>
<p>This solution is dependent on the quantity and size of the data. If they are completely independent of each other and have a considerable size, it may make sense to split the data over several instances. This way, execution will be faster and there may also be a reduction in costs based on the instances chosen over the initial larger instance.</p>
<p>It is clear in your case it is the <code>instance_count parameter</code> to set, as the <a href=""https://sagemaker.readthedocs.io/en/stable/frameworks/sklearn/sagemaker.sklearn.html"" rel=""nofollow noreferrer"">documentation</a> says:</p>
<blockquote>
<p><strong>instance_count</strong> (int or PipelineVariable) - The number of instances to
run the Processing job with. Defaults to 1.</p>
</blockquote>
<p>This should be combined with the <a href=""https://sagemaker.readthedocs.io/en/stable/api/training/processing.html#sagemaker.processing.ProcessingInput"" rel=""nofollow noreferrer"">ProcessingInput</a> split.</p>
<p>P.S.: This approach makes sense to use if the data can be retrieved before the script is executed. If the data is generated internally, the generation logic must be changed so that it is multi-instance.</p>
<h2>Combined approach</h2>
<p>One can undoubtedly combine the two previous approaches, i.e. create a script that parallelises the execution of a function on a list and have several parallel instances.</p>
<p>An example of use could be to process a number of csvs. If there are 100 csvs, we may decide to instantiate 5 instances so as to pass 20 files per instance. And in each instance decide to parallelise the reading and/or processing of the csvs and/or rows in the relevant functions.</p>
<p>To pursue such an approach, one must monitor well whether one is really bringing improvement to the system rather than wasting resources.</p>
","20249888",1
2039,75350196,2,75308992,2023-02-05 04:55:31,1,"<p>You can upload and download files from Amazon SageMaker to Amazon S3 using SageMaker Python SDK. <a href=""https://sagemaker.readthedocs.io/en/stable/api/utility/s3.html"" rel=""nofollow noreferrer"">SageMaker S3 utilities</a> provides <a href=""https://sagemaker.readthedocs.io/en/stable/api/utility/s3.html#sagemaker.s3.S3Uploader"" rel=""nofollow noreferrer"">S3Uploader</a> and <a href=""https://sagemaker.readthedocs.io/en/stable/api/utility/s3.html#sagemaker.s3.S3Downloader"" rel=""nofollow noreferrer"">S3Downloader</a> classes to easily work with S3 from within SageMaker studio notebooks.</p>
<p>A comment about the 'file system' in your question 2, the files are stored onto SageMaker Studio user profile Amazon Elastic File System (Amazon EFS) volume, and not EBS(SageMaker classic notebooks uses EBS volumes). Refer this <a href=""https://aws.amazon.com/blogs/machine-learning/dive-deep-into-amazon-sagemaker-studio-notebook-architecture/"" rel=""nofollow noreferrer"">blog</a> for more detailed overview of SageMaker architecture</p>
","3642047",1
2040,75338126,2,75337910,2023-02-03 15:53:53,1,"<p>Whole code is valid, but the issue is on side of knowledge ;-).</p>
<p>Key information is that <strong>aggregation for on-line target works from 1970-01-01T00:00:00Z till specific day and step/epoch is the time window</strong> (it is behavioral of MLRun 1.2.1).</p>
<p>You have two possible solutions:</p>
<p><strong>1. Change input data</strong> (move date from 2021 to 2023)</p>
<pre><code># Prepare data, four columns key0, key1, fn1, sysdate
data = {&quot;key0&quot;:[1,1,1,1,1,1], &quot;key1&quot;:[0,0,0,0,0,0],&quot;fn1&quot;:[1,1,2,3,1,0],
            &quot;sysdate&quot;:[datetime.datetime(2023,1,1,1), datetime.datetime(2023,1,1,1),
            datetime.datetime(2023,1,1,1), datetime.datetime(2023,1,1,1),
            datetime.datetime(2023,1,1,1), datetime.datetime(2023,1,1,1)]}
</code></pre>
<p>or</p>
<p><strong>2. Extend window for calculation</strong> (e.g. 3 years = ~1095 days)</p>
<pre><code># Add easy aggregation 'agg1'
feature_set.add_aggregation(name='fn1',column='fn1',operations=['count'],windows=['1095d'],step_name=&quot;agg1&quot;)
</code></pre>
","20266647",0
2041,75328540,2,75323889,2023-02-02 20:19:32,1,"<p>From Google <a href=""https://cloud.google.com/vertex-ai/docs/pipelines/run-pipeline#vertex-ai-sdk-for-python"" rel=""nofollow noreferrer"">docs</a>, There is no mention of how you can run a specific component on a subnetwork.</p>
<p>However, it is possible to run the entire pipeline in a subnetwork by passing in the subnetwork as part of the job submit api.</p>
<p><code>job.submit(service_account=SERVICE_ACCOUNT, network=NETWORK)</code></p>
","8303568",0
2042,75323327,2,75283767,2023-02-02 12:38:51,0,"<p>To pass a workflow argument to your script you can use the option
<code>job_arguments</code></p>
<h3>1. Step defintion</h3>
<p>Update your step definition to add the argument <code>job_arguments</code></p>
<pre><code>ProcessingStep(
    name=&quot;step-name&quot;,
    processor=my_processor,
    job_arguments=[
        &quot;--my_argument&quot;,my_argument
    ],
    ...
    code=f&quot;myscript.py&quot;
)
</code></pre>
<h3>2. Reading the argument</h3>
<p>In your script (<code>myscript.py</code> in this example), add ready the argument as follows:</p>
<pre><code>def parse_args():
    parser = argparse.ArgumentParser()

    # hyperparameters sent by the client are passed as command-line arguments to the script
    parser.add_argument('--my_argument', type=str)

    return parser.parse_known_args()
    args, _ = parse_args()

args, _ = parse_args()    
my_argument = args.my_argument
</code></pre>
","1831518",0
2043,75314345,2,75306841,2023-02-01 18:02:18,2,"<p>There are two types of experiments in DVC ecosystem that we need to distinguish and there are a few different approaches on naming them.</p>
<p>First, is what we sometimes call &quot;ephemeral&quot; experiments, those that do not create commits in your Git history unless you explicitly say so. They are described in this <a href=""https://dvc.org/doc/start/experiment-management/experiments"" rel=""nofollow noreferrer"">Get Started section</a>. For each of those experiments a name is auto-generated (e.g. <code>angry-upas</code> in one of the examples from the doc) or you could use <code>dvc exp run -n ...</code> to pass a particular name to it.</p>
<p>Another way to create those experiments (+ send them to Studio) is to use DVC logger (<code>DVCLive</code>), e.g. it's described <a href=""https://dvc.org/doc/studio/user-guide/projects-and-experiments/live-metrics-and-plots"" rel=""nofollow noreferrer"">here</a>. Those experiments will be visible in Studio with an auto-generated names (or a name that was provided when they were created).</p>
<p>Now, we have another type of an experiment - a commit. Something that someone decided to make persistent and share with the team via PR and/or via a regular commit. Those are presented in the image that is shared in the Question.</p>
<p>Since they are regular commits, the regular Git rules apply to them - they have hash, they have descriptions, and most relevant to this discussion is that they have <code>tags</code>. All this information will be reflected in Studio UI.</p>
<p>E.g. in this public <a href=""https://studio.iterative.ai/user/shcheklein/projects/example-get-started-yj0q1nba3g?columns=lWz2cHgsZZMJxOksbrqRp"" rel=""nofollow noreferrer"">example-get-started repo</a>:</p>
<p><a href=""https://i.stack.imgur.com/3jnZI.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/3jnZI.png"" alt=""enter image description here"" /></a></p>
<p>It think in your case, <code>tags</code> would be the most natural way to rename those. May be we can introduce a way to push exp name as a git tag along with the experiment when it's being saved. WDYT?</p>
<p>Let me know if that answers your question.</p>
","298182",2
2044,75311000,2,75304515,2023-02-01 13:37:07,1,"<p>Did you try to expand the square button in front of Session Initialized? For me, it's logged under the session. If it does not work try to downgrade mlflow to the version that PyCaret supports.</p>
<p>The current version of PyCaret (3.0.0rc8) only supports the mlflow between 1.24.0 and less than 2.0.0 you can reference from <a href=""https://github.com/pycaret/pycaret/blob/master/requirements-optional.txt"" rel=""nofollow noreferrer"">here</a>.</p>
<p><a href=""https://i.stack.imgur.com/pcJAY.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/pcJAY.png"" alt=""mlflow_logging"" /></a></p>
","13197914",1
2045,75301925,2,75301253,2023-01-31 18:58:12,0,"<p>You need to prepare component yaml and load it with <a href=""https://kubeflow-pipelines.readthedocs.io/en/stable/source/hkfp.components.html#kfp.components.load_component_from_file"" rel=""nofollow noreferrer"">load_component_from_file</a>.</p>
<p>It's well documented on kfp v2 Kubeflow documentation <a href=""https://www.kubeflow.org/docs/components/pipelines/v1/sdk-v2/component-development/"" rel=""nofollow noreferrer"">page</a>, it's also written <a href=""https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/pipelines/pipelines_intro_kfp.ipynb"" rel=""nofollow noreferrer"">here</a>.</p>
","20034549",0
2046,75278589,2,75261273,2023-01-29 21:40:26,1,"<p>Check out this official example: <a href=""https://github.com/aws/amazon-sagemaker-examples/blob/main/sagemaker-pipelines/tabular/train-register-deploy-pipeline-model/train%20register%20and%20deploy%20a%20pipeline%20model.ipynb"" rel=""nofollow noreferrer"">Train register and deploy a pipeline model</a>.</p>
<p>The two variations to keep in mind:</p>
<ol>
<li>For <strong>models that need training</strong> (usually for those based on tensorflow/pytorch), a TrainingStep must be used so that the output (the model artifact) is correctly (and automatically) generated with the ability to use it later for inference.</li>
<li>For <strong>models generated by a simple fitting on the data</strong> (e.g., a scaler with sklearn), you can think about creating a TrainingStep in disguised (it is an extra component in pipeline, it is not very correct to do it but it is a working round) but the more correct method is to configure the preprocessing script so that it internally saves a model.tar.gz file with the necessary files (e.g., pickle or joblib objects) inside it can then be properly used in later steps as model_data. In fact, if you have a model.tar.gz, you can define a Model of various types (e.g., an SKLearnModel) that is already fitted.</li>
</ol>
<p>At this point, you define your PipelineModel with the trained/fitted models and can either proceed to direct endpoint deployment or decide to go through the model registry and keep a more robust approach.</p>
","20249888",3
2047,75262049,2,72218563,2023-01-27 18:30:02,0,"<p>Answering my own question. This is not possible.</p>
","4374826",0
2048,75235604,2,75145898,2023-01-25 14:43:28,0,"<p>Issue was solved. Turned out to be an issue with the azure-packages, and is expected to be fixed soon. In the meantime you can work around this by creating your own dockerfile rather than env.yaml, and force the azure-core, azure-idendity and azurem-ai-ml packages to the latest versions. More info on how to do that here: <a href=""https://learn.microsoft.com/en-us/azure/machine-learning/how-to-manage-environments-v2?tabs=cli#create-an-environment-from-a-docker-build-context"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/machine-learning/how-to-manage-environments-v2?tabs=cli#create-an-environment-from-a-docker-build-context</a></p>
","8777856",0
2049,75230601,2,75224766,2023-01-25 06:59:42,1,"<ol>
<li>SageMaker Async is managed: you don't need a team/skills to choose, develop, test and maintain queuing software.</li>
<li>You can also configure autoscaling, to scale up and down (eg to reduce fleet size to zero if queue is empty and reduce costs).</li>
<li>It also does S3 interactions for you, (copy to/from S3), you don't need to learn to use S3 code. You would need to add S3 interaction to your ML inference code you were to develop the same thing from scratch.</li>
</ol>
","5331834",1
2050,75221471,2,75184058,2023-01-24 12:25:58,0,"<p>Usually this error occurs when you hit this quota, 5 ops / 10s. Which can be resolved by limiting your rate.</p>
<p>As given in the <a href=""https://cloud.google.com/bigquery/quotas#standard_tables"" rel=""nofollow noreferrer"">document</a>:</p>
<blockquote>
<p>Your project can make up to 1,500 table modifications per table per
day, whether the modification appends data, updates data, or truncates
the table. This limit includes the combined total of all load jobs,
copy jobs, and query jobs that append to or overwrite a destination
table or that use a DML DELETE, INSERT, MERGE, TRUNCATE TABLE, or
UPDATE statement to write data to a table.</p>
</blockquote>
<p>Refer to this SO <a href=""https://stackoverflow.com/questions/55844004/goolgebigquery-exceeded-rate-limits"">link</a> for more information.</p>
","15745105",0
2051,75221180,2,75106061,2023-01-24 11:59:47,1,"<p>you can use a <code>.gitignore</code> or <code>.amlignore</code> file in your working directory to specify files and directories to ignore. These files will not be included when you run the pipeline by default.</p>
<p>Here is the <a href=""https://learn.microsoft.com/en-us/azure/machine-learning/v1/how-to-create-machine-learning-pipelines#submit-the-pipeline"" rel=""nofollow noreferrer"">document</a> to prevent unnecessary files.</p>
<p>or</p>
<pre><code># Get all files in the current working directory
all_files = os.listdir()
# Remove &quot;.env&quot; file from the list of files
all_files.remove(&quot;.env&quot;)

@pipeline(default_compute=cluster_name, files=all_files)
def pipe():
    test_component()


pipeline_job = pipe()

pipeline_job = ml_client.jobs.create_or_update(
    pipeline_job, experiment_name=&quot;pipeline_samples&quot;
)
</code></pre>
","11297406",0
2052,75214598,2,75211670,2023-01-23 20:13:59,1,"<p>In order to save outputs following a certain structure, having in common the execution of the pipeline, <strong>the most robust method currently present</strong> is to use the <code>code_location</code> and <code>output_path</code> parameters of the various steps by previously creating a path that has the <code>pipeline_name</code> and possibly <strong>other details with a timestamp that guarantees its uniqueness</strong>.</p>
<p>Then, when you get your pipeline definition (e.g., with a get_pipeline() function), you can pass the pipeline_name and other variables.
An example is as follows:</p>
<pre class=""lang-py prettyprint-override""><code>import time

pipeline = your_pipeline_script.get_pipeline(
    region = region,
    role = role,
    pipeline_name = your_pipeline_name,
    pipeline_detail = some_details + &quot;-&quot; + time.strftime(&quot;%Y%m%d%H%M%S&quot;, time.gmtime()),
    )
</code></pre>
<p>your output destination may become something like this:</p>
<pre class=""lang-py prettyprint-override""><code>outputs_destination = f&quot;s3://{pipeline_session.default_bucket()}/pipeline/{pipeline_name}/{pipeline_detail}&quot;
</code></pre>
<p>This way is your path is pregenerated before the pipeline is executed and is controllable with whatever parameter you want to enter.</p>
<p>One idea might be to create subfolders that have names of some particular parameter. The important thing is that it follows a well-defined and easily recognizable structure.</p>
","20249888",0
2053,75198469,2,69113605,2023-01-22 05:55:40,0,"<p>I think the bug is in <code>Transform</code> itself, as I am encountering an OOM issue passing it a <code>preprocessing_fn</code> but do not encounter the same issue when passing the same <code>preprocessing_fn</code> to the function below.  Secondly, I face the OOM issue even when just having a training split.  In the case where you are using <code>Transform</code>, isn't the beam code essentially abstracted from you, so how can you override what it is doing?  I see the only solution here to not use <code>Transform</code> and use something like the below, the downside being that you lose all the <code>MLMetadata</code> and <code>pipeline</code> benefits, which are significant :(:</p>
<pre><code>def transform_data(preprocessing_fn, source_path, source_tfrecord_prefix , experiment_path):
    schema = get_schema_from_transform(source_path)                             
    source_tfrecord_path = f&quot;{source_path}/tfrecords/{source_tfrecord_prefix}*&quot;                                                                     
    with beam.Pipeline() as pipeline:                                           
        with tft_beam.Context(temp_dir=tempfile.mkdtemp()):                     
            tfrecord_tfxio = tfxio.TFExampleRecord(                             
                file_pattern=source_tfrecord_path , schema=schema)              
            raw_data = (                                                        
                pipeline | 'ReadData' &gt;&gt; beam.io.ReadFromTFRecord(              
                    file_pattern=source_tfrecord_path, coder=beam.coders.BytesCoder())
                | 'DecodeTrainData' &gt;&gt; tfrecord_tfxio.BeamSource())             
            raw_dataset = (raw_data, tfrecord_tfxio.TensorAdapterConfig())      
            transformed_dataset, transform_fn = (                               
                raw_dataset | tft_beam.AnalyzeAndTransformDataset(              
                    preprocessing_fn, output_record_batches=True))              
            transformed_data, _ = transformed_dataset                           
            transform_fn_output = os.path.join(experiment_path, 'transform_output')
            tfrecord_file_path_prefix = os.path.join(experiment_path, 'tfrecords', experiment_path)
            data_written = (                                                    
                transformed_data | 'EncodeTrainData' &gt;&gt; beam.FlatMapTuple(      
                    lambda batch, _: RecordBatchToExamples(batch)) | beam.io.WriteToTFRecord(
                        tfrecord_file_path_prefix, ))                           
            _ = (transform_fn | 'WriteTransformFn' &gt;&gt; tft_beam.WriteTransformFn(transform_fn_output))
</code></pre>
<p>My code that generates the OOM is as follows:</p>
<pre><code>from tfx import v1 as tfx                                                       
import tensorflow_data_validation as tfdv                                       
TRAIN_DATA = 'train_smallest.csv'
LABEL_NAME = 'fare_amount'                                                      
BATCH_SIZE=256                                                                  
                                                                                
ORIGINAL_TFRECORD_PATH='./identity_transform/tfrecords/'                                                                    
from tfx.components import ImportExampleGen                                     
from tfx.components import StatisticsGen                                        
from tfx.components import SchemaGen                                            
from tfx.components import Transform                                            
from tfx.v1 import proto                                                        
from tfx.proto import example_gen_pb2                                           
                                                                                
output = proto.Output(                                                          
             split_config=example_gen_pb2.SplitConfig(splits=[                  
                 proto.SplitConfig.Split(name='train', hash_buckets=3),         
                 proto.SplitConfig.Split(name='eval', hash_buckets=1)           
             ]))                                                                
example_gen = ImportExampleGen(input_base=ORIGINAL_TFRECORD_PATH, output_config=output)
stats_options = tfdv.StatsOptions(label_feature=LABEL_NAME)                     
statistics_gen = StatisticsGen(examples=example_gen.outputs['examples'], stats_options=stats_options)
schema_gen = SchemaGen(                                                         
      statistics=statistics_gen.outputs['statistics'], infer_feature_shape=True)
transform = Transform(examples = example_gen.outputs['examples'], schema = schema_gen.outputs['schema'], module_file='./preprocessing_fn.py')
components = [                                                                  
      example_gen,                                                              
      statistics_gen,                                                           
      schema_gen,                                                               
      transform                                                                 
  ]                                                                             
                                                                                
pipeline_name='pipeline'                                                        
pipeline_root='./pipeline'                                                      
metadata_path='./metadata/pipeline/metadata.db'                                 
pipeline = tfx.dsl.Pipeline(                                                    
      pipeline_name=pipeline_name,                                              
      pipeline_root=pipeline_root,                                              
      metadata_connection_config=tfx.orchestration.metadata                     
      .sqlite_metadata_connection_config(metadata_path),                        
      components=components,                                                    
enable_cache=False)                                                             
tfx.orchestration.LocalDagRunner().run(pipeline)                                

</code></pre>
<p>My <code>preprocessing_fn</code> is:</p>
<pre><code>import tensorflow as tf                                                         
import tensorflow_addons as tfa                                                 
import tensorflow_transform as tft                                              
NBUCKETS = 10                                                                   
                                                                                
def preprocessing_fn(inputs):                                                   
    &quot;&quot;&quot;                                                                         
    Preprocess input columns into transformed features. This is what goes       
    into tensorflow transform/apache beam.                                      
    &quot;&quot;&quot;                                                                         
    # Since we are modifying some features and leaving others unchanged, we     
    # start by setting `outputs` to a copy of `inputs.                          
    transformed = inputs.copy()                                                 
    del(transformed[&quot;key&quot;])                                                     
    transformed['passenger_count'] = tft.scale_to_0_1(                          
        inputs['passenger_count'])                                              
    # cannot use the below in tft as managing those learned values need         
    # to be                                                                     
    # managed carefully                                                         
    # normalizer = tf.keras.layers.Normalization(axis=None,                     
    # name=&quot;passenger_count_normalizer&quot;)                                        
    # normalizer.adapt(inputs['passenger_count'])                               
    # transformed['other_passenger_count'] = normalizer(                        
    #               inputs['passenger_count'])                                  
    for col in ['dropoff_longitude', 'dropoff_latitude']:                       
        transformed[col] = tft.sparse_tensor_to_dense_with_shape(inputs[col], default_value=tft.mean(inputs[col]), shape=[None, 1]) #You can make this more robust by using the shape from the feature spec
    for lon_col in ['pickup_longitude', 'dropoff_longitude']:                   
        # transformed[lon_col] = scale_longitude(inputs[lon_col])               
        transformed[lon_col] = (transformed[lon_col] + 78) / 8.                 
    for lat_col in ['pickup_latitude', 'dropoff_latitude']:                     
        transformed[lat_col] = (transformed[lat_col] - 37) / 8.                 
    position_difference = tf.square(                                            
        transformed[&quot;dropoff_longitude&quot;] -                                      
        transformed[&quot;pickup_longitude&quot;])                                        
    position_difference += tf.square(                                           
        transformed[&quot;dropoff_latitude&quot;] -                                       
        transformed[&quot;pickup_latitude&quot;])                                         
    transformed['euclidean'] = tf.sqrt(position_difference)                     
    lat_lon_buckets = [                                                         
        bin_edge / NBUCKETS                                                     
        for bin_edge in range(0, NBUCKETS)]                                     
                                                                                
    transformed['bucketed_pickup_longitude'] = tft.apply_buckets(               
        transformed[&quot;pickup_longitude&quot;],                                        
        bucket_boundaries=tf.constant([lat_lon_buckets]))                       
    transformed[&quot;bucketed_pickup_latitude&quot;] = tft.apply_buckets(                
        transformed['pickup_latitude'],                                         
        bucket_boundaries=tf.constant([lat_lon_buckets]))                       
                                                                                
    transformed['bucketed_dropoff_longitude'] = tft.apply_buckets(              
        transformed[&quot;dropoff_longitude&quot;],                                       
        bucket_boundaries=tf.constant([lat_lon_buckets]))                       
    transformed['bucketed_dropoff_latitude'] = tft.apply_buckets(               
        transformed[&quot;dropoff_latitude&quot;],                                        
        bucket_boundaries=tf.constant([lat_lon_buckets]))                       
                                                                                
    # transformed[&quot;pickup_cross&quot;]=tf.sparse.cross(                              
    # inputs=[transformed['pickup_latitude_apply_buckets'],                     
    # transformed['pickup_longitude_apply_buckets']])                           
    hash_pickup_crossing_layer = tf.keras.layers.experimental.preprocessing.HashedCrossing(
        output_mode='one_hot', num_bins=NBUCKETS**2, name='hash_pickup_crossing_layer')
    transformed['pickup_location'] = hash_pickup_crossing_layer(                
        (transformed['bucketed_pickup_latitude'],                               
         transformed['bucketed_pickup_longitude']))                             
    hash_dropoff_crossing_layer = tf.keras.layers.experimental.preprocessing.HashedCrossing(
        output_mode='one_hot', num_bins=NBUCKETS**2,                            
        name='hash_dropoff_crossing_layer')                                     
    transformed['dropoff_location'] = hash_dropoff_crossing_layer(              
        (transformed['bucketed_dropoff_latitude'],                              
         transformed['bucketed_dropoff_longitude']))                            
hash_pickup_crossing_layer_intermediary = tf.keras.layers.experimental.preprocessing.HashedCrossing(
        output_mode='int', num_bins=NBUCKETS**2, )                              
    hashed_pickup_intermediary = hash_pickup_crossing_layer_intermediary(       
        (transformed['bucketed_pickup_longitude'],                              
         transformed['bucketed_pickup_latitude']))                              
    hash_dropoff_crossing_layer_intermediary = tf.keras.layers.experimental.preprocessing.HashedCrossing(
        output_mode='int', num_bins=NBUCKETS**2, )                              
    hashed_dropoff_intermediary = hash_dropoff_crossing_layer_intermediary(     
        (transformed['bucketed_dropoff_longitude'],                             
         transformed['bucketed_dropoff_latitude']))                             
                                                                                
    hash_trip_crossing_layer = tf.keras.layers.experimental.preprocessing.HashedCrossing(
        output_mode='one_hot', num_bins=NBUCKETS ** 3,                          
        name=&quot;hash_trip_crossing_layer&quot;)                                        
    transformed['hashed_trip'] = hash_trip_crossing_layer(                      
        (hashed_pickup_intermediary,                                            
         hashed_dropoff_intermediary))                                          
                                                                                
    seconds_since_1970 = tf.cast(                                               
        tfa.text.parse_time(                                                    
            inputs[&quot;pickup_datetime&quot;],                                          
            &quot;%Y-%m-%d %H:%M:%S %Z&quot;,                                             
            output_unit=&quot;SECOND&quot;),                                              
        tf.float32)                                                             
                                                                                
    # seconds_since_1970 = fn_seconds_since_1970(inputs['pickup_datetime'])     
    seconds_since_1970 = tf.cast(seconds_since_1970, tf.float32)                
    hours_since_1970 = seconds_since_1970 / 3600.                               
    hours_since_1970 = tf.floor(hours_since_1970)                               
    hour_of_day_intermediary = hours_since_1970 % 24                            
    transformed['hour_of_day'] = hour_of_day_intermediary                       
    hour_of_day_intermediary = tf.cast(hour_of_day_intermediary, tf.int32)      
    days_since_1970 = seconds_since_1970 / (3600 * 24)                          
    days_since_1970 = tf.floor(days_since_1970)                                 
    # January 1st 1970 was a Thursday                                           
    day_of_week_intermediary = (days_since_1970 + 4) % 7                        
    transformed['day_of_week'] = day_of_week_intermediary                       
    day_of_week_intermediary = tf.cast(day_of_week_intermediary, tf.int32)      
    hashed_crossing_layer = tf.keras.layers.experimental.preprocessing.HashedCrossing(
        num_bins=24 * 7, output_mode=&quot;one_hot&quot;)                                 
    hashed_crossing_layer_intermediary = tf.keras.layers.experimental.preprocessing.HashedCrossing(
        num_bins=24 * 7, output_mode=&quot;int&quot;, name='hashed_hour_of_day_of_week_layer')
    transformed['hour_of_day_of_week'] = hashed_crossing_layer(                 
        (hour_of_day_intermediary, day_of_week_intermediary))                   
    hour_of_day_of_week_intermediary = hashed_crossing_layer_intermediary(      
        (hour_of_day_intermediary, day_of_week_intermediary))                   
                                                                                
    hash_trip_crossing_layer_intermediary = tf.keras.layers.experimental.preprocessing.HashedCrossing(
        output_mode='int', num_bins=NBUCKETS ** 3)                              
    hashed_trip_intermediary = hash_trip_crossing_layer_intermediary(           
        (hashed_pickup_intermediary, hashed_dropoff_intermediary))              
                                                                                
    hash_trip_and_time_layer = tf.keras.layers.experimental.preprocessing.HashedCrossing(
        output_mode='one_hot', num_bins=(                                       
            NBUCKETS ** 3) * 4, name='hash_trip_and_time_layer')                
    transformed['hashed_trip_and_time'] = hash_trip_and_time_layer(             
        (hashed_trip_intermediary, hour_of_day_of_week_intermediary))           
    return transformed
</code></pre>
<p>And the data that I'm using is as follows, this is before putting it into tfrecords by using a <code>preprocessing_fn</code> that is essentially <code>f(x) = x</code>:</p>
<pre><code>key,fare_amount,pickup_datetime,pickup_longitude,pickup_latitude,dropoff_longitude,dropoff_latitude,passenger_count
2010-10-19 00:01:17.0000003,11.3,2010-10-19 00:01:17 UTC,-73.948724,40.777489,-73.949569,40.814049,1
2009-08-19 08:58:00.00000031,8.5,2009-08-19 08:58:00 UTC,-74.007743,40.724717,-74.006797,40.751253,1
2011-01-24 06:48:12.0000001,10.9,2011-01-24 06:48:12 UTC,-73.986678,40.742597,-73.955101,40.766174,1
2014-09-13 09:08:00.000000126,15.5,2014-09-13 09:08:00 UTC,-74.00325,40.7083,-73.975935,40.749007,1
2013-04-12 19:12:22.0000002,10,2013-04-12 19:12:22 UTC,-74.005318,40.728261,-73.981724,40.7293,1
2015-03-11 11:09:40.0000003,13,2015-03-11 11:09:40 UTC,-73.998809814453125,40.734573364257812,-73.989830017089844,40.756542205810547,1
2014-02-26 01:06:40.0000001,8,2014-02-26 01:06:40 UTC,-73.985821,40.763299,-74.003947,40.751722,1
2011-12-03 02:34:21.0000002,15.7,2011-12-03 02:34:21 UTC,-73.940638,40.840057,-73.98792,40.768815,2
2012-12-10 15:50:03.0000002,34,2012-12-10 15:50:03 UTC,-73.872871,40.774284,-73.995264,40.739349,1
2013-09-22 21:15:18.0000001,7.5,2013-09-22 21:15:18 UTC,-73.996565,40.718924,-74.007011,40.707672,2
2011-06-13 20:19:00.00000010,8.1,2011-06-13 20:19:00 UTC,-73.981587,40.747238,-73.956932,40.771512,5
2013-04-13 02:28:55.0000006,4,2013-04-13 02:28:55 UTC,0,0,0,0,1
2013-09-08 15:49:49.0000001,5.5,2013-09-08 15:49:49 UTC,-73.96077,40.775805,-73.970084,40.76252,1
2011-10-05 21:59:00.00000098,6.5,2011-10-05 21:59:00 UTC,-74.005052,40.70663,-74.012255,40.718838,1
2014-10-03 04:04:00.00000020,5.5,2014-10-03 04:04:00 UTC,-74.000032,40.732362,-73.99655,40.736532,5
2010-06-18 13:41:28.0000001,12.1,2010-06-18 13:41:28 UTC,-73.998732,40.717089,-73.975146,40.758038,3
2014-08-07 12:06:50.0000001,13,2014-08-07 12:06:50 UTC,-74.008268,40.722489,-73.982861,40.744874,1
2009-08-30 11:56:58.0000005,5.7,2009-08-30 11:56:58 UTC,-74.007583,40.748017,-73.994615,40.751834,1
2012-01-28 01:58:00.000000132,9.7,2012-01-28 01:58:00 UTC,-74.422832,40.84662,-74.49563,40.968122,1
2012-06-01 13:33:00.00000070,6.9,2012-06-01 13:33:00 UTC,-73.987377,40.743832,-73.998615,40.739855,1
2014-07-31 20:00:00.000000140,14.5,2014-07-31 20:00:00 UTC,-73.983415,40.75677,-73.95324,40.77148,1
2009-02-06 09:25:00.000000128,6.9,2009-02-06 09:25:00 UTC,-73.994222,40.727137,-73.987398,40.744425,1
2012-01-07 20:28:30.0000003,6.9,2012-01-07 20:28:30 UTC,-73.9889,40.7591,-74.0053,40.7401,0
2012-06-13 15:57:00.000000105,8.5,2012-06-13 15:57:00 UTC,-73.982535,40.761602,-73.980302,40.748475,1
2014-05-02 10:33:00.00000042,5,2014-05-02 10:33:00 UTC,-73.985352,40.74213,-73.991562,40.742512,1
2013-03-30 00:33:00.000000133,15,2013-03-30 00:33:00 UTC,-73.98616,40.757022,-74.009022,40.707857,1
2011-05-20 23:01:00.00000071,9.3,2011-05-20 23:01:00 UTC,-73.951177,40.77465,-73.972487,40.743393,1
2011-01-27 21:51:00.000000171,8.9,2011-01-27 21:51:00 UTC,-73.989867,40.756748,-73.972143,40.786588,3
2009-03-20 12:46:25.0000001,6.9,2009-03-20 12:46:25 UTC,-73.951526,40.770003,-73.970998,40.754989,1
2013-05-01 09:32:00.000000143,7,2013-05-01 09:32:00 UTC,-73.990302,40.756552,-73.982462,40.760242,5
2010-05-25 10:10:00.00000027,4.9,2010-05-25 10:10:00 UTC,-73.980722,40.779832,-73.971522,40.787518,1
2012-12-18 07:24:00.000000148,6,2012-12-18 07:24:00 UTC,-73.965952,40.776297,-73.950533,40.774467,4
2012-04-18 08:36:00.000000182,7.7,2012-04-18 08:36:00 UTC,-73.98358,40.766182,-73.97922,40.752992,1
2009-05-23 17:11:00.00000092,42.9,2009-05-23 17:11:00 UTC,-73.781909,40.64477,-73.991234,40.687251,2
2013-04-16 08:44:39.0000006,12,2013-04-16 08:44:39 UTC,-73.961365,40.760555,-73.9805,40.753709,1
2014-04-15 18:39:47.0000002,3.5,2014-04-15 18:39:47 UTC,-74.008172,40.737866,-74.007745,40.732653,1
2013-12-22 17:42:24.0000001,8,2013-12-22 17:42:24 UTC,-73.977505,40.742731,-73.980127,40.722385,2
2011-09-07 20:12:37.0000004,14.5,2011-09-07 20:12:37 UTC,-74.002225,40.715,-73.951187,40.728798,1
2014-12-10 21:57:46.0000005,6.5,2014-12-10 21:57:46 UTC,-73.97175,40.760287,0,0,1
2012-11-12 22:11:00.000000101,6.5,2012-11-12 22:11:00 UTC,-73.983615,40.760325,-73.998885,40.760012,1
2015-02-21 19:26:56.0000005,15,2015-02-21 19:26:56 UTC,-73.9959716796875,40.686809539794922,-74.009872436523438,40.718009948730469,1
2010-10-06 03:28:42.0000001,8.9,2010-10-06 03:28:42 UTC,-73.988341,40.72886,-73.984581,40.751519,1
2010-07-09 22:01:22.0000007,4.1,2010-07-09 22:01:22 UTC,-73.959272,40.771833,-73.962304,40.773124,1
2009-05-28 10:45:00.00000032,8.9,2009-05-28 10:45:00 UTC,-73.988872,40.753367,-73.972733,40.753327,1
2013-09-24 18:25:00.000000123,25,2013-09-24 18:25:00 UTC,-74.005197,40.751602,-73.980317,40.676607,1
2009-02-05 08:23:01.0000004,7.3,2009-02-05 08:23:01 UTC,-73.975468,40.759635,-73.991854,40.749352,1
2011-07-03 22:25:04.0000003,28.5,2011-07-03 22:25:04 UTC,-73.776755,40.64523,-73.951802,40.657781,1
2010-06-07 15:20:00.000000164,17.7,2010-06-07 15:20:00 UTC,-73.969625,40.758133,-74.012548,40.713983,1
2012-09-14 01:02:00.000000106,7,2012-09-14 01:02:00 UTC,-73.982777,40.744722,-73.984505,40.732127,1
2013-03-08 23:16:00.00000051,25.5,2013-03-08 23:16:00 UTC,-73.990822,40.734702,-73.945737,40.651117,1
2009-10-30 16:06:00.00000074,10.1,2009-10-30 16:06:00 UTC,-73.981008,40.768257,-73.968412,40.80144,2
2012-12-13 19:08:47.0000004,4.5,2012-12-13 19:08:47 UTC,-73.949347,40.78134,-73.956487,40.777995,1
2009-09-03 18:08:00.000000222,5.3,2009-09-03 18:08:00 UTC,-73.985702,40.753625,-73.989385,40.741143,5
2014-05-09 14:18:00.00000085,22.5,2014-05-09 14:18:00 UTC,-73.994697,40.765992,-74.012857,40.706807,1
2010-06-13 18:07:00.00000026,5.7,2010-06-13 18:07:00 UTC,-73.98811,40.774712,-73.98207,40.763285,1
2009-05-30 22:44:00.0000003,15.7,2009-05-30 22:44:00 UTC,-73.968862,40.791558,-73.971705,40.793732,2
2009-05-03 09:58:47.0000002,3.7,2009-05-03 09:58:47 UTC,-73.966445,40.804635,-73.969422,40.798094,1
2011-02-21 22:48:57.0000004,5.7,2011-02-21 22:48:57 UTC,-73.977624,40.752278,-73.994179,40.751649,1
2009-01-09 22:16:00.00000018,4.5,2009-01-09 22:16:00 UTC,-73.961832,40.763897,-73.96975,40.759523,1
2011-06-17 10:31:00.000000154,7.3,2011-06-17 10:31:00 UTC,-73.963242,40.771425,-73.968752,40.7537,1
2014-11-24 12:38:00.000000143,5,2014-11-24 12:38:00 UTC,-73.969662,40.753422,-73.975425,40.74545,1
2015-05-03 09:46:20.0000007,14.5,2015-05-03 09:46:20 UTC,-73.9213638305664,40.745254516601562,-73.85888671875,40.751045227050781,2
2010-04-05 18:49:28.0000001,8.9,2010-04-05 18:49:28 UTC,-73.960298,40.766187,-73.981875,40.727677,1
2013-12-17 20:12:00.00000030,6.5,2013-12-17 20:12:00 UTC,-73.956007,40.779077,-73.965855,40.765742,1
2010-03-12 21:29:00.000000235,9.3,2010-03-12 21:29:00 UTC,0,0,0,0,3
2011-11-30 17:09:00.000000158,14.1,2011-11-30 17:09:00 UTC,-73.985565,40.731717,-73.981347,40.77369,1
2011-12-18 22:21:00.000000162,12.1,2011-12-18 22:21:00 UTC,-73.995165,40.753117,-73.98827,40.725642,1
2010-09-02 14:39:00.000000201,4.1,2010-09-02 14:39:00 UTC,-73.978898,40.785037,-73.970818,40.789267,1
2014-08-22 16:00:00.00000098,6,2014-08-22 16:00:00 UTC,-73.97484,40.756925,-73.972712,40.761775,3
</code></pre>
<p>@tensorflow-support let's solve this issue and make <code>Transform</code> and <code>LocalDagRunner</code> usable so people can solve cool problems with it!</p>
","16681242",1
2054,75179909,2,74979765,2023-01-20 01:50:47,0,"<p>OK, this is the solution I found. When defining the pipeline's components, you should use <code>.with_id()</code> method and give the component a custom ID. That way you can find it later on.</p>
<p>Here's an example. Let's say that I want to find the schema generated as part of the recently executed pipeline.</p>
<pre class=""lang-py prettyprint-override""><code>schema_gen = tfx.components.SchemaGen(
    statistics=statistics_gen.outputs['statistics'],
    infer_feature_shape=True).with_id('some_unique_id')
</code></pre>
<p>Then, the same function I defined above can be used like this:</p>
<pre class=""lang-py prettyprint-override""><code>def get_latest_artifact(metadata_connection_config, pipeline_name: str, component_name: str, type_name: str):
    with Metadata(metadata_connection_config) as metadata:
        context = metadata.store.get_context_by_type_and_name('node', f'{pipeline_name}.{component_name}')
        artifacts = metadata.store.get_artifacts_by_context(context.id)
        artifact_type = metadata.store.get_artifact_type(type_name)
        latest_artifact = max([a for a in artifacts if a.type_id == artifact_type.id],
                              key=lambda a: a.last_update_time_since_epoch)
        artifact = types.Artifact(artifact_type)
        artifact.set_mlmd_artifact(latest_artifact)
        return artifact

sqlite_path = './pipeline_data/metadata.db'
metadata_connection_config = tfx.orchestration.metadata.sqlite_metadata_connection_config(sqlite_path)
examples_artifact = get_latest_artifact(metadata_connection_config, 'simple_pipeline',
                                        'some_unique_id', 'Schema')
</code></pre>
","866082",0
2055,75169309,2,75162867,2023-01-19 08:11:47,1,"<p>Within the pipeline context, <a href=""https://sagemaker.readthedocs.io/en/stable/workflows/pipelines/sagemaker.workflow.pipelines.html#sagemaker.workflow.pipeline_context.PipelineSession"" rel=""nofollow noreferrer"">PipelineSession</a> should be used wherever possible.</p>
<blockquote>
<p>This class inherits the SageMaker session, it provides convenient
methods for manipulating entities and resources that Amazon SageMaker
uses, such as training jobs, endpoints, and input datasets in S3. When
composing SageMaker Model-Building Pipeline, PipelineSession is
recommended over regular SageMaker Session</p>
</blockquote>
<p>At this point, you can specify the bucket directly as a parameter:</p>
<pre class=""lang-py prettyprint-override""><code>from sagemaker.workflow.pipeline_context import PipelineSession

pipeline_session = PipelineSession(default_bucket = your_default_bucket)
</code></pre>
","20249888",2
2056,75143295,2,75061023,2023-01-17 07:48:16,0,"<p><strong>I tried in my environment and below results:</strong></p>
<p>You can be able to create compute instance in <strong><code>AzureML workspace</code></strong> with user by using <a href=""https://learn.microsoft.com/en-us/python/api/azure-identity/azure.identity.defaultazurecredential?view=azure-python"" rel=""nofollow noreferrer"">Defaultazurecredential</a> method.</p>
<p>You can follow this <a href=""https://learn.microsoft.com/en-us/azure/machine-learning/how-to-create-manage-compute-instance?tabs=python"" rel=""nofollow noreferrer"">MS-DOCS</a> to create compute instance.</p>
<p><strong>Code:</strong></p>
<pre><code>from azure.identity import DefaultAzureCredential
from azure.ai.ml.entities import ComputeInstance
import datetime
from azure.ai.ml import MLClient

subscription_id = &quot;&lt;sub id&gt;&quot;
resource_group = &quot;&lt;resource_grp&gt;&quot;
workspace_name = &quot;wrkspace_name&quot;
credential=DefaultAzureCredential()
ml_client = MLClient(subscription_id,resource_group,workspace_name,credential)
ci_basic_name = &quot;v-vsettu1&quot; + datetime.datetime.now().strftime(&quot;%Y%m%d%H%M&quot;)
ci_basic = ComputeInstance(name=ci_basic_name,size=&quot;STANDARD_DS3_v2&quot;)
ml_client.begin_create_or_update(ci_basic).result()
</code></pre>
<p><strong>Console:</strong></p>
<p><img src=""https://i.imgur.com/oGTyl4L.png"" alt=""enter image description here"" /></p>
<p>Also, you can use Cli commands(v2) to create Compute instance.</p>
<p><strong>Command:</strong></p>
<pre><code>az ml compute create -f instance1.yml
</code></pre>
<p><strong><code>yaml</code></strong></p>
<pre><code>$schema: https://azuremlschemas.azureedge.net/latest/computeInstance.schema.json 
name: v-vsettu1
type: computeinstance
size: STANDARD_DS3_v2
</code></pre>
<p><strong>Portal:</strong></p>
<p><img src=""https://i.imgur.com/UaglJCx.png"" alt=""enter image description here"" /></p>
","19144428",0
2057,75142040,2,75140385,2023-01-17 04:38:22,1,"<p>You can pass S3 remote url to the function <code>load_from_disk</code>.</p>
<p>The argument is dataset_path described below.</p>
<blockquote>
<p>dataset_path (str) — Path (e.g. &quot;dataset/train&quot;) or remote URI (e.g. &quot;s3//my-bucket/dataset/train&quot;) of the dataset directory where the dataset will be loaded from.
Reference - <a href=""https://huggingface.co/docs/datasets/v2.8.0/en/package_reference/main_classes#datasets.Dataset.load_from_disk"" rel=""nofollow noreferrer"">https://huggingface.co/docs/datasets/v2.8.0/en/package_reference/main_classes#datasets.Dataset.load_from_disk</a></p>
</blockquote>
<pre><code>from datasets import load_from_disk
# load encoded_dataset from cloud storage

dataset = load_from_disk(&quot;s3://a-public-datasets/imdb/train&quot;, storage_options=storage_options)  

print(len(dataset))
25000
</code></pre>
<p>In order to pass the S3 session details, you can look at the documentation below.
<a href=""https://huggingface.co/docs/datasets/filesystems#amazon-s3"" rel=""nofollow noreferrer"">https://huggingface.co/docs/datasets/filesystems#amazon-s3</a></p>
<pre><code>storage_options = {&quot;anon&quot;: True}  # for anonymous connection
# or use your credentials

storage_options = {&quot;key&quot;: aws_access_key_id, &quot;secret&quot;: aws_secret_access_key}  # for private buckets
# or use a botocore session

import botocore

s3_session = botocore.session.Session(profile=&quot;my_profile_name&quot;)

storage_options = {&quot;session&quot;: s3_session}
</code></pre>
","20910687",0
2058,75109064,2,75108945,2023-01-13 12:13:54,1,"<p>Add <em>recursive</em> option to your command to copy multiple files</p>
<pre><code>aws s3 cp s3://bucket-name/dependencies/ ./dependencies_from_s3/ --recursive
</code></pre>
","4840338",0
2059,75105145,2,75087428,2023-01-13 05:18:49,1,"<p>Another way would be to add &quot;tensorflow_probability&quot; to &quot;requirements.txt&quot; and both your code and requirements into <a href=""https://sagemaker.readthedocs.io/en/stable/api/inference/model.html#:%7E:text=Python%20logging%20module.-,dependencies%20(list%5Bstr%5D)%20%E2%80%93,-A%20list%20of"" rel=""nofollow noreferrer"">dependencies</a> if you are not using source_dir:</p>
<pre><code>Model(entry_point='inference.py',
  dependencies=['code', 'requirements.txt'])
</code></pre>
","9046247",4
2060,75104855,2,75096564,2023-01-13 04:27:54,2,"<p>When you are logging the model, your MLflow version is 1.24, but when you serve it as an API in Databrick's there will be a new environment created for it. This new environment is installing a 2.0+ version of MLflow. As the error message suggests, you can either specify the MLflow version or update the request format.</p>
<p>If you are using <strong>Classic Model Serving</strong>, you should specify the version, if you are using <strong>Serverless Model Serving</strong>, you should update the request format. If you must use <strong>Classic Model Serving</strong> and do not want to upgrade, scroll to the bottom.</p>
<h2>Specify the MLflow version</h2>
<p>When logging the model, you can specify a new Conda environment or add additional pip requirements that are used when the model is being served.</p>
<p><strong>pip</strong></p>
<pre class=""lang-py prettyprint-override""><code># log model with mlflow==1.* specified 
mlflow.&lt;flavor&gt;.log_model(..., extra_pip_requirements=[&quot;mlflow==1.*&quot;])
</code></pre>
<p><strong>Conda</strong></p>
<pre class=""lang-py prettyprint-override""><code># get default conda env
conda_env = mlflow.&lt;flavor&gt;.get_default_conda_env()
print(conda_env)

# specify mlflow==1.*
conda_env = {
    &quot;channels&quot;: [&quot;conda-forge&quot;],
    &quot;dependencies&quot;: [
        &quot;python=3.9.5&quot;,
        &quot;pip&lt;=21.2.4&quot;,
        {&quot;pip&quot;: [&quot;mlflow==1.*&quot;, &quot;cloudpickle==2.0.0&quot;]},
    ],
    &quot;name&quot;: &quot;mlflow-env&quot;,
}

# log model with new conda_env
mlflow.&lt;flavor&gt;.log_model(..., conda_env=conda_env)
</code></pre>
<h2>Update the request</h2>
<p>An alternative is to update the JSON request format, but this only will work if you are using Databrick's Serverless.</p>
<p>In the <a href=""https://mlflow.org/docs/latest/models.html#deploy-mlflow-models"" rel=""nofollow noreferrer"">MLflow docs</a> link at the end of the error message, you can see all the formats. From the data, you provided, I would suggest using <code>dataframe_split</code> or <code>dataframe_records</code>.</p>
<pre class=""lang-json prettyprint-override""><code>{
  &quot;dataframe_split&quot;: {
    &quot;columns&quot;: [&quot;input1&quot;, &quot;input2&quot;, &quot;input3&quot;],
    &quot;data&quot;: [[1, 2, &quot;red&quot;]]
  }
}
</code></pre>
<pre class=""lang-json prettyprint-override""><code>{
  &quot;dataframe_records&quot;: [
    {
      &quot;input1&quot;: 12,
      &quot;input2&quot;: 290,
      &quot;input3&quot;: &quot;red&quot;
    }
  ]
}

</code></pre>
<h2>Classic model serving with MLflow 2.0+</h2>
<p>If you are using Classic Model Serving, don't want to specify the MLflow version and want to use the UI for inference, DO NOT log an <code>input_example</code> when you log the model. I know this does not follow &quot;best practice&quot; for MLflow, but because of some investigating, I believe there is an issue with Databricks when you do this.</p>
<p>When you log an <code>input_example</code>, MLFlow logs information about the example including <code>type</code> and <code>pandas_orient</code>. This information is used to generate the inference recipe. As you can see in the generated curl command, it sets <code>format=pandas-records</code> (the JSON is not generated). But this returns the <em>Unrecognized content type...</em> error.</p>
<pre class=""lang-bash prettyprint-override""><code>curl \
  -u token:$DATABRICKS_TOKEN \
  -X POST \
  -H &quot;Content-Type: application/json; format=pandas-records&quot; \
  -d '{
    &quot;dataframe_split&quot;: {
        &quot;columns&quot;: [&quot;input1&quot;, &quot;input2&quot;, &quot;input3&quot;],
        &quot;data&quot;: [[12, 290, 3]]
    }
  }' \
  https://&lt;url&gt;/model/&lt;model&gt;/&lt;version&gt;/invocations
</code></pre>
<p>For me when I removed <code>format=pandas-records</code> entirely, then everything works as expected. Because of this, I believe if you log an example and use the UI then Databricks is adding this format to the request for you. Which results in an error even if you did everything correctly. While in serverless the generated curl does not include this parameter at all.</p>
","8414855",1
2061,75103386,2,75084816,2023-01-12 23:33:41,2,"<p>With MLFlow, you have to first save or log your model before you can register it. But with <code>log_model</code> you can do both in one step</p>
<pre class=""lang-py prettyprint-override""><code>  mlflow.pytorch.log_model(model, &quot;my_model_path&quot;, registered_model_name=&quot;fancy&quot;)
</code></pre>
<p>Then it is easiest to deploy it from the AzureML Studio:
<a href=""https://i.stack.imgur.com/R2DRF.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/R2DRF.png"" alt=""enter image description here"" /></a></p>
","8821969",3
2062,75098567,2,75071869,2023-01-12 15:18:13,0,"<p>I guess there is no option to make this &quot;host&quot; token last longer. So I created a class to handle my PAT following databrick's 2.O API for tokens <a href=""https://docs.databricks.com/dev-tools/api/latest/tokens.html"" rel=""nofollow noreferrer"">https://docs.databricks.com/dev-tools/api/latest/tokens.html</a></p>
<p>Thankfully PATs are automatically removed once they are expired. So I don't have to handle old PATs.</p>
<pre class=""lang-py prettyprint-override""><code>import json
from typing import Dict, List

import requests
from azure.identity import DefaultAzureCredential


class DatabricksTokenManager:
    &quot;&quot;&quot;Databricks Token Manager. Based on https://docs.databricks.com/dev-tools/api/latest/index.html
    It uses `DefaultAzureCredential` to generate a short token for Databricks. Then it can manage Databricks PATs.
    &quot;&quot;&quot;

    def __init__(self, databricks_host) -&gt; None:
        &quot;&quot;&quot;Init DatabricksTokenManager

        Args:
            databricks_host (str): Databricks host with out &quot;https&quot; or ending &quot;/&quot;
        &quot;&quot;&quot;
        self._token = self._get_databricks_token()
        self.databricks_host = databricks_host
        self._pat = None

    @property
    def token(self) -&gt; str:
        &quot;&quot;&quot;Token property

        Returns:
            str: token value
        &quot;&quot;&quot;
        return self._token

    @property
    def pat(self) -&gt; str:
        &quot;&quot;&quot;PAT property

        Returns:
            str: PAT value
        &quot;&quot;&quot;
        return self._pat

    def _get_databricks_token(self) -&gt; str:
        &quot;&quot;&quot;Get auto generated token from Default Azure Credentials.
        If you are running this code in local. You need to run `az login`. Or set Service Principal Environment Variables.

        Returns:
            str: Databricks temporary Token
        &quot;&quot;&quot;
        dbx_scope = &quot;2ff814a6-3304-4ab8-85cb-cd0e6f879c1d/.default&quot;
        return DefaultAzureCredential().get_token(dbx_scope).token


    def list_databricks_pats(self) -&gt; List[Dict]:
        &quot;&quot;&quot;List all PATs for this user in Databricks

        Returns:
            list: List of dicts containing PAT info
        &quot;&quot;&quot;
        headers = {
            &quot;Authorization&quot;: f&quot;Bearer {self.token}&quot;,
        }
        response = requests.get(
            f&quot;https://{self.databricks_host}/api/2.0/token/list&quot;, headers=headers
        )
        return response.json()[&quot;token_infos&quot;]

    def create_databricks_pat(self, comment=None) -&gt; str:
        &quot;&quot;&quot;Create and return a new PAT from Databricks

        Args:
            comment (str:Optional): Comment to link to PAT. Default None
        Returns:
            str: PAT value
        &quot;&quot;&quot;
        if comment is None:
            comment = &quot;Token created from datalab-framework&quot;

        headers = {
            &quot;Content-type&quot;: &quot;application/json&quot;,
            &quot;Authorization&quot;: f&quot;Bearer {self.token}&quot;,
        }
        json_data = {
            &quot;application_id&quot;: &quot;ce3b7e02-a406-4afc-8123-3de02807e729&quot;,
            &quot;comment&quot;: comment,
            &quot;lifetime_seconds&quot;: 86400, # 24 Hours
        }
        response = requests.post(
            f&quot;https://{self.databricks_host}/api/2.0/token/create&quot;,
            headers=headers,
            json=json_data,
        )
        self._pat = response.json()[&quot;token_value&quot;]
        return self._pat

    def remove_databricks_pat(self, pat_id):
        &quot;&quot;&quot;Remove PAT from databricks

        Args:
            pat_id str: PAT ID
        &quot;&quot;&quot;
        headers = {
            &quot;Authorization&quot;: f&quot;Bearer {self.token}&quot;,
            &quot;Content-Type&quot;: &quot;application/x-www-form-urlencoded&quot;,
        }
        data = {&quot;token_id&quot;: f&quot;{pat_id}&quot;}
        requests.post(
            f&quot;https://{self.databricks_host}/api/2.0/token/delete&quot;,
            headers=headers,
            data=json.dumps(data),
        )
</code></pre>
","5462743",0
2063,75094569,2,75006654,2023-01-12 10:05:39,2,"<p>Please refer to <a href=""https://www.tensorflow.org/tfx/transform/common_transformations#mean_imputation_for_missing_data"" rel=""nofollow noreferrer"">Mean imputation for missing data</a> to impute missing values from your data with mean.</p>
<p>In the example below, x is a feature, represented as a <code>tf.SparseTensor</code> in the <code>preprocessing_fn</code>. In order to convert it to a dense tensor, we compute its mean, and set the mean to be the default value when it is missing from an instance.</p>
<p>Answering your third question, TensorFlow Transform builds transformations into the TensorFlow graph for your model so the same transformations are performed at training and inference time.
For your mentioned use-case, the below example for imputation would work, because default_value param sets values for indices if not specified. And if default_value param is not set, it defaults to Zero.</p>
<p>Example Code:</p>
<pre><code>def preprocessing_fn(inputs):
  return {
      'x_out': tft.sparse_tensor_to_dense_with_shape(
          inputs['x'], default_value=tft.mean(x), shape=[None, 1])
  }
</code></pre>
","user11530462",0
2064,75076353,2,75010272,2023-01-10 22:06:44,1,"<p>This isn't possible using only EventBridge because the event does not contain pipeline parameters.</p>
<p>Your best bet would be a rule which triggers a Lambda. That Lambda then calls <a href=""https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_ListPipelineParametersForExecution.html"" rel=""nofollow noreferrer"">ListPipelineParametersForExecution</a> and starts the second pipeline.</p>
","895615",0
2065,75073333,2,75057477,2023-01-10 16:53:20,1,"<p>At first, it looks confusing because you have high flexibility. <br />
You can use both of them or only one of them. Let's explain it a bit more :-)</p>
<ul>
<li>The <code>--default-artifact-root</code> is a directory for storing artifacts for every new experiment.
<ul>
<li>NOTE: The default value depend if the <code>-serve-artifacts</code> is enabled or not
(<code>mlflow-artifacts:/</code>, <code>./mlruns</code>)</li>
</ul>
</li>
<li><code>--artifacts-destination</code> is used to specify the location of artifacts in HTTP requests.
<ul>
<li>NOTE: This option only applies when the tracking server is configured to stream artifacts (<code>--serve-artifacts</code> is enabled) AND the experiment’s artifact root location is <code>http</code> or <code>mlflow-artifacts URI</code></li>
</ul>
</li>
</ul>
<p><strong>Case 1:</strong> Use both <code>--default-artifact-root</code> &amp; <code>--artifacts-destination</code>:</p>
<pre><code>mlflow server
  --default-artifact-root mlflow-artifacts:/
  --artifacts-destination s3://my-root-bucket
  --host remote_host
  --serve-artifacts
</code></pre>
<p><strong>Case 2:</strong> Use only <code>--artifacts-destination</code></p>
<pre><code>mlflow server
  --artifacts-destination s3://my-root-bucket
  --host remote_host
  --serve-artifacts
</code></pre>
<p>Case 3: Use only <code>--default-artifact-root</code></p>
<pre><code>mlflow server
    --default-artifact-root is s3://my-root-bucket/mlartifacts
    --serve-artifacts
</code></pre>
<p>In this case the server can resolve all the following patterns to the configured proxied object store location of <code>s3://my-root-bucket/mlartifacts</code>:</p>
<pre><code>https://&lt;host&gt;:&lt;port&gt;/mlartifacts
http://&lt;host&gt;/mlartifacts
mlflow-artifacts://&lt;host&gt;/mlartifacts
mlflow-artifacts://&lt;host&gt;:&lt;port&gt;/mlartifacts
mlflow-artifacts:/mlartifacts
</code></pre>
","5544416",0
2066,75055883,2,75016842,2023-01-09 10:24:05,1,"<p>I am creating Azure ML components from a yaml file.
Here's how a simple yaml file to create a component will look like :</p>
<h3>prep.yaml</h3>
<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"" data-babel=""false"">
<div class=""snippet-code"">
<pre class=""snippet-code-html lang-html prettyprint-override""><code>$schema: https://azuremlschemas.azureedge.net/latest/commandComponent.schema.json
type: command

name: prep_image_classification_pytorch
display_name: Data Preparation for Image Classification Pytorch
inputs:
  input_data: 
    type: uri_folder
outputs:
  training_data:
    type: uri_folder
  val_data:
    type: uri_folder 
code: ./
command: python prep.py --input_data ${{inputs.input_data}} --training_data ${{outputs.training_data}} --val_data ${{outputs.val_data}}
environment:
  conda_file: ./conda.yaml
  image: mcr.microsoft.com/azureml/openmpi3.1.2-ubuntu18.04</code></pre>
</div>
</div>
</p>
<h1>The command type will run the source python file which is prep.py</h1>
<p>The prep.py file will take arguments from given yaml file (command line arguments) which we will parse through argparse library</p>
<h3>prep.py</h3>
<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"" data-babel=""false"">
<div class=""snippet-code"">
<pre class=""snippet-code-html lang-html prettyprint-override""><code>ef parse_args():
    # setup argparse
    parser = argparse.ArgumentParser()

    # add arguments
    parser.add_argument(""--input_data"", type=str, help=""path of input data"")
    parser.add_argument(""--training_data"", type=str, default=""./"", help=""output path of train data"")
    parser.add_argument(""--val_data"", type=str, default=""./"", help=""output path of validation data"")

    # parse args
    args = parser.parse_args()
    # return args
    return args
    
def prepare_data_component(
    input_data: Input(type=""uri_folder""),
    training_data: Output(type=""uri_folder""),
    val_data: Output(type=""uri_folder"") ):</code></pre>
</div>
</div>
</p>
<p>As shown in code above use &quot;./default&quot; for ouput URI folders. These folders will be created inside Azure MLs blob storage. Mentioning path of our choice inside azure blob storage did not work for me.</p>
","15398074",0
2067,75039089,2,75016527,2023-01-07 08:48:19,0,"<p>As of MLFlow 2.1.1 <code>pyspark.ml.recommendation.ALS</code> is not on the <a href=""https://mlflow.org/docs/latest/python_api/mlflow.pyspark.ml.html?highlight=spark%20pipeline#:%7E:text=The%20default%20log%20model%20allowlist%20in%20mlflow"" rel=""nofollow noreferrer"">allowlist</a>. Using the pipeline as you tested in #3 is the appropriate way to log unsupported spark.ml models.</p>
<p>It looks like you might have hit an environment issue when logging your model, as I was able to get the following implementation working both on Databricks and locally.</p>
<h2>Log within run</h2>
<pre class=""lang-py prettyprint-override""><code>import mlflow
from mlflow.models.signature import infer_signature
from pyspark.ml import Pipeline
from pyspark.ml.recommendation import ALS

# get a bigger test split from your data
(df_train, df_test) = df_rating.randomSplit([0.6, 0.4])

with mlflow.start_run() as run:
    # initialize als model
    als = ALS(
        maxIter=5,
        regParam=0.01,
        userCol=&quot;User&quot;,
        itemCol=&quot;Item&quot;,
        ratingCol=&quot;Rating&quot;,
        implicitPrefs=False,
        coldStartStrategy=&quot;drop&quot;,
    )

    # build and fit pipeline
    pipeline = Pipeline(stages=[als])
    pipeline_model = pipeline.fit(df_train)

    # test predict and infer signature
    predictions = pipeline_model.transform(df_test)
    signature = infer_signature(df_train, predictions)

    # log model
    mlflow.spark.log_model(
        pipeline_model, artifact_path=&quot;spark-model&quot;, signature=signature
    )

    mlflow.end_run()
</code></pre>
<p><strong>Reload for inference</strong></p>
<p>On Databricks you need to first move the model from the experiment into the model registry. This can be done via the UI or with the following commands.</p>
<pre class=""lang-py prettyprint-override""><code># construct the model_uri from generated run_id and the set artifact_path
run_id = &quot;2f9a5424b1f44435a9413a3e2762b8a9&quot;
artifact_path = &quot;spark-model&quot;
model_uri = f&quot;runs:/{run_id}/{artifact_path}&quot;

# move the model into the registry
model_details = mlflow.register_model(model_uri=model_uri, name=model_name)

# load model
version = 1
model_uri = f&quot;models:/{model_name}/{version}&quot;
loaded_model = mlflow.spark.load_model(model_uri)

# test predict
loaded_model.transform(df_test).show()
</code></pre>
<pre><code>+----+------+----+----------+
|Item|Rating|User|prediction|
+----+------+----+----------+
|   2|     3|   2| 1.0075595|
+----+------+----+----------+
</code></pre>
<h2>Log directly to registry</h2>
<p>Alternatively, you can also log directly to the model registry.</p>
<pre class=""lang-py prettyprint-override""><code># log model
model_name=&quot;als-model&quot;
mlflow.spark.log_model(
    pipeline_model, 
    artifact_path=&quot;spark-model&quot;,
    registered_model_name=model_name
)

# log model
version = 1
model_uri = f&quot;models:/{model_name}/{version}&quot;
loaded_model = mlflow.spark.load_model(model_uri)

# test predict
loaded_model.transform(df_test).show()
</code></pre>
<h2>Logging as custom pyfunc</h2>
<p>I also tried wrapping the ALS model and the pipeline in a custom pyfunc  and in both cases I received the same exact error. I believe there is something unserializable with the ALS model that prevents this...</p>
","8414855",1
2068,75002933,2,74558819,2023-01-04 08:46:29,0,"<p>After reaching out to Google support regarding this issue, it was explained to me that the quota is based on the number of vCPUs used in the batch prediction job. The formula to calculate this is:</p>
<p><em>the number of vCPUs in a machine X number of machines ( X 3 if explanations are enabled because a separate node is spun up in this case which requires additional resources)</em></p>
<p>For example if using 50 <code>e2-standard-4</code> machines to a run batch prediction with explanations results in 50 * 4 * 3 = 600 vCPUs in total being used.</p>
<p><strong>The default quota for a Google project is 2,200 vCPUs</strong> for the europe-west2 region. Moreover, this limit is not visible in the user's Google project, but instead in a hidden project only visible to Google engineers. Thus, it is required to raise a support ticket if you need the quota to be increased.</p>
","19956528",0
2069,75000268,2,74996854,2023-01-04 01:55:00,1,"<p>It took me a whole day to figure this out and I had to read TFX code for it (there was hardly any documentation). An older approach to address the same need can be found in <a href=""https://www.tensorflow.org/tfx/tutorials/tfx/penguin_tfdv#review_outputs_of_the_pipeline"" rel=""nofollow noreferrer"">TFX documentation</a> but it's dated and does not work with the latest version of TFX. I'm sure even this solution will be short-lived and soon it won't work. But for the time being:</p>
<pre class=""lang-py prettyprint-override""><code>from tfx import types
from tfx import v1 as tfx
from tfx.orchestration.metadata import Metadata
from tfx.orchestration.experimental.interactive import visualizations
from tfx.orchestration.experimental.interactive import standard_visualizations
standard_visualizations.register_standard_visualizations()


sqlite_path = './pipeline_data/metadata.db'
pipeline_name = 'simple_pipeline'
component_name = 'StatisticsGen'
type_name = 'ExampleStatistics'
metadata_connection_config = tfx.orchestration.metadata.sqlite_metadata_connection_config(sqlite_path)

with Metadata(metadata_connection_config) as metadata:
    context = metadata.store.get_context_by_type_and_name('node', f'{pipeline_name}.{component_name}')
    artifacts = metadata.store.get_artifacts_by_context(context.id)
    artifact_type = metadata.store.get_artifact_type(type_name)
    latest_artifact = max([a for a in artifacts if a.type_id == artifact_type.id], key=lambda a: a.last_update_time_since_epoch)
    artifact = types.Artifact(artifact_type)
    artifact.set_mlmd_artifact(latest_artifact)
    visualization = visualizations.get_registry().get_visualization(artifact.type_name)
    visualization.display(artifact)
</code></pre>
<p>Disclaimer, this code displays the latest artifact for the statistics component of a specific pipeline. Or if you want you can point to the artifact by its folder path (uri):</p>
<pre class=""lang-py prettyprint-override""><code>from tfx import types
from tfx import v1 as tfx
from tfx.orchestration.metadata import Metadata
from tfx.orchestration.experimental.interactive import visualizations
from tfx.orchestration.experimental.interactive import standard_visualizations
standard_visualizations.register_standard_visualizations()

sqlite_path = './pipeline_data/metadata.db'
uri = './pipeline_data/StatisticsGen/statistics/16'
component_name = 'StatisticsGen'
type_name = 'ExampleStatistics'
metadata_connection_config = tfx.orchestration.metadata.sqlite_metadata_connection_config(sqlite_path)

with Metadata(metadata_connection_config) as metadata:
    artifacts = metadata.store.get_artifacts_by_uri(uri)
    artifact_type = metadata.store.get_artifact_type(type_name)
    latest_artifact = max([a for a in artifacts if a.type_id == artifact_type.id], key=lambda a: a.last_update_time_since_epoch)
    artifact = types.Artifact(artifact_type)
    artifact.set_mlmd_artifact(latest_artifact)
    visualization = visualizations.get_registry().get_visualization(type_name)
    visualization.display(artifact)
</code></pre>
<p>At the end, maybe there is a better way to do this but I missed it.</p>
","866082",0
2070,76587491,2,75038672,2023-06-30 09:01:26,0,"<p>By default mlflow save_format is tf which will allow the model to save in pb format.
I made it work(changing to h5 format) using below code</p>
<pre><code>mlflow.tensorflow.log_model(model, &quot;models&quot;, keras_model_kwargs={&quot;save_format&quot;: &quot;h5&quot;})
</code></pre>
<p>After this save format will change to h5
<a href=""https://i.stack.imgur.com/LPnbu.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/LPnbu.png"" alt=""enter image description here"" /></a></p>
<p>and the model will be saved in model.h5</p>
<p><a href=""https://i.stack.imgur.com/MGXWJ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/MGXWJ.png"" alt=""enter image description here"" /></a></p>
<p>Tested this in mlflow v2.3.0</p>
","6096715",0
2071,76575310,2,76574120,2023-06-28 17:02:43,0,"<p>Unfortunately, there is no convenient way to do it. The only thing that you can do by Mlflow is to log a new param against the old run by using <code>mlflow.client</code> which is described <a href=""https://mlflow.org/docs/latest/python_api/mlflow.client.html#mlflow.client.MlflowClient.log_param"" rel=""nofollow noreferrer"">here</a>.</p>
<hr />
<p>As you mentioned, you can rename the param by altering the column name in the database. Clearly, you can do this only if your tracking server is a database.</p>
<p>If you store your runs in a local file path, you need to only change file names in the <code>params</code> directory for each run.</p>
","11232272",0
2072,76564750,2,76563164,2023-06-27 12:14:17,2,"<p>As per this official <a href=""https://v0-5.kubeflow.org/docs/gke/troubleshooting-gke/#404-page-not-found-when-accessing-central-dashboard"" rel=""nofollow noreferrer"">doc</a>, try below troubleshooting steps to resolve your error :</p>
<blockquote>
<p>This section provides troubleshooting information for 404s, page not
found, being return by the central dashboard which is served at</p>
<pre><code>https://${KUBEFLOW_FQDN}/
</code></pre>
<p>KUBEFLOW_FQDN is your project’s OAuth web app URI domain name
<code>&lt;name&gt;.endpoints.&lt;project&gt;.cloud.goog</code></p>
<p>We can confirm that the Ambassador reverse proxy is functioning
properly by running the following command because we were able to sign
in.</p>
<pre><code> kubectl -n ${NAMESPACE} get pods -l service=envoy
    NAME                     READY     STATUS    RESTARTS   AGE
   envoy-76774f8d5c-lx9bd   2/2       Running   2          4m
   envoy-76774f8d5c-ngjnr   2/2       Running   2          4m
   envoy-76774f8d5c-sg555   2/2       Running   2          4m
</code></pre>
<p>Now try to Attempt different administrations to check whether they're
open for instance</p>
<pre><code>   https://${KUBEFLOW_FQDN}/whoami
   https://${KUBEFLOW_FQDN}/tfjobs/ui
   https://${KUBEFLOW_FQDN}/hub
</code></pre>
<p>We know that the issue is specific to the central dashboard and not
ingress if other services are accessible.</p>
<p>Now check that the central dashboard is running</p>
<pre><code>kubectl get pods -l app=centraldashboard
NAME                                READY     STATUS    RESTARTS   AGE
centraldashboard-6665fc46cb-592br   1/1       Running   0          7h
</code></pre>
<p>Check that an Ambassador route is properly defined or not</p>
<pre><code>kubectl get service centraldashboard -o jsonpath='{.metadata.annotations.getambassador\.io/config}'

apiVersion: ambassador/v0
  kind:  Mapping
  name: centralui-mapping
  prefix: /
  rewrite: /
  service: centraldashboard.kubeflow
</code></pre>
<p>,</p>
<p>Take a look at the logs of Envoy for blunders. Check to see if there
are any errors like these that indicate a problem parsing the route.
In the event that you are utilizing the new Stackdriver Kubernetes
observing you can utilize the accompanying channel in the stackdriver
console</p>
<pre><code> resource.type=&quot;k8s_container&quot;
 resource.labels.location=${ZONE}
 resource.labels.cluster_name=${CLUSTER}
 metadata.userLabels.service=&quot;ambassador&quot;
&quot;could not parse YAML&quot;
</code></pre>
</blockquote>
<p>You can also refer to this <a href=""https://v1-5-branch.kubeflow.org/docs/distributions/gke/"" rel=""nofollow noreferrer"">doc</a> for more information aboutdeploying kubeflow on GKE.</p>
","19289375",1
2073,76526651,2,76516481,2023-06-21 19:45:45,2,"<p>We resolved this issue by deleting the JupyterServer from the user profile and then re-launching Studio which will automatically create a new one (user data isn't affected as it is stored in EFS). One caveat is that we were unable to use the UI to delete the JupyterServer. The delete option was there but nothing happened when clicked, instead we had to use the CLI to remove it.</p>
<p>This had to be done on every user profile.</p>
","5659060",2
2074,76453924,2,76431211,2023-06-12 05:53:40,0,"<p>This was caused by STS (AWS Security Token Service) not being enabled for the specific regions that I was trying to run my infrastructure. Apparently in some regions, STS is not e,nabled by default, and has to be enabled through IAM -&gt; Account Settings -&gt; STS -&gt; Endpoints as explained <a href=""https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_temp_enable-regions.html"" rel=""nofollow noreferrer"">here</a>.</p>
","4858605",0
2075,76440735,2,76440443,2023-06-09 13:26:49,1,"<blockquote>
<p>I've verified that the ecs execution role has full access to s3</p>
</blockquote>
<p>The ECS Execution Role is used by the ECS service to run your task. It needs access to things like ECR and Secrets Manager in order to create and run your ECS task. The ECS Execution Role is not provided to the actual code running inside your task.</p>
<p>You need to assign a <a href=""https://docs.aws.amazon.com/AmazonECS/latest/developerguide/task-iam-roles.html"" rel=""nofollow noreferrer"">Task Role</a> to your ECS task. The Task Role is provided to code running inside your ECS tasks to allow that code to access AWS resources.</p>
","13070",0
2076,76422180,2,76398998,2023-06-07 10:10:50,1,"<p>Looking in the documentation under &quot;<a href=""https://sagemaker.readthedocs.io/en/stable/overview.html#prepare-a-training-script"" rel=""nofollow noreferrer"">Prepare a Training script</a>&quot;:</p>
<blockquote>
<p>you can access useful properties about the training environment through various environment variables</p>
</blockquote>
<p>Therefore, when you input a data channel as in your case, you will refer to specially created environment variables.</p>
<p>Your code becomes:</p>
<pre class=""lang-py prettyprint-override""><code>parser = argparse.ArgumentParser()

# Set location of input training data
parser.add_argument(&quot;--train&quot;, type=str, default=os.environ[&quot;SM_CHANNEL_TRAIN&quot;])

# Set location of input validation data
parser.add_argument(&quot;--validation&quot;, type=str, default=os.environ[&quot;SM_CHANNEL_VALIDATION&quot;])

args, _ = parser.parse_known_args()

data_train = pd.read_csv(f&quot;{args.train}/train.csv&quot;)
data_validation = pd.read_csv(f&quot;{args.validation}/validation.csv&quot;)
</code></pre>
<hr />
<p>If you want to choose a different name for your input channel, the environment variable will change accordingly.</p>
<p>For example:</p>
<pre class=""lang-py prettyprint-override""><code># in your TrainingStep
inputs = {
            &quot;myexample&quot;: TrainingInput(
                s3_data=step_process.properties.ProcessingOutputConfig.Outputs[
                    &quot;myexample&quot;
                ].S3Output.S3Uri
            )
        }

# in your training script
parser.add_argument(&quot;--myexample&quot;, type=str, default=os.environ[&quot;SM_CHANNEL_MYEXAMPLE&quot;])
</code></pre>
","20249888",0
2077,76416977,2,76334479,2023-06-06 17:14:17,0,"<p>Finally, I ended up finding my answer on my own, This method uses the SDK 2.0. Note that it is asynchronous, so the function call returns before the stopping is done.</p>
<pre><code># MC is MLClient... compute_instance is a ComputeInstance object    
mc.compute.begin_stop(compute_instance.name)
</code></pre>
","250089",0
2078,76413598,2,76408811,2023-06-06 10:16:26,2,"<p>In short, you should be able to access the directory at <code>/root/sagepipeline_nonomethod/source_dir</code> within your notebook. Explaination follows:</p>
<hr />
<p>If you open a <em>terminal</em> in the Launcher under <em>Utilities and files</em>, you will arrive at a <strong>system terminal</strong>. It's the compute environment living in SageMaker Studio, instead of the container where your notebook (kernel) lives.</p>
<p>The <code>/home/sagemaker-user</code> directory is a mountpoint of an EFS, where your Jupyter file browser points to.</p>
<pre><code>sagemaker-user@studio$ df -h
Filesystem         Size  Used Avail Use% Mounted on
overlay             32G  5.3M   32G   1% /
tmpfs               64M     0   64M   0% /dev
tmpfs              1.9G     0  1.9G   0% /sys/fs/cgroup
shm                395M     0  395M   0% /dev/shm
/dev/nvme0n1p1     160G   26G  135G  16% /opt/.sagemakerinternal
127.0.0.1:/200005  8.0E   40M  8.0E   1% /home/sagemaker-user
devtmpfs           1.9G     0  1.9G   0% /dev/tty
tmpfs              1.9G     0  1.9G   0% /proc/acpi
tmpfs              1.9G     0  1.9G   0% /sys/firmware
</code></pre>
<p>To access the terminal of the container environment, go to <em>Launcher</em> &gt; <em>Notebooks and compute resources</em> &gt; <em>Open image terminal</em></p>
<p>The same EFS is mounted on <code>/root</code></p>
<pre><code>root@datascience-1-0-ml-t3-medium-1abf3407f667f989be9d86559395:~# df -h
Filesystem         Size  Used Avail Use% Mounted on
overlay             32G   40K   32G   1% /
tmpfs               64M     0   64M   0% /dev
tmpfs              1.9G     0  1.9G   0% /sys/fs/cgroup
shm                395M     0  395M   0% /dev/shm
127.0.0.1:/200005  8.0E   40M  8.0E   1% /root
/dev/nvme0n1p1     160G   24G  137G  15% /opt/.sagemakerinternal
devtmpfs           1.9G     0  1.9G   0% /dev/tty
tmpfs              1.9G     0  1.9G   0% /proc/acpi
tmpfs              1.9G     0  1.9G   0% /sys/firmware
</code></pre>
","10109530",0
2079,76411671,2,76409133,2023-06-06 05:33:14,0,"<p>Even I got the same error while saving it in dbfs.</p>
<p><img src=""https://i.imgur.com/0k64dVu.png"" alt=""enter image description here"" /></p>
<p>So follow below approach.</p>
<p>Save the model to local filesystem and load it in dbfs.</p>
<pre><code>model.save('/tmp/my_models/my_model.h5')
</code></pre>
<p><img src=""https://i.imgur.com/a5VaD56.png"" alt=""enter image description here"" /></p>
<pre><code>dbutils.fs.ls(&quot;file:/tmp/my_models/&quot;)
</code></pre>
<p><img src=""https://i.imgur.com/BVGbklz.png"" alt=""enter image description here"" /></p>
<p>copy the model to dbfs.</p>
<pre><code>dbutils.fs.cp(&quot;file:/tmp/my_models/my_model.h5&quot;,&quot;dbfs:/tmp/my_models/my_model.h5&quot;)
</code></pre>
<p><img src=""https://i.imgur.com/OGsa41p.png"" alt=""enter image description here"" /></p>
<p>Then load the model.</p>
<pre><code>new_model = tf.keras.models.load_model('/tmp/my_models/my_model.h5')
new_model.summary()
</code></pre>
<p><img src=""https://i.imgur.com/nBmsIdT.png"" alt=""enter image description here"" /></p>
","21588207",0
2080,76400448,2,76396664,2023-06-04 12:36:25,0,"<p>I see the solution, based on these principles:</p>
<ol>
<li>If you need to use two different engines, you have to use two different feature sets</li>
<li>Each feature set will use relation to the same target type</li>
<li>Each target type will refer to the same storage (parquet target storage)</li>
</ol>
<p>See sample:</p>
<pre><code># first feature set, engine 'storey'
fs_01 = fstore.FeatureSet(&quot;fs01&quot;, entities=[fstore.Entity(&quot;km&quot;)],
                                engine=&quot;storey&quot;)
fs_01.set_targets(targets=[mlrun.datastore.ParquetTarget(name='s01', path=f&quot;v3io:///projects/tst2/&quot;,partitioned=False)], with_defaults=False)
fs_01.save()

#second feature set, engine 'pandas'
fs_02 = fstore.FeatureSet(&quot;fs02&quot;, entities=[fstore.Entity(&quot;km&quot;)],
                                engine=&quot;pandas&quot;)
fs_02.set_targets(targets=[mlrun.datastore.ParquetTarget(name='s02', path=f&quot;v3io:///projects/tst2/&quot;,partitioned=False)], with_defaults=False)
fs_02.save()


# generate sample data
dataFrm01 = pandas.DataFrame(numpy.random.randint(low=0, high=1000,
                                                size=(100, 10)),  # rows, columns
                           columns=[f&quot;fn{i}&quot; for i in range(10)])
dataFrm02 = pandas.DataFrame(numpy.random.randint(low=0, high=1000,
                                                size=(100, 10)),  # rows, columns
                           columns=[f&quot;fn{i}&quot; for i in range(10)])

# ingest data with two different engines to the same parquet target
fstore.ingest(fs_01,dataFrm01,overwrite=False)
fstore.ingest(fs_02,dataFrm02,overwrite=False)
</code></pre>
","20266647",0
2081,76394322,2,76384215,2023-06-03 02:25:49,0,"<p>As I understand it, you'd like to display the repository prefix for your Jupyter images. For this, you can set to <code>false</code> the <code>hideRegistry</code> key in the Jupyter Web App ConfigMap. By default, the value of this key is true, which hides the image repository in the user interface.</p>
<p>Search this ConfigMap in <code>kubeflow</code> namespace. In Kubeflow Manifests GitHub repository you can found in <a href=""https://github.com/kubeflow/manifests/blob/14c0f9abe70c4d0ce3e021a5839a7cdd54dc572d/apps/jupyter/jupyter-web-app/upstream/base/configs/spawner_ui_config.yaml#L50"" rel=""nofollow noreferrer"">spawner_ui_config.yaml</a> file.</p>
","10815710",1
2082,76393652,2,76385996,2023-06-02 21:51:32,1,"<p>SageMaker training/ pipeline step always adds a unique suffix to the output path provided for the files stored in the model dir. There are 2 ways you can avoid this</p>
<ol>
<li>Rather than storing to /opt/ml/model you can define checkpoint_local_path and checkpoint_s3_uri where the files are copied as is to s3.</li>
<li>Write your own s3 uploader which will upload to a known path.</li>
</ol>
","19490330",0
2083,76382385,2,76184542,2023-06-01 13:25:20,1,"<p>The path <code>&quot;/dbfs/FileStore/path/to/model/model.pkl&quot;</code> is a Databricks File System (DBFS) path, which is not accessible from outside of Databricks. When you run the code from your local machine, you need to use a path that is accessible from your local machine. <strong>One way to do this is to download the model file from DBFS to your local machine and then use the local path to the downloaded file for registering the model.</strong></p>
<p><strong>1.</strong> You can use the <a href=""https://learn.microsoft.com/en-us/azure/databricks/dev-tools/cli/"" rel=""nofollow noreferrer"">Databricks CLI</a>  to download the file.</p>
<p>First, you need to install the <code>databricks-cli</code> package:</p>
<pre class=""lang-bash prettyprint-override""><code>pip install databricks-cli
</code></pre>
<p>Then, configure the Databricks CLI with your Databricks workspace URL and <a href=""https://learn.microsoft.com/en-us/azure/databricks/dev-tools/auth#azure-ad-tokens-for-users"" rel=""nofollow noreferrer"">access token</a>:</p>
<pre><code>bash
databricks configure --token
</code></pre>
<p>This prompt to add host and token, add them.</p>
<p><img src=""https://i.imgur.com/scOoTGx.png"" alt=""enter image description here"" /></p>
<p>Below is the model path in databricks.</p>
<p><img src=""https://i.imgur.com/y3mgo9M.png"" alt=""enter image description here"" /></p>
<pre><code>databricks fs ls dbfs:/FileStore/tables/
</code></pre>
<p>Below is the list when above command is executed.</p>
<p><img src=""https://i.imgur.com/AxHWNAP.png"" alt=""enter image description here"" /></p>
<p>Know copy it to local system.</p>
<pre class=""lang-bash prettyprint-override""><code>databricks fs cp dbfs:/FileStore/path/to/model/model.pkl ./model.pkl
</code></pre>
<p><img src=""https://i.imgur.com/x2PEtdE.png"" alt=""enter image description here"" /></p>
<p>update the <code>model_uri</code> variable in your code to point to the local file:</p>
<pre><code>model_uri = &quot;./model.pkl&quot;
</code></pre>
<p>when you run the code from your local machine, it should be able to find the model file and register it with AzureML.</p>
","21588207",0
2084,76357826,2,76356663,2023-05-29 13:32:18,0,"<p>To solve this issue, you can modify your code to ensure that the tensors are loaded onto the CPU instead of the GPU.
Add device variable in your code:</p>
<pre><code>device = torch.device(&quot;cuda&quot;  if  torch.cuda.is_available() else  &quot;cpu&quot;)
</code></pre>
<p>Replace below code in <strong>run ()</strong> function:</p>
<pre><code> tensor_a = torch.tensor(y, dtype=torch.int32)
 tensor_a = torch.unsqueeze(tensor_a, dim=0).to(&quot;device&quot;)

 tensor_t = torch.tensor(x, dtype=torch.int32)
 tensor_t = torch.unsqueeze(tensor_t, dim=0).to(&quot;device&quot;)
</code></pre>
<p>Below is the example for the error and fix.</p>
<p>Error Reproduced;
<a href=""https://i.stack.imgur.com/kSi5E.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/kSi5E.png"" alt=""enter image description here"" /></a></p>
<p>Fix:</p>
<p><a href=""https://i.stack.imgur.com/CzwnO.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/CzwnO.png"" alt=""enter image description here"" /></a></p>
","20059423",0
2085,76357387,2,76332923,2023-05-29 12:27:53,0,"<p>As per the error logs, you are attempting to deserialize an object on a CUDA device, but <code>torch.cuda.is_available()</code> is returning <strong>False</strong>, which is due to running on a CPU only machine. To resolve this issue, you need to update the <code>torch.load</code> function to specify <code>map_location=torch.device('cpu')</code> to map the storages to the CPU.</p>
<p>Since the <code>mlflow.pyfunc.load_model()</code> function does not have a <code>map_location</code> argument, you can use a <code>**kwargs</code> argument that can pass any additional keyword arguments to the <code>torch.load()</code> function.</p>
<p>To solve the issue, add <code>*{'map_location': torch.device('cpu')}</code>in your <code>score.py</code> file.</p>
<pre><code>model = mlflow.pyfunc.load_model(model_path, *{'map_location': torch.device('cpu')}) 
</code></pre>
<p><strong>or Use below code:(<em>Updated solution</em>)</strong></p>
<p><code>model = mlflow.pytorch.load_model(model_path, map_location=torch.device('cpu'))</code></p>
<p>Example:</p>
<pre><code>import  mlflow
import  torch
path=&quot;./deploy/credit_defaults_model/&quot;
model = mlflow.pyfunc.load_model(path, *{'map_location': torch.device('cpu')})
</code></pre>
","20059423",3
2086,76355878,2,76352244,2023-05-29 08:47:59,1,"<p>You are right, You can create Docker Containers to create separate environment for different libraries for users to manage them separately. You can also make use of azure ML Custom environment and create isolated environment for different packages.</p>
<p>For virtual environment,One option is to directly pip install the packages and second option is to create a requirements.txt file add all the packages in that file, and pip install the requirements.txt in venv. And third option is to use setup.py &gt; Add all your packages in setup.py and run the setup.py code to install those packages and then import them according to the users requirement.</p>
<p><em><strong>Virtual environment:-</strong></em></p>
<p>Created one requirements.txt file like below:-</p>
<pre><code>numpy==1.19.5
pandas==1.3.0
</code></pre>
<p><img src=""https://i.imgur.com/6W4MqJ4.png"" alt=""enter image description here"" /></p>
<p>In the Azure ML Notebook, Create Virtual environment and install the requirements.txt packages like below:-</p>
<pre class=""lang-py prettyprint-override""><code>!python3 -m venv myenv

!source myenv/bin/activate

!pip install -r requirements.txt
</code></pre>
<p><img src=""https://i.imgur.com/6W4MqJ4.png"" alt=""enter image description here"" /></p>
<p>You can refer my <a href=""https://stackoverflow.com/questions/75770096/sys-executable-not-working-in-azure-ml-studio/75797736#75797736"">SO thread answer</a> here where I created a <strong>setup.py</strong> python file with packages in my github repository and installed them in my notebook.</p>
<p>You can directly create different python files with packages and run it in your Notebook like below:-</p>
<p><a href=""https://stackoverflow.com/questions/75770096/sys-executable-not-working-in-azure-ml-studio/75797736#75797736""><strong>setup.py</strong></a> from my answer above:-</p>
<pre class=""lang-py prettyprint-override""><code>from setuptools import setup, find_packages

setup(

name='pycode',

version='0.1',

packages=find_packages(),

install_requires=[

'numpy',

'pandas',

'scikit-learn'

],

entry_points={

'console_scripts': [

'pycode=pycode.cli:main'

]

}

)
</code></pre>
<p><img src=""https://i.imgur.com/DQQn4e3.png"" alt=""enter image description here"" /></p>
<p><strong>Output:-</strong></p>
<p><img src=""https://i.imgur.com/b8gMEos.png"" alt=""enter image description here"" /></p>
","20849135",3
2087,76346076,2,76341954,2023-05-27 08:48:50,1,"<p>I will describe <strong>two ways of doing this</strong>.</p>
<p>Model signature <strong>can be retrieved from the associated run metadata</strong>. Here is a picture showing how to do that in UI:</p>
<p><a href=""https://i.stack.imgur.com/s63rE.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/s63rE.png"" alt=""enter image description here"" /></a></p>
<p>Now, to extract this programmatically, note that <strong><a href=""https://mlflow.org/docs/latest/tracking.html"" rel=""nofollow noreferrer"">logged model metadata is tracked under the <code>mlflow.log-model.history</code> tag</a></strong>. Once we know the corresponding run id (we keep it at hand or query the model store) we can use this code snippet :</p>
<pre class=""lang-py prettyprint-override""><code>import json
import mlflow
from mlflow.client import MlflowClient

client = MlflowClient('http://0.0.0.0:5000')
run_id = '467677aff0074955a4e75492085d52f9'
run = client.get_run(run_id)
log_model_meta = json.loads(run.data.tags['mlflow.log-model.history'])
log_model_meta[0]['signature']
</code></pre>
<p>which agrees with the figure :-)</p>
<pre><code>{'inputs': '[{&quot;type&quot;: &quot;tensor&quot;, &quot;tensor-spec&quot;: {&quot;dtype&quot;: &quot;float64&quot;, &quot;shape&quot;: [-1, 4]}}]',
 'outputs': '[{&quot;type&quot;: &quot;tensor&quot;, &quot;tensor-spec&quot;: {&quot;dtype&quot;: &quot;int64&quot;, &quot;shape&quot;: [-1]}}]'}
</code></pre>
<p>Another way is to <strong>query to the model store</strong>. The schema / signature appears under the model view, like below</p>
<p><a href=""https://i.stack.imgur.com/ug6Ek.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ug6Ek.png"" alt=""enter image description here"" /></a></p>
<p>the data can be obtained by the function <a href=""https://mlflow.org/docs/latest/python_api/mlflow.models.html#mlflow.models.get_model_info"" rel=""nofollow noreferrer""><code>mlflow.models.get_model_info</code></a>, like in this snippet</p>
<pre class=""lang-py prettyprint-override""><code>model_uri = client.get_model_version_download_uri('toy-model','10')
model_info = mlflow.models.get_model_info(model_uri)
model_info._signature_dict
</code></pre>
","10710587",1
2088,76330025,2,76322238,2023-05-25 07:52:15,0,"<p>As per function definition, the parameter <strong>code_paths</strong> is for giving a list of local filesystem paths to Python file dependencies (or directories containing file dependencies).</p>
<p>If your model having such kind dependencies you need to provide there paths in list to <strong>code_paths</strong>.</p>
<p>The error you are getting about directory not found can resolved by taking abs path as below.</p>
<pre><code>code_pth = os.path.abspath(&quot;&quot;)+&quot;/media/model/&quot;
conda_env = os.path.abspath(&quot;&quot;)+&quot;/dependencies/&quot;
print(conda_env)
print(code_pth)
</code></pre>
<p><img src=""https://i.imgur.com/Ed03SP4.png"" alt=""enter image description here"" /></p>
<p>I have used sklearn model to log and save.</p>
<pre><code>mlflow.sklearn.log_model(
sk_model=clf,
registered_model_name=registered_model_name,
artifact_path=registered_model_name,
code_paths=[code_pth],
conda_env=os.path.join(conda_env, &quot;conda.yaml&quot;)
)
</code></pre>
<p>Output:
<img src=""https://i.imgur.com/50cMAUT.png"" alt=""enter image description here"" /></p>
<p><img src=""https://i.imgur.com/uP4ygTJ.png"" alt=""enter image description here"" /></p>
<pre><code>mlflow.sklearn.save_model(
sk_model=clf,
path=os.path.join(registered_model_name, &quot;trained_model&quot;),
code_paths=[code_pth],
conda_env=os.path.join(conda_env, &quot;conda.yaml&quot;)
)
</code></pre>
<p>Output:</p>
<p><img src=""https://i.imgur.com/wqXOuot.png"" alt=""enter image description here"" /></p>
","21588207",0
2089,76314975,2,76293385,2023-05-23 12:53:37,0,"<p>I have reproduced the scenario but didn't get any error. The error you are getting is might be caused by a bug in the older version of Azure ML SDK.
Below are the possible solution you can try:</p>
<ol>
<li>Make sure you are using latest version of <strong>Azure ML SDK</strong>.</li>
</ol>
<blockquote>
<p><code>pip install --upgrade azureml-sdk</code></p>
</blockquote>
<ol start=""2"">
<li><p>Ensure that you have the proper permissions to access the blob storage, check the settings for your Azure subscription.
ref:<a href=""https://learn.microsoft.com/en-us/azure/machine-learning/v1/how-to-connect-data-ui?view=azureml-api-1&amp;tabs=credential"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/machine-learning/v1/how-to-connect-data-ui?view=azureml-api-1&amp;tabs=credential</a></p>
</li>
<li><p>Try running your script in different environment:
You can try running your script in a azure <code>&quot;remote compute&quot;</code> rather than <code>&quot;local&quot;</code>.
ref: <a href=""https://learn.microsoft.com/en-us/azure/machine-learning/how-to-create-manage-compute-instance?view=azureml-api-2&amp;tabs=python"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/machine-learning/how-to-create-manage-compute-instance?view=azureml-api-2&amp;tabs=python</a></p>
</li>
</ol>
<p>Below is the output:</p>
<p><img src=""https://i.imgur.com/oCrY3le.png"" alt=""enter image description here"" /></p>
","20059423",0
2090,76268860,2,76259858,2023-05-17 05:40:11,0,"<p>you are experiencing the very design of MLFlow: <a href=""https://mlflow.org/docs/latest/tracking.html#how-runs-and-artifacts-are-recorded"" rel=""nofollow noreferrer"">separation of metadata (like runs and params) and artefacts (like models with weights)</a>. You are expected to <strong>link that information with run ids</strong>, similarly to what you proposed.
However, <strong><a href=""https://mlflow.org/docs/latest/python_api/mlflow.artifacts.html?highlight=load%20artifacts#mlflow.artifacts.load_dict"" rel=""nofollow noreferrer"">use API to handle artefact paths</a></strong>, avoiding manual creation as much as possible. In your case:</p>
<pre><code>artifact_uri = mlflow.get_run(run_id).info.artifact_uri
mlflow.artifacts.load_dict(artifact_uri + &quot;/params.json&quot;)
# {'C': 0.1, 'random_state': 42}
</code></pre>
<p>As for the run id, you know it from the model</p>
<pre><code># get the run id of the most recent model 
client = MlflowClient(mlflow.get_tracking_uri())
model_info = client.get_latest_versions('toy-model')[0]
run_id = model_info.run_id
</code></pre>
<p>PS: MLFlow is great :-)</p>
","10710587",0
2091,76197675,2,76178347,2023-05-08 05:12:51,0,"<p>What is the version of SKLearn SageMaker Estimator that you are utilizing? You need to make sure this is compatible with the training script that you are passing. I would recommend locally trying to implement same operations and ensuring that the model trains as expected. Once you confirm the version make sure you are using that version with SKLearn SageMaker Estimator for your tuning/training job.</p>
","16504640",1
2092,76197639,2,76165275,2023-05-08 05:05:12,1,"<p>Can you add loggers in each of your handler functions? This will help us capture which function is leading to the error in your inference script, you can view these results in the CloudWatch Logs. To debug more quickly I suggest two methods:</p>
<ol>
<li><p>Host the container using Docker so you don't need to deploy an endpoint each time and wait to debug.</p>
</li>
<li><p>Utilize SageMaker Local Mode (takes care of option 1 for you for a lot of use-cases), ex: <a href=""https://towardsdatascience.com/debugging-sagemaker-endpoints-quickly-with-local-mode-2975bd55f6f7"" rel=""nofollow noreferrer"">https://towardsdatascience.com/debugging-sagemaker-endpoints-quickly-with-local-mode-2975bd55f6f7</a></p>
</li>
</ol>
","16504640",0
2093,76116923,2,76110651,2023-04-27 05:08:51,0,"<p><strong>Tag your experiment appropriately</strong>, in this case use <code>mlflow.note.content</code> tag.</p>
<pre><code>experiment_id = mlflow.create_experiment(
   'my-experiment',
   tags={'mlflow.note.content':'this experiment is doing nothing'}
)
</code></pre>
<p>PS: How to figure it out, as this seems not well documented? Assign a description with GUI, and then inspect it in the backend :-)</p>
<p><a href=""https://i.stack.imgur.com/9qgg4.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/9qgg4.png"" alt=""MLFlow experiment with description"" /></a></p>
","10710587",3
2094,76041073,2,76027182,2023-04-18 04:51:41,0,"<p>Use mlflow==2.2.2 version and your error will be gone.</p>
","21669293",0
2095,76580774,2,76576296,2023-06-29 11:51:33,0,"<p>I have reproduced the issue based on the scenario.
<img src=""https://i.imgur.com/5PC12T7.png"" alt=""enter image description here"" /></p>
<p>It seems like the image URL is incorrect: <code>mcr.microsoft.com/mcr.microsoft.com/azureml/curated/azureml-automl:141</code>. There is a duplicate <code>mcr.microsoft.com</code> in the URL.
<img src=""https://i.imgur.com/IHNiT0F.png"" alt=""enter image description here"" /></p>
<p>To resolve this issue, you should update the image URL in your batch endpoint configuration to the correct one.
The correct image URL should look like this:</p>
<p><code>mcr.microsoft.com/azureml/curated/azureml-automl:141</code>.
<img src=""https://i.imgur.com/svyxhIP.png"" alt=""enter image description here"" /></p>
<p>update the endpoint with new deployment configuration and the endpoint should work fine.
<img src=""https://i.imgur.com/grIVnt2.png"" alt=""enter image description here"" /></p>
","21964439",1
2096,76573869,2,76560332,2023-06-28 13:54:09,0,"<p>For some reason, the PYTHONPATH is different when you define a container through <code>@component</code> or through <code>create_component_from_func</code>.</p>
<p>The WORKDIR is not included for the second case.</p>
<p>So there are some solutions:</p>
<ul>
<li>Adding <code>ENV PYTHONPATH &quot;${PYTHONPATH}:${WORKDIR}&quot;</code> to the Dockerfile</li>
<li>Adding it at the beginning of the component: <code>sys.path.append(os.getcwd())</code></li>
</ul>
","7481334",0
2097,76531053,2,76516514,2023-06-22 10:38:59,0,"<p>In your case, it would be beneficial to adopt a feature store approach to manage the evolving features of your ML model. A feature store is a centralized repository that serves as a single source of truth for all the features used in machine learning models. It enables efficient feature management, versioning, and retrieval for training and inference.</p>
<p>Instead of altering the schema of your existing BigQuery table every time a new feature is added, consider separating the feature extraction step from the input table. You can create a separate pipeline or process that extracts the features from your input data and stores them in a dedicated feature store.</p>
<p>Then set up a feature store to store and manage your extracted features. This can be implemented using a dedicated database or a specialized feature store tool. The feature store should allow you to version and organize features efficiently. It should also support easy retrieval of features based on timestamps or other relevant criteria.</p>
<p>Create a feature engineering pipeline that takes the raw input data, applies the necessary transformations, and extracts the features. This pipeline should populate the feature store with the latest features and ensure that they are properly versioned.</p>
<p>When training your ML model, you can fetch the required features from the feature store based on the desired version. This allows you to consistently use the same set of features for model training, regardless of the changes made to the input data schema.</p>
<p>Instead of recreating the entire table every time, design your feature engineering pipeline to handle incremental updates. This means that for each new batch of data, the pipeline should update only the necessary features in the feature store, rather than rebuilding the entire feature set. This can significantly reduce the processing time and allow for more frequent model predictions.</p>
<p>Then finally, try to implement a mechanism to track feature versions and manage deprecated features. This ensures that you can easily track which features were used for each training run and inference, and you can remove unused or deprecated features from the feature store to keep it clean and efficient.</p>
","9517769",3
2098,76449212,2,76201782,2023-06-11 06:06:40,0,"<p>So after good research I have it working.</p>
<p>To help others with these problems, some things you might consider:</p>
<ol>
<li>Make sure all modules are installed, in the above example I didn't load the <code>azure-core</code> module. But don't forget the <code>azure-ai-ml</code>.</li>
<li>The <code>DefaultAzureCredential</code> Class consist of multiple Classes for authentication. Mine was stuck at the <code>SharedToken</code> which gave a token but an expired one. So I created a <code>service principal</code> by registering an app and giving this app authorization for Azure ML. Thereafter I loaded these variable as environmental variables from a <code>.env</code> file.</li>
</ol>
<p>I did some testing with the <code>EnvironmentalCredential</code> Class (part of the <code>DefaultAzureCredential</code>) and this gave me a token which I could use in the <code>DefaultAzureCredential</code>.</p>
<p>So main problems were:</p>
<ul>
<li>Not loading variable for authentication as environmental variables.</li>
<li>Getting an expired token on the authentication method which I didn't want to use.</li>
</ul>
","18676995",0
